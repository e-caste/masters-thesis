{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c04ea0-1634-4323-be1a-f8b373a8a00b",
   "metadata": {},
   "source": [
    "# BART Denoising Language Modeling Domain Adaptation\n",
    "Taking inspiration from HuggingFace's [run_mlm.py](https://github.com/huggingface/transformers/blob/7032e0203262ebb2ebf55da8d2e01f873973e835/examples/pytorch/language-modeling/run_mlm.py) script, replacing the Masked Language Modeling task with Denoising Language Modeling using the [run_bart_dlm.py](https://github.com/huggingface/transformers/blob/ecd7de3dff7ea5713004b2f05e3869c24b8eb6e2/examples/flax/language-modeling/run_bart_dlm_flax.py) script written for Flax and adapted for PyTorch, we continue BART's pretraining on the MITOCW+OpenHPI+VT-SSum+Yale transcripts dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59c532de-c059-4f3a-b1c4-c133be35969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers datasets evaluate accelerate torch torchvision numpy nltk scikit-learn tensorboard wandb ray[tune] hyperopt ipynbname gdown matplotlib ipywidgets rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df04b78-4bf3-4901-9045-80ac88913f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=BART_DLM_domain_adaptation\n",
      "env: WANDB_LOG_MODEL=true\n",
      "env: WANDB_WATCH=all\n",
      "Platform is local\n",
      "Running in directory /home/caste/Documents/thesis/src/project/experiments/text-mlm-denoising\n",
      "PyTorch version 1.13.0+cu117\n",
      "Transformers version 4.25.1\n",
      "Datasets version 2.7.1\n",
      "GPU available\n",
      "GPU memory available 90.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/caste/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 20 09:19:36 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.60.11    Driver Version: 525.60.11    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:2B:00.0  On |                  N/A |\n",
      "|  0%   43C    P2    46W / 350W |   2306MiB / 24576MiB |     12%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3091      G   /usr/lib/xorg/Xorg                991MiB |\n",
      "|    0   N/A  N/A      3207      G   /usr/bin/gnome-shell              248MiB |\n",
      "|    0   N/A  N/A      3781      G   ...RendererForSitePerProcess       50MiB |\n",
      "|    0   N/A  N/A      4092      G   /usr/bin/nextcloud                 14MiB |\n",
      "|    0   N/A  N/A      5811      G   firefox                           624MiB |\n",
      "|    0   N/A  N/A    241586      G   ...RendererForSitePerProcess      114MiB |\n",
      "|    0   N/A  N/A    316240      C   ...ts/thesis/venv/bin/python      254MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "/dev/nvme0n1p3  449G  280G  147G  66% /\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from sys import stderr\n",
    "import torch\n",
    "from torch.cuda import OutOfMemoryError\n",
    "import gc\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BartConfig,\n",
    "    BartForConditionalGeneration,\n",
    "    BatchEncoding,\n",
    "    PreTrainedTokenizerBase,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import datasets\n",
    "from datasets import DatasetDict, Dataset\n",
    "import evaluate\n",
    "from huggingface_hub import login as notebook_login\n",
    "import gdown\n",
    "from glob import glob\n",
    "import subprocess\n",
    "from subprocess import Popen, PIPE, CalledProcessError\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "import ipynbname\n",
    "import wandb\n",
    "from wandb import AlertLevel\n",
    "import random\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from itertools import chain\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "COLAB = 'google.colab' in sys.modules  # True if on Google Colab\n",
    "os.environ['COLAB'] = \"true\" if COLAB else \"\"\n",
    "PLATFORM = \"Colab\" if COLAB else \"local\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# whether to train one model or run a hyperparameter search with Ray Tune\n",
    "search_hyperparams = False\n",
    "\n",
    "# https://docs.wandb.ai/guides/integrations/huggingface\n",
    "# set the wandb project name\n",
    "try:\n",
    "    current_notebook_name = ipynbname.name()\n",
    "except:\n",
    "    current_notebook_name = \"BART_DLM_domain_adaptation\"  # TODO: set manually\n",
    "if search_hyperparams:\n",
    "    current_notebook_name += \"_HPO\"\n",
    "%env WANDB_PROJECT=$current_notebook_name\n",
    "# save the best model as an artifact on wandb\n",
    "%env WANDB_LOG_MODEL=true\n",
    "# log everything\n",
    "%env WANDB_WATCH=all\n",
    "\n",
    "def get_gpu_mem_free_percent():\n",
    "    mem_free, mem_total = torch.cuda.mem_get_info() if torch.cuda.is_available() else (0, 1)\n",
    "    return mem_free / mem_total * 100\n",
    "\n",
    "def print_gpu_mem_free_percent():\n",
    "    print(f\"GPU memory available {get_gpu_mem_free_percent():.2f}%\")\n",
    "\n",
    "def empty_gpu_mem():\n",
    "    # try to avoid RuntimeError: CUDA out of memory.\n",
    "    \"\"\"try:\n",
    "        del model\n",
    "    except NameError:\n",
    "        pass\n",
    "    try:\n",
    "        del trainer\n",
    "    except NameError:\n",
    "        pass\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"Platform is {PLATFORM}\")\n",
    "print(f\"Running in directory {os.getcwd()}\")\n",
    "print(f\"PyTorch version {torch.__version__}\")\n",
    "print(f\"Transformers version {transformers.__version__}\")\n",
    "print(f\"Datasets version {datasets.__version__}\")\n",
    "print(f\"GPU {'available' if torch.cuda.is_available() else 'unavailable'}\")\n",
    "print_gpu_mem_free_percent()\n",
    "if get_gpu_mem_free_percent() < 80:\n",
    "    print(\"Trying to free a bit of GPU memory...\")\n",
    "    empty_gpu_mem()\n",
    "    print_gpu_mem_free_percent()\n",
    "\n",
    "# export the current GPU name to the environment, may be used later (e.g. for the batch size)\n",
    "# should be Tesla T4 (-> 16GB), Tesla K80 (-> 12GB), GeForce GTX 1060 (-> 6GB)\n",
    "!nvidia-smi --query-gpu=gpu_name --format=csv,noheader > /tmp/gpu_name\n",
    "os.environ['GPU_NAME'] = open(\"/tmp/gpu_name\", \"r\").read().strip()\n",
    "\n",
    "!nvidia-smi\n",
    "!df -h /\n",
    "\n",
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')  # used to save model checkpoints\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3cef50e-d57a-4f35-9a37-2f06e9691a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_bart_large = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95003806-abaf-411e-9db2-047884d754a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(os.path.expanduser(\"~/.huggingface/token\")):\n",
    "    notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f7a761-9909-4dab-a5bd-ff6fe02797ef",
   "metadata": {},
   "source": [
    "### Convert the DataCollator for Flax to a PyTorch-compatible one\n",
    "Here we can also see that while Flax works with numpy arrays internally, PyTorch works with (obviously) PyTorch tensors.  \n",
    "The difference in code can be overcome by simply converting the numpy arrays to PyTorch tensors at the end of the `DataCollatorForBartDenoisingLM.__call__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3acc26f-5074-48f9-92c7-b23d07840d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "def _pytorch_shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "\n",
    "    return shifted_input_ids\n",
    "\n",
    "\n",
    "# Flax\n",
    "def _flax_shift_tokens_right(input_ids: np.array, pad_token_id: int, decoder_start_token_id: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = np.zeros_like(input_ids)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1]\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    shifted_input_ids = np.where(shifted_input_ids == -100, pad_token_id, shifted_input_ids)\n",
    "    return shifted_input_ids\n",
    "\n",
    "\n",
    "# custom wrapper\n",
    "def shift_tokens_right(input_ids, pad_token_id, decoder_start_token_id):\n",
    "    return _flax_shift_tokens_right(input_ids, pad_token_id, decoder_start_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db0250e0-7ad8-42b4-9e37-2827631b709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForBartDenoisingLM:\n",
    "    \"\"\"\n",
    "    From https://github.com/huggingface/transformers/blob/ecd7de3dff7ea5713004b2f05e3869c24b8eb6e2/examples/flax/language-modeling/run_bart_dlm_flax.py\n",
    "\n",
    "    Data collator used for BART denoising language modeling. The code is largely copied from\n",
    "    `<https://github.com/morganmcg1/rotobart/blob/main/data_collator.py#L223>`__.\n",
    "    For more information on how BART denoising language modeling works, one can take a look\n",
    "    at the `official paper <https://arxiv.org/pdf/1910.13461.pdf>`__\n",
    "    or the `official code for preprocessing <https://github.com/facebookresearch/fairseq/blob/main/fairseq/data/denoising_dataset.py>`__ .\n",
    "    Args:\n",
    "        tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):\n",
    "            The tokenizer used for encoding the data\n",
    "        mask_ratio (:obj:`float`):\n",
    "            The probability with which to (randomly) mask tokens in the input\n",
    "        poisson_lambda (:obj:`float`):\n",
    "            Mean parameter of Poisson distribution used to generate span-lengths to be masked\n",
    "        permute_sentence_ratio (:obj:`float`):\n",
    "            Ratio of sentences to be permuted in each document\n",
    "        decoder_start_token_id: (:obj:`int):\n",
    "            The decoder start token id of the model\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    decoder_start_token_id: int\n",
    "    mask_ratio: float = 0.3\n",
    "    poisson_lambda: float = 3.0\n",
    "    permute_sentence_ratio: float = 1.0\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.tokenizer.mask_token is None or self.tokenizer.eos_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token or eos token token which is necessary for denoising\"\n",
    "                \" language modeling. \"\n",
    "            )\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, List[int]]]) -> BatchEncoding:\n",
    "        # convert list to dict and tensorize input\n",
    "        batch = BatchEncoding(\n",
    "            {k: np.array([examples[i][k] for i in range(len(examples))]) for k, v in examples[0].items()}\n",
    "        )\n",
    "        batch[\"labels\"] = batch[\"input_ids\"].copy()\n",
    "        batch[\"decoder_input_ids\"] = shift_tokens_right(\n",
    "            batch[\"labels\"], self.tokenizer.pad_token_id, self.decoder_start_token_id\n",
    "        )\n",
    "        # permuting sentences\n",
    "        do_permute = False\n",
    "        if self.permute_sentence_ratio > 0.0:\n",
    "            batch[\"input_ids\"] = self.permute_sentences(batch[\"input_ids\"])\n",
    "            do_permute = True\n",
    "\n",
    "        # masking span of tokens (text infilling in the paper)\n",
    "        if self.mask_ratio:\n",
    "            batch[\"input_ids\"], batch[\"labels\"] = self.span_mask_tokens(\n",
    "                batch[\"input_ids\"], batch[\"labels\"], do_permute\n",
    "            )\n",
    "\n",
    "        # ignore pad tokens\n",
    "        batch[\"attention_mask\"] = (batch[\"input_ids\"] != self.tokenizer.pad_token_id).astype(int)\n",
    "        batch[\"decoder_attention_mask\"] = (batch[\"decoder_input_ids\"] != self.tokenizer.pad_token_id).astype(int)\n",
    "\n",
    "        # NOTE: we need to convert the BatchEncoding to PyTorch Tensor to make this Flax dataloader work with PyTorch\n",
    "        # all the previous computing in this method is done with numpy.ndarray lists\n",
    "        return batch.convert_to_tensors(tensor_type=\"pt\")\n",
    "\n",
    "    def permute_sentences(self, input_ids):\n",
    "        \"\"\"\n",
    "        Shuffle sentences in each document.\n",
    "        \"\"\"\n",
    "        results = input_ids.copy()\n",
    "\n",
    "        # find end locations of sentences\n",
    "        end_sentence_mask = input_ids == self.tokenizer.pad_token_id\n",
    "        sentence_ends = np.argwhere(end_sentence_mask)\n",
    "        sentence_ends[:, 1] += 1\n",
    "        example_has_multiple_sentences, num_sentences = np.unique(sentence_ends[:, 0], return_counts=True)\n",
    "        num_sentences_map = {sent_idx: count for sent_idx, count in zip(example_has_multiple_sentences, num_sentences)}\n",
    "\n",
    "        num_to_permute = np.ceil(num_sentences * self.permute_sentence_ratio).astype(int)\n",
    "        num_to_permute_map = {\n",
    "            sent_idx: count for sent_idx, count in zip(example_has_multiple_sentences, num_to_permute)\n",
    "        }\n",
    "\n",
    "        sentence_ends = np.split(sentence_ends[:, 1], np.unique(sentence_ends[:, 0], return_index=True)[1][1:])\n",
    "        sentence_ends_map = {sent_idx: count for sent_idx, count in zip(example_has_multiple_sentences, sentence_ends)}\n",
    "\n",
    "        for i in range(input_ids.shape[0]):\n",
    "            if i not in example_has_multiple_sentences:\n",
    "                continue\n",
    "            substitutions = np.random.permutation(num_sentences_map[i])[: num_to_permute_map[i]]\n",
    "            ordering = np.arange(0, num_sentences_map[i])\n",
    "            ordering[substitutions] = substitutions[np.random.permutation(num_to_permute_map[i])]\n",
    "\n",
    "            # write shuffled sentences into results\n",
    "            index = 0\n",
    "            for j in ordering:\n",
    "                sentence = input_ids[i, (sentence_ends_map[i][j - 1] if j > 0 else 0):sentence_ends_map[i][j]]\n",
    "                results[i, index:index + sentence.shape[0]] = sentence\n",
    "                index += sentence.shape[0]\n",
    "        return results\n",
    "\n",
    "    def span_mask_tokens(self, input_ids, labels, do_permute):\n",
    "        \"\"\"\n",
    "        Sampling text spans with span lengths drawn from a Poisson distribution and masking them.\n",
    "        \"\"\"\n",
    "        special_tokens_mask_labels = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        ]\n",
    "        special_tokens_mask_inputs = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in input_ids.tolist()\n",
    "        ]\n",
    "        special_tokens_mask_labels = np.array(special_tokens_mask_labels, dtype=bool)\n",
    "        special_tokens_mask_inputs = np.array(special_tokens_mask_inputs, dtype=bool)\n",
    "\n",
    "        # determine how many tokens we need to mask in total\n",
    "        is_token_mask = ~(input_ids == self.tokenizer.pad_token_id) & ~special_tokens_mask_inputs\n",
    "        num_tokens_to_mask = int(math.ceil(is_token_mask.astype(float).sum() * self.mask_ratio))\n",
    "        if num_tokens_to_mask == 0:\n",
    "            return input_ids, labels\n",
    "\n",
    "        # generate a sufficient number of span lengths\n",
    "        span_lengths = np.random.poisson(lam=self.poisson_lambda, size=(num_tokens_to_mask,))\n",
    "        while np.cumsum(span_lengths, 0)[-1] < num_tokens_to_mask:\n",
    "            span_lengths = np.concatenate(\n",
    "                [span_lengths, np.random.poisson(lam=self.poisson_lambda, size=(num_tokens_to_mask,))]\n",
    "            )\n",
    "\n",
    "        # remove all spans of length 0\n",
    "        # note that BART inserts additional mask tokens where length == 0,\n",
    "        # which we do not implement for now as it adds additional complexity\n",
    "        span_lengths = span_lengths[span_lengths > 0]\n",
    "\n",
    "        # trim to about num_tokens_to_mask tokens\n",
    "        cutoff_idx = np.argmin(np.abs(np.cumsum(span_lengths, 0) - num_tokens_to_mask)) + 1\n",
    "        span_lengths = span_lengths[:cutoff_idx]\n",
    "\n",
    "        # randomly choose starting positions for masking\n",
    "        token_indices = np.argwhere(is_token_mask == 1)\n",
    "        span_starts = np.random.permutation(token_indices.shape[0])[: span_lengths.shape[0]]\n",
    "        # prepare mask\n",
    "        masked_indices = np.array(token_indices[span_starts])\n",
    "        mask = np.full_like(input_ids, fill_value=False)\n",
    "\n",
    "        # mask starting positions\n",
    "        for mi in masked_indices:\n",
    "            mask[tuple(mi)] = True\n",
    "        span_lengths -= 1\n",
    "\n",
    "        # fill up spans\n",
    "        max_index = input_ids.shape[1] - 1\n",
    "        remaining = (span_lengths > 0) & (masked_indices[:, 1] < max_index)\n",
    "        while np.any(remaining):\n",
    "            masked_indices[remaining, 1] += 1\n",
    "            for mi in masked_indices:\n",
    "                mask[tuple(mi)] = True\n",
    "            span_lengths -= 1\n",
    "            remaining = (span_lengths > 0) & (masked_indices[:, 1] < max_index)\n",
    "\n",
    "        # place the mask tokens\n",
    "        mask[np.where(special_tokens_mask_inputs)] = False\n",
    "        input_ids[np.where(mask)] = self.tokenizer.mask_token_id\n",
    "        if not do_permute:\n",
    "            labels[np.where(mask == 0)] = -100\n",
    "        else:\n",
    "            labels[np.where(special_tokens_mask_labels)] = -100\n",
    "\n",
    "        # remove mask tokens that are not starts of spans\n",
    "        to_remove = (mask == 1) & np.roll((mask == 1), 1, 1)\n",
    "        new_input_ids = np.full_like(input_ids, fill_value=self.tokenizer.pad_token_id)\n",
    "        for i, example in enumerate(input_ids):\n",
    "            new_example = example[~to_remove[i]]\n",
    "            new_input_ids[i, : new_example.shape[0]] = new_example\n",
    "\n",
    "        return new_input_ids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef1ed78-086d-427b-be00-2b75b366e4e2",
   "metadata": {},
   "source": [
    "### Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0395756-e705-4e3d-8202-88c76c731a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration e-caste--mitocw_openhpi_vtssum_yale-lecture_transcripts-09e7b44998532bac\n",
      "Found cached dataset csv (/home/caste/.cache/huggingface/datasets/e-caste___csv/e-caste--mitocw_openhpi_vtssum_yale-lecture_transcripts-09e7b44998532bac/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9322b84eb29496f9ffed0a37f5568b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'text'],\n",
       "        num_rows: 13209\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Unnamed: 0', 'text'],\n",
       "        num_rows: 1468\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/main/en/tasks/language_modeling\n",
    "dataset_id = \"e-caste/mitocw_openhpi_vtssum_yale-lecture_transcripts\"\n",
    "\n",
    "text_column_name = \"text\"\n",
    "raw_datasets = datasets.load_dataset(dataset_id, use_auth_token=True)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af4cf293-9855-4ca0-9d91-b29ba4feba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = f\"facebook/bart-{'large' if use_bart_large else 'base'}\"\n",
    "\n",
    "config = BartConfig.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11efeb2e-c7e4-47e0-b429-739af42bd1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "\n",
    "\n",
    "def sentence_split_function(example):\n",
    "    sents = sentence_tokenizer.tokenize(example[\"text\"])\n",
    "    # use pad token as end of sentence indicator\n",
    "    new_text = tokenizer.bos_token + f\"{tokenizer.pad_token}\".join(sents) + tokenizer.eos_token\n",
    "    return {\"text\": new_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b692e894-10a4-4b74-a605-81e6712ff254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d8bb5b241948a2bce2b2dae08753b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13209 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72902795b97f475cbfdc4f21ef4f3814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1468 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 13209\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1468\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets = raw_datasets.map(\n",
    "    sentence_split_function,\n",
    "    batched=False,\n",
    "    num_proc=1,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    "    load_from_cache_file=True,\n",
    ")\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ab9cc17-17e5-4e88-8a81-cb429e4f4690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45637e7d58724c07a85122fde911a722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3242 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06eb748ddd1d4b9395f375c060acb0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 13209\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1468\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize every text, then concatenate them together before splitting them in smaller parts.\n",
    "# Since we make sure that all sequences are of the same length, no attention_mask is needed.\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name], add_special_tokens=False, return_attention_mask=False)\n",
    "\n",
    "\n",
    "tokenized_datasets = split_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=text_column_name,\n",
    "    load_from_cache_file=True,\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7693be3a-c656-4973-b543-84789e9b0b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0291145f56c4fefb55dbd581cb432cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d3072637374c8cb69bdbdcfc3a345e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 59694\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 6236\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = min(tokenizer.model_max_length, 1024)\n",
    "\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= max_seq_length:\n",
    "        total_length = (total_length // max_seq_length) * max_seq_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i:i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    load_from_cache_file=True,\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21e4f4e3-b721-4a02-b0f5-c88723af3f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "# This one will take care of randomly masking the tokens and permuting the sentences.\n",
    "data_collator = DataCollatorForBartDenoisingLM(\n",
    "    tokenizer=tokenizer,\n",
    "    decoder_start_token_id=config.decoder_start_token_id,\n",
    "    mask_ratio=0.30,  # mlm_probability, Ratio of tokens to mask for span masked language modeling loss\n",
    "    poisson_lambda=3.0,  # Mean of Poisson distribution used to generate span-lengths to be masked\n",
    "    permute_sentence_ratio=1.0,  # Ratio of sentences to be permuted in each document\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c385357b-33de-46df-972d-49c81fe4578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"accuracy\"\n",
    "metric = evaluate.load(metric_name)\n",
    "\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
    "    # by preprocess_logits_for_metrics\n",
    "    labels = labels.reshape(-1)\n",
    "    preds = preds.reshape(-1)\n",
    "    mask = labels != -100\n",
    "    labels = labels[mask]\n",
    "    preds = preds[mask]\n",
    "    return metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623c78f1-391a-421c-9316-03773a7f2756",
   "metadata": {},
   "source": [
    "### Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72ec9c7d-9644-48dc-a94a-890efbc72ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local_NVIDIA-GeForce-RTX-3090_facebook-bart-large_2023-01-20-09_21_14.832994'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_name = \"_\".join([PLATFORM, os.environ['GPU_NAME'], model_id, str(datetime.now())]).replace(\"\\n\", \"\").replace(\"/\", \"-\").replace(\":\", \"_\").replace(\" \", \"-\")\n",
    "tensorboard_logging_dir = None  # f\"./tensorboard/{run_name}\"\n",
    "epochs = 1\n",
    "eval_delay = 0\n",
    "batch_size_train = 1 if use_bart_large else 4\n",
    "batch_size_eval = 2 if use_bart_large else 8\n",
    "gradient_accumulation_steps = 64 if use_bart_large else 1\n",
    "evaluations_per_epoch = 10\n",
    "total_steps = len(tokenized_datasets['train']) * epochs / (batch_size_train * gradient_accumulation_steps)\n",
    "warmup_steps = total_steps // 100\n",
    "eval_and_save_steps = int(total_steps / (evaluations_per_epoch * epochs))\n",
    "early_stopping_patience = epochs * evaluations_per_epoch // 2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=current_notebook_name,\n",
    "    run_name=run_name,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=eval_and_save_steps,\n",
    "    save_total_limit=evaluations_per_epoch,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=eval_and_save_steps,\n",
    "    eval_delay=eval_delay,\n",
    "    auto_find_batch_size=False,\n",
    "    per_device_train_batch_size=batch_size_train,\n",
    "    per_device_eval_batch_size=batch_size_eval,\n",
    "    num_train_epochs=epochs,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-8,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=warmup_steps,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    # make the model fit in the GPU\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=use_bart_large,\n",
    "    # fp16 vs bf16: https://www.reddit.com/r/MachineLearning/comments/vndtn8/comment/ie6dr2u/?utm_source=share&utm_medium=web2x&context=3\n",
    "    # for full precision use tf32=True: https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/\n",
    "    fp16=False,\n",
    "    bf16=False,  # needs RTX 3090\n",
    "    fp16_full_eval=False,\n",
    "    bf16_full_eval=False,  # Whether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm metric values.\n",
    "    half_precision_backend=\"cuda_amp\",  # apex is deprecated: https://discuss.pytorch.org/t/torch-cuda-amp-vs-nvidia-apex/74994/2\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    logging_dir=tensorboard_logging_dir,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,  # crucial for logging training loss -- Number of update steps between two logs if `logging_strategy=\"steps\"`\n",
    "    remove_unused_columns=True,\n",
    "    report_to=\"wandb\",\n",
    "    disable_tqdm=search_hyperparams,\n",
    ")\n",
    "\n",
    "run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5a0c67e-f4a2-41d8-b317-2ceb022145f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_trainer(model_name: str):\n",
    "    return Trainer(\n",
    "        model=BartForConditionalGeneration.from_pretrained(model_name, config=config),\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets['validation'],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = get_new_trainer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b13cbfe4-99ff-4b7d-b89d-a1d2d35875f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_perplexity_on_test_dataset(log_metrics: bool = False):\n",
    "    metrics = trainer.evaluate(eval_dataset=tokenized_datasets['validation'])\n",
    "    metrics[\"eval_samples\"] = len(tokenized_datasets['validation'])\n",
    "\n",
    "    try:\n",
    "        perplexity = math.exp(metrics[\"eval_loss\"])\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    metrics[\"perplexity\"] = perplexity\n",
    "\n",
    "    if log_metrics:\n",
    "        trainer.log_metrics(\"eval\", metrics)\n",
    "        trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "    print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bea7f5-f964-4c9e-a4e1-fbfb2e5c31f9",
   "metadata": {},
   "source": [
    "### Test pretrained model performance\n",
    "Uncomment the cell to evaluate the pretrained model. Running the training directly after the evaluation results in an OutOfMemoryError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7cc35cb-d148-4136-81da-1f4d81e60e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 6236\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3118' max='3118' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3118/3118 09:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33me-caste\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/caste/Documents/thesis/src/project/experiments/text-mlm-denoising/wandb/run-20230120_093102-erz35jhw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/e-caste/BART_DLM_domain_adaptation/runs/erz35jhw\" target=\"_blank\">local_NVIDIA-GeForce-RTX-3090_facebook-bart-large_2023-01-20-09_21_14.832994</a></strong> to <a href=\"https://wandb.ai/e-caste/BART_DLM_domain_adaptation\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 191.39\n"
     ]
    }
   ],
   "source": [
    "# evaluate_perplexity_on_test_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88521d5f-2d67-4dd6-955a-5c6af36898f6",
   "metadata": {},
   "source": [
    "### Train the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41b3760b-8d7f-4374-80a0-45783de7def8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caste/Documents/thesis/venv/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 59694\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 64\n",
      "  Total optimization steps = 932\n",
      "  Number of trainable parameters = 406291456\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33me-caste\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/caste/Documents/thesis/src/project/experiments/text-mlm-denoising/wandb/run-20230119_195936-30s865ap</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/e-caste/BART_DLM_domain_adaptation/runs/30s865ap\" target=\"_blank\">local_NVIDIA-GeForce-RTX-3090_facebook-bart-large_2023-01-19-19_59_31.545026</a></strong> to <a href=\"https://wandb.ai/e-caste/BART_DLM_domain_adaptation\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='932' max='932' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [932/932 8:03:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.992100</td>\n",
       "      <td>0.876833</td>\n",
       "      <td>0.818435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.858755</td>\n",
       "      <td>0.820923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>0.894600</td>\n",
       "      <td>0.845033</td>\n",
       "      <td>0.822847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>0.905100</td>\n",
       "      <td>0.837078</td>\n",
       "      <td>0.824371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>0.888200</td>\n",
       "      <td>0.829452</td>\n",
       "      <td>0.825726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>558</td>\n",
       "      <td>0.876500</td>\n",
       "      <td>0.822582</td>\n",
       "      <td>0.826261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>651</td>\n",
       "      <td>0.894200</td>\n",
       "      <td>0.819928</td>\n",
       "      <td>0.827048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>744</td>\n",
       "      <td>0.885600</td>\n",
       "      <td>0.817075</td>\n",
       "      <td>0.827398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>837</td>\n",
       "      <td>0.832100</td>\n",
       "      <td>0.812009</td>\n",
       "      <td>0.828041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.812800</td>\n",
       "      <td>0.812546</td>\n",
       "      <td>0.828188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 6236\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to BART_DLM_domain_adaptation/checkpoint-93\n",
      "Configuration saved in BART_DLM_domain_adaptation/checkpoint-93/config.json\n",
      "Model weights saved in BART_DLM_domain_adaptation/checkpoint-93/pytorch_model.bin\n",
      "tokenizer config file saved in BART_DLM_domain_adaptation/checkpoint-93/tokenizer_config.json\n",
      "Special tokens file saved in BART_DLM_domain_adaptation/checkpoint-93/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6236\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to BART_DLM_domain_adaptation/checkpoint-186\n",
      "Configuration saved in BART_DLM_domain_adaptation/checkpoint-186/config.json\n",
      "Model weights saved in BART_DLM_domain_adaptation/checkpoint-186/pytorch_model.bin\n",
      "tokenizer config file saved in BART_DLM_domain_adaptation/checkpoint-186/tokenizer_config.json\n",
      "Special tokens file saved in BART_DLM_domain_adaptation/checkpoint-186/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6236\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to BART_DLM_domain_adaptation/checkpoint-279\n",
      "Configuration saved in BART_DLM_domain_adaptation/checkpoint-279/config.json\n",
      "Model weights saved in BART_DLM_domain_adaptation/checkpoint-279/pytorch_model.bin\n",
      "tokenizer config file saved in BART_DLM_domain_adaptation/checkpoint-279/tokenizer_config.json\n",
      "Special tokens file saved in BART_DLM_domain_adaptation/checkpoint-279/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6236\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to BART_DLM_domain_adaptation/checkpoint-372\n",
      "Configuration saved in BART_DLM_domain_adaptation/checkpoint-372/config.json\n",
      "Model weights saved in BART_DLM_domain_adaptation/checkpoint-372/pytorch_model.bin\n",
      "tokenizer config file saved in BART_DLM_domain_adaptation/checkpoint-372/tokenizer_config.json\n",
      "Special tokens file saved in BART_DLM_domain_adaptation/checkpoint-372/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6236\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to BART_DLM_domain_adaptation/checkpoint-465\n",
      "Configuration saved in BART_DLM_domain_adaptation/checkpoint-465/config.json\n",
      "Model weights saved in BART_DLM_domain_adaptation/checkpoint-465/pytorch_model.bin\n",
      "tokenizer config file saved in BART_DLM_domain_adaptation/checkpoint-465/tokenizer_config.json\n",
      "Special tokens file saved in BART_DLM_domain_adaptation/checkpoint-465/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6236\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to BART_DLM_domain_adaptation/checkpoint-558\n",
      "Configuration saved in BART_DLM_domain_adaptation/checkpoint-558/config.json\n",
      "Model weights saved in BART_DLM_domain_adaptation/checkpoint-558/pytorch_model.bin\n",
      "tokenizer config file saved in BART_DLM_domain_adaptation/checkpoint-558/tokenizer_config.json\n",
      "Special tokens file saved in BART_DLM_domain_adaptation/checkpoint-558/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6236\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to BART_DLM_domain_adaptation/checkpoint-651\n",
      "Configuration saved in BART_DLM_domain_adaptation/checkpoint-651/config.json\n",
      "Model weights saved in BART_DLM_domain_adaptation/checkpoint-651/pytorch_model.bin\n",
      "tokenizer config file saved in BART_DLM_domain_adaptation/checkpoint-651/tokenizer_config.json\n",
      "Special tokens file saved in BART_DLM_domain_adaptation/checkpoint-651/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6236\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to BART_DLM_domain_adaptation/checkpoint-744\n",
      "Configuration saved in BART_DLM_domain_adaptation/checkpoint-744/config.json\n",
      "Model weights saved in BART_DLM_domain_adaptation/checkpoint-744/pytorch_model.bin\n",
      "tokenizer config file saved in BART_DLM_domain_adaptation/checkpoint-744/tokenizer_config.json\n",
      "Special tokens file saved in BART_DLM_domain_adaptation/checkpoint-744/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6236\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to BART_DLM_domain_adaptation/checkpoint-837\n",
      "Configuration saved in BART_DLM_domain_adaptation/checkpoint-837/config.json\n",
      "Model weights saved in BART_DLM_domain_adaptation/checkpoint-837/pytorch_model.bin\n",
      "tokenizer config file saved in BART_DLM_domain_adaptation/checkpoint-837/tokenizer_config.json\n",
      "Special tokens file saved in BART_DLM_domain_adaptation/checkpoint-837/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6236\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to BART_DLM_domain_adaptation/checkpoint-930\n",
      "Configuration saved in BART_DLM_domain_adaptation/checkpoint-930/config.json\n",
      "Model weights saved in BART_DLM_domain_adaptation/checkpoint-930/pytorch_model.bin\n",
      "tokenizer config file saved in BART_DLM_domain_adaptation/checkpoint-930/tokenizer_config.json\n",
      "Special tokens file saved in BART_DLM_domain_adaptation/checkpoint-930/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from BART_DLM_domain_adaptation/checkpoint-930 (score: 0.8281876922507254).\n",
      "Saving model checkpoint to /tmp/tmpbw8opw3w\n",
      "Configuration saved in /tmp/tmpbw8opw3w/config.json\n",
      "Model weights saved in /tmp/tmpbw8opw3w/pytorch_model.bin\n",
      "tokenizer config file saved in /tmp/tmpbw8opw3w/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmpbw8opw3w/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# we can run a grid search for the best hyperparameter combination\n",
    "# https://huggingface.co/blog/ray-tune\n",
    "if search_hyperparams:\n",
    "    # Default objective is the sum of all metrics\n",
    "    # when metrics are provided, so we have to maximize it.\n",
    "    # this doesn't save a model artifact to wandb\n",
    "    trainer.hyperparameter_search(\n",
    "        direction=\"maximize\",\n",
    "        backend=\"ray\",\n",
    "        n_trials=3,  # number of trials\n",
    "        # search_alg=HyperOptSearch(metric=\"objective\", mode=\"max\"),\n",
    "        scheduler=PopulationBasedTraining(metric=\"objective\", mode=\"max\"),\n",
    "    )\n",
    "else:\n",
    "    try:\n",
    "        # this saves the best model as an artifact on wandb\n",
    "        train_result = trainer.train(resume_from_checkpoint=None)\n",
    "    except OutOfMemoryError as oome:\n",
    "        print(\"Out of memory error: trying to free PyTorch cache...\", file=stderr)\n",
    "        empty_gpu_mem()\n",
    "        wandb.alert(\n",
    "            title=f\"Run out of memory for {current_notebook_name}\",\n",
    "            text=f\"Run name: {run_name}.\\nSee details at https://wandb.ai/e-caste/{current_notebook_name}\",\n",
    "            level=AlertLevel.ERROR,\n",
    "        )\n",
    "        wandb.finish()\n",
    "        raise oome\n",
    "    except KeyboardInterrupt:\n",
    "        wandb.alert(\n",
    "            title=f\"Run interrupted for {current_notebook_name}\",\n",
    "            text=f\"Run name: {run_name}.\\nManually interrupted.\\nSee details at https://wandb.ai/e-caste/{current_notebook_name}\",\n",
    "            level=AlertLevel.WARN,\n",
    "        )\n",
    "        wandb.finish()\n",
    "    except Exception as e:\n",
    "        wandb.alert(\n",
    "            title=f\"Run errored out for {current_notebook_name}\",\n",
    "            text=f\"Run name: {run_name}.\\nException: {e}.\\nSee details at https://wandb.ai/e-caste/{current_notebook_name}\",\n",
    "            level=AlertLevel.ERROR,\n",
    "        )\n",
    "        wandb.finish()\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "438aafcc-b70e-4db6-bf8e-7399fd4170df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to BART_DLM_domain_adaptation\n",
      "Configuration saved in BART_DLM_domain_adaptation/config.json\n",
      "Model weights saved in BART_DLM_domain_adaptation/pytorch_model.bin\n",
      "tokenizer config file saved in BART_DLM_domain_adaptation/tokenizer_config.json\n",
      "Special tokens file saved in BART_DLM_domain_adaptation/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         1.0\n",
      "  total_flos               = 120385974GF\n",
      "  train_loss               =      0.9186\n",
      "  train_runtime            =  8:03:59.22\n",
      "  train_samples            =       59694\n",
      "  train_samples_per_second =       2.056\n",
      "  train_steps_per_second   =       0.032\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "metrics = train_result.metrics\n",
    "metrics[\"train_samples\"] = len(tokenized_datasets['train'])\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8308e7-2e5e-462f-8dfc-b1f8a46305df",
   "metadata": {},
   "source": [
    "### Test trained model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79ab808b-138d-429a-915c-47984c1f2c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 6236\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3118' max='3118' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3118/3118 09:59]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_accuracy           =      0.828\n",
      "  eval_loss               =     0.8125\n",
      "  eval_runtime            = 0:10:13.27\n",
      "  eval_samples            =       6236\n",
      "  eval_samples_per_second =     10.168\n",
      "  eval_steps_per_second   =      5.084\n",
      "  perplexity              =     2.2535\n",
      "Perplexity: 2.25\n"
     ]
    }
   ],
   "source": [
    "# load saved model from output directory\n",
    "#trainer = get_new_trainer(model_name=current_notebook_name)\n",
    "try:\n",
    "    evaluate_perplexity_on_test_dataset(log_metrics=True)\n",
    "except:\n",
    "    wandb.alert(\n",
    "        title=f\"Evaluation error for {current_notebook_name}\",\n",
    "        text=f\"Run name: {run_name}.\\nSee details at https://wandb.ai/e-caste/{current_notebook_name}\",\n",
    "        level=AlertLevel.WARN,\n",
    "    )\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39ccb37f-4512-4c58-b6ee-cfd99569306c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca263eb2ad354b87a2073eda8abf95ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1553.453 MB of 1553.453 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▃▄▅▆▇▇▇███</td></tr><tr><td>eval/loss</td><td>█▆▅▄▃▂▂▂▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▆▆██▂▂▂▂▂▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▃▃▁▁▇▇▇▇▇█</td></tr><tr><td>eval/steps_per_second</td><td>▁▃▃▁▁▇▇▇▇▇█</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▇███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▄▄▃▃▃▃▂▃▃▂▃▂▃▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▂▂▁▂▁▁▂▂▁▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.82799</td></tr><tr><td>eval/loss</td><td>0.8125</td></tr><tr><td>eval/runtime</td><td>613.2715</td></tr><tr><td>eval/samples_per_second</td><td>10.168</td></tr><tr><td>eval/steps_per_second</td><td>5.084</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>932</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.8769</td></tr><tr><td>train/total_flos</td><td>1.2926345530677658e+17</td></tr><tr><td>train/train_loss</td><td>0.91862</td></tr><tr><td>train/train_runtime</td><td>29039.2231</td></tr><tr><td>train/train_samples_per_second</td><td>2.056</td></tr><tr><td>train/train_steps_per_second</td><td>0.032</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">local_NVIDIA-GeForce-RTX-3090_facebook-bart-large_2023-01-19-19_59_31.545026</strong>: <a href=\"https://wandb.ai/e-caste/BART_DLM_domain_adaptation/runs/30s865ap\" target=\"_blank\">https://wandb.ai/e-caste/BART_DLM_domain_adaptation/runs/30s865ap</a><br/>Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230119_195936-30s865ap/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    tmp = json.load(open(f\"{current_notebook_name}/eval_results.json\", \"r\"))\n",
    "    eval_results = \"\"\n",
    "    for k, v in tmp.items():\n",
    "        eval_results += f\"{k}: {v}\\n\"\n",
    "except:\n",
    "    eval_results = \"No eval_results.json.\"\n",
    "wandb.alert(\n",
    "    title=f\"Run finished for {current_notebook_name}\",\n",
    "    text=f\"Run name: {run_name}.\\n\\n{eval_results}\\nSee details at https://wandb.ai/e-caste/{current_notebook_name}\",\n",
    "    level=AlertLevel.INFO,\n",
    ")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0eba23-c4ca-4a08-9772-f1c46a7c1276",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "| Model         | Perplexity     | Accuracy     | Loss     | Notes |\n",
    "|--------------|-----------|------------|------------|------------|\n",
    "| facebook/bart-base | 36.81 | ? | 3.606 | without training        |\n",
    "| facebook/bart-base      | 2.636 | 0.8001 | 0.9693 | epochs=1, trained on 80% split, lr=1e-4, bs=4       |\n",
    "| facebook/bart-base      | 2.934 | 0.7849 | 1.0764 | epochs=1, trained on 90% split, lr=5e-4, bs=4       |\n",
    "| facebook/bart-base      | 2.6317 | 0.8002 | 0.9676 | epochs=1, trained on 90% split, lr=7.5e-5, bs=4       |\n",
    "| facebook/bart-base      | ? | 0.799149 | 0.976746 | epochs=1, trained on 90% split, lr=5e-5, bs=4       |\n",
    "| facebook/bart-base      | 2.5718 | 0.8036 | 0.9446 | epochs=2, trained on 90% split, lr=5e-5, bs=4       |\n",
    "| facebook/bart-base      | 2.6234 | 0.8008 | 0.9645 | epochs=1, trained on 90% split, lr=1e-4, bs=4       |\n",
    "| facebook/bart-base      | 2.654 | 0.7993 | 0.9761 | epochs=1, trained on 90% split, lr=2e-4, bs=4       |\n",
    "| facebook/bart-base      | 2.6298 | 0.8006 | 0.9669 | epochs=1, trained on 90% split, lr=1.5e-4, bs=4       |\n",
    "| facebook/bart-base      | 2.6248 | 0.8006 | 0.965 | epochs=1, trained on 90% split, lr=9e-5, bs=4       |\n",
    "| facebook/bart-base      | 2.5443 | 0.8056 | 0.9339 | epochs=2, trained on 90% split, lr=1e-4, bs=4       |\n",
    "| facebook/bart-large     | 191.39 | ? | 5.254 | without training       |\n",
    "| facebook/bart-large     | 2.2535 | 0.8282 | 0.8125 | epochs=1, trained on 90% split, lr=1e-4, bs=64       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62602d7f-6057-419f-8ac0-faf0461a7688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
