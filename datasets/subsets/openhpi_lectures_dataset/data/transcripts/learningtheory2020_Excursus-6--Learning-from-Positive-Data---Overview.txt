WEBVTT

1
00:00:00.000 --> 00:00:06.280 
We have seen a learning setting where we learn
from positive and negative information

2
00:00:06.490 --> 00:00:10.240 
so we see for example pictures of
butterflies and pictures that are

3
00:00:10.640 --> 00:00:15.290 
not with butterflies and then we
are told which ones contain

4
00:00:15.290 --> 00:00:18.410 
a butterfly and which ones do
not and then learn what is a

5
00:00:18.410 --> 00:00:22.600 
butterfly and what is not. So now we want
to look at a slightly different setting

6
00:00:23.010 --> 00:00:27.640 
and learn about cats. So this
is called learning from

7
00:00:27.660 --> 00:00:31.030 
positive data since we now only
have the positive information

8
00:00:31.040 --> 00:00:36.290 
yes this is a cat. So now the
question is is there a cat on

9
00:00:36.290 --> 00:00:40.200 
the image that is what the computer
should learn in this case,

10
00:00:40.200 --> 00:00:43.070 
or the learner should learn. So
it gets fat a lot of pictures

11
00:00:43.070 --> 00:00:46.000 
of cats. So this is a nice
picture of a cat and there's a

12
00:00:46.700 --> 00:00:51.320 
grumpy cat and then some other
cats and more cats and as you

13
00:00:51.320 --> 00:00:53.900 
can see there is no negative
information, there is no picture

14
00:00:53.900 --> 00:00:57.840 
that does not contain a cat. So
we see more and more pictures

15
00:00:57.840 --> 00:01:02.890 
of cats and now the task is to
say ok now I can identify what

16
00:01:02.890 --> 00:01:07.360 
a cat looks like. So one can imagine
children for example learning like this

17
00:01:07.730 --> 00:01:12.560 
uh they see lots of cats and told
by their parents look it's a cat

18
00:01:12.940 --> 00:01:17.080 
and when they then see an unknown animal
for example some kind of badger

19
00:01:17.530 --> 00:01:21.370 
they are able to say that this is
not a cat even though they have

20
00:01:21.370 --> 00:01:24.920 
never been told that badgers are
not cats. They can just infer

21
00:01:24.920 --> 00:01:29.400 
from lots of sightings of cats what
a cat generally should look like.

22
00:01:30.250 --> 00:01:35.890 
So we are now interested in formalizing
this for our use in the computer.

23
00:01:37.070 --> 00:01:40.730 
In general we have a learner
given and the learner gets some

24
00:01:40.730 --> 00:01:44.720 
kind of input and for the sake
of abstraction we are learning

25
00:01:45.170 --> 00:01:48.830 
numbers. So integer
numbers not negative,

26
00:01:49.350 --> 00:01:54.500 
sixteen, two fifty six, sixteen
again maybe four. So this is all

27
00:01:54.500 --> 00:02:00.330 
our positive data and this is what is
in the concept that we want to learn

28
00:02:00.770 --> 00:02:04.860 
and then the learner has to think
about it and come up with a guess.

29
00:02:05.400 --> 00:02:09.240 
So the guess could be well it
all looks like powers of two.

30
00:02:09.690 --> 00:02:13.560 
Sixteen is two to four, two hundred
fifty six is two to the eight and

31
00:02:13.560 --> 00:02:17.610 
four is two to the two.
So seems to check out.

32
00:02:18.290 --> 00:02:22.940 
Now the input doesn't stop here,
it keeps on going forever since

33
00:02:22.940 --> 00:02:26.310 
powers of two would be an infinite
language this kind of makes sense

34
00:02:26.610 --> 00:02:31.450 
if the concept. So maybe four, two
hundred, twenty four, ninety six.

35
00:02:31.960 --> 00:02:35.930 
So powers of two doesn't seem
to be too correct. So now the

36
00:02:35.930 --> 00:02:38.230 
learner might want to
change his mind.

37
00:02:38.950 --> 00:02:42.910 
Multiples of four would
be an example guess so

38
00:02:43.730 --> 00:02:47.070 
making the set
smaller and smaller

39
00:02:47.910 --> 00:02:53.430 
to contain only the stuff that
we have seen so far. Actually

40
00:02:53.430 --> 00:02:57.570 
it makes the set of course
bigger powers of two, most

41
00:02:57.570 --> 00:03:01.380 
powers of two are multiples of
four but it now kicked out

42
00:03:01.380 --> 00:03:03.770 
the two as a
possible option.

43
00:03:04.430 --> 00:03:09.200 
Then some more numbers come six,
eight, one forty six, two twelve,

44
00:03:09.350 --> 00:03:13.340 
and multiples of four is not
correct any more so the six

45
00:03:13.340 --> 00:03:17.710 
is not a multiple of four so
maybe now multiples of two

46
00:03:17.910 --> 00:03:21.290 
that would then include again the two
just as powers of two included the two,

47
00:03:21.670 --> 00:03:25.450 
multiples of four did not but multiples
of two now includes it again.

48
00:03:25.810 --> 00:03:30.210 
The learner hasn't seen that datum yet so
it can't really decide based on this

49
00:03:30.810 --> 00:03:33.030 
but it has to come up
with a guess somehow.

50
00:03:34.310 --> 00:03:40.520 
So this continues on basically
without end and the

51
00:03:40.520 --> 00:03:44.240 
general setting is that incomes
some kind of sequence of

52
00:03:44.240 --> 00:03:48.140 
data x0, x1, x2 and so on
and the learner outputs

53
00:03:48.140 --> 00:03:52.830 
some kind of sequence of
hypotheses, some kind of guesses

54
00:03:52.830 --> 00:03:54.980 
that makes p0, p1, p2

55
00:03:55.600 --> 00:04:00.810 
and of course we are interested in
computers doing the work and not

56
00:04:01.050 --> 00:04:05.370 
humans we want to analyze and
understand how computers currently

57
00:04:05.370 --> 00:04:08.430 
learn in this setting
from positive data.

58
00:04:09.580 --> 00:04:15.550 
Now this set mostly was growing
the output hypothesis.

59
00:04:15.750 --> 00:04:18.720 
it could be strictly growing in
this case it wasn't strictly

60
00:04:18.720 --> 00:04:23.100 
growing, the two was kicked
out and then back in. One

61
00:04:23.100 --> 00:04:24.820 
can also think of other

62
00:04:25.750 --> 00:04:31.060 
ways of learning so for example
incomes one two three

63
00:04:31.720 --> 00:04:36.440 
and then what comes out well maybe
all numbers from one to infinity,

64
00:04:36.920 --> 00:04:42.350 
then more numbers can come
five six seven nine ten and

65
00:04:42.350 --> 00:04:47.180 
then this very big set, an all
encompassing set of all numbers

66
00:04:47.940 --> 00:04:53.560 
can be narrowed down to all numbers
except multiples of four. So

67
00:04:53.880 --> 00:04:58.290 
a formally large set is being shrunk
which is kind of the opposite

68
00:04:58.290 --> 00:05:01.350 
of what we saw before that we
start with a rather small set

69
00:05:01.350 --> 00:05:05.890 
powers of two and then generally
get bigger as we see more data.

70
00:05:06.460 --> 00:05:09.160 
But learning doesn't have
to progress this way.

71
00:05:09.620 --> 00:05:13.540 
In general we allow whatever
kind of sequence of hypothesis

72
00:05:14.000 --> 00:05:16.590 
hypotheses that the
learner wants to output.

73
00:05:17.790 --> 00:05:21.240 
Yeah this continues of
course on our ability.

74
00:05:21.900 --> 00:05:25.700 
Now as I said the learner can
have various properties that

75
00:05:25.710 --> 00:05:29.320 
will see in more detail
in a different video.

76
00:05:29.790 --> 00:05:34.660 
So we can have these increasing
hypotheses, we can have decreasing

77
00:05:34.660 --> 00:05:40.290 
hypothesis' in the second example. We
can have consistency with the data

78
00:05:40.610 --> 00:05:46.060 
so that the learner always
makes a hypothesis that is

79
00:05:46.060 --> 00:05:49.060 
at least correct on the
data already presented,

80
00:05:49.640 --> 00:05:53.490 
um maybe it's not perfectly
correct yet when more data comes

81
00:05:53.490 --> 00:05:56.900 
maybe the hypothesis has to be
revised. But at least for the

82
00:05:56.900 --> 00:05:59.040 
data given so far
is consistent.

83
00:05:59.940 --> 00:06:04.960 
Another interesting property is
that the hypothesis should only

84
00:06:05.070 --> 00:06:11.890 
change when it becomes inconsistent, so you don't
the learner shouldn't make hypothesis change

85
00:06:12.400 --> 00:06:14.640 
even though the data
was still covered.

86
00:06:15.320 --> 00:06:20.140 
That for example was not the
case in our second example

87
00:06:20.140 --> 00:06:23.980 
where we guessed all numbers from
one to infinity and then as

88
00:06:23.980 --> 00:06:29.260 
more data came in we changed the
hypothesis to exclude multiples of four

89
00:06:29.790 --> 00:06:34.010 
even though the data presented
was still consistent and there

90
00:06:34.010 --> 00:06:40.100 
might be the four and the eight
being shown later in the data.

91
00:06:41.350 --> 00:06:46.460 
We will discuss these properties or
similar properties in different videos.

92
00:06:47.530 --> 00:06:53.810 
Now for the general definition
we say that a language L

93
00:06:54.190 --> 00:06:56.760 
from a big class of
languages is learned

94
00:06:57.200 --> 00:07:03.870 
or a learner learns this set of data, if
for all possible so called texts of L

95
00:07:04.020 --> 00:07:06.760 
and the text is an infinite
listing of all and only the data

96
00:07:06.760 --> 00:07:09.890 
of L in any order whatsoever. It
doesn't need to be increasing, it can

97
00:07:09.890 --> 00:07:14.230 
be in any order. The learner eventually
settles on the hypothesis i

98
00:07:14.820 --> 00:07:20.020 
that is a correct description for
this language. So this definition

99
00:07:20.020 --> 00:07:24.900 
is completely analogous to the case of
learning from positive and negative data.

100
00:07:25.830 --> 00:07:32.540 
Same for the general learning an
entire set, of learning tasks;

101
00:07:32.780 --> 00:07:36.080 
this is also completely analogous
to the case of learning from

102
00:07:36.080 --> 00:07:40.240 
positive and negative data. We say
a learner learns the set of tasks

103
00:07:40.240 --> 00:07:42.680 
if he learns every
member of this set.

104
00:07:45.430 --> 00:07:49.840 
I'll give you one example. So we
want to learn this set of learning

105
00:07:49.840 --> 00:07:55.080 
tasks. So the first learning task
looks like for a equal one

106
00:07:55.300 --> 00:08:00.080 
zero one two three four and so on.
The second learning task looks like

107
00:08:00.380 --> 00:08:05.590 
zero two four six eight and so on,
the third learning tasks zero

108
00:08:05.590 --> 00:08:11.820 
three six nine twelve and so on. So
all multiples of a for a sum number

109
00:08:12.240 --> 00:08:13.560 
is our learning task.

110
00:08:14.240 --> 00:08:16.980 
Now the question is which
learner could you learn

111
00:08:17.670 --> 00:08:19.820 
all these different
learning tasks.

112
00:08:20.540 --> 00:08:24.960 
And I propose the following
learner, given a set of data

113
00:08:24.960 --> 00:08:28.930 
always output the greatest common
factor of all input data.

114
00:08:29.930 --> 00:08:35.320 
So for example if we see the eight
and the twelve, the greatest common

115
00:08:35.320 --> 00:08:39.160 
factor is four so it yes
is multiples of four

116
00:08:39.650 --> 00:08:41.820 
is what is to put
to be learned.

117
00:08:42.450 --> 00:08:48.330 
This of course then excludes
the zero in a sense

118
00:08:48.330 --> 00:08:50.830 
every factor is a
factor of zero,

119
00:08:51.590 --> 00:08:53.080 
zero is a multiple
of every factor.

120
00:08:53.780 --> 00:08:58.840 
So this learner can be incorrect
to start with for example

121
00:08:58.840 --> 00:09:03.020 
when seeing the data 4a and 6a
then it would incorrectly

122
00:09:03.020 --> 00:09:05.250 
guess that it's
multiples of 2a. So

123
00:09:05.960 --> 00:09:11.170 
this is incorrect for
learning multiples of a but

124
00:09:11.170 --> 00:09:15.480 
is corrected at some point
when we see for example 7a.

125
00:09:15.880 --> 00:09:20.940 
And then the greatest common factor will
be a and then it will be correct.

126
00:09:21.580 --> 00:09:26.310 
So in general one can say that
there is a finite point in time

127
00:09:26.310 --> 00:09:30.370 
such that the learner will be
correct, since one can never know

128
00:09:30.370 --> 00:09:35.940 
how late this data comes. In the text
we can never say whether learning is

129
00:09:36.260 --> 00:09:41.760 
finished yet. But the learner always makes a
guess, it always makes a consistent guess

130
00:09:42.330 --> 00:09:46.800 
and it will only change its
mind if the data contradicts

131
00:09:47.500 --> 00:09:52.660 
the guess. That is just one
example of a learning task.

132
00:09:53.410 --> 00:09:58.070 
So the learner has less
information than learning with

133
00:09:58.190 --> 00:10:03.540 
labelled data. So positive and
negative information that is clear.

134
00:10:03.770 --> 00:10:07.570 
So it will be a bit
harder to learn. It's a

135
00:10:07.990 --> 00:10:12.840 
weaker paradigm in the sense
that less is learnable but

136
00:10:12.850 --> 00:10:14.980 
still we want to learn
as well as possible

137
00:10:15.490 --> 00:10:19.670 
and many of such instances of
learning are around us, so

138
00:10:19.670 --> 00:10:24.190 
it is interesting to analyze this
setting and understand how

139
00:10:24.190 --> 00:10:25.870 
learning can proceed
in this case.

140
00:10:26.490 --> 00:10:30.630 
So for example when children
learn a natural language they

141
00:10:30.640 --> 00:10:35.400 
typically learn by listening to
others speak. They learn correct

142
00:10:35.410 --> 00:10:39.700 
sentences, they learn correct
words by listening to lots

143
00:10:39.700 --> 00:10:43.700 
of correct words and correct
sentences. And they don't get told

144
00:10:43.710 --> 00:10:47.460 
now here is an incorrect sentence
and then the parents speak

145
00:10:47.460 --> 00:10:49.700 
some incorrect sentences
and then maybe some more.

146
00:10:50.100 --> 00:10:53.600 
And certainly not all incorrect
words and incorrect sentences

147
00:10:53.600 --> 00:10:56.650 
are being presented to
the child but instead

148
00:10:57.200 --> 00:10:58.980 
it learns just from

149
00:11:00.060 --> 00:11:04.200 
listening to correct sentences.
Similarly I gave the example

150
00:11:04.200 --> 00:11:07.650 
in the beginning, learning cats.
Yes sometimes it's pointed out

151
00:11:07.650 --> 00:11:10.260 
no this is a dog that's not a
cat, so there's some negative

152
00:11:10.260 --> 00:11:12.260 
information in the
real world typically

153
00:11:12.900 --> 00:11:17.420 
but again if the child sees a
badger or whatever animal that

154
00:11:17.420 --> 00:11:22.330 
is not featured so far it can typically
say that no this is not a cat.

155
00:11:23.200 --> 00:11:29.000 
Learning customs of a group of people
by observation also with adults

156
00:11:29.380 --> 00:11:33.330 
this is typically something where
you well look at what other

157
00:11:33.330 --> 00:11:37.360 
customs of the people around me and
then you kind of try to blend in.

158
00:11:37.900 --> 00:11:45.710 
It's not like there also negative examples
really provided, though it sometimes happens.

159
00:11:46.360 --> 00:11:51.470 
Now more interesting for us is how
it's coming up in technology.

160
00:11:51.830 --> 00:11:55.330 
So for computer science there are
a lot of examples coming up.

161
00:11:55.680 --> 00:12:00.620 
For example recognizing entries in
a database. So you look through

162
00:12:00.620 --> 00:12:03.960 
all kinds of entries and you say
that looks like first names to me.

163
00:12:04.070 --> 00:12:06.320 
First names is the
general concept of this

164
00:12:06.730 --> 00:12:10.970 
data and you say that just by
looking at well a bunch of them

165
00:12:10.970 --> 00:12:14.000 
and say, well, these are all first
names, I'm guessing that this is

166
00:12:14.310 --> 00:12:19.300 
data for first names. Or you see lots
of dates and then you say well this

167
00:12:19.580 --> 00:12:22.470 
row of data or some
such is about dates.

168
00:12:23.610 --> 00:12:29.020 
Also you might have seen some
automatic detection of

169
00:12:29.020 --> 00:12:31.400 
language. For example
when you write an email

170
00:12:31.900 --> 00:12:34.810 
and your spell checker automatically
says oh you're writing

171
00:12:34.810 --> 00:12:38.340 
in German, so I now do German
spell-check, or you're writing in

172
00:12:38.340 --> 00:12:41.800 
English then I do English spell-check,
and today I'm writing French

173
00:12:41.800 --> 00:12:45.730 
so I do French spell-check. So
this detection of language is

174
00:12:45.730 --> 00:12:50.930 
basically you see lots of positive data,
lots of words and then you identify

175
00:12:51.050 --> 00:12:52.520 
what language are
these words from.

176
00:12:53.800 --> 00:12:59.240 
Finally also inference of documents
structure for collection of documents

177
00:12:59.860 --> 00:13:04.320 
is a task that is a bit
more involved. So

178
00:13:04.900 --> 00:13:08.040 
given a set of documents and you
want to process those, you

179
00:13:08.040 --> 00:13:12.530 
want to understand those, sometimes make
sense to infer the document structure then

180
00:13:12.750 --> 00:13:17.750 
with some kind of automatic procedure and
it's also covered by our setting of

181
00:13:18.110 --> 00:13:19.670 
learning from
positive data.
