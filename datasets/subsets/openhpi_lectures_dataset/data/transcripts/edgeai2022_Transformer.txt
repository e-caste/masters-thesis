WEBVTT

1
00:00:00.940 --> 00:00:03.620 
Hello and welcome! In this video,

2
00:00:03.629 --> 00:00:12.099 
we will introduce Transformer model which first had a significant influence in the field of natural language processing and

3
00:00:12.099 --> 00:00:16.839 
then began to influence computer vision.

4
00:00:16.839 --> 00:00:22.559 
Because the attention mechanism plays an essential role in the transformer model structure.

5
00:00:22.940 --> 00:00:34.619 
Let's first learn the attention mechanism and the difference between the visual task and NLP task. In the year 2015, the

6
00:00:34.619 --> 00:00:43.259 
"show, attend and tell" paper maybe the first work that proposed the attention mechanism in the context of computer vision

7
00:00:44.039 --> 00:00:47.880 
it is more like the mechanism of the human retina.

8
00:00:48.149 --> 00:00:58.850 
There will be a focus in the visual image, where the focus is the clearest, and the surroundings are blurred.

9
00:01:00.039 --> 00:01:04.469 
The visual attention tries to imitate such a mechanism.

10
00:01:04.480 --> 00:01:12.250 
Find the focal area in the image and find the connection between those focal points and the natural language.

11
00:01:13.739 --> 00:01:22.819 
The algorithm tries to understand the semantic meaning of a sentence and uses the key words to be aligned with the visual

12
00:01:22.819 --> 00:01:25.159 
focus in the corresponding image.

13
00:01:28.939 --> 00:01:36.049 
The attention mechanism in NLP domain describes the correlations between tokens of sentences.

14
00:01:36.840 --> 00:01:44.250 
It is related to how to define query, key, and value vectors in the NLP problems.

15
00:01:44.840 --> 00:01:48.590 
For example, on the reading comprehension problem,

16
00:01:48.599 --> 00:01:57.670 
the query can be a representation of a question or a representation of a question, combine it with an option and for key

17
00:01:57.670 --> 00:02:03.359 
and value they are often the same and they all refer to the context information.

18
00:02:03.840 --> 00:02:10.360 
So for reading comprehension problem, the context is the article. For this case,

19
00:02:10.370 --> 00:02:20.159 
The purpose of attention is to find out the relevant fragments in the context, which is for example an article to the given

20
00:02:20.939 --> 00:02:24.810 
question to estimate the best answer.

21
00:02:26.240 --> 00:02:34.659 
Using this attention scores, you can get a weighted representation and then put it into a feed forward neural network to

22
00:02:34.659 --> 00:02:40.150 
get a new representation which takes into a current contextual information.

23
00:02:41.039 --> 00:02:44.280 
Another example here is a machine translation.

24
00:02:45.139 --> 00:02:52.000 
The input sentence will be processed by a neural network and to predict each output token,

25
00:02:52.009 --> 00:03:00.969 
it will not only get the information of the corresponding input token but also consider the context information from the

26
00:03:00.969 --> 00:03:10.330 
surrounding tokens but not all the input token will be equally considerate for certain output token and you will learn

27
00:03:10.330 --> 00:03:20.060 
a softmax attention distribution and to re-weight the correlation between an output token and several different input tokens.

28
00:03:20.539 --> 00:03:27.259 
Here, for example, the tokens located far away may be relatively less relevant.

29
00:03:30.639 --> 00:03:39.550 
Self attention for short will capture the correlation of a given tokens to all the other tokens in the same sentence.

30
00:03:40.240 --> 00:03:46.150 
For example, the token "it" here refers to "animal" or the "street".

31
00:03:46.840 --> 00:03:57.460 
This requires us to reach the context when we see the state is "tired" we should know that it refers to the "animal" with the

32
00:03:57.460 --> 00:03:58.860 
higher probability.

33
00:03:59.840 --> 00:04:08.539 
In the recurrent neural network, we need to process all the token step by step and when they are far apart, the

34
00:04:08.539 --> 00:04:18.720 
effect of recurrent neural network is often poor and its sequential processing efficiency is also very low and self attention

35
00:04:18.720 --> 00:04:26.250 
uses the attention mechanism to calculate the association between each token and all the other token in the sentence.

36
00:04:26.939 --> 00:04:32.889 
In the first sentence, the word animal has the highest attention score for the token

37
00:04:32.899 --> 00:04:45.699 
"it". On the contrary, adjective from "tired" changed to "wide" in the second sentence and token with the highest attention

38
00:04:45.699 --> 00:04:50.160 
score also changed from animal to the street.

39
00:04:51.339 --> 00:05:00.850 
So therefore we can see that the model trained by self attention can capture this context information very well and the

40
00:05:00.850 --> 00:05:03.990 
efficiency will not be drastically reduced

41
00:05:03.990 --> 00:05:09.339 
once the sentence became very long.

42
00:05:09.339 --> 00:05:18.410 
Transformer is essentially an encoder decoder structure and the encoder is composed of six encoding blocks and the decoder

43
00:05:18.410 --> 00:05:21.189 
is also composed of six decoding blocks.

44
00:05:21.199 --> 00:05:28.750 
Like all generative models, the output of the encoder will be used as the input of the decoder.

45
00:05:29.740 --> 00:05:36.550 
The encoder block consists of multi head self attention module and feed forward network layer.

46
00:05:37.439 --> 00:05:42.759 
Multi head attention is the combination of multiple self attention structures.

47
00:05:43.240 --> 00:05:47.949 
Each head learns features in different representation spaces.

48
00:05:47.959 --> 00:05:57.079 
As shown in the figure, the focus of attention learned by the two heads may be slightly different, giving the model more

49
00:05:57.079 --> 00:05:57.850 
capacity.

50
00:05:58.639 --> 00:06:07.519 
The difference between the decoder and the encoder block is that the decoder has one more connection attention

51
00:06:07.519 --> 00:06:10.850 
module between the encoder and the decoder.

52
00:06:10.860 --> 00:06:16.149 
The functional difference between these two different attention blocks are following.

53
00:06:16.839 --> 00:06:25.110 
In machine translation task self attention captures the relationship between the current translation and the previous text

54
00:06:25.110 --> 00:06:33.790 
that has been translated. The encoder decoder attention focused on the relationship between the current translation and the

55
00:06:33.790 --> 00:06:35.660 
encoded future vector.

56
00:06:39.540 --> 00:06:44.139 
We already introduced the three components for many NLP tasks.

57
00:06:44.230 --> 00:06:46.350 
They are query, key, and value.

58
00:06:46.939 --> 00:06:49.060 
How are they arranged in transformer?

59
00:06:49.839 --> 00:06:58.779 
Query, Key, and Value vectors are computed from the same word with the same length and in the self attention module. They are

60
00:06:58.779 --> 00:07:07.490 
obtained by multiplying the embedding vector by three different weight matrices. And the dimensions of three matrices are the

61
00:07:07.490 --> 00:07:07.949 
same.

62
00:07:09.139 --> 00:07:18.100 
The computation is according to the equation and softmax attention will be further multiplied by the value vector

63
00:07:18.110 --> 00:07:27.920 
and the output of multi head attention module will be fed into a classic feed forward network layer. Multi-head attention

64
00:07:27.930 --> 00:07:36.550 
may learn different features for each head and which can further enhance the expression ability of the model.

65
00:07:37.040 --> 00:07:41.470 
But it also introduced some more computation overhead.

66
00:07:41.480 --> 00:07:46.250 
So we need to make a trade off of accuracy and efficiency here.

67
00:07:49.230 --> 00:07:57.610 
As aforementioned, the convolutional neural network have two important inductive bias. The locality,

68
00:07:57.620 --> 00:08:01.569 
it has a locally restricted receptive field.

69
00:08:01.579 --> 00:08:06.660 
This means that the linear convolution filter can only see the neighbor values.

70
00:08:07.740 --> 00:08:17.720 
The second is weight sharing across the whole image, which makes the convolution filter translation invariance. Here invariants

71
00:08:17.720 --> 00:08:24.160 
means that you can recognize an object in an image regardless of the specific positions.

72
00:08:25.040 --> 00:08:29.319 
CNN works pretty well when the object's appearance change.

73
00:08:29.329 --> 00:08:39.259 
On the other hand, the transformer is by design  permutation in variant transformer is for sequential data and the missing

74
00:08:39.259 --> 00:08:41.960 
position information of visual objects.

75
00:08:44.740 --> 00:08:53.350 
So if we want to also apply transformer architecture on image, we need to make some adaption on its structure.

76
00:08:54.340 --> 00:09:03.049 
Recently, the author of transformer further proposed vision transformer which reformulated the image classification problem

77
00:09:03.059 --> 00:09:07.960 
at the sequential problem using image patches as word tokens.

78
00:09:08.559 --> 00:09:10.830 
Let's take a look how it works.

79
00:09:11.940 --> 00:09:22.570 
First it splits an image into patches and then flattened the patches, create the linear embedding of the patches and then

80
00:09:22.570 --> 00:09:28.649 
define and add a positional embedding information on top of the linear embedding.

81
00:09:28.659 --> 00:09:34.259 
Note that the extra learnable class embedding are also utilized here.

82
00:09:34.269 --> 00:09:37.850 
It is fed into the sequence at the position zero.

83
00:09:38.940 --> 00:09:42.980 
Feed the sequential input into a standard transformer encoders.

84
00:09:42.990 --> 00:09:46.950 
And get the classification result from a MLP-Head

85
00:09:46.950 --> 00:09:50.059 
on top of the transformer encoders.

86
00:09:52.440 --> 00:09:59.960 
So with some simple adaption modification, transformer can also be used for image classification task.

87
00:10:04.110 --> 00:10:12.080 
They also claim that this structure does not directly produce impressive results on datasets such as CIFAR and ImageNet.

88
00:10:12.740 --> 00:10:22.149 
But if pre training is done on a larger data set, the situation will fundamentally change while larger ViT

89
00:10:22.149 --> 00:10:27.450 
model perform worse than BiT ResNets baseline.

90
00:10:28.240 --> 00:10:31.860 
And the pre-trained on the small data set.

91
00:10:31.919 --> 00:10:36.990 
It performs better and even much better when pre training on the larger dataset.

92
00:10:37.000 --> 00:10:39.549 
Similarly,  larger ViT

93
00:10:39.549 --> 00:10:46.860 
variants overtake similar ones as the dataset grows. In the figure on the right hand side,

94
00:10:47.240 --> 00:10:49.559 
We can see that ViTs

95
00:10:49.559 --> 00:10:53.360 
outperform ResNet across the FLOPs landscapes.

96
00:10:57.039 --> 00:11:01.750 
ConvNets like AlexNet contains two separate stream of processing.

97
00:11:02.440 --> 00:11:11.370 
An apparent consequence of this architecture is that one stream developed the high frequency grayscale features and the

98
00:11:11.370 --> 00:11:13.860 
other low frequency color features.

99
00:11:15.139 --> 00:11:25.429 
The realization of the first linear embedding filters of visual transformer shows that early layer represents may share similar

100
00:11:25.429 --> 00:11:26.639 
features as ConvNets.

101
00:11:26.639 --> 00:11:37.120 
It demonstrates very well learned smooth filters and also compute attention distance at the average distance between

102
00:11:37.120 --> 00:11:48.830 
the query pixel and the rest of the patch, multiplied by attention weights. And they use 128 example images and average their

103
00:11:48.830 --> 00:11:49.559 
results.

104
00:11:50.340 --> 00:12:01.210 
So it can be seen that the area of inherent interest obtained in each photo is roughly consistent with the contour shape

105
00:12:01.210 --> 00:12:10.700 
of the object, indicating that the learned attention has a reasonable semantic meaning and the relatively higher interpretability.

106
00:12:14.539 --> 00:12:22.759 
Okay, in the practical session of this week, we will learn how to implement an image classification model using PyTorch.

107
00:12:23.740 --> 00:12:32.009 
The detailed task description can be found in the next learning unit and the time required to complete the practical task

108
00:12:32.019 --> 00:12:34.360 
is about 3-6 hours.

109
00:12:34.940 --> 00:12:38.370 
I wish you all have fun and have great success.

110
00:12:41.139 --> 00:12:41.649 
Thank you
