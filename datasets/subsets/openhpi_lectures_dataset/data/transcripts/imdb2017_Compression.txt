WEBVTT

1
00:00:00.400 --> 00:00:04.800 
So we want to talk a little bit
about how do we compress the

2
00:00:04.800 --> 00:00:08.120 
data in the main data
storage, how do we

3
00:00:09.130 --> 00:00:15.190 
build attribute vectors and
what is happening in the

4
00:00:15.190 --> 00:00:17.210 
dictionary and in the
attribute vectors.

5
00:00:18.220 --> 00:00:24.280 
So there are many
compression techniques

6
00:00:24.280 --> 00:00:29.330 
available, I think there is
a huge amount of literature

7
00:00:29.330 --> 00:00:33.370 
about compression techniques,
really became popular

8
00:00:33.370 --> 00:00:38.420 
with relational databases
for the index of relational

9
00:00:38.420 --> 00:00:42.460 
databases. So when in
relational databases became

10
00:00:42.460 --> 00:00:47.510 
popular 25 years ago or
actually longer already, some

11
00:00:47.510 --> 00:00:50.540 
mid '80s so
nearly 30 ago

12
00:00:51.550 --> 00:00:55.590 
computer scientists all
over the world built

13
00:00:56.600 --> 00:01:00.640 
clever index
techniques and

14
00:01:02.660 --> 00:01:05.690 
there are different
techniques for

15
00:01:08.720 --> 00:01:10.740 
indices which are used
for direct access.

16
00:01:10.740 --> 00:01:15.790 
So they are basically trees and
you have learned about trees

17
00:01:15.790 --> 00:01:19.830 
I hope, so you have
learned about trees and

18
00:01:19.830 --> 00:01:26.900 
how we find, with a few

19
00:01:27.910 --> 00:01:31.950 
with a few comparisons we find
the location and boom, we can

20
00:01:31.950 --> 00:01:35.990 
access the data directly
instead of having a disk key

21
00:01:35.990 --> 00:01:39.103 
we have the location,
the physical location

22
00:01:40.104 --> 00:01:45.109 
of the logical position

23
00:01:46.110 --> 00:01:50.114 
of a row in the table and
then we can access field

24
00:01:50.114 --> 00:01:53.117 
by field by field
applying this position

25
00:01:56.120 --> 00:02:02.126 
to all columns of a
row. So that is pretty

26
00:02:03.000 --> 00:02:06.130 
easy, how do we compress
the other columns?

27
00:02:07.131 --> 00:02:10.134 
This is the question
and there are,

28
00:02:11.135 --> 00:02:14.138 
just for distinction, heavy
weight and light weight techniques

29
00:02:15.139 --> 00:02:20.144 
in the case of HANA
and SanssouciDB as

30
00:02:20.144 --> 00:02:26.150 
our research database we
decided to mainly only

31
00:02:26.150 --> 00:02:30.154 
use light weight techniques or
prefer to use light weight techniques

32
00:02:30.154 --> 00:02:34.158 
because the system should be
used for OLTP direct access

33
00:02:34.158 --> 00:02:37.161 
and you can compress
much higher when

34
00:02:38.162 --> 00:02:41.165 
access is only sequential,
when access has to be

35
00:02:42.166 --> 00:02:47.171 
sequential and
direct then focus on

36
00:02:48.172 --> 00:02:51.175 
light weight techniques.

37
00:02:53.177 --> 00:02:55.179 
Let's look what we can do
to the attribute-vector

38
00:02:56.180 --> 00:03:02.186 
the first thing is we
we code everything,

39
00:03:03.187 --> 00:03:08.192 
every attribute value with
the help of a dictionary into

40
00:03:08.192 --> 00:03:14.198 
a binary value into an
integer. In this case we do it

41
00:03:14.198 --> 00:03:19.203 
variable length one bit, two
bit, three bit up to I don't know

42
00:03:19.203 --> 00:03:24.000 
how many bits, probably 32 bits,
that is a pretty large number

43
00:03:24.000 --> 00:03:30.214 
already. So let us
look here at the world

44
00:03:30.214 --> 00:03:31.215 
table of people

45
00:03:34.218 --> 00:03:37.221 
and this is,

46
00:03:38.222 --> 00:03:41.225 
I am running ahead
already a little bit so we

47
00:03:42.226 --> 00:03:45.229 
we take 8 billion
people into account

48
00:03:46.230 --> 00:03:50.234 
and China is already at
1.4 billion in this case

49
00:03:50.234 --> 00:03:54.238 
it's easier to
calculate a few things.

50
00:03:55.239 --> 00:04:01.245 
So just a few numbers we have,
round about 200 countries,

51
00:04:01.245 --> 00:04:04.248 
we need 8 bits for that we
have one million cities,

52
00:04:05.249 --> 00:04:07.251 
this is 20 bits.

53
00:04:11.255 --> 00:04:15.259 
We have 100 different,
second nationalities.

54
00:04:18.262 --> 00:04:22.266 
This is 7 bit but not many
people have a second nationality.

55
00:04:24.268 --> 00:04:30.274 
In Germany there are quite a
few, in Berlin there are a lot.

56
00:04:32.276 --> 00:04:34.278 
We have only 5
million first names,

57
00:04:38.282 --> 00:04:42.286 
still 23 bits.

58
00:04:42.286 --> 00:04:48.292 
So these are a few numbers,
if we encode the column

59
00:04:49.293 --> 00:04:56.300 
country for the 8 billion
people we end up with

60
00:04:56.300 --> 00:05:03.307 
7.45 gigabytes of
data, easy calculation.

61
00:05:04.308 --> 00:05:10.314 
If it's sorted by country
then for quite some time

62
00:05:11.315 --> 00:05:15.319 
we have the number
37, the Chinese ones

63
00:05:15.319 --> 00:05:18.322 
Chinese, Chinese, Chinese,
Chinese, I stop now

64
00:05:18.322 --> 00:05:22.326 
and then 1396, 97

65
00:05:22.326 --> 00:05:26.330 
98 and then
we come at 1.4

66
00:05:26.330 --> 00:05:33.337 
billion. What we can do is,
in this case we memorize

67
00:05:33.337 --> 00:05:38.342 
it is 37 China and
it's 1.4 billion times

68
00:05:41.345 --> 00:05:45.349 
so we can
dramatically reduce

69
00:05:45.349 --> 00:05:49.353 
the storage, dramatically, we
can reduce it here by nearly

70
00:05:49.353 --> 00:05:54.358 
1.4 gigabytes. We
can do this now

71
00:05:54.358 --> 00:05:59.363 
several times and do run
length encoding and so

72
00:05:59.363 --> 00:06:02.366 
we have 37 then we
have 74 then we have

73
00:06:02.366 --> 00:06:08.372 
195 and the for
smaller countries

74
00:06:08.372 --> 00:06:11.375 
it's not very efficient.

75
00:06:12.376 --> 00:06:17.381 
Last year we had one
in openHPI coming

76
00:06:17.381 --> 00:06:22.386 
from the Solomon Islands
participating in the online

77
00:06:22.386 --> 00:06:29.393 
HANA or Sanssouci, HPI
seminar, lecture from the

78
00:06:29.393 --> 00:06:32.396 
Solomon Islands. I wanted to
write him an email because

79
00:06:32.396 --> 00:06:36.400 
we had actually his email but
he checked out after two weeks

80
00:06:36.400 --> 00:06:40.404 
so otherwise Solomon
Islands would be a 100% on.

81
00:06:43.407 --> 00:06:46.410 
That was interesting,
this is a new world of

82
00:06:47.411 --> 00:06:50.414 
studying via
the internet.

83
00:06:50.414 --> 00:06:54.418 
So we keep the value and we
keep the number of occurrences

84
00:06:54.418 --> 00:06:58.422 
and the standing position position
and then we basically can reconstruct

85
00:06:58.422 --> 00:07:05.429 
the attribute-vector on the fly
and reduce it now dramatically

86
00:07:05.429 --> 00:07:11.435 
down to one kilobyte of data
so we can do this only for one

87
00:07:12.436 --> 00:07:15.439 
column because
we can sort the

88
00:07:15.439 --> 00:07:19.443 
whole table only by
one attribute and not

89
00:07:20.444 --> 00:07:25.449 
by many so we will
pick one where this

90
00:07:25.449 --> 00:07:29.453 
is very efficient. Typically
a selection by country could

91
00:07:29.453 --> 00:07:33.457 
also be for other
purposes, come in handy,

92
00:07:34.458 --> 00:07:38.462 
because in a world wide
people database, most of the

93
00:07:38.462 --> 00:07:42.466 
applications will be country
specific so therefore in this

94
00:07:42.466 --> 00:07:44.468 
mini application here
we take this one.

95
00:07:47.471 --> 00:07:53.477 
Actually, it is tiring
to go through all these

96
00:07:53.477 --> 00:07:59.483 
encoding techniques. The
more complicated they get,

97
00:07:59.483 --> 00:08:05.489 
the better they compress,
the less suitable

98
00:08:05.489 --> 00:08:07.491 
they are for direct access
and we have to make a

99
00:08:08.492 --> 00:08:11.495 
judgment call, do we
spend this, is the

100
00:08:12.496 --> 00:08:16.500 
reduction of space
worth the additional

101
00:08:17.501 --> 00:08:21.505 
computation to find
the position and then

102
00:08:21.505 --> 00:08:26.510 
continue with the search
until we have found the

103
00:08:26.510 --> 00:08:29.513 
right tuple in the
case of direct access.

104
00:08:30.514 --> 00:08:33.517 
On the other hand
direct access is not

105
00:08:34.518 --> 00:08:37.521 
the dominant any more and
therefore we do all these, develop

106
00:08:37.521 --> 00:08:41.525 
here these tools that we really
can judge an application, how

107
00:08:41.525 --> 00:08:45.529 
much direct access do we do,
why do we do direct access, and

108
00:08:45.529 --> 00:08:49.533 
how much do we do, range select
and only then we can answer

109
00:08:49.533 --> 00:08:52.536 
whether this or another
compression would be

110
00:08:52.536 --> 00:08:54.538 
the right compression.

111
00:08:57.541 --> 00:09:01.545 
And I repeat this again and
again this is completely changing

112
00:09:01.545 --> 00:09:04.548 
when you change the
architecture of the application.

113
00:09:05.549 --> 00:09:09.553 
There is another one, an
interesting one, very often in

114
00:09:09.553 --> 00:09:13.557 
in these applications
we have a standard value

115
00:09:14.558 --> 00:09:18.562 
in an attribute
and not much more.

116
00:09:21.565 --> 00:09:25.569 
So if we remove the
standard value and the

117
00:09:26.570 --> 00:09:31.575 
set default is the standard
value and we look then all only

118
00:09:31.575 --> 00:09:34.578 
at the deviations from
the standard value

119
00:09:34.578 --> 00:09:39.583 
and there are only a few. Then we
can have a bit vector describing

120
00:09:40.584 --> 00:09:45.589 
where we have a deviation
from the standard vector

121
00:09:46.590 --> 00:09:52.596 
and a case here is, well in this
case here is the second language

122
00:09:52.596 --> 00:09:55.599 
because most, a second
country, because most people

123
00:09:56.600 --> 00:09:59.603 
do not have a second
nationality so in this case 99%

124
00:09:59.603 --> 00:10:02.606 
of people do not have
a second nationality.

125
00:10:03.607 --> 00:10:06.610 
So for all the ones who do not
have a second nationality they

126
00:10:06.610 --> 00:10:12.616 
have no, there
is no entry and

127
00:10:13.617 --> 00:10:18.622 
we can compress the
dictionary attribute-vector

128
00:10:18.622 --> 00:10:23.627 
significantly. Actually I propose
that we do this for default

129
00:10:23.627 --> 00:10:28.632 
values, because SAP always worked
with default values in the past

130
00:10:28.632 --> 00:10:31.635 
and not totally correctly
with the NULL value.

131
00:10:33.637 --> 00:10:37.641 
So NULL value is a strange guy
(datatype) we did either not

132
00:10:37.641 --> 00:10:41.645 
fully understand or not
really like. So we had

133
00:10:41.645 --> 00:10:44.648 
default values typically in
commercial application it's blank

134
00:10:44.648 --> 00:10:48.652 
or zero and the zero you can
decide between a character zero

135
00:10:48.652 --> 00:10:54.658 
and a binary zero. We have a
lot of columns which are of that

136
00:10:54.658 --> 00:11:02.666 
type. The rule is if a
column has only one value

137
00:11:04.668 --> 00:11:10.674 
either a blank zero or character
zero then the column doesn't

138
00:11:10.674 --> 00:11:15.679 
exist, should not exist.
Jürgen, could you check

139
00:11:15.679 --> 00:11:17.681 
this, whether
that is true.

140
00:11:19.683 --> 00:11:23.687 
We discussed this here at length
four years ago or longer ago.

141
00:11:23.687 --> 00:11:28.692 
If you do this then
a lot of columns

142
00:11:29.693 --> 00:11:34.698 
which are only populated
by a default value

143
00:11:34.698 --> 00:11:39.703 
they just collapse to, there
is a column to metadata,

144
00:11:40.704 --> 00:11:44.708 
there is a column but no data and
then there is no attribute-vector,

145
00:11:45.709 --> 00:11:48.712 
which is good.

146
00:11:51.715 --> 00:11:55.719 
This means that columns which
are not being used don't count.

147
00:11:58.722 --> 00:12:01.725 
Regardless of the discussion
we have sparse ones and we

148
00:12:01.725 --> 00:12:05.729 
can compress a lot but then
we have to calculate a bit for

149
00:12:05.729 --> 00:12:09.733 
direct access. We
can actually say

150
00:12:10.734 --> 00:12:14.738 
that one of the deficiencies
of standard systems and SAP is

151
00:12:14.738 --> 00:12:19.743 
a bad candidate for this,
we over size tables.

152
00:12:20.744 --> 00:12:23.747 
There are many
compression techniques.

153
00:12:24.748 --> 00:12:29.753 
Primary key has to be, for direct
access, has to be a tree structure.

154
00:12:29.753 --> 00:12:35.759 
There is a separate
index which is a

155
00:12:35.759 --> 00:12:41.765 
tree on top of
the table columns

156
00:12:42.766 --> 00:12:45.769 
and every single attribute
can be used then as an index

157
00:12:47.771 --> 00:12:51.775 
and the compression of the of
the columns can be relatively

158
00:12:51.775 --> 00:12:55.779 
high compressions because
they are only used

159
00:12:55.779 --> 00:13:00.784 
in sequential
processing, so

160
00:13:03.787 --> 00:13:05.789 
for filtering and
sequential processing.

161
00:13:07.791 --> 00:13:13.797 
For the reconstruction,
that I don't say something

162
00:13:13.797 --> 00:13:15.799 
wrong, for the
reconstruction of a tuple

163
00:13:15.799 --> 00:13:21.805 
we need the direct access,
that is a penalty we have

164
00:13:21.805 --> 00:13:26.810 
to pay, I forgot about that.
When we, out of the 300

165
00:13:26.810 --> 00:13:31.815 
columns, we want to get now
10 and we have the position

166
00:13:32.816 --> 00:13:35.819 
of the tuple we are working on
then we have only the relative

167
00:13:35.819 --> 00:13:40.824 
position in a column and the
relative position can not translate

168
00:13:40.824 --> 00:13:43.827 
to direct access if we use some
of these then we have to use some

169
00:13:48.832 --> 00:13:52.836 
approximate position and
then we have to figure out

170
00:13:52.836 --> 00:13:56.840 
the position
by scanning,

171
00:13:59.843 --> 00:14:04.848 
scanning and counting.
Which ones do we actually

172
00:14:04.848 --> 00:14:06.850 
use now in in
HANA, is that

173
00:14:12.856 --> 00:14:15.859 
yeah this is a nice one

174
00:14:17.861 --> 00:14:22.866 
and this one was
actually the first

175
00:14:22.866 --> 00:14:26.870 
one which was introduced
in the dictionary

176
00:14:27.871 --> 00:14:33.877 
to translate back, because
in the in the dictionary

177
00:14:33.877 --> 00:14:37.881 
we have to translate
from the attribute-value

178
00:14:37.881 --> 00:14:41.885 
to the integer and then from the
integer to the attribute-value

179
00:14:41.885 --> 00:14:45.889 
this is very easy so we, if
we have the valueID, we find

180
00:14:45.889 --> 00:14:49.893 
the character, the
original attribute value.

181
00:14:49.893 --> 00:14:53.897 
But the other way around if
we want to search for the

182
00:14:53.897 --> 00:14:56.900 
character to find whether
we have it already,

183
00:14:56.900 --> 00:15:00.904 
we sort the dictionary
by the value

184
00:15:00.904 --> 00:15:03.907 
and in order to compress
it as much as possible

185
00:15:04.908 --> 00:15:08.912 
we use this delta encoding
for the dictionary

186
00:15:08.912 --> 00:15:12.916 
where we write down,

187
00:15:13.917 --> 00:15:18.922 
where we always re-use the
previous character string

188
00:15:19.923 --> 00:15:22.926 
and extend it. So the
game goes like, the first

189
00:15:23.927 --> 00:15:27.931 
these are cities obviously,
the first city is Aach

190
00:15:28.932 --> 00:15:33.937 
in it's four characters long,
we re-use four characters of the

191
00:15:33.937 --> 00:15:37.941 
city and put two new ones
on top and we have Aachen.

192
00:15:37.941 --> 00:15:40.944 
We take to one of the
previous ones which is Aachen

193
00:15:41.945 --> 00:15:46.950 
and we take the "a,a"
and put six new ones on Aalbourg.

194
00:15:46.950 --> 00:15:49.953 
We take one of the
previous one and add

195
00:15:51.955 --> 00:15:55.959 
then at two and then we
get Alba, I don't know

196
00:15:55.959 --> 00:15:59.963 
what city that
is, and it's nice

197
00:15:59.963 --> 00:16:04.968 
when we have Haarlem, and in
Holland, there is a Haarlem Meer

198
00:16:04.968 --> 00:16:08.972 
we can off haarlem, we can reuse
seven and add seven and then

199
00:16:08.972 --> 00:16:11.975 
we have
haarlemmermeer, this

200
00:16:11.975 --> 00:16:17.981 
is how the coding works and
it is nearly factor of five

201
00:16:17.981 --> 00:16:23.987 
to compress the dictionary. The
calculation is not bad, the computer is

202
00:16:23.987 --> 00:16:27.991 
so fast in calculations, scanning
through main memory is the

203
00:16:27.991 --> 00:16:32.996 
computing cost so if
we reduce the scanning,

204
00:16:33.997 --> 00:16:35.999 
this is from one million
cities here, if we

205
00:16:35.999 --> 00:16:39.100 
reduce the scanning in the
dictionary in the character strings

206
00:16:39.100 --> 00:16:44.100 
from 46 megabytes
down to 5.4 megabytes

207
00:16:44.100 --> 00:16:48.101 
it is a huge, basically
this is a factor

208
00:16:48.101 --> 00:16:50.101 
ten in performance.

209
00:16:53.101 --> 00:16:56.102 
This is the melody
again and again.

210
00:16:57.102 --> 00:17:01.102 
The only thing we have to
consider is can we calculate

211
00:17:01.102 --> 00:17:05.102 
the direct access in the, when
we do the column compression,

212
00:17:05.102 --> 00:17:08.103 
can we calculate

213
00:17:08.103 --> 00:17:14.103 
directly the position or do
we have to find the position

214
00:17:14.103 --> 00:17:17.104 
and how expensive
is this find loop.

215
00:17:22.104 --> 00:17:30.105 
Using, carefully, these compressions,
the compression rate is,

216
00:17:30.105 --> 00:17:34.105 
as I said, typically
between five and ten

217
00:17:35.105 --> 00:17:38.106 
other in-memory databases have
round about the same numbers.

218
00:17:39.106 --> 00:17:44.106 
The question when you hear
that somebody is having a

219
00:17:44.106 --> 00:17:48.107 
much higher compression rate, the
question is whether this database

220
00:17:48.107 --> 00:17:52.107 
is still usable for
transactional processing.

221
00:17:56.108 --> 00:18:00.108 
So that was checked, most
compression techniques

222
00:18:00.108 --> 00:18:04.108 
require sorted sets but a
table can only be sorted by one

223
00:18:05.108 --> 00:18:09.109 
so only one column we
can really use the sort

224
00:18:10.109 --> 00:18:15.109 
no direct access to rows in
some cases but offset has

225
00:18:15.109 --> 00:18:19.110 
to be computed, I said
this and this has to be

226
00:18:19.110 --> 00:18:21.310 
kept in mind.
