WEBVTT

1
00:00:00.000 --> 00:00:03.190 
Welcome. Let's talk a
bit about consistency.

2
00:00:04.370 --> 00:00:07.950 
So we have already seen some
examples for learning processes.

3
00:00:08.170 --> 00:00:10.580 
I want to show you one
again to remind you.

4
00:00:11.260 --> 00:00:14.570 
So after one image may be the
half space the learner guesses

5
00:00:14.820 --> 00:00:20.310 
is a vertical one and after two
images the learner changes its mind

6
00:00:20.480 --> 00:00:25.040 
and guesses this half-space
which is consistent

7
00:00:25.040 --> 00:00:29.470 
with the two images it sees and
after three images it stays

8
00:00:29.480 --> 00:00:32.960 
it sticks to this hypothesis
because its still consistent

9
00:00:33.120 --> 00:00:36.500 
and after four images it changes
its mind again because it

10
00:00:36.500 --> 00:00:40.480 
realised it was not consistent
and after five images it again

11
00:00:40.480 --> 00:00:43.180 
changes its mind because
it was not consistent.

12
00:00:43.970 --> 00:00:47.300 
Ok so this learning strategy
which for example could be the

13
00:00:47.300 --> 00:00:49.840 
learning by
enumeration strategy

14
00:00:50.840 --> 00:00:55.810 
yeah has the property that after
the examples the learner has seen

15
00:00:55.970 --> 00:00:59.580 
it has always output a prediction model
that is consistent with the data.

16
00:01:00.280 --> 00:01:04.110 
But what happens for example if
we have a delayed learner which

17
00:01:04.110 --> 00:01:07.130 
will only consider the
first half of the data it

18
00:01:07.130 --> 00:01:13.310 
has seen or it just repeats, yeah, every
hypothesis twice. So what happens?

19
00:01:13.520 --> 00:01:18.080 
Well it will still output this
after it has seen two images.

20
00:01:18.290 --> 00:01:22.900 
When it sees the third image it
will change to this hypothesis

21
00:01:23.320 --> 00:01:27.020 
and after four images it will
stick to this hypothesis and

22
00:01:27.020 --> 00:01:30.490 
after five images it would also
stick to this hypothesis because

23
00:01:30.490 --> 00:01:33.380 
um well it converges
more slowly.

24
00:01:34.360 --> 00:01:39.570 
Ok so still it learns the half space
right? It just needs more time.

25
00:01:39.820 --> 00:01:44.160 
But what the learner does it is not
consistent as you can obviously see

26
00:01:44.510 --> 00:01:48.080 
some images are not correctly
classified by the half-space.

27
00:01:49.810 --> 00:01:55.770 
So a learner is consistent if for
every training sequence it sees

28
00:01:55.950 --> 00:02:00.110 
the prediction model it outputs is
consistent with the training sequence.

29
00:02:00.450 --> 00:02:05.960 
The first line learner is consistent,
the second line one is not.

30
00:02:08.490 --> 00:02:13.210 
So let me show you one more
example for a hypothesis space.

31
00:02:13.350 --> 00:02:16.060 
We fix enumeration
of the

32
00:02:16.710 --> 00:02:21.120 
even finite sets. So the finite sets
that contain only even numbers.

33
00:02:21.340 --> 00:02:25.540 
We can do that just in a similar way
as we did for the finite sets.

34
00:02:27.470 --> 00:02:29.980 
We identify subsets

35
00:02:31.190 --> 00:02:34.500 
with their characteristic function
as we already talked about

36
00:02:34.690 --> 00:02:38.700 
and now we define this
funny collection of sets.

37
00:02:38.880 --> 00:02:44.290 
So we call it double even finite
odd singleton collection

38
00:02:44.670 --> 00:02:49.330 
because we collect for every i
and j in the natural numbers

39
00:02:49.410 --> 00:02:55.750 
we collect e i and e j. So two finite
sets of even numbers and we also

40
00:02:56.080 --> 00:03:00.120 
add the odd numbers that correspond
to i and j to the set.

41
00:03:00.970 --> 00:03:05.430 
So this may seem a bit
arbitrary but we'll see

42
00:03:05.430 --> 00:03:08.830 
that this is a quite useful
collection of sets very soon.

43
00:03:10.370 --> 00:03:13.970 
Ok so first of all this is
the hypothesis space. You

44
00:03:13.970 --> 00:03:16.970 
can read the formal proof in the
handout, I will go through

45
00:03:16.970 --> 00:03:21.330 
it quickly now and stick more
to explaining the idea.

46
00:03:21.710 --> 00:03:26.860 
So the idea is that if we receive
an input k we can assume

47
00:03:26.860 --> 00:03:29.500 
that it stands for i and
j by unmerging it.

48
00:03:30.710 --> 00:03:35.260 
Then if the second input so the
input of the decision procedure

49
00:03:35.530 --> 00:03:39.470 
is one of the two odd numbers
that correspond to i and j then

50
00:03:39.470 --> 00:03:42.300 
of course we know ok n
is like in the set

51
00:03:43.430 --> 00:03:49.400 
and otherwise we just run the
decision procedures for this

52
00:03:49.660 --> 00:03:54.810 
hypothesis space of even finite
sets and then if says n is in

53
00:03:54.810 --> 00:04:00.970 
is in e i then we output yes or if
we get the answer n is in e j

54
00:04:01.350 --> 00:04:05.840 
then we also output yes and this is
already the decision procedure

55
00:04:05.990 --> 00:04:10.960 
for this double even finite odd
singleton hypothesis space.

56
00:04:11.210 --> 00:04:14.210 
Ok so why do we need this
funny collection of sets?

57
00:04:14.940 --> 00:04:21.180 
Well we want to prove
something like this.

58
00:04:23.740 --> 00:04:25.790 
Ok so what does
it mean?

59
00:04:26.750 --> 00:04:30.220 
We remember the abbreviations and
we will now use the abbreviation

60
00:04:30.220 --> 00:04:32.180 
cons for consistent
learners.

61
00:04:32.830 --> 00:04:38.570 
So we argued already but maybe you
haven't noticed so I repeat it now

62
00:04:38.900 --> 00:04:44.800 
that the hypothesis space spaces
are learnable by enumeration

63
00:04:44.820 --> 00:04:49.510 
and this enumeration strategy
indeed is consistent because

64
00:04:49.520 --> 00:04:54.450 
we always use the smallest
consistent hypothesis

65
00:04:54.620 --> 00:04:58.250 
with the data we have seen. So
this is nice. We have already

66
00:04:58.250 --> 00:05:01.800 
found a consistent learner. So
the question whether everything

67
00:05:01.800 --> 00:05:07.360 
that can be learned can be learned
consistently, can be answered with yes

68
00:05:07.590 --> 00:05:10.510 
in this setting of
hypothesis spaces.

69
00:05:12.660 --> 00:05:13.330 
Okay.

70
00:05:15.640 --> 00:05:20.230 
Now this is again the learning
by enumeration strategy

71
00:05:20.570 --> 00:05:24.560 
and I hope you are convinced
that it is always consistent.

72
00:05:26.960 --> 00:05:29.890 
So what we will prove
in the rest of

73
00:05:30.430 --> 00:05:36.220 
this video is that the
hypothesis space that contains

74
00:05:36.230 --> 00:05:42.470 
all even natural numbers and the double even
finite odd singleton collection of sets

75
00:05:42.770 --> 00:05:45.310 
that this can be learned
by an iterative learner

76
00:05:46.230 --> 00:05:50.810 
but it cannot be learned consistently
by an iterative learner.

77
00:05:53.310 --> 00:05:58.810 
How do we do it? Well first we look
at the positive part of the result

78
00:05:59.030 --> 00:06:03.410 
namely there is an iterative
learner for this hypothesis space

79
00:06:03.630 --> 00:06:07.690 
containing all even
numbers and the double

80
00:06:07.690 --> 00:06:10.320 
even finite odd singleton
collection of sets.

81
00:06:12.590 --> 00:06:16.350 
So the idea is
quite simple

82
00:06:17.070 --> 00:06:22.560 
namely we guess that it's this whole
set, this large set of all even

83
00:06:22.730 --> 00:06:27.670 
natural numbers until I know
that it's not, so until

84
00:06:27.680 --> 00:06:30.500 
I see there is an odd
number in this set.

85
00:06:30.980 --> 00:06:36.280 
Then I, well, then I adjust the
prediction model and just guess the

86
00:06:36.650 --> 00:06:42.910 
first element of double even finite odd
singleton collection that makes sense

87
00:06:43.200 --> 00:06:45.100 
with respect to
the odd number.

88
00:06:47.290 --> 00:06:53.010 
Ok so this is the algorithm.
K is now our variable for

89
00:06:53.160 --> 00:06:57.880 
the hypothesis. So first we assume that this
is the whole set of even natural numbers

90
00:06:58.370 --> 00:07:00.900 
and then if the
input is an odd

91
00:07:01.790 --> 00:07:05.490 
natural number that is positively
labeled, so which is

92
00:07:05.490 --> 00:07:07.330 
in the set then we
change our minds.

93
00:07:08.270 --> 00:07:12.950 
If this is the first odd number
we see so if before we

94
00:07:12.950 --> 00:07:16.590 
thought it's all the even
natural numbers then we will

95
00:07:16.600 --> 00:07:21.510 
assume that now it's an hypothesis
for e j and this odd natural

96
00:07:21.510 --> 00:07:22.740 
number that we
have seen.

97
00:07:24.820 --> 00:07:31.020 
Just a quick note on the side,
I mean with i and j coincide

98
00:07:31.160 --> 00:07:35.480 
then we get exactly this set. Like
if i equals j then we have e j

99
00:07:35.640 --> 00:07:40.180 
union 2j plus one,
also as a part of

100
00:07:40.620 --> 00:07:42.300 
this collection
of sets.

101
00:07:43.430 --> 00:07:47.710 
Ok so if we had already seen an
odd number before we have seen

102
00:07:47.710 --> 00:07:54.110 
2j plus one then our
hypothesis is four sum Ei

103
00:07:54.400 --> 00:07:58.360 
union 2i plus one where two 2i
plus one is this odd natural

104
00:07:58.360 --> 00:08:01.890 
number that we have seen positively
labelled before. And then

105
00:08:02.000 --> 00:08:06.870 
we will update our hypothesis
to be Ei and Ej and 2i

106
00:08:06.870 --> 00:08:11.890 
plus one, 2j plus one
which has to be correct

107
00:08:12.460 --> 00:08:14.780 
if you think about
it for a minute.

108
00:08:15.840 --> 00:08:19.110 
Ok so this is the algorithm for
the positive result if you

109
00:08:19.110 --> 00:08:23.470 
want to see approve but maybe
first think about it yourself,

110
00:08:23.530 --> 00:08:26.390 
you can find it in the handout that
this is the correct learner.

111
00:08:28.880 --> 00:08:32.880 
So for the rest of the video
I want to talk about why

112
00:08:32.880 --> 00:08:36.830 
isn't there a consistent iterative
learner for this collection of sets?

113
00:08:38.530 --> 00:08:40.920 
We will again use the
locking sequence argument

114
00:08:41.440 --> 00:08:46.060 
and assume that it's yeah that
what we state is wrong.

115
00:08:46.170 --> 00:08:50.410 
So we assume that there is an iterative
learner learning this collection.

116
00:08:51.750 --> 00:08:57.090 
Ok then there is a locking
sequence for the whole set

117
00:08:57.090 --> 00:08:58.420 
of even natural numbers

118
00:09:00.390 --> 00:09:04.150 
but if we assume
now that

119
00:09:04.780 --> 00:09:09.000 
yeah that there is or, I mean we
know that there is some i that

120
00:09:09.010 --> 00:09:14.090 
such that the Ei union this
odd number given by i

121
00:09:14.310 --> 00:09:18.290 
is consistent with this 𝜏0
and we just fix this i.

122
00:09:19.260 --> 00:09:22.980 
So we have some
i such that

123
00:09:24.410 --> 00:09:27.750 
such that Ei
only contains

124
00:09:29.130 --> 00:09:33.100 
natural numbers that are probably
contains all natural numbers

125
00:09:33.230 --> 00:09:35.130 
positively labeled
by 𝜏0.

126
00:09:37.770 --> 00:09:41.450 
Ok so then again we can, I mean
it is basically very similar

127
00:09:41.450 --> 00:09:45.080 
to the argument we have seen
in the last video. So we

128
00:09:45.080 --> 00:09:49.180 
make this sequence longer and
have a locking sequence for

129
00:09:49.180 --> 00:09:50.370 
the set that we fixed.

130
00:09:51.580 --> 00:09:56.990 
And then again we pick a j
such that this is again

131
00:09:57.320 --> 00:10:01.010 
yes at the set that corresponds
to i and j in our collection

132
00:10:01.200 --> 00:10:05.010 
is consistent with this locking
sequence that we get when we add

133
00:10:05.310 --> 00:10:11.070 
𝜏i to 𝜏0 and and we
will also get that

134
00:10:11.080 --> 00:10:15.640 
we have some even number, I mean
this is a requirement we put on j

135
00:10:15.870 --> 00:10:21.010 
that there is an even number that
is in Ej that has not been in Ei.

136
00:10:21.170 --> 00:10:25.170 
So we require Ej to be to
contain a new element

137
00:10:26.230 --> 00:10:28.100 
that is not labelled
by 𝜏0 𝜏i.

138
00:10:28.960 --> 00:10:33.180 
Ok but if we have all these
ingredients so if we pick 𝜏0

139
00:10:33.400 --> 00:10:38.990 
i, 𝜏i and j like this, then
then the iterative learner

140
00:10:39.160 --> 00:10:43.880 
will have to output an
hypothesis for the first set

141
00:10:43.880 --> 00:10:49.180 
we fix Ei with 2i+1 after
having observed the sequence

142
00:10:49.330 --> 00:10:53.670 
where we just magically inserted
the number 2l positively

143
00:10:53.670 --> 00:10:56.780 
labeled after 𝜏0
because this will not

144
00:10:57.240 --> 00:11:00.900 
like change the mind of the
learner because after 𝜏0

145
00:11:01.050 --> 00:11:04.160 
it guesses the whole
even natural numbers

146
00:11:04.670 --> 00:11:08.860 
and this is a positive labelled even
natural number so the learner

147
00:11:08.860 --> 00:11:14.710 
will not change its mind. So it's just
the same hypothesis as after 𝜏0

148
00:11:14.980 --> 00:11:19.960 
and yeah and then if you add
𝜏i it will just output

149
00:11:19.960 --> 00:11:23.470 
the same as it did
after 𝜏0 𝜏i

150
00:11:25.140 --> 00:11:30.670 
and um yeah but this means
that m is not consistent. So

151
00:11:30.700 --> 00:11:32.190 
why is it not
consistent?

152
00:11:32.930 --> 00:11:41.080 
Well after 𝜏0 𝜏i the learner
output the hypothesis for Ei

153
00:11:41.360 --> 00:11:45.390 
union 2i+1 but 2l is not
inside of this set.

154
00:11:47.700 --> 00:11:52.030 
It was positively labeled in the sequence
but the learner doesn't consider it

155
00:11:52.190 --> 00:11:54.830 
in its output and so the
learner was not consistent.

156
00:12:00.740 --> 00:12:04.440 
We proved every hypothesis space is
learnable by a consistent learner.

157
00:12:05.480 --> 00:12:09.310 
We proved also that consistent
mini batch or iterative learners

158
00:12:09.530 --> 00:12:12.970 
have less learning power than
the iterative learners which

159
00:12:12.970 --> 00:12:16.650 
already had less learning power than
the full information learners.

160
00:12:17.950 --> 00:12:19.790 
And we did that, well,

161
00:12:20.940 --> 00:12:26.030 
so I mean, we used locking sequence
argument but what we also

162
00:12:26.030 --> 00:12:30.870 
did is we kind of can or what
can we gain from that?

163
00:12:30.880 --> 00:12:36.750 
We can gain from that that maybe
sometimes we can learn more when we

164
00:12:37.040 --> 00:12:41.280 
when we are inconsistent in
between the learning process. So

165
00:12:41.280 --> 00:12:46.170 
this is something one has to ask keep in
mind. So maybe a small inconsistency

166
00:12:46.400 --> 00:12:48.690 
may give more
learning power.

167
00:12:50.520 --> 00:12:55.100 
Ok so what can be done is
one can consider a lot

168
00:12:55.100 --> 00:12:58.260 
more additional requirements or
consistency is one example

169
00:12:58.260 --> 00:13:02.030 
for them, but there are a lot
more. Some of them are also

170
00:13:02.030 --> 00:13:05.530 
mentioned in some of the excursions
and you can investigate

171
00:13:05.650 --> 00:13:07.720 
how this affects the
learning power.
