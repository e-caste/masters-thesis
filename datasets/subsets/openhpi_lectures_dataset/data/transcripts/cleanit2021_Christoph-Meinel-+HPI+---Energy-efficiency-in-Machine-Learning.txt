WEBVTT

1
00:00:00.840 --> 00:00:04.860 
This is a clean AI chapter
in our cleanIT forum.

2
00:00:05.680 --> 00:00:12.310 
And the focus is how we can make
artificial intelligence application more

3
00:00:12.460 --> 00:00:13.500 
energy efficient.

4
00:00:14.930 --> 00:00:18.660 
And of course it starts with
the question, why we need AI,

5
00:00:19.150 --> 00:00:22.050 
why we need machine learning,
why we need that technology?

6
00:00:22.610 --> 00:00:27.190 
And there's a simple answer, we need it
to be able to manage the data flood.

7
00:00:28.130 --> 00:00:29.990 
And the data flood is

8
00:00:30.640 --> 00:00:35.630 
rising and rising. Here are some
figures from the year 2020.

9
00:00:36.120 --> 00:00:42.250 
Every minute 400 hours of video
are uploaded

10
00:00:42.840 --> 00:00:49.790 
to Youtube. 325,000
new malware files were

11
00:00:50.150 --> 00:00:52.320 
designed and
distributed every day,

12
00:00:53.210 --> 00:00:58.640 
or 35 billion IoT(internet of things)
devices

13
00:00:59.080 --> 00:01:03.930 
collect data. These are sensors that
collect data that need to be

14
00:01:04.160 --> 00:01:05.710 
computed and connected.

15
00:01:06.340 --> 00:01:11.210 
Or 2.3 billion people
use smartphones and collect

16
00:01:11.210 --> 00:01:17.700 
data with their smartphones. Smartphones have
more than 10 senders which collect

17
00:01:18.030 --> 00:01:21.460 
information, collect
data every second.

18
00:01:21.920 --> 00:01:23.580 
Or if we look to Amazon, it

19
00:01:24.860 --> 00:01:27.910 
offers more than 550
million products.

20
00:01:28.410 --> 00:01:30.120 
Or if we look to the

21
00:01:30.990 --> 00:01:37.070 
healthcare system, a medical
scan of a single organ creates

22
00:01:37.180 --> 00:01:41.070 
10 gigabytes of raw
data each second.

23
00:01:43.230 --> 00:01:45.120 
So the question is, what
is machine learning?

24
00:01:45.680 --> 00:01:50.750 
The idea is that the
machines learn from the data

25
00:01:51.360 --> 00:01:56.650 
without programming. So not following
a program they are able to

26
00:01:57.510 --> 00:02:03.530 
understand and analyze
huge data sets.

27
00:02:04.210 --> 00:02:07.540 
Computers become in
this way able to see

28
00:02:08.310 --> 00:02:12.250 
or to read, or to listen,
or to understand,

29
00:02:12.700 --> 00:02:16.270 
and interact with this data with
other machines.

30
00:02:18.310 --> 00:02:24.830 
Machine learning is so successful because
we have really big data meanwhile,

31
00:02:25.390 --> 00:02:29.240 
by all these cloud applications,

32
00:02:30.310 --> 00:02:34.330 
the increasing internet of things by
social media, there are huge

33
00:02:34.330 --> 00:02:38.630 
amount of data sets available which
can be used to train machines.

34
00:02:39.560 --> 00:02:44.830 
The hardware is improved over
the year in such a way that

35
00:02:44.860 --> 00:02:49.810 
with new components, multi-core components
with new processors CPUs, etc

36
00:02:50.740 --> 00:02:55.670 
that the hardware is able
in a very short time

37
00:02:55.920 --> 00:03:01.270 
inspect a huge amount of
data. We have new storage

38
00:03:01.900 --> 00:03:07.030 
and data management ideas, for
example the in-memory data management,

39
00:03:07.230 --> 00:03:11.740 
for real time big data analytics.
And we have cloud computing,

40
00:03:12.060 --> 00:03:16.860 
which provides
unlimited access to processing power

41
00:03:16.980 --> 00:03:23.580 
anytime and anywhere. And we have deep
learning algorithms that really

42
00:03:24.070 --> 00:03:27.680 
are able to learn
from the data.

43
00:03:29.080 --> 00:03:32.580 
If the people speak
about AI,

44
00:03:33.070 --> 00:03:35.040 
then there are

45
00:03:36.020 --> 00:03:41.040 
different techniques which are
subsumed under this title.

46
00:03:41.650 --> 00:03:49.670 
So we focus and this is a success today
on such deep learning architectures.

47
00:03:50.560 --> 00:03:53.900 
These are architectures,
convolutional neural networks,

48
00:03:54.540 --> 00:03:57.810 
recurrent neural
networks, that are able

49
00:03:58.240 --> 00:04:02.660 
to learn out of data which
are presented to them.

50
00:04:03.410 --> 00:04:05.190 
But AI is also

51
00:04:06.820 --> 00:04:10.870 
connected to ideas in the
representation learning,

52
00:04:11.380 --> 00:04:16.310 
in machine learning, and control
these are different fields. So the

53
00:04:16.640 --> 00:04:20.060 
heirarchical learning features
from large-scale data,

54
00:04:20.520 --> 00:04:25.960 
this is the success story of
the last years which brought

55
00:04:26.940 --> 00:04:32.310 
the AI application to a level
that's comparable in some areas

56
00:04:32.320 --> 00:04:33.890 
to what humans can do.

57
00:04:34.570 --> 00:04:41.610 
So machine learning is data driven,
and deep learning in progress is

58
00:04:42.440 --> 00:04:44.640 
the method of choice.

59
00:04:45.470 --> 00:04:48.520 
If you have a closer look to this
deep learning, to the current

60
00:04:48.520 --> 00:04:52.580 
state of deep learning, then
within neural networks

61
00:04:53.080 --> 00:04:59.770 
we are able to classify images, we are
able to classify images extremely well.

62
00:05:00.380 --> 00:05:02.590 
It is shown

63
00:05:03.270 --> 00:05:06.430 
that machines can do this
better than humans.

64
00:05:07.250 --> 00:05:10.390 
Neural networks can
beat the strongest

65
00:05:10.860 --> 00:05:16.420 
human players for example,
in games like Chess or like Go.

66
00:05:17.870 --> 00:05:21.490 
Networks can generate
realistic looking images

67
00:05:21.910 --> 00:05:23.690 
of non existing people,

68
00:05:24.510 --> 00:05:28.870 
and are able to
serve a variety of other tasks.

69
00:05:29.450 --> 00:05:33.270 
So this is a very active
actual research work.

70
00:05:33.940 --> 00:05:40.420 
And this work is inspired by
looking how the nature, how

71
00:05:40.420 --> 00:05:42.850 
we as humans are
able to learn.

72
00:05:43.610 --> 00:05:46.610 
And if we have a
look to the brain,

73
00:05:47.260 --> 00:05:50.780 
then we see and this is
very similar

74
00:05:51.240 --> 00:05:57.470 
how deep learning is working, that when
we see and realize for example a face,

75
00:05:57.890 --> 00:06:04.220 
we start in the different areas in
the brain to first recognize edges,

76
00:06:04.630 --> 00:06:10.140 
and such simple structures. Then
in another

77
00:06:10.430 --> 00:06:15.140 
area of the brain, we are
able to put simple

78
00:06:15.970 --> 00:06:21.160 
geometric things together to
recognize eyes or nose or mouth,

79
00:06:21.460 --> 00:06:26.680 
and finally to face. So if we
look here, this is the area as

80
00:06:26.680 --> 00:06:31.490 
in the human brain where such
simple structures are recognized.

81
00:06:31.700 --> 00:06:35.280 
Here the simple structures
are combined to

82
00:06:35.750 --> 00:06:40.870 
pieces, we can recognize. And here
finally we see the full face.

83
00:06:41.240 --> 00:06:45.810 
So with the deep learning
they follow this approach,

84
00:06:46.030 --> 00:06:48.430 
how we notice
from the brain.

85
00:06:49.630 --> 00:06:54.460 
Also in the way how it's computed,
it follows in the way, here

86
00:06:54.770 --> 00:06:55.740 
I want to show

87
00:06:56.960 --> 00:07:03.550 
the learning more and more
abstractions. Because if a computer

88
00:07:03.550 --> 00:07:07.320 
inspect such an image, it can only
see the pixels and then the

89
00:07:07.320 --> 00:07:11.080 
pixels are connected to
such low level features.

90
00:07:11.520 --> 00:07:15.760 
Then in the next step, they
combined these low level features

91
00:07:15.760 --> 00:07:18.730 
to mid level features to
high level features,

92
00:07:19.330 --> 00:07:24.750 
to train a classifier
so that at the end

93
00:07:24.980 --> 00:07:27.910 
the machines
are able to

94
00:07:28.790 --> 00:07:35.380 
recognize images.
For example, to see oh, that is a

95
00:07:35.380 --> 00:07:37.990 
child crossing the
street or a cat.

96
00:07:38.650 --> 00:07:44.540 
And the structure by
which we are used to this are such

97
00:07:44.690 --> 00:07:51.150 
perceptrons.These are basic
artificial neural networks, neural

98
00:07:51.150 --> 00:07:54.720 
networks so it's similar
working like our brain.

99
00:07:55.150 --> 00:08:01.070 
There are computing nodes which are able
to compute very simple operations,

100
00:08:01.360 --> 00:08:05.740 
it's a convolution operation,
it's described here. And

101
00:08:05.740 --> 00:08:08.290 
then there are inputs
from the neighbor

102
00:08:09.620 --> 00:08:14.710 
perceptrons and then a certain
activation function is started

103
00:08:15.040 --> 00:08:20.570 
and an output is created that the input
in the next step for the next layer

104
00:08:20.840 --> 00:08:24.620 
of such a network.

105
00:08:25.260 --> 00:08:31.880 
So the idea is that the weights
here which in this formula are used

106
00:08:32.210 --> 00:08:38.480 
to combine, to multiply with a
input information, that this

107
00:08:38.480 --> 00:08:42.260 
weights need to be brought
on the right level.

108
00:08:42.950 --> 00:08:49.650 
If the signal is to be increased, or the
signal to be decreased, this is learning.

109
00:08:49.810 --> 00:08:53.480 
So at the end, when all these
weights

110
00:08:53.940 --> 00:08:58.320 
are on the right way and
in such a network we have

111
00:08:58.550 --> 00:09:01.750 
10,000 of such perceptrons with

112
00:09:01.750 --> 00:09:03.830 
all these weights are
on the right level,

113
00:09:04.280 --> 00:09:10.100 
then this system, the architecture is
able for example to classify images.

114
00:09:10.950 --> 00:09:15.750 
Let's have a look here, we have
different levels of such

115
00:09:15.750 --> 00:09:19.410 
a neural network,

116
00:09:20.780 --> 00:09:26.310 
such neurons in a neural network.
We have here in the raw pixel

117
00:09:26.580 --> 00:09:30.640 
which have input and the question that
is asked what is on the picture?

118
00:09:30.960 --> 00:09:32.880 
Is it a dog or is it a cat?

119
00:09:33.550 --> 00:09:36.590 
And this is what
is done in

120
00:09:38.090 --> 00:09:43.630 
a network is trained, there are
different databases which are

121
00:09:43.630 --> 00:09:47.830 
available in the internet with tens of thousands
of different images

122
00:09:48.190 --> 00:09:54.160 
and then the system has to learn
at hand on this databases

123
00:09:54.920 --> 00:10:02.340 
to understand the images. And then
the image in form of the pixels

124
00:10:02.460 --> 00:10:06.780 
is sent through the network and if the
network is trained or the network

125
00:10:07.260 --> 00:10:14.100 
is training phase, it is told to
the network that this is a

126
00:10:14.650 --> 00:10:20.340 
dog or this a cat. By the way when
the system decides this is a dog,

127
00:10:20.640 --> 00:10:24.720 
then it gets the answer it's wrong
so it needs to be reorganized.

128
00:10:25.020 --> 00:10:29.940 
If it gets the answer
that's a cat, then the next image is

129
00:10:30.590 --> 00:10:36.310 
used for train. And in every
step here the weights of

130
00:10:37.140 --> 00:10:42.450 
connecting lines on which the
data that are

131
00:10:42.450 --> 00:10:47.930 
produced in the different neurons are
transmitted to the neighbors are adapted.

132
00:10:49.480 --> 00:10:51.660 
The mathematical
operation

133
00:10:53.190 --> 00:10:56.070 
in the neurons, these
are convolution.

134
00:10:56.510 --> 00:11:01.390 
So these architectures are called
convolutional neural networks.

135
00:11:01.850 --> 00:11:05.560 
So they share weights of the whole image,

136
00:11:06.110 --> 00:11:11.560 
and similar to a sliding window,
more efficient, then connected

137
00:11:11.590 --> 00:11:17.780 
to all and all the inputs. So this network
for image often use this convolution

138
00:11:18.180 --> 00:11:22.040 
operation and for that reason it's
called convolution networks.

139
00:11:22.630 --> 00:11:27.650 
And here we see that the networks
become more and more accurate.

140
00:11:28.810 --> 00:11:32.770 
In the moment they have a
larger size, so they can

141
00:11:33.510 --> 00:11:41.140 
be trained more specifically. So the reason
CNN's have hundreds of different layers,

142
00:11:41.590 --> 00:11:48.030 
and we see with the number of increasing
layers the accuracy

143
00:11:48.670 --> 00:11:51.430 
of the results is increasing.
And the ability

144
00:11:52.080 --> 00:11:55.380 
to be trained and to
give right answers.

145
00:11:56.070 --> 00:12:00.380 
So a very reasoned
and very powerful

146
00:12:01.390 --> 00:12:06.860 
network convolutional network architecture
is ResNet-152

147
00:12:07.320 --> 00:12:14.880 
which surpasses human performance on image
classification. But this network needs

148
00:12:14.980 --> 00:12:20.630 
240MBs for storage
and 11.3 billion

149
00:12:20.850 --> 00:12:22.850 
floating point
operations.

150
00:12:23.880 --> 00:12:30.570 
And therefore such a network needs to run
on very powerful servers, it's available, the

151
00:12:30.840 --> 00:12:36.540 
computational power in the cloud. And
this huge number of operations

152
00:12:37.050 --> 00:12:41.280 
makes clear that
such a deep learning model

153
00:12:41.750 --> 00:12:45.430 
needs a lot of energy in
the phase to be trained.

154
00:12:45.940 --> 00:12:52.330 
To compare this to other activities, here
is an interesting study

155
00:12:52.680 --> 00:12:56.590 
chart from MIT
technology review,

156
00:12:57.160 --> 00:13:04.030 
which compares different activities, here
the largest one is the transformer

157
00:13:04.610 --> 00:13:10.690 
with a neural architecture search, and
this first that is the amount of

158
00:13:11.370 --> 00:13:17.190 
carbon footprint of a round-trip flight
from New York to San Francisco.

159
00:13:17.910 --> 00:13:21.930 
This is in comparison
about 10 times more

160
00:13:23.430 --> 00:13:27.340 
footprint of a human
life over one year.

161
00:13:28.030 --> 00:13:32.380 
Here the next one is the American
lives average over one year,

162
00:13:32.690 --> 00:13:36.790 
you know there is a lot of
energy needed and used in US.

163
00:13:37.310 --> 00:13:42.530 
Then the US car including fuel

164
00:13:43.440 --> 00:13:46.240 
over all its lifetime
needs that

165
00:13:48.500 --> 00:13:52.420 
amount or produce
that amount of carbon

166
00:13:53.650 --> 00:13:56.690 
emissions. And
this is AI

167
00:13:58.140 --> 00:14:02.840 
And this is where we have to think
about and where we have to come

168
00:14:03.180 --> 00:14:08.370 
up with new ideas and better
approaches. And to start with this

169
00:14:08.370 --> 00:14:15.980 
discussion one idea would be to make
this computation more energy saving.

170
00:14:16.480 --> 00:14:20.320 
And an extreme idea
is to replace this

171
00:14:20.960 --> 00:14:24.010 
convolution operation on
a 32-bit basis

172
00:14:24.500 --> 00:14:30.080 
by an operation on 1-bit basis. And
these are the so-called

173
00:14:30.220 --> 00:14:31.930 
binary neural networks.

174
00:14:32.970 --> 00:14:37.370 
The state of the art in the neural
networks they are trained

175
00:14:37.370 --> 00:14:42.520 
and operated with 32-bit models
and the question is can we design

176
00:14:42.920 --> 00:14:47.030 
similar powerful neural
networks on binary level?

177
00:14:47.610 --> 00:14:52.740 
And ofcourse, if this would be possible
and we will show that it is possible,

178
00:14:53.010 --> 00:14:57.970 
then we can get a huge
amount of energy saving.

179
00:14:59.410 --> 00:15:03.650 
So reduced energy requirements
save ofcourse the

180
00:15:04.090 --> 00:15:09.110 
environment and by the way also
produce other positive effects.

181
00:15:09.590 --> 00:15:13.920 
So to save the environment
energy of deep learning models

182
00:15:14.170 --> 00:15:21.850 
are needed. And this energy reduction can be
reached by a lower number of operations

183
00:15:22.150 --> 00:15:28.190 
while raising the energy efficiency
and by lower memory requirements.

184
00:15:28.670 --> 00:15:36.790 
And if one can reach this, then there is
a potential that such AI application

185
00:15:37.220 --> 00:15:41.780 
can be run on such
small devices.

186
00:15:42.570 --> 00:15:46.310 
So the energy
saving can be

187
00:15:47.380 --> 00:15:52.710 
provided in such a way that the models
do not need huge computing centres

188
00:15:52.990 --> 00:15:58.870 
to be computed, such binary neural
networks they can be run on mobile

189
00:15:59.320 --> 00:16:01.130 
and embedded devices.

190
00:16:02.940 --> 00:16:07.110 
So the approach to achieve more
energy efficiency

191
00:16:07.590 --> 00:16:12.220 
by deep learning models is
knowledge distillation.

192
00:16:12.720 --> 00:16:16.970 
This is a large model teacher
into a smaller model student.

193
00:16:17.900 --> 00:16:21.100 
Another approach is network
pruning techniques,

194
00:16:21.540 --> 00:16:27.880 
to remove non-essential weights. And
finally is a compact network design

195
00:16:28.320 --> 00:16:34.290 
that use layer structure with less weights
and operations. These are all approaches

196
00:16:34.510 --> 00:16:41.520 
one can investigate to find out
whether it's possible to design more

197
00:16:41.790 --> 00:16:46.760 
energy efficient artificial
intelligence architectures.

198
00:16:47.470 --> 00:16:51.870 
So we show later on
low bit quantization

199
00:16:52.610 --> 00:16:57.700 
activities where we quantified
that 32-bit floating point

200
00:16:57.700 --> 00:17:04.580 
weights to a lower bit with
for example with -1,

201
00:17:04.940 --> 00:17:10.210 
one can work with 2-bits
as another approach.

202
00:17:10.860 --> 00:17:16.570 
All bits lower than 32 and
hopefully have the same efficiency

203
00:17:16.880 --> 00:17:18.990 
is good and helps
to save energy.

204
00:17:19.950 --> 00:17:24.810 
Such kind of research work, such
kind of information ideas,

205
00:17:25.280 --> 00:17:30.880 
knowledge is shared in this
clean AI chapter

206
00:17:31.270 --> 00:17:33.610 
and you are invited to

207
00:17:34.900 --> 00:17:41.990 
get informed about that. And you are also
invited to send us your proposals

208
00:17:42.320 --> 00:17:49.830 
to be shown here in this chapter a
with the idea that we make AI more

209
00:17:50.010 --> 00:17:54.820 
energy efficient, more
environmental resistant.
