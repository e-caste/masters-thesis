WEBVTT

1
00:00:00.970 --> 00:00:04.580 
In this video we will talk about software and systems architecture.

2
00:00:05.220 --> 00:00:09.410 
Before it comes to writing actual code designing a good architecture

3
00:00:09.410 --> 00:00:12.730 
for your software is crucial to its overall efficiency.

4
00:00:13.440 --> 00:00:17.170 
The architecture of a software includes among others decisions

5
00:00:17.170 --> 00:00:21.230 
on how to structure your application into models and components,

6
00:00:21.440 --> 00:00:23.120 
and how they relate to each other.

7
00:00:23.840 --> 00:00:28.520 
Its architecture has effects on how well the software can be evolved and maintained

8
00:00:28.920 --> 00:00:33.230 
by an engineering team and very important also effects the

9
00:00:33.230 --> 00:00:35.760 
software's performance and efficiency.

10
00:00:36.650 --> 00:00:40.820 
We will use cloud native and web applications as an example

11
00:00:40.850 --> 00:00:44.890 
which design choices are affecting the resource efficiency of your software,

12
00:00:45.160 --> 00:00:49.150 
because they are quite popular today. But these concepts are

13
00:00:49.150 --> 00:00:52.960 
adaptable for other software applications for example command

14
00:00:52.960 --> 00:00:54.180 
line applications.

15
00:00:56.470 --> 00:01:02.340 
Modern web applications for example handle requests from many different users simultaneously.

16
00:01:02.990 --> 00:01:06.540 
That's why scalability is an important aspect when it comes

17
00:01:06.540 --> 00:01:08.500 
to designing a software architecture.

18
00:01:09.160 --> 00:01:14.680 
To be resource efficient a system should only use as much resources as necessary.

19
00:01:15.120 --> 00:01:21.160 
However a resource demand correlates with among other things user request load,

20
00:01:21.320 --> 00:01:25.790 
which is not easily predictable. Resources for such web applications

21
00:01:25.790 --> 00:01:29.010 
should be scalable at run-time to be resource efficient.

22
00:01:29.690 --> 00:01:32.750 
There are two common strategies to scale a system-

23
00:01:33.190 --> 00:01:35.480 
horizontal scaling and vertical

24
00:01:36.100 --> 00:01:41.290 
scaling. Both refer to the physical machines that your applications are using.

25
00:01:42.250 --> 00:01:46.840 
Their resources typically are CPU power, RAM or storage.

26
00:01:47.610 --> 00:01:51.300 
Vertical staging, which is also called scaling up

27
00:01:51.790 --> 00:01:54.580 
means to add more resources to a single machine.

28
00:01:55.190 --> 00:01:59.130 
While this is hard to do at runtime and points to another question

29
00:01:59.250 --> 00:02:04.920 
that must be thought of up front. Is it possible to let multiple of your applications

30
00:02:05.130 --> 00:02:07.710 
share the same resources and the same machine

31
00:02:08.320 --> 00:02:12.020 
it can be more efficient to use one larger machine then to

32
00:02:12.020 --> 00:02:13.720 
use two smaller ones.

33
00:02:14.620 --> 00:02:17.810 
Horizontal scaling, also called scaling out

34
00:02:18.280 --> 00:02:21.260 
means to add more machines to your pool of resources.

35
00:02:21.730 --> 00:02:26.210 
However this is not easily done because to run on multiple machines,

36
00:02:26.450 --> 00:02:30.620 
your application needs to be broken up into logically separate modules,

37
00:02:30.970 --> 00:02:33.900 
which can then be distributed to various machines.

38
00:02:34.480 --> 00:02:38.730 
If your application does not have a shared volatile state meaning

39
00:02:38.730 --> 00:02:42.250 
that all data can be fetched from a persistence layer component

40
00:02:42.620 --> 00:02:48.480 
it is also possible to run multiple instances of your application on various machines.

41
00:02:49.050 --> 00:02:55.260 
However this makes the deployment of a load balancer or task scheduler component necessary.

42
00:02:56.870 --> 00:03:01.470 
To sum up horizontal scaling adds complexity to your application,

43
00:03:01.850 --> 00:03:05.410 
but it also helps designing a well de-coupled application

44
00:03:06.160 --> 00:03:10.290 
Horizontal scaling has several other advantages for example

45
00:03:10.290 --> 00:03:14.660 
due redundancy and rolling updates to be resource efficient.

46
00:03:14.890 --> 00:03:18.670 
It is most important to choose a scaling strategy that fits

47
00:03:18.670 --> 00:03:20.210 
the needs of your application.

48
00:03:22.490 --> 00:03:28.540 
A good example for horizontally well scalable and resource efficient software architecture

49
00:03:28.760 --> 00:03:30.620 
is a micro service architecture.

50
00:03:31.440 --> 00:03:36.620 
The approach is to develop a single application as a suite of small services,

51
00:03:36.860 --> 00:03:39.980 
which can be developed and deployed independently.

52
00:03:41.050 --> 00:03:44.490 
This encourages a high modularity of your application.

53
00:03:45.190 --> 00:03:49.370 
These micro services communicate over a lightweight mechanism

54
00:03:49.500 --> 00:03:53.340 
such as http resources or a message queue.

55
00:03:54.350 --> 00:03:58.810 
There is no exact definition on how big a micro service needs to be

56
00:03:59.150 --> 00:04:03.500 
however micro services are designed around business capabilities.

57
00:04:04.210 --> 00:04:09.040 
The opposite of a micro service architecture is the monolithic architecture.

58
00:04:09.540 --> 00:04:13.670 
Here the application is developed and deployed as a single unit.

59
00:04:14.410 --> 00:04:19.110 
While this is a logically simple approach it bears some disadvantages

60
00:04:19.300 --> 00:04:21.010 
when it comes to scalability.

61
00:04:21.690 --> 00:04:26.490 
As stated earlier in order to use horizontal scaling an application

62
00:04:26.490 --> 00:04:29.620 
must be split up into logically separate modules.

63
00:04:30.310 --> 00:04:33.390 
In a micro service architecture this already

64
00:04:33.810 --> 00:04:38.630 
is established because the micro services and body business capabilities.

65
00:04:39.220 --> 00:04:42.780 
That's where every micro service can be scaled individually

66
00:04:42.780 --> 00:04:47.260 
depending on its use of resources and the load on that micro service.

67
00:04:47.740 --> 00:04:52.360 
Because micro services are small the overhead of replicating

68
00:04:52.360 --> 00:04:56.430 
them if needed is smaller than with monolithic systems.

69
00:04:58.180 --> 00:05:02.560 
Because micro services use well defined mechanisms to communicate with

70
00:05:03.160 --> 00:05:07.890 
each other, the lot of a particular micro service is easily monitored.

71
00:05:08.480 --> 00:05:12.530 
Then the micro service can be scaled horizontally independently

72
00:05:12.530 --> 00:05:14.500 
from another parts of the application.

73
00:05:15.510 --> 00:05:18.880 
No redeployment of the entire application is necessary.

74
00:05:19.760 --> 00:05:23.190 
In a monolith, the whole application needs to be replicated,

75
00:05:23.210 --> 00:05:28.070 
if lord demands it. This includes a large overhead because then

76
00:05:28.070 --> 00:05:32.560 
the parts of the application which are not under high load are replicated to

77
00:05:33.520 --> 00:05:37.770 
micro services are good example of resource efficient architectures,

78
00:05:38.090 --> 00:05:41.470 
because they allow for scaling only the parts of the application

79
00:05:41.470 --> 00:05:42.770 
that need to be scaled.

80
00:05:45.880 --> 00:05:49.850 
So far we only talked about the server side of applications

81
00:05:50.480 --> 00:05:54.370 
but an also important design choice is how much of the logic

82
00:05:54.660 --> 00:05:59.280 
and therefore energy consumption should be outsourced to the client side.

83
00:06:00.140 --> 00:06:04.330 
In web applications high bandwidth consumption is correlated

84
00:06:04.330 --> 00:06:05.850 
with high energy usage.

85
00:06:06.710 --> 00:06:11.930 
It is important to state that this is also impacted by the network distance

86
00:06:12.250 --> 00:06:15.500 
of client and server, meaning the number of routers

87
00:06:15.910 --> 00:06:18.370 
a piece of data passes for request.

88
00:06:19.680 --> 00:06:23.700 
One approach to decrease this network distance is edge computing.

89
00:06:24.370 --> 00:06:28.210 
In short edge computing is an approach where the processing or

90
00:06:28.210 --> 00:06:32.500 
storage of data is moved nearer to the client, thus creating

91
00:06:32.500 --> 00:06:38.040 
a layer between cloud and client. Often this includes the partitioning of data

92
00:06:38.390 --> 00:06:40.870 
based on its geo spatial properties.

93
00:06:41.650 --> 00:06:47.320 
For example if some content on a video streaming platform is consumed especially often

94
00:06:47.580 --> 00:06:51.610 
in a certain area, it is beneficial to move this data to the

95
00:06:51.920 --> 00:06:54.060 
edge data center in this area.

96
00:06:54.740 --> 00:06:59.040 
On the other hand if data is produced by the clients, it may

97
00:06:59.040 --> 00:07:03.500 
be the case that this data can be pre processed and aggregated at the edge,

98
00:07:03.690 --> 00:07:08.630 
and less data or even just meta data must be sent to the cloud.

99
00:07:09.730 --> 00:07:13.830 
Edge data center can be fine tuned to fit the need of a certain

100
00:07:13.830 --> 00:07:17.090 
area and therefore be more efficient for its direct users.

101
00:07:17.790 --> 00:07:21.960 
For example they can be scaled for the area specific usage time,

102
00:07:22.250 --> 00:07:27.550 
and shut down at night time which is not possible with a globally used data center.

103
00:07:28.250 --> 00:07:32.760 
Furthermore edge data centers are way smaller and may be more

104
00:07:32.920 --> 00:07:35.720 
energy efficient when it comes to cooling.

105
00:07:37.160 --> 00:07:42.070 
Lastly it can be more efficient to compute data not until it reached a client,

106
00:07:42.410 --> 00:07:44.710 
taking load from the service in the cloud.

107
00:07:45.210 --> 00:07:48.910 
Another important design choice and IT architectures is the

108
00:07:48.910 --> 00:07:51.450 
usage of blocking or non blocking I/O.

109
00:07:52.150 --> 00:07:55.810 
We will now look at these concepts regarding resource efficiency,

110
00:07:56.050 --> 00:07:58.180 
using the example of a web server.

111
00:07:59.040 --> 00:08:03.540 
First let's describe the difference between blocking and non blocking I/O.

112
00:08:04.290 --> 00:08:09.750 
Input Output or I/O refers to access to devices such as hard drives

113
00:08:09.950 --> 00:08:12.290 
networks or usb devices.

114
00:08:13.020 --> 00:08:17.320 
This access is not happening in the CPU alone and therefore

115
00:08:17.320 --> 00:08:22.530 
comes with a delay. In blocking I/O, the program flow of an executed

116
00:08:22.530 --> 00:08:27.120 
program is blocked as long as the access to a device is not concluded.

117
00:08:27.990 --> 00:08:32.280 
For the web server example when multiple requests to the web server are

118
00:08:32.580 --> 00:08:37.110 
done using blocking I/O, for each call a new fret is created.

119
00:08:37.840 --> 00:08:43.260 
On the other hand non blocking I/O is an implementation of I/O access

120
00:08:43.380 --> 00:08:49.010 
which does not block the executing fret and the program flow continues immediately.

121
00:08:49.910 --> 00:08:53.640 
The finished I/O access is then handled by a call back function

122
00:08:53.900 --> 00:08:57.410 
or a similar programming language-specific construct.

123
00:08:57.940 --> 00:09:01.280 
Now one might ask how a program calls these

124
00:09:01.990 --> 00:09:05.470 
call back functions and how it knows when to call it.

125
00:09:06.030 --> 00:09:10.770 
Many non-blocking frameworks use an infinite loop that constantly checks

126
00:09:10.960 --> 00:09:14.060 
or pause if data is returned from I/O.

127
00:09:14.660 --> 00:09:19.230 
This is often called an event loop which effectively is a wild true loop.

128
00:09:20.100 --> 00:09:25.170 
At each iteration this loop checks if there is data from the I/O available.

129
00:09:26.290 --> 00:09:30.220 
This construct may seem very inefficient at first sight

130
00:09:30.820 --> 00:09:35.440 
but major operating systems nowadays provide system level APIs

131
00:09:35.590 --> 00:09:41.010 
to optimize these calls leading to approximate computational complexity,

132
00:09:41.210 --> 00:09:45.160 
that only depends on the amount of new data from the I/O, and

133
00:09:45.160 --> 00:09:49.260 
not how many different I/O calls are happening simultaneously.

134
00:09:51.570 --> 00:09:56.990 
In conclusion these optimizations is better scalable than using blocking I/O

135
00:09:57.370 --> 00:10:02.390 
because each fret costs around one megabyte of memory and there

136
00:10:02.390 --> 00:10:06.290 
is computational overhead of switching between frets to

137
00:10:06.850 --> 00:10:11.190 
for web servers which execute very I/O heavy programs this

138
00:10:11.190 --> 00:10:14.980 
is way more efficient than using blocking I/O which needs a

139
00:10:14.980 --> 00:10:15.930 
lot of frets.
