WEBVTT

1
00:00:00.840 --> 00:00:01.600 
Hello.

2
00:00:01.610 --> 00:00:09.560 
In this video we will use some examples to show you how the computation graph works in neural network computation workflow.

3
00:00:12.339 --> 00:00:18.969 
In the first example we have five nodes. a and b are two starting notes and c

4
00:00:18.969 --> 00:00:19.839 
d and e

5
00:00:19.850 --> 00:00:24.250 
are based on the very simple addition and multiplication operations.

6
00:00:25.239 --> 00:00:28.920 
So as you can see, c equals a plus b.

7
00:00:28.929 --> 00:00:33.549 
d equals b plus one and e equals c times d.

8
00:00:34.539 --> 00:00:38.759 
These five notes from a simple directed computational graph.

9
00:00:40.539 --> 00:00:44.920 
Now we assign values to the initial nodes a and b.

10
00:00:44.929 --> 00:00:48.250 
So in this case a equals two and b equals one.

11
00:00:48.740 --> 00:00:58.460 
As you can see by following the bottom up direction, we can easily calculate the corresponding value for the note c, d and e.

12
00:01:00.840 --> 00:01:06.920 
Now we calculate the derivative of the top note with respect to the bottom one.

13
00:01:06.930 --> 00:01:10.340 
For example the derivative of note e

14
00:01:10.349 --> 00:01:12.750 
with respect to c and d.

15
00:01:13.540 --> 00:01:21.599 
Similarly, we can further compute the derivatives for each edge between two connected notes.

16
00:01:21.609 --> 00:01:25.959 
At this point we have made all the preparations for you.

17
00:01:28.840 --> 00:01:37.060 
So I will create the detailed computation progress and show you in the slides.

18
00:01:38.340 --> 00:01:43.099 
Now the question is how to calculate the derivative of note e

19
00:01:43.109 --> 00:01:46.260 
with respect with respect to the note b.

20
00:01:46.840 --> 00:01:48.159 
So very bottom one.

21
00:01:49.439 --> 00:01:56.359 
Let's see. We can use the forward mode differentiation from the note b app to the note e.

22
00:01:56.739 --> 00:02:03.060 
So bottom map this gives us a derivative of every note with respect to b.

23
00:02:03.840 --> 00:02:11.210 
The derivative derivative of d with respect to b equals one and then the derivative of c

24
00:02:11.210 --> 00:02:15.960 
with respect to be have been already calculated in the previous step.

25
00:02:16.439 --> 00:02:20.740 
Similarly the derivatives between e and node c

26
00:02:20.750 --> 00:02:23.460 
And a also have been calculated.

27
00:02:24.340 --> 00:02:29.389 
Finally according to the chain rule we can calculate the derivative of e

28
00:02:29.389 --> 00:02:32.349 
with respect to b, which equals five.

29
00:02:34.900 --> 00:02:38.849 
Now how about how about the derivative of e

30
00:02:38.849 --> 00:02:40.860 
with respect to the note a.

31
00:02:42.639 --> 00:02:45.599 
You may have discovered the pattern.

32
00:02:45.610 --> 00:02:46.449 
Yes.

33
00:02:46.460 --> 00:02:50.710 
We need to recalculate the derivative of all the edge nodes.

34
00:02:50.719 --> 00:02:53.689 
All the edge note between the note e

35
00:02:53.699 --> 00:02:54.780 
And note a.

36
00:02:54.789 --> 00:02:58.449 
And then we use the chain rule to merge them together.

37
00:02:59.039 --> 00:03:09.009 
But this forward computing mode also brings a lot of recalculation because every time we need to recalculate all the edge

38
00:03:09.020 --> 00:03:14.639 
on the path between two nodes.

39
00:03:14.639 --> 00:03:19.550 
What if we do the reverse mode differentiation from the note e

40
00:03:19.550 --> 00:03:21.050 
down to the input node

41
00:03:21.060 --> 00:03:21.449 
a

42
00:03:21.939 --> 00:03:28.259 
And this give us the derivative of e with respect to every node in one path.

43
00:03:28.750 --> 00:03:41.370 
In contrast forward no differentiation give us the derivative of our output with respect a single input. For this graph that's

44
00:03:41.370 --> 00:03:45.349 
only a factor of 2 speed up,

45
00:03:45.360 --> 00:03:54.930 
but imagine a function with the millions of inputs and one output forward mode differentiation would request to go through

46
00:03:54.930 --> 00:04:04.650 
the Graph 1,000,000 times and together derivatives. Reverse mode differentiation can get all of them in one reversal.

47
00:04:05.539 --> 00:04:13.949 
So when the training neural networks we think of the cost function of the parameters we want to calculate the derivatives

48
00:04:13.949 --> 00:04:22.360 
of the cost function with respect to all the parameters all the weights for using gradient descent.

49
00:04:22.740 --> 00:04:29.230 
And now there are often millions of even 10 of the millions of parameters in the neural network.

50
00:04:29.240 --> 00:04:38.180 
So reverse mode differentiation can also be called back propagation in the context of neural networks gives us a massive

51
00:04:38.180 --> 00:04:38.949 
speed up.

52
00:04:41.939 --> 00:04:47.759 
Okay, now let's take a simple example to recap how the back propagation works.

53
00:04:49.339 --> 00:04:57.649 
So we have three input nodes. So we define it and and the first do the sum of then we do the multiplication.

54
00:04:58.040 --> 00:05:01.569 
Just two very simple basic operators.

55
00:05:01.579 --> 00:05:09.259 
We already assigned the value to each note and compute the intermediate value and also the final output.

56
00:05:11.839 --> 00:05:18.139 
So we use the green color to denote the forward path

57
00:05:18.139 --> 00:05:26.759 
and then we can calculate the derivatives of the output node f with respect to the input node x, y and z.

58
00:05:26.769 --> 00:05:28.360 
So we have three inputs.

59
00:05:29.139 --> 00:05:36.550 
And for backward pass we use the color red to represent the intermedia and also the final result.

60
00:05:36.939 --> 00:05:49.879 
So for simplicity, I just keep the detail calculation progress and you can take a closer look to the details of the

61
00:05:49.879 --> 00:05:58.860 
calculation. Here, we have summarized the three most commonly used the fusion operators in the neural networks.

62
00:05:59.240 --> 00:06:03.350 
So the addition gate, max gate and multiply gate.

63
00:06:04.139 --> 00:06:07.959 
They have very different characteristics in the back propagation.

64
00:06:07.970 --> 00:06:16.660 
I hope everyone that can remember these characteristics so that when designing neural network structure architectures in

65
00:06:16.660 --> 00:06:24.959 
the future, everybody can use them directly, rapidly to avoid performance bottleneck.

66
00:06:25.939 --> 00:06:36.620 
So in the backward pass, we can see that the advocates serve as a gradient distributor, and it will just pass the gradient

67
00:06:36.620 --> 00:06:46.990 
to every subsequent passes and max gate as the gradient router gradient view only go through the past with a larger

68
00:06:46.990 --> 00:06:52.949 
input value and the multiplication multiply gate serve as a gradient feature.

69
00:06:57.240 --> 00:06:58.649 
Thank you for watching the video.
