WEBVTT

1
00:00:00.000 --> 00:00:03.300 
This is the last one in terms of
trends that I wanted to share.

2
00:00:03.300 --> 00:00:06.600 
This is about deep
reinforcement learning that was

3
00:00:06.600 --> 00:00:09.900 
the second to last one, I still
have memory networks coming,

4
00:00:09.900 --> 00:00:12.120 
sorry about that. Reinforcement
learning is about agents in

5
00:00:12.120 --> 00:00:16.160 
their environment and you
typically have observations of that

6
00:00:16.160 --> 00:00:20.200 
environment. They tend to be
visual so the video camera feeds

7
00:00:20.200 --> 00:00:24.240 
or simulated screen contents
in games or whatever.

8
00:00:24.240 --> 00:00:26.260 
But they can be anything, they
can also be any kind of time

9
00:00:26.260 --> 00:00:30.300 
series sensor signal.
Your agent can typically

10
00:00:30.300 --> 00:00:33.330 
trigger a bunch of actions in
the environment like turn left,

11
00:00:33.330 --> 00:00:36.360 
turn right, climb up the
stairs, hit the opponent,

12
00:00:36.360 --> 00:00:40.400 
fire your gun, or send
a dunning letter to that

13
00:00:40.400 --> 00:00:44.440 
customer. You can
then get rewards like

14
00:00:45.450 --> 00:00:47.470 
hey you've been successfully
driving and not hit the other

15
00:00:47.470 --> 00:00:51.510 
cars yet or your high
score has gone up

16
00:00:51.510 --> 00:00:55.550 
or hey, payment inflow
from the customer. What

17
00:00:55.550 --> 00:01:00.600 
you did over the last five actions
actually was somehow meaningful

18
00:01:00.600 --> 00:01:02.620 
because the customer
has paid you. This is

19
00:01:02.620 --> 00:01:05.650 
what reinforcement
learning excels at

20
00:01:05.650 --> 00:01:09.690 
with very very rich environments
which require complex

21
00:01:09.690 --> 00:01:13.730 
strategies in order to win and
just a trickle of very very

22
00:01:13.730 --> 00:01:17.770 
sparse rewards feedback we
can actually learn models

23
00:01:17.770 --> 00:01:21.810 
tha do this. Technique
behind this is called deep

24
00:01:21.810 --> 00:01:25.850 
q reinforcement learning.
It was pioneered probably

25
00:01:25.850 --> 00:01:29.890 
ten years ago and a team

26
00:01:30.900 --> 00:01:32.920 
in the UK
called deep mind

27
00:01:33.930 --> 00:01:37.970 
has successfully made a few
tweaks to this which led

28
00:01:37.970 --> 00:01:41.101 
to a rather spectacular
paper on atari game

29
00:01:41.101 --> 00:01:45.105 
playing and rather spectacular
acquisition by google

30
00:01:45.105 --> 00:01:49.109 
because again. small tweaks
in how we do reinforcement

31
00:01:49.109 --> 00:01:52.112 
learning have suddenly
made it real world relevant

32
00:01:52.112 --> 00:01:55.115 
in order to win. These more
tweaks that they made were,

33
00:01:56.116 --> 00:01:59.119 
hey we don't have enough training
data maybe we add a replay

34
00:01:59.119 --> 00:02:02.122 
buffer and the last one million
things we've seen we will just

35
00:02:02.122 --> 00:02:06.126 
periodically re-look at. Hey,
this computer vision thing

36
00:02:06.126 --> 00:02:10.130 
on graphics cards seems to work
well maybe we can formulate

37
00:02:10.130 --> 00:02:13.133 
our reward function as one of
these computer vision things

38
00:02:13.133 --> 00:02:17.137 
on graphics cards, those are
really grossly simplifying a bit.

39
00:02:17.137 --> 00:02:20.140 
The fundamental changes
that a deep mind made

40
00:02:20.140 --> 00:02:23.143 
and the core training
algorithm fits on a single

41
00:02:23.143 --> 00:02:27.147 
page in their paper. This
is what reinforcement

42
00:02:27.147 --> 00:02:30.150 
learning is about and
sort of the abstraction

43
00:02:30.150 --> 00:02:32.152 
layers that the
algorithm has to provide.

44
00:02:32.152 --> 00:02:35.155 
It's about
understanding rewards

45
00:02:35.155 --> 00:02:38.158 
and rewards are the signal that
you're getting into a concrete

46
00:02:38.158 --> 00:02:41.161 
problem. For example
in maze running

47
00:02:41.161 --> 00:02:45.165 
you could say that my reward
function is going to be a -1

48
00:02:45.165 --> 00:02:49.169 
for every time step that I
haven't reached the goal yet.

49
00:02:50.170 --> 00:02:53.173 
The longer you take
the worse you are off.

50
00:02:53.173 --> 00:02:57.177 
Everything else like physical
proximity to the goal or something

51
00:02:57.177 --> 00:03:00.180 
could mislead you because there
are blind alleys and other things.

52
00:03:00.180 --> 00:03:04.184 
We are just saying clock is ticking
and the longer you take the worse

53
00:03:04.184 --> 00:03:09.189 
you're off. The agent
then has to learn a policy

54
00:03:09.189 --> 00:03:13.193 
and the policy is very simply
spoken what's the button on the

55
00:03:13.193 --> 00:03:16.196 
game controller that I
need to push next. Do I go

56
00:03:16.196 --> 00:03:18.198 
forward, left, right, or back
in order to solve this thing?

57
00:03:19.199 --> 00:03:23.203 
That policy function is what
we really want because when

58
00:03:23.203 --> 00:03:26.206 
I see a certain
environment and I sense it

59
00:03:27.207 --> 00:03:30.210 
I want to be able to apply the
next action and do that really

60
00:03:30.210 --> 00:03:34.214 
quickly. But learning the policy
function is actually fairly

61
00:03:34.214 --> 00:03:38.218 
hard because what you
would have to do is to

62
00:03:38.218 --> 00:03:42.222 
quickly calculate in your head
what are the expected outcomes

63
00:03:43.223 --> 00:03:46.226 
from each of the actions
that I could take

64
00:03:46.226 --> 00:03:50.230 
and the expected outcomes of
that actually correspond to

65
00:03:50.230 --> 00:03:54.234 
sort of the the infinite summation
of the sum of all sequences

66
00:03:54.234 --> 00:03:57.237 
of actions that I could
take in the future.

67
00:03:57.237 --> 00:04:00.240 
You have that huge
combinatorical explosion,

68
00:04:01.241 --> 00:04:06.246 
the good thing is that combinatorical
space has a lot of locality.

69
00:04:06.246 --> 00:04:10.250 
What you can actually do is try
and learn these value functions

70
00:04:10.250 --> 00:04:13.253 
that give you the value of
each step in the environment

71
00:04:13.253 --> 00:04:16.256 
and each action that you
can take in the environment

72
00:04:16.256 --> 00:04:20.260 
in a neural network way.
That leads to a model

73
00:04:20.260 --> 00:04:23.263 
that will immediately give
you sort of an optimal

74
00:04:23.263 --> 00:04:26.266 
or near optimal
path towards your

75
00:04:26.266 --> 00:04:30.270 
goal. The way folks do
this today is, guess what,

76
00:04:31.271 --> 00:04:34.274 
they have a convolutional
neural network that looks at the

77
00:04:34.274 --> 00:04:37.277 
last four frames of, for example,
a video game this is how the

78
00:04:37.277 --> 00:04:42.282 
atari game playing was done.
That convolutional network

79
00:04:42.282 --> 00:04:46.286 
leads down to the action
states for the game controller

80
00:04:46.286 --> 00:04:48.288 
to trigger the next
action in the game.

81
00:04:49.289 --> 00:04:52.292 
Then there is a bit of
sophistication in the learning around

82
00:04:52.292 --> 00:04:56.296 
sort of replaying contents and
having a special optimization

83
00:04:56.296 --> 00:04:59.299 
to optimize the
gradient for finding

84
00:04:59.299 --> 00:05:01.301 
the improvements to
make on the network

85
00:05:01.301 --> 00:05:05.305 
and essentially we're
done. The beauty if this is

86
00:05:06.306 --> 00:05:09.309 
no knowledge about how
to play atari video games

87
00:05:09.309 --> 00:05:15.315 
was inside, ever, at any point in
time so the method is universal

88
00:05:15.315 --> 00:05:18.318 
and it applies to just about everything.
Here is some stuff you can do

89
00:05:18.318 --> 00:05:22.322 
with it. They are taken from
openAI gym and openAI universe,

90
00:05:22.322 --> 00:05:26.326 
the video on the left hand side
shows how we control a human

91
00:05:26.326 --> 00:05:30.330 
in running and the goal function
is only, hey the farther you get

92
00:05:30.330 --> 00:05:33.333 
the better you are off.
It starts by falling down

93
00:05:33.333 --> 00:05:37.337 
a lot and sort of swaying
and it doesn't quite know

94
00:05:37.337 --> 00:05:40.340 
how to control it's body and
then it learns how to do that.

95
00:05:40.340 --> 00:05:43.343 
You can see it running faster
and faster, this countervailing

96
00:05:43.343 --> 00:05:46.346 
movements, there is sort of
the upper body going forward

97
00:05:46.346 --> 00:05:49.349 
and all the things you would, there
are longer strides, all the things

98
00:05:49.349 --> 00:05:55.355 
you would expect from a sprinter.
Not a single biomechanical

99
00:05:55.355 --> 00:05:57.357 
expert was involved in
building this stupid model.

100
00:05:58.358 --> 00:06:02.362 
The model takes a physics
simulator and that's it. It learns

101
00:06:02.362 --> 00:06:05.365 
from the more meters you run in a
given time the better you are off.

102
00:06:08.368 --> 00:06:09.369 
The scary thing is that you
can do that to other things

103
00:06:10.370 --> 00:06:12.372 
and what you see on
the right hand side is

104
00:06:13.373 --> 00:06:16.376 
that Video didn't run,
something broken here.

105
00:06:17.377 --> 00:06:19.379 
We see the right
hand side is

106
00:06:20.380 --> 00:06:24.384 
the openAI universe environment
which can't do just games.

107
00:06:24.384 --> 00:06:28.388 
It's based on a screen capture and
keyboard mouse sharing environment

108
00:06:28.388 --> 00:06:31.391 
for arbitrary applications.
What they're playing with now

109
00:06:32.392 --> 00:06:36.396 
is small simple
UI mobile apps and

110
00:06:36.396 --> 00:06:40.400 
instructions like can
you book me a flight to

111
00:06:40.400 --> 00:06:45.405 
New York tonight. With reinforcement
learning, learning what

112
00:06:45.405 --> 00:06:49.409 
the human agent is doing in
these kinds of environments in

113
00:06:49.409 --> 00:06:52.412 
order to fulfill the
actions and get the reward

114
00:06:52.412 --> 00:06:54.414 
which is the customer saying,
"I'm happy."

115
00:06:55.415 --> 00:07:02.422 
This is super scary, the left
hand side is funny, is inspiring,

116
00:07:02.422 --> 00:07:05.425 
is something that shows us what
the future of all this robotic

117
00:07:05.425 --> 00:07:08.428 
stuff and how we train robots
and environments is going to

118
00:07:08.428 --> 00:07:12.432 
be. It's not about physical embodied
robots, it's not about making

119
00:07:12.432 --> 00:07:15.435 
things cute and cuddly and human
and having them live through

120
00:07:15.435 --> 00:07:18.438 
an infant experience because we
cuddle them through five years.

121
00:07:19.439 --> 00:07:22.442 
It's about hey, we create
a simulation environment

122
00:07:22.442 --> 00:07:25.445 
that's based on the laws of
physics and we can create millions

123
00:07:25.445 --> 00:07:29.449 
of these training samples of
how the robot controls it's body

124
00:07:29.449 --> 00:07:32.452 
and how the body reacts. It's
going to figure this out, there's

125
00:07:32.452 --> 00:07:36.456 
now other ones of humanoids
standing up which is actually a

126
00:07:36.456 --> 00:07:40.460 
fairly hard thing to do and
traditional hardcoded rules-based

127
00:07:40.460 --> 00:07:43.463 
robot control has never been
able to do this really well.

128
00:07:43.463 --> 00:07:47.467 
There is now examples of these
humanoids running parkour

129
00:07:47.467 --> 00:07:50.470 
in simulated environments and
sort of climbing, navigating

130
00:07:50.470 --> 00:07:54.474 
obstacle courses. The
reinforcement learning thing at

131
00:07:54.474 --> 00:07:58.478 
robot and agent control
is pretty cool and scary.

132
00:07:58.478 --> 00:08:01.481 
The scary thing for a
company like SAP is ,

133
00:08:01.481 --> 00:08:03.483 
hey if you can do that on
screen grabs and pixels

134
00:08:04.484 --> 00:08:09.489 
our key value has been integration
and if you can just integrate

135
00:08:09.489 --> 00:08:12.492 
stuff by learning on a
frame-grab of the thing from the

136
00:08:12.492 --> 00:08:16.496 
outside then we might
have a problem because

137
00:08:16.496 --> 00:08:19.000 
then you can actually
do integration in a much

138
00:08:19.000 --> 00:08:21.501 
simpler way. Therefore
we are working very

139
00:08:21.501 --> 00:08:24.504 
hard to bring the
value into the product

140
00:08:24.504 --> 00:08:27.507 
and do that from the inside
before somebody comes

141
00:08:27.507 --> 00:08:28.508 
and does it from
the outside.

142
00:08:31.511 --> 00:08:34.514 
Last one on trends, spoke a
lot about environments where

143
00:08:34.514 --> 00:08:39.519 
you have a lot of data and when
you have only a very few examples

144
00:08:39.519 --> 00:08:43.523 
or very few instructions like,
find me a man with a dark hat

145
00:08:43.523 --> 00:08:46.526 
or something like that, the
neural networks don't work well

146
00:08:46.526 --> 00:08:49.529 
because they typically expect to
be fed a couple hundred thousand

147
00:08:49.529 --> 00:08:53.533 
examples with neat labels, a lot of
number crunching in order to train

148
00:08:53.533 --> 00:08:56.536 
that. What folks
have started out

149
00:08:57.537 --> 00:09:00.540 
end of 2014 with the first
papers on memory networks

150
00:09:00.540 --> 00:09:05.545 
was to add a memory and the
hard thing about adding memory

151
00:09:05.545 --> 00:09:08.548 
was memory does not match
neural networks very

152
00:09:08.548 --> 00:09:12.552 
well. Memory sort of 1 0, you
write to it, you can always

153
00:09:12.552 --> 00:09:16.556 
retrieve. Neural networks
is hey iIhave these 32 bit

154
00:09:16.556 --> 00:09:19.559 
floating-point weights and with
each training step I'm going

155
00:09:19.559 --> 00:09:21.561 
to go a little tiny delta
into some direction.

156
00:09:21.561 --> 00:09:24.564 
But it's all very differentiable,
very tractable, you can do

157
00:09:24.564 --> 00:09:28.568 
almost anything gradually
over time. What folks

158
00:09:28.568 --> 00:09:32.572 
did with memory networks was
to say I'm going to create a

159
00:09:32.572 --> 00:09:36.576 
memory which has a
differentiable write-equation

160
00:09:36.576 --> 00:09:40.580 
where we're basically taking
the thing you want to write

161
00:09:40.580 --> 00:09:43.583 
and we're doing an
analog write with sort of

162
00:09:43.583 --> 00:09:47.587 
70% superimposing that
over the current memory

163
00:09:47.587 --> 00:09:51.591 
contents of the thing, or 50
or 30 or whatever your learning

164
00:09:51.591 --> 00:09:55.595 
rate may be. So that the original
state still is faintly there

165
00:09:55.595 --> 00:09:59.599 
and faintly recognizable but
if you successively write stuff

166
00:09:59.599 --> 00:10:03.603 
and do that over time, it's becoming
more solid and more available.

167
00:10:03.603 --> 00:10:06.606 
This has evolved into
a couple directions,

168
00:10:07.607 --> 00:10:09.609 
the memory network guys did
that with a big matrix that you

169
00:10:09.609 --> 00:10:14.614 
could write over, simultaneously
there were a few folks thinking

170
00:10:14.614 --> 00:10:17.617 
about neural turing machines
where you have a write head

171
00:10:17.617 --> 00:10:20.620 
and read head and you can go left
and right and you can trigger

172
00:10:20.620 --> 00:10:24.624 
these partial override actions
in a very similar manner.

173
00:10:24.624 --> 00:10:27.627 
This has now
evolved end of 2016

174
00:10:28.628 --> 00:10:32.632 
into a DNC architecture or
differentiable neural computing

175
00:10:32.632 --> 00:10:35.635 
architecture where
you have a controller

176
00:10:35.635 --> 00:10:40.640 
with one or multiple
input/output read/write heads

177
00:10:40.640 --> 00:10:45.645 
and a memory that is accessible by
these heads and then a differentiable

178
00:10:45.645 --> 00:10:49.649 
function determining how
we write and how we read.

179
00:10:50.650 --> 00:10:53.653 
For one-shot learning
for trying to figure out

180
00:10:53.653 --> 00:10:57.657 
what we can do or should do
based on a very few examples,

181
00:10:57.657 --> 00:11:01.661 
this is currently the hottest
thing. It's also hot from another

182
00:11:01.661 --> 00:11:06.666 
perspective because we
can learn stuff from data

183
00:11:06.666 --> 00:11:10.670 
and in this case we can learn
algorithms, so the thing if fed

184
00:11:10.670 --> 00:11:14.674 
the proper structures and if
fed the proper reward functions

185
00:11:14.674 --> 00:11:18.678 
can learn a list sorting function,
it can learn a graph traversal

186
00:11:18.678 --> 00:11:21.681 
algorithm whether it's depth
first or breadth first.

187
00:11:21.681 --> 00:11:25.685 
You can learn a lot of sort
of simple programming tasks

188
00:11:25.685 --> 00:11:28.688 
and if you are
able to break down

189
00:11:29.689 --> 00:11:33.693 
the description of what you want
in the form of a specification

190
00:11:33.693 --> 00:11:37.697 
to a very low level than this
thing could already today

191
00:11:37.697 --> 00:11:40.700 
probably code 80 %
of your specification

192
00:11:40.700 --> 00:11:44.704 
on very very
simple toy domains.

193
00:11:44.704 --> 00:11:48.708 
It's very far away from sort
of completely replacing human

194
00:11:48.708 --> 00:11:51.711 
programmers and
turning them into

195
00:11:51.711 --> 00:11:55.715 
the next generation of unemployed
but it could be a fantastic

196
00:11:55.715 --> 00:12:00.720 
tool to enhance the productivity
of all of us in creating code

197
00:12:00.720 --> 00:12:04.724 
and sort of gaining
more leverage

198
00:12:04.724 --> 00:12:08.728 
as we focus on describing
problems and relying ever more

199
00:12:08.728 --> 00:12:11.731 
computer support to turn
that into executable code.

200
00:12:12.732 --> 00:12:15.735 
Because none of us
write binary code today,

201
00:12:16.736 --> 00:12:17.737 
right? Maybe one
in the audience.

202
00:12:18.738 --> 00:12:20.740 
None of us right assembly
code today, right?

203
00:12:21.741 --> 00:12:24.744 
Maybe two in the audience. A
few of us may still write c or

204
00:12:24.744 --> 00:12:28.748 
c++ or CUDA code but the
majority are probably working

205
00:12:28.748 --> 00:12:33.753 
in some kind of fourth, fifth
generation language, scripting

206
00:12:33.753 --> 00:12:37.757 
interpreted i python
notebook, whatever.

207
00:12:37.757 --> 00:12:39.759 
Which is many many layers of
abstraction from the concrete

208
00:12:39.759 --> 00:12:43.000 
machine doing the
execution. The differential

209
00:12:43.763 --> 00:12:46.766 
neural computing and
being able to learn,

210
00:12:46.766 --> 00:12:50.770 
how to solve programming
tasks but also solving

211
00:12:50.770 --> 00:12:52.772 
other tasks that rely
on single-shot data

212
00:12:52.772 --> 00:12:54.774 
pools tremendous
potential.
