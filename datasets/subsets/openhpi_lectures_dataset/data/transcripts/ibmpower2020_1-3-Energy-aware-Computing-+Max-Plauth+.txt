WEBVTT

1
00:00:00.930 --> 00:00:05.320 
Hello. My name is Max Plauth and I'm a
PhD student at the operating

2
00:00:05.320 --> 00:00:07.920 
systems and middleware group at
the Hasso Plattner Institute.

3
00:00:08.880 --> 00:00:11.640 
In this session I am going to
talk a little bit about the

4
00:00:11.640 --> 00:00:16.240 
topic of energy aware computing and why
it's such a relevant topic right now

5
00:00:16.860 --> 00:00:20.280 
and in this context I'm going to
present some of the research

6
00:00:20.280 --> 00:00:23.460 
projects that are currently
going on at our research group.

7
00:00:27.070 --> 00:00:32.980 
So, of course a very important driver for
this project is, or for this topic is

8
00:00:33.260 --> 00:00:38.060 
that we all want to attain carbon neutral
compute infrastructures one day.

9
00:00:38.740 --> 00:00:43.840 
And to achieve this goal of course renewable
energy sources are a vital ingredient.

10
00:00:45.830 --> 00:00:51.010 
As nice as a renewable energy
sources are, they also come with

11
00:00:51.010 --> 00:00:55.150 
a slightly problematic attribute and
that is that they are highly volatile.

12
00:00:55.650 --> 00:00:59.940 
So of course you can imagine that wind
isn't blowing constantly all day long,

13
00:01:00.190 --> 00:01:03.620 
sun isn't shining every day and
of course it doesn't shine

14
00:01:03.620 --> 00:01:04.490 
at night at all.

15
00:01:06.440 --> 00:01:10.460 
So this problem aggravates if you
have to deal with an energy grid

16
00:01:10.750 --> 00:01:15.780 
where a very large fraction of energy
comes from renewable energy sources.

17
00:01:16.370 --> 00:01:19.810 
So this is an example from
Germany, from two thousand and

18
00:01:19.810 --> 00:01:23.750 
seventeen and already there
roughly one third of the energy

19
00:01:24.150 --> 00:01:27.700 
in the electricity grid comes
from renewable energy sources.

20
00:01:28.760 --> 00:01:33.340 
And now assuming it's, some time
has passed, probably this

21
00:01:33.340 --> 00:01:36.230 
share is even
aising

22
00:01:36.970 --> 00:01:41.210 
and traditional energy sources
are slightly declining, so

23
00:01:41.350 --> 00:01:44.770 
this volatility has
even stronger effects.

24
00:01:45.240 --> 00:01:49.460 
Now you might ask yourself why
is this volatility important

25
00:01:49.460 --> 00:01:53.340 
for me as a data centre operator? Isn't
this something that the

26
00:01:53.500 --> 00:01:56.740 
operators of the electricity
grid have to consider?

27
00:01:57.530 --> 00:02:02.160 
Yes and no. So of course the
electricity grid providers will

28
00:02:02.160 --> 00:02:04.720 
make sure that
you have stable

29
00:02:05.390 --> 00:02:09.860 
supply at your energy sockets, in
your home and at your company

30
00:02:09.880 --> 00:02:17.140 
but this comes of course at a cost
and that is that, so there's this

31
00:02:17.630 --> 00:02:22.660 
there are energy markets and
usually there's this volatility of

32
00:02:22.810 --> 00:02:27.570 
energy available on
the grid comes

33
00:02:28.110 --> 00:02:32.980 
associated with varying costs.
So whenever there is an over

34
00:02:32.980 --> 00:02:35.860 
demand of energy, of course
energy will become cheaper.

35
00:02:36.390 --> 00:02:40.300 
When there is not as much
energy produced currently, then

36
00:02:40.300 --> 00:02:42.800 
energy might become a
little bit more expensive

37
00:02:43.270 --> 00:02:47.730 
and so here we have a plot from
two thousand and seventeen

38
00:02:48.010 --> 00:02:53.280 
where we ran in Germany into the strange
situation that over the weekend

39
00:02:53.840 --> 00:02:58.150 
energy price turned negative. So
this is important to consider

40
00:02:58.150 --> 00:03:02.500 
because as a data center operator
consuming a lot of energy

41
00:03:02.500 --> 00:03:06.050 
and you probably have special contracts
with the electricity provider.

42
00:03:06.580 --> 00:03:11.030 
So you might ask yourself, how do we end
up with natural negative energy prices?

43
00:03:11.510 --> 00:03:15.440 
So you can see it was by the
end of two thousand seventeen,

44
00:03:15.440 --> 00:03:20.670 
so some week during fall and
probably it was a week and

45
00:03:20.680 --> 00:03:24.400 
with nice weather, a lot of
sunshine and strong wind blowing

46
00:03:24.870 --> 00:03:30.070 
and perhaps some industries
reduced their production

47
00:03:30.070 --> 00:03:34.440 
workloads over the weekend, so
there was less energy demand

48
00:03:34.450 --> 00:03:37.530 
on the grid. So we had this
negative energy price.

49
00:03:38.690 --> 00:03:42.430 
So if you're taking this
effect and applying it to

50
00:03:42.850 --> 00:03:46.880 
the point of view of a data
centre operator, this results

51
00:03:47.290 --> 00:03:53.620 
in contracts with your electricity
provider that you have to

52
00:03:54.300 --> 00:03:56.410 
make sure that
you're going to

53
00:03:57.440 --> 00:04:01.120 
consume certain amounts of
energy from the grid. So

54
00:04:01.940 --> 00:04:05.250 
you may not exceed a
certain upper threshold

55
00:04:05.810 --> 00:04:11.330 
and you may not surpass
a certain lower threshold.

56
00:04:11.490 --> 00:04:15.570 
And of course now with the
volatility of renewable energies,

57
00:04:15.910 --> 00:04:21.280 
this isn't a constant zone, so
you have this power target

58
00:04:21.280 --> 00:04:25.430 
zone in this plot and if you're
running into an excess demand

59
00:04:25.430 --> 00:04:30.020 
situation and under supply, you
have to pay penalty fees and

60
00:04:30.020 --> 00:04:33.560 
if you run into under
demand and excess supply,

61
00:04:34.280 --> 00:04:37.730 
then you also have to pay a
penalty fee. So of course you

62
00:04:37.730 --> 00:04:42.600 
want to stay in this region were
you have to pay the lowest

63
00:04:42.600 --> 00:04:47.410 
amount of yeah the lowest
amount of a money for your energy.

64
00:04:48.550 --> 00:04:52.500 
Now this brings us to
the next question.

65
00:04:53.190 --> 00:04:59.040 
How can you influence the power consumption
of compute infrastructures

66
00:04:59.430 --> 00:05:01.930 
from a software
perspective?

67
00:05:04.030 --> 00:05:09.790 
So we have thought of three
different categories how you can

68
00:05:10.370 --> 00:05:15.550 
imagine this. So one approach would
be to work at a different time.

69
00:05:15.680 --> 00:05:19.610 
So you probably all know that
from your phones or notebooks, so

70
00:05:19.770 --> 00:05:22.500 
when they are on but battery
they are not going to perform

71
00:05:22.500 --> 00:05:26.010 
any updates or backups. So they
are going to wait with that

72
00:05:26.370 --> 00:05:28.900 
until they are connected
back to the energy grid.

73
00:05:29.460 --> 00:05:35.550 
And this can be applied also to very
large systems or also embedded systems

74
00:05:36.030 --> 00:05:40.960 
but this strategy depends on
situations where you know

75
00:05:40.960 --> 00:05:46.740 
very well what jobs occur at what
time and where it's also possible

76
00:05:46.740 --> 00:05:50.380 
to defer a certain job without
violating a service level

77
00:05:50.380 --> 00:05:51.780 
agreement or
something like that.

78
00:05:53.910 --> 00:05:57.950 
Another strategy would be to work
less or more. So again from

79
00:05:57.950 --> 00:06:02.540 
our notebooks we know this that if I
pull the plug from this notebook

80
00:06:02.860 --> 00:06:04.980 
the processor will

81
00:06:05.680 --> 00:06:10.470 
be switched down to a lower power setting,
where the CPU will get clock down

82
00:06:10.720 --> 00:06:16.890 
and compressing video like this will
take significantly longer. So

83
00:06:17.050 --> 00:06:18.460 
once my notebook
is back

84
00:06:19.240 --> 00:06:23.830 
to the power supply, everything
will run much smoother again. So

85
00:06:24.190 --> 00:06:27.840 
again this can be applied
to the large data center.

86
00:06:28.350 --> 00:06:32.590 
If you imagine that you're just
booking additional compute

87
00:06:32.590 --> 00:06:37.540 
nodes in your from your cloud
provider of your choice or in your

88
00:06:37.940 --> 00:06:40.590 
local data center, you're spinning
up additional resources.

89
00:06:41.730 --> 00:06:46.810 
So this is also a quite
well study topic.

90
00:06:47.370 --> 00:06:52.670 
So another topic that is becoming
increasingly irrelevant is a third approach

91
00:06:52.870 --> 00:06:58.960 
that is to work on some different
kind of hardware. So work elsewhere,

92
00:06:59.160 --> 00:07:01.680 
use other kinds of
computer resources.

93
00:07:02.470 --> 00:07:08.240 
So and this is the topic
that we are currently

94
00:07:08.320 --> 00:07:13.560 
investigating in our operating
systems and middleware research group

95
00:07:14.190 --> 00:07:18.920 
and so I just want to present
two colleagues who are very

96
00:07:18.920 --> 00:07:22.390 
active in this area, who have
done a lot of the work that I

97
00:07:22.390 --> 00:07:27.500 
am presenting here. So there
are Sven KÃ¶hler and Lukas Wenzel,

98
00:07:27.500 --> 00:07:29.640 
my colleagues. So

99
00:07:30.300 --> 00:07:33.690 
keep them in mind that they doing
lot of heavy lifting of

100
00:07:33.890 --> 00:07:36.350 
what you can hear in
this talk now.

101
00:07:38.650 --> 00:07:46.650 
So when we want to compute elsewhere
and use different compute resources,

102
00:07:46.930 --> 00:07:51.260 
it makes sense to quickly go
through what different kinds

103
00:07:51.260 --> 00:07:54.010 
of computer resources
are there currently. So

104
00:07:54.940 --> 00:07:59.610 
let me talk about three major categories.
So starting with the CPUs,

105
00:07:59.820 --> 00:08:03.760 
so you're probably very
familiar with the CPUs. So

106
00:08:04.210 --> 00:08:08.320 
they are very flexible so you
can run any kind of algorithm

107
00:08:08.320 --> 00:08:12.610 
or application on them which
is really great and its

108
00:08:13.030 --> 00:08:14.760 
quite easy to
program for them.

109
00:08:15.360 --> 00:08:20.110 
The issue with this is that
this strategy allows

110
00:08:20.110 --> 00:08:23.280 
less specialisation. So the CPU
has to be able to process

111
00:08:23.290 --> 00:08:27.890 
any kind of workload regardless of
what it might be.

112
00:08:28.540 --> 00:08:32.520 
And that means also there's less
room for optimising your CPU

113
00:08:32.520 --> 00:08:37.020 
for a certain task. So this is a
inherent problem that comes

114
00:08:37.240 --> 00:08:39.370 
with this extreme
degree of flexibility.

115
00:08:41.870 --> 00:08:45.870 
Another very popular approach
right now is GPU based computing.

116
00:08:46.760 --> 00:08:50.990 
So GPUs very great when
it comes to high throughput

117
00:08:50.990 --> 00:08:55.700 
computing, so they they can bring
really high throughput but

118
00:08:56.350 --> 00:09:00.230 
only for very data
parallel workloads. So

119
00:09:01.540 --> 00:09:05.190 
when you can use them, they provide
good performance, but the

120
00:09:05.190 --> 00:09:09.160 
problem is they are not applicable
to arbitrary workloads. So

121
00:09:09.160 --> 00:09:13.080 
you have to make sure that
you're that you are applying

122
00:09:13.080 --> 00:09:17.030 
the same kinds of computations
to a very large dataset.

123
00:09:17.940 --> 00:09:20.100 
So machine learning is
a good example here.

124
00:09:22.170 --> 00:09:26.850 
Thirdly there are a FPGAs
that are currently

125
00:09:27.480 --> 00:09:34.700 
being considered as a very relevant
third category. So FPGAs are

126
00:09:34.880 --> 00:09:40.290 
over simplified programmable
hardware circuits. So

127
00:09:41.690 --> 00:09:47.020 
the advantage of this is that you can
create highly optimized hardware for a

128
00:09:47.150 --> 00:09:49.030 
certain workload
or algorithm,

129
00:09:49.650 --> 00:09:55.610 
but this high optimization
degree comes at the cost that

130
00:09:55.620 --> 00:10:00.730 
FPGAs are really hard to program,
at least that is for software

131
00:10:00.730 --> 00:10:06.770 
developers. So if you have a software mindset,
software development mindset

132
00:10:07.030 --> 00:10:10.750 
it might take quite some time to
get used to FPGA development.

133
00:10:12.150 --> 00:10:16.990 
Yeah, so these are the three
major categories are that we

134
00:10:17.530 --> 00:10:19.040 
want to consider
for a moment.

135
00:10:19.980 --> 00:10:24.580 
And now we asked ourselves if we
have these three categories,

136
00:10:25.650 --> 00:10:29.060 
how would one workload
perform in those? So

137
00:10:29.620 --> 00:10:35.310 
we used as an example a simple heat
dissipation simulation workload

138
00:10:35.880 --> 00:10:40.650 
and we try to measure the
power efficiency of this

139
00:10:41.080 --> 00:10:43.800 
simulation workload on
different kinds of

140
00:10:44.540 --> 00:10:51.830 
compute devices. So we have one example
for a GPU based implementation,

141
00:10:52.340 --> 00:10:55.910 
two different kinds of
CPU based implementations

142
00:10:56.340 --> 00:11:02.190 
which only vary in the kind
of a data type that is used

143
00:11:02.200 --> 00:11:07.160 
to implement the simulation
to form a bridge to the FPGA

144
00:11:07.160 --> 00:11:09.520 
based implementation

145
00:11:10.190 --> 00:11:13.740 
that is also using character
data types instead of floating

146
00:11:13.740 --> 00:11:15.780 
point data types

147
00:11:16.420 --> 00:11:19.290 
but at first view, this

148
00:11:19.290 --> 00:11:20.320 
table

149
00:11:20.330 --> 00:11:23.720 
looks very
impressive because you can see that

150
00:11:24.020 --> 00:11:29.160 
FPGAs can give you almost double
the throughput for this

151
00:11:29.480 --> 00:11:34.140 
heat dissipation simulation
at really a fraction

152
00:11:34.140 --> 00:11:35.350 
of the power draw.

153
00:11:36.760 --> 00:11:40.750 
So the efficiency here of
the FPGAs is much higher.

154
00:11:42.960 --> 00:11:48.440 
You might ask yourself why did we choose to
see dissipations simulation workload. So

155
00:11:48.790 --> 00:11:53.400 
this workload uses a common
compute pattern that is

156
00:11:53.400 --> 00:11:57.250 
used in many simulation and
scientific workload. So

157
00:11:57.680 --> 00:12:01.590 
of course this is a little bit
simplified but you can think

158
00:12:01.590 --> 00:12:05.910 
of weather forecasts and weather simulations
working quite similar

159
00:12:06.350 --> 00:12:07.380 
to this workload.

160
00:12:10.910 --> 00:12:15.980 
So the power draw values that
are denoted in this table

161
00:12:15.990 --> 00:12:20.660 
were retrieved from various resources.
So for GPUs, we were

162
00:12:20.660 --> 00:12:23.570 
able to use the vendor
specific tool that reports

163
00:12:23.970 --> 00:12:27.820 
the current power draw for the
CPU based implementations,

164
00:12:28.190 --> 00:12:32.360 
we looked up on the
CPU datasheet and

165
00:12:33.120 --> 00:12:36.580 
the power draw that is
allowed for the CPU there

166
00:12:37.490 --> 00:12:40.910 
and for FPGAs it worked
again differently, so

167
00:12:41.420 --> 00:12:47.110 
there we had to rely on
simulation reports from the

168
00:12:47.110 --> 00:12:48.820 
FPGA development
tool chain.

169
00:12:49.640 --> 00:12:57.390 
So all these power draw values come
from very different sources and

170
00:12:59.320 --> 00:13:06.120 
in this context it still looks nice
that FPGAs yield the best performance

171
00:13:06.470 --> 00:13:07.880 
and energy efficiency

172
00:13:08.490 --> 00:13:11.750 
but very important thing that
you should keep in mind, this

173
00:13:11.750 --> 00:13:14.840 
experiment
is just for

174
00:13:15.570 --> 00:13:20.360 
a very certain workload so it's
just for this simulation

175
00:13:20.560 --> 00:13:24.370 
and it's also for very specific
hardware configurations that

176
00:13:24.380 --> 00:13:25.720 
we used in our setup.

177
00:13:28.240 --> 00:13:32.470 
This is a snapshot from summer
two thousand and nineteen

178
00:13:33.160 --> 00:13:39.090 
and this led us with a mixed feeling
because we thought, well, now these

179
00:13:39.520 --> 00:13:45.230 
amplified power draw measurements
that doesn't feel right. So

180
00:13:46.740 --> 00:13:50.990 
the differences here in this
table they seem nice but they

181
00:13:50.990 --> 00:13:54.910 
are also a little bit hard to believe
and we wanted to verify them if they

182
00:13:55.130 --> 00:13:59.400 
actually hold up against
much more

183
00:14:00.060 --> 00:14:01.700 
thorough evaluation.

184
00:14:02.540 --> 00:14:08.750 
So we dug a little bit deeper and
ran into another problem

185
00:14:09.170 --> 00:14:14.740 
and that is at the very basics
measuring power and energy

186
00:14:15.070 --> 00:14:16.730 
of a computer system.

187
00:14:17.670 --> 00:14:21.550 
So the problem is that even today
there are no standardized

188
00:14:21.560 --> 00:14:24.980 
programming interfaces available
for measuring power draw

189
00:14:24.990 --> 00:14:26.920 
energy consumption of
computer hardware

190
00:14:27.890 --> 00:14:32.090 
and each and any hardware
platform provides different

191
00:14:32.090 --> 00:14:35.860 
means for measurements,
if there are any at all.

192
00:14:36.780 --> 00:14:40.560 
So here is a brief
example. We used

193
00:14:41.090 --> 00:14:46.810 
low power hardware board an
embedded system with a

194
00:14:46.820 --> 00:14:51.950 
GPU embedded into it
and we executed the same

195
00:14:51.950 --> 00:14:58.410 
heat map simulation using a CPU based
and a GPU based implementation.

196
00:14:59.240 --> 00:15:02.100 
So a nice thing about this platform
that will be used is that

197
00:15:02.100 --> 00:15:07.080 
it has internal sensors that
can provide individual

198
00:15:07.080 --> 00:15:11.030 
readings for each subsystem
such as CPU, GPU,

199
00:15:11.470 --> 00:15:15.590 
system on chip components
and the memory subsystem.

200
00:15:18.640 --> 00:15:22.790 
And in addition we also perform measurements
using an external measurement device

201
00:15:23.150 --> 00:15:26.420 
hooked up right in front of
the power supply of this board

202
00:15:27.250 --> 00:15:30.450 
and what we learn from these
measurements is that of course,

203
00:15:30.450 --> 00:15:35.580 
the most obvious insight here
is that using this hardware,

204
00:15:35.870 --> 00:15:40.700 
the GPU based implementation is both
faster and more energy efficient.

205
00:15:42.510 --> 00:15:46.770 
Now if you're looking a little bit
more closely and ask yourself what

206
00:15:46.920 --> 00:15:51.020 
what might go wrong and why did we use
two different measurement techniques.

207
00:15:51.570 --> 00:15:55.380 
If you're looking more closely
at the internal sensors, you

208
00:15:55.380 --> 00:16:01.660 
can see that there is a certain
slope when we are starting workload

209
00:16:02.130 --> 00:16:07.130 
before the plateau of energy
consumption is reached. So

210
00:16:07.320 --> 00:16:11.850 
this seems a little bit unexpected
considering that the workload

211
00:16:11.850 --> 00:16:15.920 
starts immediately and there's a
load immediately on the CPU or GPU.

212
00:16:16.350 --> 00:16:22.060 
So why should the power draw only
start to increase after a certain wire?

213
00:16:23.010 --> 00:16:25.390 
So we compared this with
the external measurements

214
00:16:25.860 --> 00:16:32.250 
and then you can see that this slope
is much shorter and steeper. So

215
00:16:32.570 --> 00:16:37.310 
there is not this
strange averaging effect.

216
00:16:37.940 --> 00:16:42.050 
So what you are supposed to
take away from this light is

217
00:16:42.460 --> 00:16:46.870 
that with all the different measurement
techniques that might be out there

218
00:16:47.020 --> 00:16:48.250 
they all have different

219
00:16:49.040 --> 00:16:53.420 
attributes, different properties and
in case of these internal sensors

220
00:16:54.110 --> 00:16:57.050 
they might be nice because they
are they offer individual

221
00:16:57.050 --> 00:17:01.300 
readings for all subsystems but
they also come with a certain

222
00:17:01.870 --> 00:17:07.350 
degree of unprecision or
lacking precision because

223
00:17:07.350 --> 00:17:10.130 
if you're interested
in very momentary

224
00:17:10.830 --> 00:17:17.270 
effects on power draw then the
sensors don't give you

225
00:17:17.270 --> 00:17:20.020 
the right picture of what's
actually going to happen.

226
00:17:21.400 --> 00:17:23.900 
So this is one thing
that we learned

227
00:17:24.600 --> 00:17:29.120 
and another thing that we learned
is we repeated the same tests

228
00:17:29.740 --> 00:17:33.270 
on a large high-performance
server also equipped with a GPU

229
00:17:34.330 --> 00:17:40.570 
and the first thing that we learned
there is that there were no internal

230
00:17:40.910 --> 00:17:45.820 
measurement techniques at all. So we
had to use again an external power

231
00:17:46.550 --> 00:17:50.790 
power draw measurement device to
measure the power supply level

232
00:17:51.870 --> 00:17:58.160 
and what we learned here is that using
the same workload, this different machine

233
00:17:58.390 --> 00:18:02.900 
performs better both in terms of faster
performance and higher energy efficiency

234
00:18:03.180 --> 00:18:05.680 
if you're using the CPU
based implementation.

235
00:18:06.210 --> 00:18:10.640 
So there the GPU based implementation
performed less optimal.

236
00:18:11.360 --> 00:18:14.720 
So again keep in mind

237
00:18:16.520 --> 00:18:19.140 
the properties might change

238
00:18:19.770 --> 00:18:23.630 
both based on what workload you're
using and what kind of hardware

239
00:18:23.630 --> 00:18:26.050 
you're going
to use. So

240
00:18:26.710 --> 00:18:32.830 
this makes the situation very complicated
to make generally applicable statements.

241
00:18:33.060 --> 00:18:37.400 
So a lot of additional investigations
have to go into that direction.

242
00:18:39.770 --> 00:18:45.220 
And one thing where we started
of from that point is

243
00:18:45.510 --> 00:18:50.460 
that we were already quite
unhappy with the situation that

244
00:18:50.470 --> 00:18:54.510 
we now had different energy
measurement techniques and

245
00:18:54.510 --> 00:18:58.550 
had to use different
methods for

246
00:18:59.020 --> 00:19:03.570 
accessing these measurements. So
we developed a little tool

247
00:19:03.940 --> 00:19:07.900 
called the Perf-inspired energy
profiling tool, in short PINPOINT,

248
00:19:08.680 --> 00:19:13.400 
which is a cross platform tool that
allows us to measure energy consumption

249
00:19:13.680 --> 00:19:15.760 
of applications

250
00:19:16.490 --> 00:19:20.150 
and the idea of this tool is that
it's not just cross platform

251
00:19:20.370 --> 00:19:25.740 
but that it's also able to
collect readings from various

252
00:19:25.910 --> 00:19:30.900 
measurement sources and that these
readings are then aggregated

253
00:19:31.260 --> 00:19:37.660 
into a unified representation, a
little bit to what Perf does for

254
00:19:37.910 --> 00:19:42.480 
performance based profiling
for CPU-based workloads

255
00:19:43.010 --> 00:19:48.020 
in linux. So this tool is freely
available on GitHub, so you

256
00:19:48.020 --> 00:19:51.030 
can check it out if you're
interested in this topic and what

257
00:19:51.030 --> 00:19:55.380 
this tool does, it executes
a certain workload,

258
00:19:55.850 --> 00:20:00.920 
you can specify it various parameters
such as the measurement interval,

259
00:20:01.250 --> 00:20:05.580 
the number of repetitions that the
experiment should be repeated and then

260
00:20:05.830 --> 00:20:07.950 
of course the actual
workload itself.

261
00:20:09.020 --> 00:20:15.400 
And then it provides you with
energy draws for the individual

262
00:20:16.190 --> 00:20:19.250 
measurements also that have been
available during the execution

263
00:20:19.250 --> 00:20:20.130 
of this workload.

264
00:20:23.400 --> 00:20:29.000 
So we learnt that measuring energy
and power draw is a very

265
00:20:29.000 --> 00:20:34.690 
important aspect in to understanding where
there could be additional potentials for

266
00:20:35.230 --> 00:20:39.580 
increasing energy efficiency of
our systems. So this brought

267
00:20:39.580 --> 00:20:45.030 
us to an additional project
where we realized that

268
00:20:45.780 --> 00:20:51.420 
if you're mildly reducing the clock
frequency of your execution unit, so

269
00:20:51.800 --> 00:20:56.310 
in our case it was GPUs, but
probably it might also apply for CPUs,

270
00:20:56.860 --> 00:21:01.050 
then this reduced clock
frequency results in a

271
00:21:01.620 --> 00:21:03.240 
much lower power draw

272
00:21:04.160 --> 00:21:06.980 
but at just slightly

273
00:21:07.620 --> 00:21:11.860 
increased execution
times. So in effect

274
00:21:12.660 --> 00:21:17.560 
if you have a workload that is not
strictly bound by deadlines,

275
00:21:17.860 --> 00:21:21.840 
and were not the last bit
of performance matters

276
00:21:22.270 --> 00:21:27.350 
then it may be worth to
consider if you can, yeah,

277
00:21:28.250 --> 00:21:36.530 
if you can live without a
little bit of a fewer

278
00:21:37.030 --> 00:21:40.730 
performance at a much
highly increased energy

279
00:21:41.270 --> 00:21:46.540 
efficiency levels. So here is
a little example where we

280
00:21:46.550 --> 00:21:52.720 
ran a fine tuning stage of the BERT
machine learning workload on GPUs

281
00:21:53.460 --> 00:21:57.570 
and on the left hand diagram
you can see the effect on the

282
00:21:57.580 --> 00:22:01.890 
clock frequency of a GPU on the
energy demand of this workload

283
00:22:02.230 --> 00:22:06.290 
and then you can see that
in the bottom right

284
00:22:07.050 --> 00:22:10.780 
edge of the diagram
you can see that

285
00:22:11.780 --> 00:22:16.220 
the reducing the clock frequency
doesn't have a large impact

286
00:22:16.220 --> 00:22:17.530 
on the execution time.

287
00:22:18.300 --> 00:22:21.230 
And if you then again
look at the right hand

288
00:22:21.680 --> 00:22:27.860 
diagram you can see that slightly
reducing the clock frequency

289
00:22:27.860 --> 00:22:32.320 
and the performance of your GPU
significantly lowers the amount

290
00:22:32.320 --> 00:22:37.550 
of energy that is consumed in
order to perform your workload.

291
00:22:38.560 --> 00:22:45.330 
Yeah, so this is also a very interesting
example of how not just

292
00:22:45.700 --> 00:22:49.160 
the kind of machine or the kind
of hardware that you're

293
00:22:49.160 --> 00:22:54.540 
using but also the way how you are configuring
it can affect energy efficiency.

294
00:22:56.110 --> 00:23:01.130 
So this is just a very brief
introduction that's supposed to

295
00:23:01.820 --> 00:23:07.580 
get you interested into
the topic of energy aware computing.

296
00:23:08.110 --> 00:23:11.700 
So we've talked a little bit
about how the growing degree

297
00:23:11.700 --> 00:23:17.610 
of heterogeneity in data centers
can also be exploited

298
00:23:17.960 --> 00:23:23.640 
to use this heterogeneity, to use the
most efficient hardware for each task.

299
00:23:24.420 --> 00:23:28.330 
This of course comes at the cost
that applications need to support

300
00:23:28.450 --> 00:23:32.180 
a larger number of hardware
targets in the future and that

301
00:23:32.190 --> 00:23:35.150 
tooling and infrastructure
needs to improve

302
00:23:35.710 --> 00:23:40.230 
in order to allow this dynamic
placement of jobs according

303
00:23:40.230 --> 00:23:43.390 
to the current situation and the
workload in the data center.

304
00:23:45.170 --> 00:23:48.370 
Then of course we also learned
that being able to quantify

305
00:23:48.370 --> 00:23:52.590 
energy demand of applications
is of course the first

306
00:23:53.240 --> 00:23:58.000 
fundamental step. So you need to
know how much energy is consumed

307
00:23:58.000 --> 00:24:02.020 
by application in order to change
anything about it. So with this

308
00:24:02.910 --> 00:24:05.190 
we think that with
tools like PINPOINT

309
00:24:05.760 --> 00:24:11.820 
it is easier to raise awareness of
how much carbon footprint a certain

310
00:24:12.240 --> 00:24:16.410 
a certain part of code can
cause and that is

311
00:24:16.900 --> 00:24:21.790 
raising awareness, and then of course
this tool also allows developers to

312
00:24:22.120 --> 00:24:28.390 
improve the situation. And if
now energy hotspots are

313
00:24:28.580 --> 00:24:33.030 
identified in an application
base, it can be more easily

314
00:24:33.590 --> 00:24:37.450 
improved and yeah workarounds
can be found to improve

315
00:24:37.460 --> 00:24:38.970 
the overall energy
efficiency.

316
00:24:40.290 --> 00:24:44.960 
And finally we have seen that
if you're willing to sacrifice

317
00:24:44.960 --> 00:24:50.850 
a little bit of performance, it can also
be possible to increase energy efficiency.

318
00:24:51.140 --> 00:24:56.000 
However it remains to be
investigated if

319
00:24:56.000 --> 00:25:00.460 
this approach is applicable to
a wider range of workloads or

320
00:25:00.770 --> 00:25:08.950 
if this is, yeah, only applicable to
fewer hardware and workload configurations.

321
00:25:10.010 --> 00:25:14.890 
So but these are three interested
directions that we are investigating

322
00:25:15.320 --> 00:25:18.270 
and that are
worth looking into.

323
00:25:22.150 --> 00:25:25.530 
So, thank you very much for
listening and watching

324
00:25:26.060 --> 00:25:30.920 
and keep in mind that every
individual counts when you're

325
00:25:31.640 --> 00:25:34.470 
opening your code editor
next time and

326
00:25:35.060 --> 00:25:38.880 
yeah each line of code
can make a difference

327
00:25:39.730 --> 00:25:45.040 
and whether we're going to be successful
in moving to an entirely carbon neutral

328
00:25:45.470 --> 00:25:46.680 
compute infrastructures.

329
00:25:47.910 --> 00:25:49.580 
That's it from me.
Thank you very much.
