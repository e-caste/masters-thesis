WEBVTT

1
00:00:00.540 --> 00:00:00.910 
Hello.

2
00:00:00.910 --> 00:00:01.560 
Welcome.

3
00:00:03.339 --> 00:00:08.259 
Even during the winter of AI, important research results have emerged.

4
00:00:08.839 --> 00:00:18.410 
One of the most exciting findings comes from the diploma thesis of Sepp Hochreiter in 1991. In this thesis,

5
00:00:18.420 --> 00:00:23.350 
he found the important reason why we couldn't train deep neural networks.

6
00:00:23.940 --> 00:00:32.000 
He was actually working on the recurrent neural network which is supposed to be very effective for processing sequential

7
00:00:32.000 --> 00:00:32.460 
data.

8
00:00:33.340 --> 00:00:36.960 
So let's take a look at the working idea of RNN.

9
00:00:39.039 --> 00:00:48.840 
It has similar problems as the multi layer feed forward networks but instead of stocking layers on top of each other, RNN

10
00:00:48.840 --> 00:00:53.759 
just expand itself repeatedly in the time domain.

11
00:00:54.140 --> 00:01:03.729 
So in the output of one layer of you come back concatenate with the input of the next time stamp and being fed

12
00:01:03.729 --> 00:01:05.670 
into the layer again and again.

13
00:01:06.739 --> 00:01:09.250 
And this will create some new states.

14
00:01:09.260 --> 00:01:12.760 
We call them hidden state between each time stamp.

15
00:01:15.140 --> 00:01:21.689 
So, do you still remember we do apply chain rule in the back propagation. Here,

16
00:01:21.689 --> 00:01:30.760 
the problem is when the chain of time steps gets too long then too many times of multiplication will be applied.

17
00:01:31.140 --> 00:01:39.810 
So this will cause gradient managing when the gradient value are too small, smaller than one or cause the gradient exploding

18
00:01:39.819 --> 00:01:44.840 
when the gradients are greater than one.

19
00:01:44.840 --> 00:01:46.250 
More intuitively,

20
00:01:46.640 --> 00:01:53.090 
so let's look at the graphics. This figure shows that Vanilla RNN are on the time stamp

21
00:01:53.090 --> 00:01:53.549 
T.

22
00:01:54.040 --> 00:02:01.269 
The hidden state comes from the previous time step T-1. Concatenate with the input xt

23
00:02:01.269 --> 00:02:01.750 


24
00:02:02.120 --> 00:02:11.900 
then activated by the tanh function and the hidden state will be passed to the next step. In the back propagation

25
00:02:11.909 --> 00:02:21.479 
we have to multiply the derivatives of the tanh function of all the time steps together which actually caused this

26
00:02:21.490 --> 00:02:22.750 
gradient problem.

27
00:02:24.830 --> 00:02:26.439 
To overcome this problem,

28
00:02:26.449 --> 00:02:29.740 
Hochreiter and Schmidhuber proposed

29
00:02:29.740 --> 00:02:29.919 
LSTM

30
00:02:29.919 --> 00:02:30.129 


31
00:02:30.129 --> 00:02:30.409 


32
00:02:30.409 --> 00:02:33.150 
Network - Long Short Term Memory.

33
00:02:34.240 --> 00:02:37.860 
Let's take a look at the most exciting insights of this work.

34
00:02:38.639 --> 00:02:42.030 
First, they introduced the new cell state C.

35
00:02:42.039 --> 00:02:51.020 
To represent the long term information and they also introduced three learnable gate operators to add more control to

36
00:02:51.020 --> 00:02:56.159 
the information flow. Intended to make the network more intelligent,

37
00:02:56.539 --> 00:03:05.960 
we can just ignore those scales for now. Let's look take a look at what exactly makes a difference in the new cell state.

38
00:03:06.639 --> 00:03:09.759 
So we can save long term information.

39
00:03:10.319 --> 00:03:18.680 
The answer is the add operators if you look at in the graphic it avoid multiplication in the back propagation.

40
00:03:19.439 --> 00:03:26.159 
We know that add operator in backward path is like data or information distributor.

41
00:03:27.039 --> 00:03:36.270 
Just let the gradient flow pass through it and being fed into each subsequent pass away and keeping the gradient

42
00:03:36.270 --> 00:03:37.449 
value unchanged.

43
00:03:39.840 --> 00:03:46.849 
Now we just figure out the gradient problem in Oregon. Let's go back to the multi layer feed forward network.

44
00:03:49.139 --> 00:03:55.159 
The reason we can now train deeper network is mainly caused by the nonlinear activation function.

45
00:03:55.539 --> 00:03:57.789 
For example the sigmoid function.

46
00:03:58.439 --> 00:04:06.800 
Please pay attention to the derivative function of sigmoid so we can see that the maximum is 0.25.

47
00:04:06.810 --> 00:04:08.960 
We will recall this value later.

48
00:04:11.039 --> 00:04:13.860 
Let's take the same example as we used before.

49
00:04:14.539 --> 00:04:18.170 
So the target is to calculate the derivative of dc

50
00:04:18.170 --> 00:04:18.699 


51
00:04:18.709 --> 00:04:20.110 
with respect to b1.

52
00:04:20.110 --> 00:04:20.560 


53
00:04:21.639 --> 00:04:34.829 
And for demonstration purpose I just prepared each derivative steps and for simply for dy4 with respect to

54
00:04:34.829 --> 00:04:40.129 
dz four, recall that y4 actually equals sigma z4.

55
00:04:40.139 --> 00:04:47.139 
So its derivative equals the derivative of sigma Z function.

56
00:04:47.139 --> 00:04:56.730 
and z4 to the x4 where z4 is w4 times x4 plus b4 with respect to x4 that

57
00:04:56.740 --> 00:04:58.339 
the derivative is w4.

58
00:04:58.339 --> 00:04:58.850 


59
00:04:59.240 --> 00:05:04.459 
So the subsequent dear aviation steps are performed in a similar manner.

60
00:05:04.839 --> 00:05:07.160 
For now, we just keep overview of them

61
00:05:07.399 --> 00:05:10.339 
for simplicity.

62
00:05:10.339 --> 00:05:14.750 
Let's recall the derivative curve of sigmoid activation function.

63
00:05:15.740 --> 00:05:18.769 
We know that the maximum is 0.25.

64
00:05:18.779 --> 00:05:19.360 
Right?

65
00:05:19.839 --> 00:05:24.550 
And we know that the absolute initial ways are smaller than one.

66
00:05:24.939 --> 00:05:29.759 
We normally will normalize the initial ways from 0-1.

67
00:05:30.339 --> 00:05:34.220 
Thus their product is smaller than 0.25.

68
00:05:34.230 --> 00:05:34.750 
Right?

69
00:05:35.529 --> 00:05:37.689 
Then let's go back to the formula.

70
00:05:37.759 --> 00:05:48.680 
We can see that are three times multiplication of w and the derivative of sigma and each of one hidden layer

71
00:05:48.680 --> 00:05:49.850 
respectively.

72
00:05:49.949 --> 00:05:53.629 
Remember that there are three hidden layers in the network.

73
00:05:53.639 --> 00:05:55.350 
What does this mean?

74
00:05:56.740 --> 00:05:57.939 
It actually means,

75
00:05:57.949 --> 00:06:07.389 
it indicates that the more front of the layer, the slower the gradient change and this is why we were not able to train a

76
00:06:07.389 --> 00:06:12.949 
very deep network previously and now the question is how to solve it.

77
00:06:14.939 --> 00:06:24.959 
Things have gradually turned around. Nair and Hinton proposed ReLU activation function, the rectified linear unit in 2010.

78
00:06:25.540 --> 00:06:28.689 
The advantage of ReLU function is very obvious.

79
00:06:29.240 --> 00:06:37.360 
Look at its derivative, the value is always One for the positive access and zero for the negative.

80
00:06:37.839 --> 00:06:42.550 
It can naturally and effectively avoid the gradient vanishing problem.

81
00:06:43.139 --> 00:06:50.740 
And even now, ReLU is still the most commonly used activation function for the most latest deep neural networks?

82
00:06:50.750 --> 00:06:57.149 
It seems that this should be the beginning of the revival of neural networks.

83
00:06:57.639 --> 00:07:04.449 
On the other hand, we can also figure out that the disadvantage of ReLU is this negative access.

84
00:07:05.240 --> 00:07:07.949 
There is no information can pass through it.

85
00:07:08.009 --> 00:07:10.459 
We call this death value problem.

86
00:07:13.439 --> 00:07:22.579 
So to overcome this problem, they are different derived approach proposed in the following years as a follow up study such

87
00:07:22.579 --> 00:07:27.660 
as leaky ReLU, parametric ReLU and exponential linear unit.

88
00:07:28.139 --> 00:07:35.980 
They all try to enable information flow through the negative access but still keep the derivative of the function as simple

89
00:07:35.980 --> 00:07:36.959 
as possible.

90
00:07:40.040 --> 00:07:42.779 
Time has come to 2012.

91
00:07:42.899 --> 00:07:51.740 
And this year something happened that changed the rules of the game. In this year, probably the first famous deep convolutional

92
00:07:51.740 --> 00:08:02.259 
neural network AlexNet turned out. It won the image net challenge and surpass all the competitors with 10% accuracy improvement

93
00:08:02.839 --> 00:08:12.620 
from the figure, we can see that and clearly see the gap between deep convolutional neural networks and the traditional computer

94
00:08:12.620 --> 00:08:13.759 
vision approaches.

95
00:08:14.139 --> 00:08:21.439 
I personally think that AlexNet kicked off the current AI revolution,

96
00:08:21.439 --> 00:08:26.250 
AlexNet has eight layers, so more than 16 million parameters,

97
00:08:26.259 --> 00:08:36.600 
where the three fully connected layers are count for 94% of the overall parameters, But the five convolutional layer accounted

98
00:08:36.600 --> 00:08:39.350 
for 95% of the total computation.

99
00:08:40.139 --> 00:08:46.669 
The network has been traded in six days on the 2x Media GTX 580 GPUs.

100
00:08:46.799 --> 00:08:49.559 
Don't be scared by the these numbers.

101
00:08:50.139 --> 00:08:51.570 
We can do it much faster

102
00:08:51.570 --> 00:09:00.529 
today. A current example fujitsu lab claimed that they have successfully trained a deep network with 50 layers in just 75

103
00:09:00.529 --> 00:09:03.470 
seconds using 2048 GPUs.

104
00:09:03.470 --> 00:09:03.629 


105
00:09:03.629 --> 00:09:03.789 


106
00:09:03.789 --> 00:09:04.149 


107
00:09:05.039 --> 00:09:12.529 
Nowadays, thanks to the rapid development of hardware acceleration and high performance computing techniques,

108
00:09:12.539 --> 00:09:16.559 
we are able to train such a deep neural network much faster.

109
00:09:18.539 --> 00:09:20.059 
Thank you for watching the radio.
