WEBVTT

1
00:00:00.420 --> 00:00:06.780 
Hi, this talk is about programming
models for massively parallel systems.

2
00:00:07.710 --> 00:00:12.570 
My name is Thilo Maurer
and I work at IBM

3
00:00:13.480 --> 00:00:18.460 
in bubbling in which is running
of our worldwide research labs

4
00:00:18.690 --> 00:00:24.170 
in a department that deals with the
acceleration of power systems

5
00:00:24.340 --> 00:00:29.090 
at the integration of massively parallel
accelerators into this architecture.

6
00:00:30.520 --> 00:00:38.250 
Massive parallel systems, what is it? I
will give you a short interpretation with

7
00:00:38.690 --> 00:00:43.750 
what the term means, what memory models
are employed to realize implementations

8
00:00:44.200 --> 00:00:48.320 
of algorithms using massively
parallel systems. I'll talk about

9
00:00:49.040 --> 00:00:52.220 
the task decomposition
and the challenges that

10
00:00:52.700 --> 00:00:54.470 
you will encounter.

11
00:00:55.180 --> 00:00:59.390 
Then later I'll talk about easy ways
of paralyzing applications and

12
00:00:59.680 --> 00:01:03.240 
go in detail about what multiple programming
models can suppose do recently.

13
00:01:05.860 --> 00:01:11.860 
So massively parallel processing is
the term that is of interest and

14
00:01:13.150 --> 00:01:16.760 
principally it says
what it says. It's

15
00:01:17.400 --> 00:01:23.660 
the distribution of or the processing
over task using an extremely

16
00:01:23.660 --> 00:01:28.530 
large number of independent workers,
which are known as processes. And

17
00:01:30.330 --> 00:01:35.530 
well this is a configuration
between standard processing and

18
00:01:35.810 --> 00:01:40.770 
massively parallel processing, which shifts very
much with the availability of new hardware.

19
00:01:41.450 --> 00:01:44.640 
That's bad for a
definition. I mean

20
00:01:45.130 --> 00:01:49.410 
the feelings that one has the
feeling the programmers have

21
00:01:49.410 --> 00:01:54.510 
in twenty nineteen is like sequential
would be parallelism of one.

22
00:01:54.740 --> 00:01:59.350 
A multicore would be parallelism of less
than sixteen because the largest number of

23
00:01:59.550 --> 00:02:03.650 
CPU was available to use today
is more or less sixteen cores.

24
00:02:04.230 --> 00:02:08.380 
and then there's maybe in the
future systems that are many-core

25
00:02:08.570 --> 00:02:11.420 
that have more than
sixteen cores.

26
00:02:12.240 --> 00:02:17.910 
And well that's a category, that's it's
kind of in the decline right now, since

27
00:02:18.180 --> 00:02:22.590 
this is announced in the
five architecture.

28
00:02:23.420 --> 00:02:25.110 
and then currently

29
00:02:25.950 --> 00:02:30.960 
parallelism system is
more than sixty-four,

30
00:02:31.640 --> 00:02:35.470 
which is massively parallel, but
the main point is that it can

31
00:02:35.470 --> 00:02:39.770 
be much larger than 64, like
thousands and millions. But

32
00:02:40.580 --> 00:02:42.980 
this feeling of these numbers shifts
very much. It's not a clear

33
00:02:42.980 --> 00:02:47.060 
disambiguation of what is massively
parallel and what not. so what is

34
00:02:47.630 --> 00:02:50.830 
actually massively parallel what do
we mean what is the background.

35
00:02:52.400 --> 00:02:56.870 
The background is that also a
transistor is extremely cheap

36
00:02:57.230 --> 00:03:00.310 
and I mean really
extremely cheap.

37
00:03:01.880 --> 00:03:05.970 
People use a lot of them, but there
is a limit to how much you can

38
00:03:06.550 --> 00:03:11.920 
economically fabricated into a
single computer device. So

39
00:03:11.920 --> 00:03:16.260 
the largest devices today are like
twenty, thirty billion transistors.

40
00:03:16.420 --> 00:03:19.240 
But then if you want to
have more than these.

41
00:03:20.220 --> 00:03:24.300 
the cost of these
integrations

42
00:03:24.300 --> 00:03:26.870 
just explode, so that each
point in time there is

43
00:03:27.650 --> 00:03:30.610 
as a limit of what is
economically feasible.

44
00:03:31.220 --> 00:03:35.050 
But and one who will have
to live with this amount.

45
00:03:35.560 --> 00:03:42.000 
But the applications that you
want to run on these are one

46
00:03:42.000 --> 00:03:46.040 
calculate using these transistors
they are different in all

47
00:03:46.040 --> 00:03:50.250 
kinds of. weather they have there's a lot of
applications they are totally different.

48
00:03:50.820 --> 00:03:53.790 
and some of them might
require a different

49
00:03:54.680 --> 00:04:00.460 
set up of these transistors, I mean a different
interconnect between these transistors

50
00:04:02.240 --> 00:04:05.170 
then other applications. So,
for example, there's a

51
00:04:05.630 --> 00:04:07.190 
deep neural
networks which

52
00:04:07.890 --> 00:04:12.850 
do very symmetric processing
of information.

53
00:04:13.450 --> 00:04:17.680 
On the other hand, there are
applications like databases

54
00:04:18.110 --> 00:04:24.220 
which need to deal with a huge
amount of different requests

55
00:04:24.220 --> 00:04:29.190 
at the same time that all may be different
in nature. But still, need to be

56
00:04:29.560 --> 00:04:30.790 
processed in parallel.

57
00:04:31.530 --> 00:04:34.460 
so that means applications have
different demands. and therefore

58
00:04:34.460 --> 00:04:38.160 
it may make sense and it actually
is that that's what people

59
00:04:38.160 --> 00:04:42.780 
do right now. It is the good generate
multiple architectures, compute

60
00:04:42.780 --> 00:04:47.010 
architectures using the
same transistors. so in

61
00:04:47.010 --> 00:04:52.690 
the past there was
regular CPU. as one is

62
00:04:52.690 --> 00:04:57.460 
using in laptops and service
and mobile phones and watches

63
00:04:57.650 --> 00:04:58.750 
and pretty much
everything.

64
00:04:59.710 --> 00:05:02.770 
and then on the other hand there
is this new architecture type

65
00:05:02.780 --> 00:05:08.570 
which is commonly called measurably parallel
systems which were used to different

66
00:05:08.970 --> 00:05:12.850 
compute architecture that
uses a reduced complexity

67
00:05:13.740 --> 00:05:17.210 
for each processor. so that means
there are fewer transistors

68
00:05:17.680 --> 00:05:19.540 
for each of these
processes. but then

69
00:05:20.600 --> 00:05:25.750 
given the still valid
maximum transistor count

70
00:05:25.750 --> 00:05:30.730 
that is economically feasible, you will
have a higher processor count per system.

71
00:05:31.330 --> 00:05:34.380 
in comparison to the more
traditional architecture we try to

72
00:05:34.390 --> 00:05:39.700 
make very strong processes with a high
transistor count each processor.

73
00:05:40.420 --> 00:05:47.140 
and now this is the
disambiguation. when you do

74
00:05:47.440 --> 00:05:53.230 
a special-purpose implementation towards high
core counts of high process accounts,

75
00:05:53.600 --> 00:05:56.820 
then this is what is called a
massively parallel system.

76
00:06:00.510 --> 00:06:01.500 
now what

77
00:06:03.040 --> 00:06:08.670 
permissive parallel systems are available
today. the latest silicon based

78
00:06:08.900 --> 00:06:12.770 
massively parallel system
is in videos tesla GPU

79
00:06:13.290 --> 00:06:18.930 
as after day, the latest generation is
walter a code name we one hundred

80
00:06:19.040 --> 00:06:22.660 
and dismissive parallelism
features five thousand.

81
00:06:23.360 --> 00:06:29.910 
independent workers that can collectively
work on solving a problem.

82
00:06:31.150 --> 00:06:34.740 
they are the leaders in the field
at the moment regarding massive

83
00:06:34.740 --> 00:06:37.950 
parallelism on a single
chip or single cpu.

84
00:06:38.770 --> 00:06:43.220 
and it is possible to then combine
these two larger systems.

85
00:06:43.690 --> 00:06:48.390 
but historically these
are rather new

86
00:06:48.500 --> 00:06:51.400 
of these types of
so-called GPUs.

87
00:06:52.130 --> 00:06:57.020 
and in the past one
didn't have this

88
00:06:57.020 --> 00:07:01.970 
massively parallel chips available. but
nevertheless one required to generate

89
00:07:03.330 --> 00:07:08.130 
large parallelism for
computing large problems.

90
00:07:08.480 --> 00:07:12.090 
one of these machines that could
do so was to have been bleaching

91
00:07:12.090 --> 00:07:16.170 
huge featured up to 1.6
million independent workers.

92
00:07:16.670 --> 00:07:20.880 
but this time that this was a big
machine room filling machine

93
00:07:21.220 --> 00:07:26.250 
with a high a power
consumption of megawatts.

94
00:07:27.030 --> 00:07:30.480 
but in 2010 or 2011,
there wasn't

95
00:07:30.480 --> 00:07:34.690 
the best one
could do. Today

96
00:07:35.610 --> 00:07:40.300 
this has changed a bit
in the sense that

97
00:07:40.310 --> 00:07:44.450 
systems have become more hybrid.
so current installations

98
00:07:44.460 --> 00:07:49.390 
of a big cluster machine have become
hybrid and use a combination of

99
00:07:49.940 --> 00:07:58.310 
classical CPU uses those issues in laptops
end accelerating a massively parallel

100
00:07:59.070 --> 00:08:01.510 
devices, like technology
in GPU use.

101
00:08:03.660 --> 00:08:09.960 
and well there are other massively
parallel systems today, actually

102
00:08:10.350 --> 00:08:14.400 
i'm using one of it which is my
brain. it consists of sixty eight

103
00:08:14.590 --> 00:08:18.550 
billion transistors which
is, no transistors. it's

104
00:08:19.440 --> 00:08:22.030 
neurons and well

105
00:08:23.310 --> 00:08:25.950 
neurons compute
data like maybe

106
00:08:26.650 --> 00:08:31.020 
one doesn't really understand
how in detail that

107
00:08:31.270 --> 00:08:34.120 
the human brain creates the
understanding of things.

108
00:08:34.130 --> 00:08:37.840 
but what one should understands is
how they process information and

109
00:08:37.840 --> 00:08:40.280 
one understands that these
neurons are very tiny

110
00:08:41.120 --> 00:08:46.340 
data processing units that use
electric potentials to communicate

111
00:08:46.810 --> 00:08:50.860 
data further to
neighbouring neurons. and

112
00:08:51.280 --> 00:08:57.030 
this flow of filters
that is generated by

113
00:08:57.630 --> 00:09:03.630 
this connection of neural networks of neurons,
then somehow processes information in

114
00:09:03.970 --> 00:09:09.490 
reduces it to a more or less meaning
full output that can be done

115
00:09:09.750 --> 00:09:13.050 
derived further. so

116
00:09:14.370 --> 00:09:19.110 
fix the aid billion it is a number
that is vastly larger than

117
00:09:19.190 --> 00:09:22.460 
that any that anything one
can currently fabricate in

118
00:09:23.530 --> 00:09:29.620 
a transistors. but still an
incredible amount less then the

119
00:09:29.620 --> 00:09:32.610 
largest massively
parallel systems

120
00:09:33.520 --> 00:09:37.930 


121
00:09:37.930 --> 00:09:41.480 
that one knows today which is
the universe itself and it has

122
00:09:41.980 --> 00:09:44.860 
hundreds of billions of billions
of observable stars. and they

123
00:09:44.860 --> 00:09:46.920 
all calculate something
in parallel.

124
00:09:47.710 --> 00:09:52.480 
mainly I mean the
laws of physics

125
00:09:52.490 --> 00:09:55.390 
they just live it these
laws of physics. but

126
00:09:56.210 --> 00:09:58.130 
someone can say that they
calculate something, but

127
00:09:59.150 --> 00:10:01.840 
what is it doesn't really understand
what it means. maybe in the future

128
00:10:02.380 --> 00:10:05.700 
one will also understand what this
massively parallel system is actually

129
00:10:05.970 --> 00:10:06.580 
computing.

130
00:10:08.650 --> 00:10:14.490 
now back to small things. the task
of the composition is one of

131
00:10:14.660 --> 00:10:19.970 
the most important things that you have to do
when you start are paralyzing applications.

132
00:10:21.200 --> 00:10:26.350 
so an algorithm is usually
made up of a lot of parts.

133
00:10:27.170 --> 00:10:32.220 
and they build upon
each other and

134
00:10:34.830 --> 00:10:36.960 
they use the input of
the previous one.

135
00:10:38.470 --> 00:10:42.300 
and compute some changes and
generate the output and then

136
00:10:42.300 --> 00:10:43.960 
forward this
information to

137
00:10:44.950 --> 00:10:47.680 
work it down

138
00:10:48.420 --> 00:10:50.410 
the train. and

139
00:10:51.050 --> 00:10:54.420 
as you can see on the

140
00:10:55.030 --> 00:10:57.040 
charge under on the
drawing on the right,

141
00:10:57.590 --> 00:11:02.060 
what you can see there is an example
of a typical task decomposition

142
00:11:02.060 --> 00:11:05.050 
where the data comes in from
the top. and then it is a

143
00:11:05.800 --> 00:11:10.560 
process by a few workers and
then goes down to the bottom,

144
00:11:10.770 --> 00:11:13.880 
and information
flows as

145
00:11:14.690 --> 00:11:21.270 
depicted using this so-called
directed acyclic graph.

146
00:11:22.950 --> 00:11:28.630 
but as you can see, there is a
parallelism of like five in this

147
00:11:28.780 --> 00:11:33.710 
depiction. so you have like
fifteen different tasks,

148
00:11:34.040 --> 00:11:37.610 
starting from the top to the bottom
is like fifteen tasks, but

149
00:11:38.040 --> 00:11:40.030 
they all don't be
run in parallel.

150
00:11:40.650 --> 00:11:45.330 
and it's not necessarily the case that you
can always decompose your problem into

151
00:11:45.900 --> 00:11:54.820 
perfectly symmetric tasks that
can be well run in parallel.

152
00:11:55.590 --> 00:12:01.450 
so usually it's the case that there
are more like difficult things that

153
00:12:01.680 --> 00:12:06.250 
are not parallelisable, difficult
subtasks that are not parallel

154
00:12:06.440 --> 00:12:10.110 
realizable. and
therefore

155
00:12:11.190 --> 00:12:17.290 
the best possible
decomposition of a

156
00:12:17.440 --> 00:12:20.450 
specific problem.

157
00:12:21.080 --> 00:12:26.350 
it creates a maximal possible
execution efficiency.

158
00:12:26.760 --> 00:12:32.940 
so in what I'm showing here in this
depiction, you can see there's a

159
00:12:34.180 --> 00:12:41.240 
parallelism of like five, when that means that
the maximum achievable acceleration by this

160
00:12:41.430 --> 00:12:44.550 
penalization would be
like a factor of five.

161
00:12:45.180 --> 00:12:50.120 
but since not all of the workers work at
the same time, they realized if it

162
00:12:51.540 --> 00:12:55.090 
speeds up will be less than
five obviously. I mean

163
00:12:57.230 --> 00:13:01.170 
not all of the tasks, there
are empty spots that the

164
00:13:01.460 --> 00:13:04.360 
where workers are just waiting for
input, but they don't have anything.

165
00:13:04.830 --> 00:13:08.520 
so they are just adding and the speed
up will be less than optimal.

166
00:13:08.840 --> 00:13:12.150 
and that's an innocence

167
00:13:12.630 --> 00:13:16.970 
what is formulated in our models'
law which just says if there

168
00:13:16.970 --> 00:13:19.170 
is only parallelism of
like two in application.

169
00:13:19.800 --> 00:13:22.460 
you will also only be able to
speed up your application by

170
00:13:22.460 --> 00:13:31.610 
a factor of two one. if there's just ninety-five percent
of the application parallelisable, then the remaining

171
00:13:31.610 --> 00:13:36.170 
five percent will need to
remain unparalyzed and only

172
00:13:36.170 --> 00:13:39.100 
ninety five percent can be paralyzed,
but on the other hand, that means

173
00:13:40.070 --> 00:13:43.800 
there are only be able to gain a
factor of twenty in execution

174
00:13:43.800 --> 00:13:46.950 
speed, because five percent
times twenty is one.

175
00:13:47.960 --> 00:13:50.900 
he could only be the totally five
percent they will remain and

176
00:13:51.230 --> 00:13:54.350 
the best case would be that you
reduce ninety five percent

177
00:13:54.360 --> 00:13:57.530 
to zero and the five percent
will remain and that's

178
00:13:57.540 --> 00:13:58.560 
speedup factor
of twenty.

179
00:13:59.680 --> 00:14:03.200 
on the theoretical side, this has been
worked extensively in the past with

180
00:14:03.640 --> 00:14:08.670 
so-called extensions to the
so-called random model which

181
00:14:08.670 --> 00:14:14.240 
is called the parallel random
access memory model.

182
00:14:16.800 --> 00:14:20.910 
well everything is pretty
clear in that sense.

183
00:14:21.440 --> 00:14:28.310 
let's continue a bit about what
hard features are required

184
00:14:28.780 --> 00:14:32.940 
in order to actually execute
tasks in parallel.

185
00:14:33.370 --> 00:14:37.690 
so in the past when one started
with computing, one

186
00:14:37.690 --> 00:14:40.310 
had just a single worker
in a single chip,

187
00:14:40.880 --> 00:14:43.210 
and it means that

188
00:14:44.010 --> 00:14:49.010 
one didn't have the infrastructure
to actually communicate between

189
00:14:49.150 --> 00:14:51.940 
multiple workers in a single field,
because there was nothing like that.

190
00:14:52.100 --> 00:14:53.730 
so one had to create

191
00:14:54.730 --> 00:14:57.980 
a different architecture
that allows

192
00:14:58.560 --> 00:15:02.770 
multiple workers to work on a common
data set. but they still execute

193
00:15:02.770 --> 00:15:07.390 
their own instructions and have
to realize their own tasks.

194
00:15:07.660 --> 00:15:11.920 
and there have been and
are still today three

195
00:15:12.470 --> 00:15:16.320 
process interaction memory
model set up employed.

196
00:15:16.320 --> 00:15:18.630 
The first one is shared
memory processing,

197
00:15:20.950 --> 00:15:26.960 
which means that all of the tasks work
on common knowledge and common memory

198
00:15:27.550 --> 00:15:33.080 
and they use variables to
access these variables

199
00:15:33.080 --> 00:15:39.050 
and then to communicate to the other workers
by modifying these shared variables.

200
00:15:39.620 --> 00:15:43.700 
and then there's the other thing
which is message passing.

201
00:15:44.180 --> 00:15:46.650 
this means that one of the
workers sends a message

202
00:15:47.190 --> 00:15:52.460 
that it be a block of data, like
a hundred bytes and contains

203
00:15:52.780 --> 00:15:57.400 
please do this for me and then
feedback the output there.

204
00:15:57.650 --> 00:16:00.810 
and this message is communicated
to one hundred different worker

205
00:16:00.810 --> 00:16:02.640 
and this worker
will execute

206
00:16:03.110 --> 00:16:07.300 
do what is requested
to do, this is

207
00:16:08.930 --> 00:16:12.690 
logically difficult different
from shared memory processing,

208
00:16:13.270 --> 00:16:20.510 
but today the common way of
realizing a large computer is by

209
00:16:20.980 --> 00:16:25.040 
is using hybrid hardware, which
means that is a mixture of

210
00:16:25.040 --> 00:16:28.040 
shared memory processing
and message passing.

211
00:16:29.520 --> 00:16:31.400 
and I will talk
about these three

212
00:16:32.100 --> 00:16:36.370 
things in detail now.

213
00:16:37.090 --> 00:16:41.980 
so shared memory processing is
usually abbreviated just by SNP.

214
00:16:44.310 --> 00:16:48.120 
not many people just say that shared
memory processing the change

215
00:16:48.130 --> 00:16:49.540 
of just SNP.

216
00:16:50.760 --> 00:16:55.890 
that's the short term, as said
the tasks were with shared

217
00:16:55.900 --> 00:16:59.010 
data. so each of
the tasks sees

218
00:16:59.470 --> 00:17:03.730 
all the information at the same time
and can access these information

219
00:17:03.730 --> 00:17:08.090 
at the same time by processor instructions.
and the communication between these

220
00:17:08.290 --> 00:17:11.780 
tasks is done using common
variables that control and

221
00:17:12.320 --> 00:17:16.900 
still the behaviour of these
set of tasks. and the typical

222
00:17:17.870 --> 00:17:23.190 
programming task is then to realize
this basic graph was shown earlier

223
00:17:23.600 --> 00:17:30.470 
using communication and your
communication variables to steal the

224
00:17:31.090 --> 00:17:35.010 
orchestration of the
execution of the task.

225
00:17:35.550 --> 00:17:38.800 
this shared memory
processing is

226
00:17:39.720 --> 00:17:45.030 
well the most widely used way
of doing parallel processing.

227
00:17:45.660 --> 00:17:50.730 
and it is used in processes multi-socket
servers in mainframes and

228
00:17:51.160 --> 00:17:56.330 
phones and everything that data
processing does in these

229
00:17:56.330 --> 00:17:59.710 
days in this memory
processing.

230
00:18:01.120 --> 00:18:05.020 
typical MPI programmers have
to use Pthreads which

231
00:18:05.020 --> 00:18:09.700 
is a very old topic, but it is
still used today extensively

232
00:18:10.320 --> 00:18:15.600 
and very successful. and building
on these Pthreads is a different

233
00:18:15.600 --> 00:18:19.020 
model called openMP, which is a
little bit easier for programmers

234
00:18:19.020 --> 00:18:23.720 
to use, because it doesn't require deep
knowledge of how operating systems work.

235
00:18:25.350 --> 00:18:27.760 
and in the field of
the message passing,

236
00:18:28.480 --> 00:18:32.320 
as I said the tasks work with
distributed data which

237
00:18:32.320 --> 00:18:35.550 
means that the data
set is divided into

238
00:18:36.030 --> 00:18:39.410 
multiple parts and each of
the workers can only see

239
00:18:40.100 --> 00:18:42.080 
their individual
part of the data.

240
00:18:43.690 --> 00:18:48.110 
and the communication between these
workers is done using explicit

241
00:18:48.110 --> 00:18:51.130 
messages, which means
that one worker uses

242
00:18:52.080 --> 00:18:54.050 
a message consisting of

243
00:18:54.990 --> 00:18:59.420 
an instruction and sending
that to another worker,

244
00:18:59.420 --> 00:19:01.980 
and then this worker
also will receive it

245
00:19:02.530 --> 00:19:05.510 
and do the job that
was requested.

246
00:19:07.580 --> 00:19:13.910 
so this is a method that can be used
when the shared message processing

247
00:19:14.820 --> 00:19:17.990 
is not available. and
it is typically

248
00:19:17.990 --> 00:19:21.400 
used in co-location of multiple
services, like a data center

249
00:19:21.470 --> 00:19:28.830 
where you have wrecks of multiple services in them and they
are connected with network interconnects, like Infiniband

250
00:19:25.020 --> 00:19:28.830 
are connected with network
interconnects, like Infiniband

251
00:19:28.830 --> 00:19:29.880 
and the Internet, or
something else.

252
00:19:30.520 --> 00:19:33.870 
and so this is called something
like clusters today.

253
00:19:34.320 --> 00:19:38.350 
it's a common term the people
use. so these clusters message

254
00:19:38.350 --> 00:19:42.440 
passing is the way to go
and the application MPI.

255
00:19:42.900 --> 00:19:47.580 
It has been survived
until now is

256
00:19:48.180 --> 00:19:54.800 
both MPI and spark. and MPI
is very old, as well.

257
00:19:55.280 --> 00:19:59.630 
so it was one of the oldest
programming models for doing

258
00:20:00.290 --> 00:20:05.460 
parallel processing. and the program
has to do explicit data distribution.

259
00:20:07.080 --> 00:20:10.020 
on the other hand, the
spark which is a very new

260
00:20:10.460 --> 00:20:13.980 
programming model that
doesn't require

261
00:20:13.990 --> 00:20:17.360 
much attention by
the programmer

262
00:20:18.020 --> 00:20:22.510 
for the method passing, because the
message passing is done implicitly by

263
00:20:22.660 --> 00:20:28.240 
the run-time of spark and
the proper execution

264
00:20:28.240 --> 00:20:32.250 
of the tasks requested is then
realized by the so-called

265
00:20:32.250 --> 00:20:34.160 
a resilient
distributed data set

266
00:20:34.920 --> 00:20:38.600 
by another runtime
of spark. and then

267
00:20:40.020 --> 00:20:43.880 
today the most common thing is
actually hybrid hardware. so

268
00:20:44.080 --> 00:20:48.680 
in HPC it is usually the case
that you have large clusters,

269
00:20:48.760 --> 00:20:51.680 
where each cluster has
a shared memory node,

270
00:20:52.670 --> 00:20:57.550 
and each node has a shared
memory subsystem. and

271
00:20:57.930 --> 00:21:02.450 
this shared memory subsystem can be
accessed by the host processors,

272
00:21:03.400 --> 00:21:08.780 
because an attached massively
parallel accelerator

273
00:21:09.230 --> 00:21:11.250 
can either be
attached most

274
00:21:12.070 --> 00:21:15.480 
by shared memory or by message
passing within the same

275
00:21:15.480 --> 00:21:16.900 
node of this cluster.

276
00:21:18.310 --> 00:21:21.780 
so that's the typical
setup today, so the case

277
00:21:21.790 --> 00:21:27.230 
with CPU plus GPU or that's
the case for CPU plus FPGA.

278
00:21:28.090 --> 00:21:33.810 
but the trend today is towards
integrating these accelerators

279
00:21:33.810 --> 00:21:39.820 
like GPU refugees not by message
passing so by means of attaching

280
00:21:39.820 --> 00:21:42.880 
these accelerators to
the processor using

281
00:21:43.660 --> 00:21:47.540 
an external processor bus like
an express. but rather to

282
00:21:47.540 --> 00:21:51.480 
integrate them into
the shared memory

283
00:21:51.480 --> 00:21:56.000 
system of the host processor
using new technologies, like

284
00:21:56.540 --> 00:22:02.180 
linking which allows accelerators
to use the load-store model,

285
00:22:02.700 --> 00:22:06.190 
in that way, the host
processor is accessing

286
00:22:06.780 --> 00:22:11.490 
its own memory. and the MPI can be
used for this hybrid hardware

287
00:22:12.010 --> 00:22:16.320 
today's OpenACC
and OpenCL.

288
00:22:24.590 --> 00:22:28.900 
all is very theoretic until
one then tries actually to

289
00:22:28.900 --> 00:22:33.130 
do a task decomposition, and
one realizes that one can

290
00:22:33.130 --> 00:22:38.550 
run into a lot of difficulties
and create issues in

291
00:22:38.550 --> 00:22:41.030 
the past opposition
that one chooses.

292
00:22:41.710 --> 00:22:45.820 
and the most common thing is a
so-called race condition. so

293
00:22:45.820 --> 00:22:49.780 
this is a typical bug that arises
when there are two threads

294
00:22:49.780 --> 00:22:54.930 
in SMP and for example
the second thread loads

295
00:22:54.930 --> 00:22:57.390 
data before the first

296
00:22:58.150 --> 00:23:02.630 
thread stores the data, which means
that the second thread eventually

297
00:23:02.630 --> 00:23:05.440 
reads the wrong data since the
first one hasn't written it yet.

298
00:23:05.660 --> 00:23:09.300 
and then the result will be that the second
thread does wrong calculations, and then

299
00:23:09.790 --> 00:23:13.650 
things go bad. there
are two ways

300
00:23:14.880 --> 00:23:19.660 
to treat this box. either one
does it explicitly reach

301
00:23:19.660 --> 00:23:21.310 
meaning that the
programmer has to work

302
00:23:21.840 --> 00:23:27.010 
and changes this
application, such that

303
00:23:27.330 --> 00:23:31.060 
the thread request temporary
exclusive access to these

304
00:23:31.060 --> 00:23:35.310 
data sets, to audits parts of
the whole dataset such that it

305
00:23:35.310 --> 00:23:39.850 
is guaranteed that the second thread
always comes after the first thread.

306
00:23:40.290 --> 00:23:45.320 
and then there's more implicit
treatments to this issue

307
00:23:45.640 --> 00:23:47.960 
which is by support of
programming languages.

308
00:23:48.370 --> 00:23:52.430 
for example there is this new
pony language which provides

309
00:23:52.430 --> 00:23:57.740 
a lot of guarantees to the programmer
like race free and deadlock-free.

310
00:23:58.630 --> 00:24:03.120 
although the language itself
is still compatible,

311
00:24:04.170 --> 00:24:07.220 
for example, it
is not the case

312
00:24:07.820 --> 00:24:10.370 
with all the programming
languages like c and c++.

313
00:24:11.390 --> 00:24:14.060 
the other thing
is deadlocks,

314
00:24:14.720 --> 00:24:16.310 
well and deadlocks,

315
00:24:17.990 --> 00:24:21.080 
they are actually pretty
easy to understand.

316
00:24:21.540 --> 00:24:25.940 
when they have multiple
properties and you have

317
00:24:26.400 --> 00:24:30.580 
mutually blocked resources between
them, or let's say one of

318
00:24:31.130 --> 00:24:34.470 
the processes one wants
to use the monitor and

319
00:24:34.470 --> 00:24:37.130 
the other process is currently using
the printer, but they need to do

320
00:24:37.130 --> 00:24:42.160 
exclusively for some case,
and then at some point that

321
00:24:42.370 --> 00:24:47.450 
the monitor will also want to
access the print exclusively,

322
00:24:47.870 --> 00:24:51.460 
then the first one would be
waiting in the second place, but

323
00:24:52.990 --> 00:24:56.870 
the situation is not going to
be resolved by itself and

324
00:24:56.870 --> 00:24:59.960 
the only way to
escape that

325
00:25:01.120 --> 00:25:05.270 
his problem is something called
pre-emption which just means

326
00:25:05.430 --> 00:25:09.560 
to have a third
the person

327
00:25:10.180 --> 00:25:15.260 
in the game that a controlled
access to these devices and then

328
00:25:15.570 --> 00:25:22.870 
just pause his execution of
one of these processes,

329
00:25:23.070 --> 00:25:28.110 
to allow transfer of rights to one
of these processes until this one,

330
00:25:28.270 --> 00:25:33.130 
so that this one can complete its job
and then once this one is done,

331
00:25:33.640 --> 00:25:36.640 
then to resume is the
second process and

332
00:25:37.590 --> 00:25:38.980 
they won't get
into conflict.

333
00:25:41.280 --> 00:25:44.900 
then the third thing that
usually creates problems

334
00:25:44.940 --> 00:25:50.330 
with paralyzing applications that can be
resolved by so-called critical sections.

335
00:25:50.700 --> 00:25:55.660 
so for example, when a program instructs
the sequence of instructions.

336
00:25:56.010 --> 00:26:00.410 
it is usually the case that
only the combination

337
00:26:00.640 --> 00:26:05.980 
of these instructions make well
logical sense. for example

338
00:26:06.130 --> 00:26:08.700 
a creating a user and then
changing its password,

339
00:26:09.210 --> 00:26:14.800 
to initialize value, but it is a two-step
process but isn't an essential thing.

340
00:26:14.940 --> 00:26:18.690 
and if the
second thread

341
00:26:20.060 --> 00:26:25.730 
watches this process
from the outside and

342
00:26:27.300 --> 00:26:31.690 
and uses the
password before

343
00:26:32.230 --> 00:26:35.560 
the second part of the first thread
is done, it is the meaning that

344
00:26:35.570 --> 00:26:39.280 
the password is actually set, then also
a bad thing is going to happen. so

345
00:26:40.560 --> 00:26:46.090 
it needs to be the case that there
are groupings of instructions that

346
00:26:46.520 --> 00:26:50.990 
must be executed as can one
piece, although looking at them

347
00:26:50.990 --> 00:26:54.050 
in detail, they are actually
multiple steps of the same thing.

348
00:26:54.560 --> 00:27:00.480 
so one needs to have a
guarantee that the second

349
00:27:00.480 --> 00:27:04.340 
process can't watch the first process
while executing these steps.

350
00:27:05.780 --> 00:27:12.350 
and the method is used
to treat this problem

351
00:27:12.350 --> 00:27:16.390 
is to use so called critical
sections which essentially just a

352
00:27:16.600 --> 00:27:22.380 
mark these stream of group instruction,
says it is a single block.

353
00:27:23.130 --> 00:27:26.650 
and

354
00:27:27.690 --> 00:27:32.660 
one successfully can make these
transactions look like one thing by using

355
00:27:33.120 --> 00:27:38.970 
two things, either
one using

356
00:27:40.350 --> 00:27:44.890 
requesting temporary access to the
variables used by the first thread.

357
00:27:45.190 --> 00:27:47.810 
such as a second threat can't read
or write to them, it has to

358
00:27:47.810 --> 00:27:49.540 
wait until they
become available.

359
00:27:50.310 --> 00:27:54.700 
and that resolves a problem or
a more modern way is to use

360
00:27:54.700 --> 00:27:59.270 
so-called transactional memory which
is a hardware feature. but that

361
00:27:59.620 --> 00:28:07.290 
virtually attributes are the
duplicate applications of the

362
00:28:07.540 --> 00:28:12.290 
memory space to these threats,
so they can seemingly work on

363
00:28:12.830 --> 00:28:17.830 
different data sets, although
it's the same dataset. and

364
00:28:17.830 --> 00:28:22.440 
once conflicts arise, the
hardware guarantees

365
00:28:22.840 --> 00:28:29.450 
the correct order of execution
which is actually fulfilled

366
00:28:29.500 --> 00:28:31.870 
through its defined by
these critical sections.
