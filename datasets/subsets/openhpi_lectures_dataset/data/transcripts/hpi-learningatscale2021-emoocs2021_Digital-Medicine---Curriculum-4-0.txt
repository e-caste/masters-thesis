WEBVTT

1
00:00:00.510 --> 00:00:03.280 
okay. so hi everyone my name is Kerstin.

2
00:00:04.530 --> 00:00:08.960 
but -- and I would like to welcome you to the session on

3
00:00:08.960 --> 00:00:12.980 
digital medicine and curriculum 4.0 today.

4
00:00:12.980 --> 00:00:17.100 
we want to talk about digital health education.

5
00:00:17.100 --> 00:00:23.570 
and recent advancements in the development of
online programs aiming to teach digital
and AI competences.

6
00:00:24.100 --> 00:00:31.850 
we invited four guests as speakers and
panelists. Unfortunately one of them

7
00:00:32.320 --> 00:00:35.910 
namely professor doctor Anne Hermann-Werner had to cancel her

8
00:00:35.910 --> 00:00:39.970 
participation. she had an acute surgery yesterday and she is

9
00:00:39.970 --> 00:00:42.250 
very sorry that you can't be here today.

10
00:00:42.890 --> 00:00:47.820 
I will kind of replace her with
a short presentation on an online learning

11
00:00:48.220 --> 00:00:51.610 
program that I developed together with the AI campus.

12
00:00:52.210 --> 00:00:57.560 
the structure today is as follows.
we will start with short impulse talks

13
00:00:58.090 --> 00:01:02.690 
which will be last about roughly seven minutes.

14
00:01:02.690 --> 00:01:07.270 
Afterwards you can ask one or two questions
directly linked to the talk.

15
00:01:07.720 --> 00:01:13.010 
and then after we had our presentations,
we have the panel discussion

16
00:01:13.010 --> 00:01:17.880 
and you are very welcome to
contribute to share your experience

17
00:01:17.880 --> 00:01:22.180 
or to ask questions, if you want to say something,

18
00:01:22.610 --> 00:01:23.950 
you can directly speak.

19
00:01:25.430 --> 00:01:28.380 
Because it's not to such a large number here.
But you can also raise

20
00:01:28.380 --> 00:01:30.200 
your hands or write in the chat.

21
00:01:31.390 --> 00:01:33.000 
you have anything to add?

22
00:01:34.350 --> 00:01:36.120 
This was a very nice introduction,

23
00:01:36.790 --> 00:01:37.520 
so thanks a lot.

24
00:01:38.770 --> 00:01:44.430 
yeah, I think we then soon will proceed
with introducing our or panelists right.

25
00:01:46.040 --> 00:01:49.530 
Yes, so the first talk will be given by
professor doctor Sebastian Kuhn.

26
00:01:50.330 --> 00:01:51.470 
very happy that you're here.

27
00:01:53.450 --> 00:01:59.990 
the title of the talk is medicine
in the digital age transformation by education.

28
00:02:00.260 --> 00:02:03.040 
since 2020 he is a professor for digital

29
00:02:03.040 --> 00:02:06.720 
medicine at the medical faculty at Bielefeld university.

30
00:02:07.350 --> 00:02:12.000 
He as a medical background and as an orthopedic and trauma surgeon.

31
00:02:13.740 --> 00:02:17.500 
His research focus very much on topics we are also interested

32
00:02:17.510 --> 00:02:22.390 
in today, namely educational aspects of digital
transformation and medicine.

33
00:02:22.640 --> 00:02:27.220 
and he is also very active in shaping medical education in germany.

34
00:02:27.380 --> 00:02:31.770 
For example he is the deputy chairman for digitalisation technology

35
00:02:31.770 --> 00:02:36.310 
assisted learning and teaching at the german
society for medical education.

36
00:02:36.910 --> 00:02:37.870 
Thanks that you're here.

37
00:02:38.930 --> 00:02:41.430 
Thank you for the invitation, I look forward for today.

38
00:02:43.380 --> 00:02:48.680 
Okay, then I'm happy to introduce professor Anna Moen.

39
00:02:49.270 --> 00:02:53.430 
So I hope I did the pronunciation nearly right
we practiced it before.

40
00:02:54.330 --> 00:03:00.430 
So she's a full professor at the faculty of medicine
at the university of Oslo,

41
00:03:01.070 --> 00:03:07.330 
and adjunct professor in the norwegian

42
00:03:07.570 --> 00:03:08.050 
centre for health research in tomsk.

43
00:03:09.610 --> 00:03:17.010 
she has led the design and deployment of
several digital learning resources including MOOCs.

44
00:03:18.010 --> 00:03:23.450 
So, two examples to mention is a systematic clinical assessment

45
00:03:23.450 --> 00:03:29.850 
to strengthen interdisciplinary collaboration
and patient safety in primary care.

46
00:03:30.450 --> 00:03:35.840 
For me this interdisciplinary aspect is a very interesting one,

47
00:03:36.160 --> 00:03:40.160 
because a lot of what we do actually almost everything we do in digital

48
00:03:40.160 --> 00:03:43.760 
house is somehow interdisciplinary.

49
00:03:44.210 --> 00:03:47.440 
That's why I find it so important to have it as a MOOC.

50
00:03:48.030 --> 00:03:53.780 
so that many can learn from that.
And the second exemplary MOOCs from her is

51
00:03:53.920 --> 00:03:58.790 
how to write a phd proposal.
And I'm sure, if you have guests

52
00:03:58.790 --> 00:04:01.960 
today that are at the beginning of their scientific research.

53
00:04:02.360 --> 00:04:07.520 
Such a MOOC would be very very helpful.
so very happy to have you here.

54
00:04:07.520 --> 00:04:11.620 
Welcome and we're all looking

55
00:04:12.400 --> 00:04:13.450 
forward to your input presentation later.

56
00:04:15.100 --> 00:04:18.120 
And of course to your contributions in the discussion.

57
00:04:19.190 --> 00:04:20.680 
Thank you! Thank you for having me!

58
00:04:23.320 --> 00:04:26.950 
Oh, so then I'm going briefly to introduce myself as the search

59
00:04:26.950 --> 00:04:29.790 
speaker and replacement of professor Anna Hammon Bana.

60
00:04:30.340 --> 00:04:34.360 
So I'm junior professor for computation neuroscience charity and

61
00:04:34.640 --> 00:04:40.430 
it's medecine belin and I'm leading a group on machine learning
and clinical neuroimaging.

62
00:04:40.640 --> 00:04:45.960 
I don't have a medical background,
but a background in mathematics and machine learning

63
00:04:46.580 --> 00:04:52.100 
and yes and acquired some experiences.
Now an education of medical students with respect to AI topics.

64
00:04:52.100 --> 00:04:55.720 
And I'm going to show you

65
00:04:56.380 --> 00:05:01.650 
some works here and then the last talk will be given by a Jenny Brandt.

66
00:05:01.960 --> 00:05:07.820 
She is a medical student eager to learn about AI
and assault some experiences

67
00:05:08.050 --> 00:05:13.570 
here already she studies at university minds.
she is co-founder

68
00:05:13.570 --> 00:05:16.500 
and head of the project digital medicine within the federal

69
00:05:16.500 --> 00:05:21.240 
association of medical students in germany
and it is involved in several project

70
00:05:21.390 --> 00:05:25.800 
working groups on the topic of AI medicine
and university teaching.

71
00:05:26.120 --> 00:05:31.230 
We also met via the AI campus
and I think it's very good that

72
00:05:31.230 --> 00:05:34.580 
you bring in your student perspective here today.

73
00:05:34.580 --> 00:05:37.780 
so, thank you very much for being here today.
Thanks for the invitation.

74
00:05:38.840 --> 00:05:41.830 
So then we start with a talk by sebastian kuhn.

75
00:05:42.900 --> 00:05:43.740 
Thank you kerstin.

76
00:05:45.080 --> 00:05:48.960 
I'm going to talk about medicine the digital age and how education

77
00:05:48.960 --> 00:05:53.250 
really can advance the really big transformation process we

78
00:05:53.250 --> 00:05:58.330 
have ahead of us. so medicine or the profession of being a doctor

79
00:05:58.330 --> 00:06:02.790 
is one of the oldest professions of mankind you can go back in history

80
00:06:03.170 --> 00:06:07.370 
for decades centuries and even more and always find a doctor,

81
00:06:07.370 --> 00:06:11.530 
and also medical education goes back quite a long time.

82
00:06:11.530 --> 00:06:15.110 
I would  like to start my talk with going back in history
to 1296,

83
00:06:15.800 --> 00:06:20.170 
where we find a very good description what it takes to be a

84
00:06:20.170 --> 00:06:23.260 
doctor? what it takes to be a surgeon and magna?

85
00:06:23.960 --> 00:06:28.170 
It says the competency profile a surgeon should have

86
00:06:28.170 --> 00:06:31.960 
well-formed hands, long slender fingers, strong body,

87
00:06:32.470 --> 00:06:36.520 
not be inclined to tremble be well grounded in natural science

88
00:06:36.560 --> 00:06:40.290 
and should know not only medicine, but every part of philosophy.

89
00:06:41.040 --> 00:06:45.990 
So from this quote. we move into the future more than seven

90
00:06:45.990 --> 00:06:49.230 
hundred years into the future and say well.

91
00:06:49.230 --> 00:06:54.770 
this competency profile. I would still say.
It's a great profile even for today's doctors.

92
00:06:55.180 --> 00:06:57.730 
But it is missing something.

93
00:06:58.390 --> 00:07:01.690 
and you might have noticed that the last change

94
00:07:01.690 --> 00:07:05.140 
set people grounded in natural science not only medicine,

95
00:07:05.140 --> 00:07:09.680 
but every part of philosophy and have digital competencies,
because I truly believe

96
00:07:09.960 --> 00:07:13.810 
that digital competence will be essential to be a good doctor

97
00:07:13.830 --> 00:07:15.700 
in 2021 and beyond.

98
00:07:16.350 --> 00:07:22.030 
so how is medicine changing?
and we have very very many different changes

99
00:07:22.360 --> 00:07:25.610 
happening at the moment and a lot of them are technology driven.

100
00:07:26.390 --> 00:07:30.820 
But one of the biggest drivers of change are patients themselves.

101
00:07:31.110 --> 00:07:35.040 
and we know nowadays that most of the diagnostic processes

102
00:07:35.190 --> 00:07:39.380 
do not start in GPS office or in emergency room,

103
00:07:39.440 --> 00:07:44.020 
but actually happen by patients themselves
before they see me as a doctor.

104
00:07:44.100 --> 00:07:46.900 
They usually see doctor google. so about seventy percent of

105
00:07:46.930 --> 00:07:50.450 
patients already started diagnostic process ahead of that.

106
00:07:50.450 --> 00:07:52.300 
and that is not only true in their home.

107
00:07:52.930 --> 00:07:57.010 
this is a situation, I encountered about two years ago when

108
00:07:57.010 --> 00:08:00.910 
I was doing my trauma round as an attending.
and the patient

109
00:08:00.910 --> 00:08:05.590 
I took care of she's an elderly lady with a pelvic fracture.

110
00:08:06.170 --> 00:08:11.040 
she had the currents three guideline on osteoporotic treatment

111
00:08:11.040 --> 00:08:14.830 
after fractures on her iphone, and her husband was googling

112
00:08:14.840 --> 00:08:18.150 
public ring fractures and asking if that's the injury

113
00:08:18.880 --> 00:08:23.300 
his wife is having. so the inform patient became a normality

114
00:08:23.450 --> 00:08:26.960 
and at the same time doctors are still struggling with accepting

115
00:08:27.010 --> 00:08:31.410 
inform patients or also misinform patients.
so this is a big driver of change.

116
00:08:31.480 --> 00:08:33.950 
But it's also a big challenge.

117
00:08:35.020 --> 00:08:38.960 
Absent smart devices, I think it's really really very

118
00:08:39.700 --> 00:08:43.820 
like a big driver at the moment.
and it started out kind of like as

119
00:08:44.620 --> 00:08:46.640 
can like getting intersection towards like

120
00:08:47.590 --> 00:08:52.240 
physical fitness and health.

121
00:08:52.240 --> 00:08:53.570 
But it really becomes part of real medicine at the moment.

122
00:08:54.320 --> 00:08:57.950 
And in germany since last year we can prescribe apps,

123
00:08:58.710 --> 00:09:05.160 
such as instead come I normally.
We're only able to prescribe medications or

124
00:09:05.330 --> 00:09:08.680 
shoe installs and other things,
but since about nine months ago,

125
00:09:08.680 --> 00:09:12.180 
we actually can prescribe abs for treatments.

126
00:09:12.640 --> 00:09:16.870 
and this enables very many different treatment options and

127
00:09:16.870 --> 00:09:18.940 
it is not just games.

128
00:09:20.300 --> 00:09:26.380 
For me my main focus besides qualifying a doctor for the digital

129
00:09:26.380 --> 00:09:31.240 
future is really also implementing change and when covid nineteen

130
00:09:31.240 --> 00:09:35.390 
happened for me. It was a big motivation to actually use apps

131
00:09:35.390 --> 00:09:39.660 
and smart devices to improve acute patient treatment patients

132
00:09:39.660 --> 00:09:42.920 
who are suffering from acute covid infections in their home.

133
00:09:43.170 --> 00:09:46.890 
Because a telephone call is not good enough for telemedicine

134
00:09:46.890 --> 00:09:51.430 
nowadays, we cannot detect when people are getting a viral pneumonia

135
00:09:51.620 --> 00:09:54.440 
or when they are getting trump embolisms, which are the most

136
00:09:54.760 --> 00:09:58.260 
common causes of death in covid nineteen acute and also at the

137
00:09:58.260 --> 00:10:01.350 
moment you hear a lot about post- acute kohut syndromes

138
00:10:01.840 --> 00:10:06.020 
where we can introduce with apps we can ask patientsm,

139
00:10:06.030 --> 00:10:08.650 
we can check symptoms. we can do long note

140
00:10:09.430 --> 00:10:13.040 
screenings with those connected devices and through

141
00:10:13.450 --> 00:10:17.060 
telemedicine we could connect them to the GPS to emergency room

142
00:10:17.160 --> 00:10:21.190 
and also to competency centers and at the same time acquire information

143
00:10:21.360 --> 00:10:25.020 
for research. and this is one of the projects I currently do

144
00:10:25.020 --> 00:10:29.920 
as part of the german network on research.

145
00:10:29.930 --> 00:10:31.780 
So absent smart devices are much more than just gadgets.

146
00:10:32.430 --> 00:10:35.140 
and I think the biggest driver at the moment, I think probably

147
00:10:35.140 --> 00:10:38.950 
also the focus of today's discussion is artificial intelligence,

148
00:10:39.320 --> 00:10:43.510 
because a lot of things when we do diagnostic processes do not end

149
00:10:43.510 --> 00:10:47.440 
up with the correct diagnosis. and there speake hope with different

150
00:10:47.440 --> 00:10:51.920 
forms of artificial intelligence to integrate large information

151
00:10:51.930 --> 00:10:57.570 
image analysis to really enhance a doctor's ability

152
00:10:58.060 --> 00:11:01.890 
to make the correct diagnosis.

153
00:11:03.450 --> 00:11:08.610 
so  as kasten mentioned I'm a trauma surgeon
and I'm also a medical educator.

154
00:11:08.890 --> 00:11:11.490 
and about five six years ago I really wondered

155
00:11:12.160 --> 00:11:16.450 
how can we educate doctors for the digital future and at that

156
00:11:16.450 --> 00:11:19.410 
time I did an analysis of the current state of the medical

157
00:11:19.410 --> 00:11:21.790 
schools in germany, austria and switzerland.

158
00:11:22.420 --> 00:11:27.240 
How they introduce apps telemedicine and AI into medical curricula and

159
00:11:27.390 --> 00:11:31.590 
the result of the survey was very short, they don't.

160
00:11:32.530 --> 00:11:37.020 
so we have a really huge change of basically what doctors need

161
00:11:37.020 --> 00:11:40.190 
and what doctors have as tools to treat patients and at the

162
00:11:40.190 --> 00:11:42.620 
same time a lot of the medical schools were not responding.

163
00:11:43.090 --> 00:11:46.600 
so that was my big motivation to start a project called medicine

164
00:11:46.600 --> 00:11:50.350 
in the digital age.
and I would like to give you a brief

165
00:11:50.380 --> 00:11:51.910 
overview of what we're doing.

166
00:11:52.570 --> 00:11:57.410 
so the main thing was basically to add new components to

167
00:11:57.410 --> 00:12:02.380 
the curriculum and we introduced a brief modular curriculum

168
00:12:03.530 --> 00:12:07.230 
with modules on digital communications between doctors and

169
00:12:07.230 --> 00:12:11.690 
patients, but also between doctors a module on apps and smart devices

170
00:12:12.630 --> 00:12:17.000 
on telemedicine on virtual or augmented and robotic technologies

171
00:12:17.110 --> 00:12:21.060 
and finally five and six these are quite interconnected basically

172
00:12:21.060 --> 00:12:24.960 
data rich medicine and intelligent artificial intelligence systems

173
00:12:25.170 --> 00:12:28.380 
really in clinical processes.

174
00:12:30.090 --> 00:12:33.980 
so this was not done through just a lecture series or something

175
00:12:33.980 --> 00:12:37.680 
like that we also want to use innovative didactics

176
00:12:37.680 --> 00:12:40.780 
and we went for blended learning curriculum where a lot of

177
00:12:40.790 --> 00:12:45.660 
the knowledge based aspects.
we introduce through e-learning before hand

178
00:12:46.010 --> 00:12:49.160 
to really have time when we meet at the same time in the same

179
00:12:49.170 --> 00:12:52.750 
room with students and doctors that not one person lectures.

180
00:12:52.750 --> 00:12:57.500 
But rather we learn through experience hands
on really testing digital technologies

181
00:12:57.650 --> 00:13:01.870 
and going into critical discussions amongst students,

182
00:13:01.880 --> 00:13:06.880 
but also exposing to very very different experts
which are also showing the next slide.

183
00:13:07.470 --> 00:13:10.640 
and the final step is basically to really

184
00:13:11.660 --> 00:13:15.160 
try to get them to work together and to think about what kind

185
00:13:15.160 --> 00:13:17.150 
of doctors they want to be and how

186
00:13:17.770 --> 00:13:20.540 
treatment of patients will look into the future and all of

187
00:13:20.550 --> 00:13:24.080 
those research projects end up into e-learning again.

188
00:13:24.080 --> 00:13:27.390 
because a lot of student research projects are quite interesting to

189
00:13:27.390 --> 00:13:28.770 
use for the next semester.

190
00:13:29.500 --> 00:13:34.180 
so this is a curriculum I think you cannot teach solely as a doctor,

191
00:13:34.380 --> 00:13:38.380 
but we introduce different people from the health sector doctor

192
00:13:38.390 --> 00:13:41.780 
psychologist and health care professionals.

193
00:13:41.780 --> 00:13:45.940 
so interpersonal with bert mentioned is very
important different people from the technology

194
00:13:46.350 --> 00:13:52.320 
arena such as medical informatics data analysis
and also startups app developers

195
00:13:52.710 --> 00:13:58.670 
and very important.
I think for ethical legal social aspects introduce ethicist

196
00:13:58.850 --> 00:14:01.700 
data protection experts, but also patients as lecturers.

197
00:14:02.380 --> 00:14:05.980 
because they can tell a very very important story

198
00:14:06.100 --> 00:14:10.800 
of patient continuum and how patient care can
change through digital health.

199
00:14:12.110 --> 00:14:14.090 
so we did quite a lot of

200
00:14:15.220 --> 00:14:19.560 
educational research on that
and I cannot go into too much detail today.

201
00:14:19.890 --> 00:14:24.350 
But in a nutshell it really is not only about technology,

202
00:14:24.350 --> 00:14:27.670 
but really about changing the world new division of labour

203
00:14:27.670 --> 00:14:30.330 
between what doctors do what healthcare professionals do,
but especially

204
00:14:30.330 --> 00:14:33.340 
also what patients do themselves they record data and they

205
00:14:33.340 --> 00:14:37.730 
get insight into data and also new division of labour
between men and machine.

206
00:14:38.480 --> 00:14:41.580 
so this really asked for a different competency profile where

207
00:14:41.580 --> 00:14:45.220 
it's not about memorization just information acquisition like

208
00:14:45.220 --> 00:14:48.640 
it was often in the past with medical education.

209
00:14:48.990 --> 00:14:50.990 
but rather the intelligent use of system

210
00:14:51.830 --> 00:14:56.190 
corporation interdisciplinarity into professionalism and inter-sectoral

211
00:14:56.190 --> 00:15:00.540 
cooperation are very important. and a lot of the AI system really

212
00:15:00.920 --> 00:15:04.160 
rotates around to really make a decision with the numbers.

213
00:15:04.160 --> 00:15:07.710 
you're getting and how you're dealing with probabilities.

214
00:15:07.710 --> 00:15:10.500 
and it really asks for a lot of adaptability,

215
00:15:11.430 --> 00:15:15.310 
a lot of openness for change to really question
your own role as a doctor

216
00:15:15.310 --> 00:15:19.280 
and to have it ongoing to go with the change.

217
00:15:21.410 --> 00:15:26.500 
we had in germany an ongoing reform process
with basically new curricula

218
00:15:26.830 --> 00:15:31.810 
and a lot of them when you went to analysis.

219
00:15:32.050 --> 00:15:33.210 
They did not really have this change  on board.

220
00:15:33.940 --> 00:15:37.120 
they really ignored kind of like the digital change
into a high change.

221
00:15:37.570 --> 00:15:40.720 
and I was glad that we after a lot of discussion were able

222
00:15:40.720 --> 00:15:44.690 
to introduce AI and digital competencies as part of it and

223
00:15:44.690 --> 00:15:48.610 
it made it into the new core curriculum,
however I want to say

224
00:15:48.610 --> 00:15:51.930 
we're not there, yet.
it's still marginal a lot of people

225
00:15:51.930 --> 00:15:54.750 
are still questioning.
A lot of faculties are still struggling

226
00:15:54.750 --> 00:15:58.840 
to introduce it.
so it will be a challenge for a lot of

227
00:15:58.840 --> 00:16:00.610 
place to really qualified doctors.

228
00:16:01.220 --> 00:16:06.750 
and also it's not only a question of medical students,
but also very much important

229
00:16:07.050 --> 00:16:10.890 
about educating the whole health workforce.

230
00:16:10.890 --> 00:16:14.860 
we have more than four hundred thousand doctors.
we have about three million health care professionals.

231
00:16:15.190 --> 00:16:19.290 
they need to be educated and
I did a transfer project where

232
00:16:19.290 --> 00:16:23.580 
we do a project for continuous medical education.

233
00:16:24.570 --> 00:16:29.700 
and I'm also chairing a european doctors ambitions
on digital competencies for doctors.

234
00:16:29.830 --> 00:16:31.750 
so  I think there's a lot of work  for us to do.

235
00:16:33.180 --> 00:16:37.410 
so sometimes it's good to go back in history
and and see what history's teaching us

236
00:16:37.410 --> 00:16:42.570 
and medicine has always been the field

237
00:16:42.740 --> 00:16:46.830 
where we have to transfer technology into purposeful patient treatment.

238
00:16:46.830 --> 00:16:50.790 
and in the last century the introduction of x ray

239
00:16:50.790 --> 00:16:53.530 
was really the biggest technology advancement.

240
00:16:54.330 --> 00:16:58.640 
but the pure technology itself basically the first x ray image in 1899.

241
00:16:58.640 --> 00:17:01.360 
the left hand of miss franklin

242
00:17:01.850 --> 00:17:05.740 
did not directly lead to improvement in patient care.

243
00:17:06.260 --> 00:17:10.310 
In the beginning x ray was often an entertainment for the highest

244
00:17:10.310 --> 00:17:16.310 
society where you would get x rays during a dinner

245
00:17:16.420 --> 00:17:21.470 
or at a fair just for fun and it took quite a while

246
00:17:21.710 --> 00:17:25.960 
and 39 nobel laureates actually did research with x ray.
and thousands millions of doctors had to learn it.

247
00:17:25.960 --> 00:17:30.190 
as a core competency to really make

248
00:17:30.190 --> 00:17:32.860 
sure that every day's patients really benefit from it.

249
00:17:33.350 --> 00:17:37.360 
and this basically transfer of technology to purposeful

250
00:17:37.820 --> 00:17:42.360 
patient treatments. the same challenge we have
at the moment with digital technologies.

251
00:17:43.490 --> 00:17:48.430 
so I think covid 19 is a big challenge at the moment.

252
00:17:48.430 --> 00:17:51.370 
but it's also a great opportunity for change and

253
00:17:52.220 --> 00:17:56.070 
I think this quote resembles.
it quite well and

254
00:17:56.070 --> 00:18:00.740 
says times have changed greatest danger
is to act with yesterday's logic.

255
00:18:00.770 --> 00:18:05.770 
I think we should use the opportunity
to we have at the moment.

256
00:18:05.910 --> 00:18:09.410 
where things really change in the medical system to really

257
00:18:09.410 --> 00:18:14.020 
use it for the benefit of the patient and to really question,

258
00:18:14.590 --> 00:18:16.120 
if the current status quo was the right one.

259
00:18:16.940 --> 00:18:21.230 
and one person who really did it. I think was kasparov when

260
00:18:21.230 --> 00:18:25.640 
he lost against deep blue about 25 years ago in chess.

261
00:18:26.040 --> 00:18:30.850 
and kasparov the reigning chess world champion
at the time did not resign.

262
00:18:31.050 --> 00:18:37.950 
but he rather use this for learning example and
he used basically also artificial intelligence

263
00:18:38.430 --> 00:18:43.440 
to combine it with his own intelligence
and to play chess as a hybrid.

264
00:18:43.770 --> 00:18:48.640 
and as a hybrid he was undefeated
by for quite a while by artificial

265
00:18:48.640 --> 00:18:52.930 
intelligence such as deep blue.
so to combine your human knowledge

266
00:18:53.880 --> 00:18:58.500 
with basically digital or artificial knowledge I think is the

267
00:18:58.500 --> 00:19:01.680 
way we have to think into the future.
we have to strengthen

268
00:19:01.680 --> 00:19:06.080 
our human abilities our communication skills seeing hearing

269
00:19:06.080 --> 00:19:11.320 
and our intelligence. but at the same time
to integrated with artificial intelligence.

270
00:19:11.820 --> 00:19:16.050 
so I think a lot of,
when we think about patient journeys of futures,

271
00:19:16.390 --> 00:19:19.930 
we still the core will still be the human interaction, kind

272
00:19:19.930 --> 00:19:23.290 
of like patients doing research themselves interactions in

273
00:19:23.300 --> 00:19:27.650 
gp offices or medical experts,
such as radiologist and pathologist

274
00:19:27.650 --> 00:19:31.450 
using their senses to make diagnosis,
but at the same time at the same step.

275
00:19:31.450 --> 00:19:34.620 
we will use app we will use clinical decision

276
00:19:34.760 --> 00:19:38.610 
support system and different AI technologies to improve our

277
00:19:38.610 --> 00:19:40.930 
skills and to improve our decision making.

278
00:19:41.520 --> 00:19:44.700 
and this is what we currently also try to introduce in the

279
00:19:44.700 --> 00:19:48.110 
medical curriculum with one of the AI campus projects，

280
00:19:48.110 --> 00:19:52.640 
also in cooperation with openHPI would
basically do blended learning

281
00:19:52.640 --> 00:19:57.910 
curriculum for medical school.
we do hackathon formats very interdisciplinary.

282
00:19:58.050 --> 00:20:04.560 
and we will have an mooc of an openHPI
towards the end of the year.

283
00:20:05.670 --> 00:20:10.440 
so I come to the end of my talk
with a not easy translate question.

284
00:20:10.500 --> 00:20:14.540 
and it says basically
how will the future of medicine look like?

285
00:20:17.040 --> 00:20:19.970 
the united of vinci the human man was

286
00:20:21.220 --> 00:20:25.360 
often or is a very big symbol
for the profession of doctors

287
00:20:25.360 --> 00:20:29.210 
and medicine in general.
and I think this will look different

288
00:20:29.210 --> 00:20:34.790 
in the future. and I think it will look similar to
that I think a future doctor

289
00:20:35.250 --> 00:20:38.300 
will have to have excellent abilities with core

290
00:20:38.910 --> 00:20:44.390 
competencies. we have used for centuries like
how to use a stethoscope?

291
00:20:44.400 --> 00:20:48.130 
how to use a scalpel？
but at the same time we have to understand

292
00:20:48.130 --> 00:20:52.220 
the interconnection of data apps
and artificial intelligence.

293
00:20:52.350 --> 00:20:56.530 
use both hands use both arms to really ensure patients are

294
00:20:56.530 --> 00:20:59.700 
getting the best care possible.
Thank you very much!

295
00:21:02.520 --> 00:21:06.920 
Thank you, excellent presentation.
very interesting and very interesting perspective.

296
00:21:07.390 --> 00:21:09.520 
Thanks. so are there questions?

297
00:21:17.160 --> 00:21:20.630 
I'm questioned to the curriculum and so

298
00:21:21.500 --> 00:21:26.050 
here at HPI we have a digital health master program

299
00:21:26.720 --> 00:21:28.270 
and we accept students from

300
00:21:28.970 --> 00:21:33.040 
the medical field and also students from my team.

301
00:21:34.130 --> 00:21:39.350 
and then so it's a two year master program that the students

302
00:21:39.420 --> 00:21:44.950 
do in addition to their as standards education right
from medical or from the IT field.

303
00:21:45.510 --> 00:21:51.490 
so the curriculum, you showed us would this be a similar

304
00:21:51.500 --> 00:21:55.340 
kind of additional program or do you foresee that this should

305
00:21:55.350 --> 00:22:00.890 
be actually part of the medical education.
and if yes, then I

306
00:22:00.890 --> 00:22:05.670 
mean we all know that medical education
already takes quite some time.

307
00:22:06.050 --> 00:22:10.700 
because they have to learn a lot.
so I mean would this than not

308
00:22:11.160 --> 00:22:16.120 
increase their in old learning time to an extent that you know

309
00:22:16.120 --> 00:22:19.180 
there might be quite always they start practicing ?

310
00:22:19.920 --> 00:22:22.460 
yeah, I think that's a very good question.

311
00:22:23.430 --> 00:22:25.640 
I think what we're doing I think it's basically

312
00:22:26.220 --> 00:22:30.990 
a similar direction.But at a very different scale.
I think what I do within

313
00:22:31.190 --> 00:22:34.810 
like the core curriculum or we do so as long as the curriculum

314
00:22:35.170 --> 00:22:39.060 
is a very basic qualification in that sense.
I think every doctor

315
00:22:39.180 --> 00:22:43.960 
needs to understand like basic principles how to integrate

316
00:22:43.960 --> 00:22:48.560 
this into diagnostic and treatment decisions,
such this is kind of like.

317
00:22:48.890 --> 00:22:52.780 
I think a core requirement as a basic qualification.

318
00:22:52.780 --> 00:22:54.280 
I think  what you do it openHPI is very much more advanced.

319
00:22:55.030 --> 00:23:00.080 
So I think we need a basic qualification for everybody.

320
00:23:00.540 --> 00:23:04.430 
and then we need some like digital health master programs for people

321
00:23:04.580 --> 00:23:07.320 
who have a big focus on that and I think.

322
00:23:08.230 --> 00:23:11.220 
I'd like to think in basically like three levels and

323
00:23:12.040 --> 00:23:15.270 
what I show now was like the basic level, then like special course.

324
00:23:15.280 --> 00:23:18.740 
we do say with the AI campus that it's a little bit

325
00:23:18.740 --> 00:23:23.740 
more than that and if people really want to focus on that.

326
00:23:24.880 --> 00:23:29.420 
I really tell them basically go to them
and do a masters in that.

327
00:23:30.160 --> 00:23:33.260 
Because this is really if you want to do a deep dive into it.

328
00:23:34.560 --> 00:23:38.120 
A short follow up question you short covid nineteen project

329
00:23:38.570 --> 00:23:43.380 
where it was about combining verbals and maybe IT devices with

330
00:23:43.780 --> 00:23:45.260 
clinical data we do similar

331
00:23:45.870 --> 00:23:50.530 
stuff at other projects. and we see the big challenge that

332
00:23:50.540 --> 00:23:54.950 
even combining clinical hospital
information systems is difficult.

333
00:23:54.950 --> 00:23:57.690 
because we have so many data silos.

334
00:23:58.610 --> 00:24:00.970 
these days and  we now propose that actually at

335
00:24:01.620 --> 00:24:03.010 
additional data sources.

336
00:24:03.760 --> 00:24:06.280 
the problem does not get smaller right?

337
00:24:07.090 --> 00:24:10.610 
and so we discussing this,
there's a lot of, you know, how you know

338
00:24:10.620 --> 00:24:13.730 
making interoperability
between hospital information systems.

339
00:24:13.730 --> 00:24:17.330 
But I have the feeling that
no one is really prepared for the

340
00:24:17.340 --> 00:24:21.330 
future patients that come with
the apple watch to the cardiologists

341
00:24:21.440 --> 00:24:24.180 
and say please, can I have a check up?
because my watch tells me that there might be a problem.

342
00:24:24.180 --> 00:24:28.510 
so I mean how do we see this?

343
00:24:28.510 --> 00:24:33.290 
you  know big change that's front of us.

344
00:24:34.410 --> 00:24:38.490 
but no one seems to  discuss it.
except maybe we when we have those kind of panels

345
00:24:38.490 --> 00:24:42.190 
like they have today.
yeah, I think I mean the integration

346
00:24:42.190 --> 00:24:48.300 
of data into basically not like a hundred silos
or ten thousand silos two is a big challenge.

347
00:24:48.700 --> 00:24:51.860 
and it's an ongoing challenge and also we

348
00:24:51.940 --> 00:24:56.370 
as like a heart package besides actually treating

349
00:24:56.370 --> 00:25:00.960 
patients with acute and post- acute covid 19.
we try to link with the

350
00:25:01.140 --> 00:25:07.990 
the MI initiative to address like the special aspect of
patient generated data through

351
00:25:08.360 --> 00:25:12.840 
patient reports or also sensor data by patients integration.

352
00:25:13.360 --> 00:25:17.730 
But I think,
this is a long term project

353
00:25:17.980 --> 00:25:21.960 
and it's very difficult at the same time.
I think with the current situation with covid 19 for me

354
00:25:22.740 --> 00:25:26.680 
last year in March.
I was still very busy organizing things

355
00:25:26.680 --> 00:25:31.630 
in the emergency room and
were struggling with kind like are we

356
00:25:31.630 --> 00:25:35.050 
gonna have enough ventilators
and enough ICU bits.

357
00:25:35.050 --> 00:25:40.220 
But at that time I really realized that
we will have really to test for feet

358
00:25:40.610 --> 00:25:44.530 
not in the hospitals,
But actually in the patients own home.

359
00:25:44.810 --> 00:25:48.870 
because ninety five percent of
the patients are treated at home,

360
00:25:49.660 --> 00:25:54.680 
and basically when they saturate
when they get to viral pneumonia

361
00:25:54.680 --> 00:25:57.910 
and when they get to thrum embolism
from acute covid nineteen.

362
00:25:58.280 --> 00:26:01.590 
It's often silent.
They cannot detect it, they will tell the

363
00:26:01.590 --> 00:26:06.200 
doctor on the phone.
yes, I feel weak, feel tired I have

364
00:26:06.210 --> 00:26:09.590 
high fevers etcetera.
the doctor will say ok rest,

365
00:26:10.010 --> 00:26:13.760 
drinking of fluids.
But if you record basically

366
00:26:14.730 --> 00:26:19.220 
oxygen saturation through basically a thirty euro device and

367
00:26:19.220 --> 00:26:24.260 
you transfer a two digit number between doctor and patient.

368
00:26:24.670 --> 00:26:28.340 
you will know, if the situation is below 92

369
00:26:29.150 --> 00:26:34.130 
the patient has mortality of 25 percent,
one out of four will die .

370
00:26:34.590 --> 00:26:37.270 
and this is a situation where patients often do not realize

371
00:26:37.270 --> 00:26:41.630 
that they have it, and at the same time we transfer text images

372
00:26:41.640 --> 00:26:46.530 
video between germany and timbuktu
per whatsapp for private communication.

373
00:26:46.980 --> 00:26:51.320 
But we're not able to transfer two digit code

374
00:26:51.630 --> 00:26:55.380 
between a patient a doctor in a life
threatening condition.

375
00:26:55.980 --> 00:26:59.750 
and so we found a solution to do it
to connect the doctors and connect them.

376
00:27:00.070 --> 00:27:04.470 
But the data integration,
what you said is a long term challenge

377
00:27:04.470 --> 00:27:08.570 
we need to address.
and we cannot build ten thousand different ceos for every

378
00:27:08.850 --> 00:27:11.320 
every cause and every every every sector.

379
00:27:12.810 --> 00:27:16.120 
it's not the answer.
There's a question from the shedd

380
00:27:16.520 --> 00:27:19.910 
is your course on kochi campus open to everyone is it down

381
00:27:19.910 --> 00:27:24.290 
to a period of time when you need to take it all it self-paced.

382
00:27:25.710 --> 00:27:29.620 
we plan basically to do,
when we introduce it to have

383
00:27:29.620 --> 00:27:33.480 
a course which is also guided where we also have to

384
00:27:33.490 --> 00:27:38.090 
have interactions and direct exchanges.

385
00:27:38.090 --> 00:27:42.130 
but after the initial phase,
it will remain online for self-paced calls.

386
00:27:42.130 --> 00:27:45.960 
so  basically surface will be available immediately.

387
00:27:46.550 --> 00:27:49.490 
but we will do also some interactions from time to time.

388
00:27:50.980 --> 00:27:56.030 
and let us only be in german or english or maybe even french

389
00:27:57.490 --> 00:28:01.390 
will start and start in german.
we also thought it would be

390
00:28:01.390 --> 00:28:03.550 
great to also do it in english or

391
00:28:04.300 --> 00:28:08.270 
french at the moment I must say no.
because I had french in

392
00:28:08.270 --> 00:28:12.810 
school for five years.And it's terrible.
so maybe I think this is something

393
00:28:13.790 --> 00:28:17.100 
we need to think about in the future.
But I think german will

394
00:28:17.100 --> 00:28:21.530 
be the start, english I think is
a good good second and then we'll see.

395
00:28:23.570 --> 00:28:27.010 
ok. thanks a lot.
I think we should go to the next talk by.

396
00:28:28.250 --> 00:28:33.990 
thank you. thank you for the opportunity to
share some of our experiences from

397
00:28:34.230 --> 00:28:40.710 
creating a book to solve or to address a specific problem of
systematic clinical assessment

398
00:28:41.970 --> 00:28:46.670 
as professor kuhn pointed out and
illustrated quite well. we see that the

399
00:28:47.230 --> 00:28:50.690 
there are massive shifts going on
and that the shift that we

400
00:28:51.090 --> 00:28:57.770 
focused on for this room was actually to tackle.
and meet the needs and the

401
00:28:57.900 --> 00:29:03.340 
challenges of this ongoing massive shift
of site of activity and

402
00:29:03.720 --> 00:29:09.960 
also a commitment to increase or
to support the interpromotional collaboration.

403
00:29:10.970 --> 00:29:16.860 
we see that not only medical care,
but I would say health care is changing profoundly.

404
00:29:17.110 --> 00:29:21.280 
and for the clean print MOOC which was the name of this MOOC we

405
00:29:21.460 --> 00:29:25.780 
had three goals.
we wanted to support life-long learning knowing

406
00:29:25.780 --> 00:29:28.490 
that there is a competency gap.

407
00:29:30.450 --> 00:29:35.550 
In a competency gap and the requirements for more specific

408
00:29:35.550 --> 00:29:40.500 
and shared competencies and
insights to meet the challenges as we see

409
00:29:40.780 --> 00:29:45.210 
this shift of activity.
In our faculty and in our curriculum,

410
00:29:45.280 --> 00:29:50.870 
we've had an increasingly focus on preparing
our graduates for primary care.

411
00:29:51.460 --> 00:29:54.150 
because we know that as also professor Kuhn

412
00:29:54.690 --> 00:29:59.240 
pointed to 95 percent or a large proportion of the

413
00:29:59.240 --> 00:30:03.700 
health care activity's  takes place outside
of the health care system

414
00:30:04.270 --> 00:30:06.820 
or the not the health care setting.
But the hospital setting,

415
00:30:07.060 --> 00:30:13.490 
so we wanted to support life-long learning.
we wanted to stimulate into professional inter-disciplinary

416
00:30:13.950 --> 00:30:18.730 
a collaboration with problem solving
and a competence development

417
00:30:18.760 --> 00:30:23.670 
in the workplace as well as
in the students education offerings.

418
00:30:24.100 --> 00:30:29.040 
and we chose to focus on communication
and practical skills.

419
00:30:30.280 --> 00:30:36.050 
to reach goals of shared
understanding standardization use

420
00:30:36.050 --> 00:30:41.070 
of a common language and
also using the same or similar

421
00:30:41.640 --> 00:30:44.920 
systematic approaches to

422
00:30:45.980 --> 00:30:48.420 
assessment techniques.

423
00:30:49.290 --> 00:30:55.100 
so in the kleene framework we have it's very practical.
it's authentic.

424
00:30:55.300 --> 00:30:57.950 
we have five models.
it's self place.

425
00:30:58.610 --> 00:31:00.320 
we build it,

426
00:31:01.520 --> 00:31:06.530 
so that there is built in the scaffolding

427
00:31:06.950 --> 00:31:10.750 
and a increasing or difficult
or increasing the difficulties of the activities.

428
00:31:11.470 --> 00:31:15.540 
so start by introduction and model

429
00:31:16.390 --> 00:31:19.780 
why is this important?
why do we need the shared language?

430
00:31:20.080 --> 00:31:22.710 
why do we need to have a common
with the systematic assessment?

431
00:31:23.260 --> 00:31:26.770 
we also establish a common frame of reference.

432
00:31:28.090 --> 00:31:30.630 
what does that imply for which

433
00:31:31.590 --> 00:31:33.170 
signs and systematic observations?

434
00:31:33.830 --> 00:31:36.820 
what are the important ?

435
00:31:37.940 --> 00:31:42.220 
and relevant observations and assessments

436
00:31:43.460 --> 00:31:46.650 
that can be helpful from the site

437
00:31:47.870 --> 00:31:54.440 
of activity outside the hospital setting
to help also the pre-hospital

438
00:31:54.440 --> 00:31:59.010 
services to determine severity and urgency.

439
00:31:59.280 --> 00:32:02.510 
we focussed on physical examination.
and then also an in-depth focus.

440
00:32:02.510 --> 00:32:08.270 
and i'll give you a small sneak peek into
what we had in store.

441
00:32:11.660 --> 00:32:17.910 
we used shorter focused videos quite extensively
to demonstrate that.

442
00:32:18.150 --> 00:32:21.760 
of course the challenge ahead of us,
or in front of us was that

443
00:32:22.390 --> 00:32:28.040 
we explored how we could use the mooc environment
with self-paced learning

444
00:32:29.910 --> 00:32:35.280 
to demonstrate and stimulate clinical skills development

445
00:32:36.510 --> 00:32:41.920 
with unsupervised instructions really.
so here you can see some

446
00:32:42.300 --> 00:32:44.420 
examples of data collection

447
00:32:45.280 --> 00:32:52.710 
and assessment during that learners were exposed to demos.

448
00:32:53.130 --> 00:32:56.450 
and of course through this we wanted to demonstrate also

449
00:32:57.140 --> 00:33:00.520 
sort of a best practice of the we also learned know that people

450
00:33:00.520 --> 00:33:02.620 
learn a lot from looking at mistakes.

451
00:33:03.230 --> 00:33:08.140 
But we wanted to keep in and demonstrate

452
00:33:10.850 --> 00:33:15.190 
the best practices.

453
00:33:15.630 --> 00:33:19.100 
we used the common ABCDE scheme for the trial.

454
00:33:20.140 --> 00:33:29.770 
part and actually a focus the lot on those areas
on airway breathing circulation,

455
00:33:30.730 --> 00:33:32.710 
disability and exposure.

456
00:33:35.660 --> 00:33:39.630 
a prominent case or what we

457
00:33:40.320 --> 00:33:44.790 
took as a starting point
was a sufficiently complex project program or

458
00:33:44.990 --> 00:33:48.580 
problem.
and that's the deterioration of a clinical condition.

459
00:33:49.760 --> 00:33:54.120 
and then if you suspect that there is something

460
00:33:54.120 --> 00:34:00.240 
going on, or if you come to a patient or a user that has changed

461
00:34:00.250 --> 00:34:05.510 
from how you saw that patient previously.

462
00:34:05.510 --> 00:34:07.510 
we wanted to alert and to bring in the

463
00:34:09.220 --> 00:34:11.740 
techniques and the understanding of

464
00:34:13.460 --> 00:34:15.990 
and the systematic assessment to

465
00:34:17.120 --> 00:34:20.110 
detect that deterioration

466
00:34:21.160 --> 00:34:24.240 
as early as possible and actually to be focused,

467
00:34:24.240 --> 00:34:26.790 
so that you're not saying I think this patient.

468
00:34:26.790 --> 00:34:29.830 
this there is something with this patient.
It's better than to say.

469
00:34:29.830 --> 00:34:34.480 
There is something with this patient.
I check them, the skin is kind of sweaty.

470
00:34:35.070 --> 00:34:39.250 
It doesn't feel right, or it is not warm
as it used to be I checked the pulse.

471
00:34:39.250 --> 00:34:44.460 
it's weaker or
I even did the blood pressure to establish

472
00:34:44.460 --> 00:34:50.410 
what that looked like. A use of assessment forms
was an important standardized the

473
00:34:50.650 --> 00:34:53.370 
assessment forms were an important component

474
00:34:54.010 --> 00:34:57.360 
of vital signs as I mentioned.

475
00:34:58.050 --> 00:35:00.210 
and then improved the communication,

476
00:35:00.610 --> 00:35:04.690 
so that you can share the findings
in such a way that they also make

477
00:35:04.690 --> 00:35:08.360 
sense for other colleagues in the interdisciplinary team.

478
00:35:10.460 --> 00:35:14.620 
so that was the overall.
and it was like a deterioration in a clinical

479
00:35:14.620 --> 00:35:17.730 
condition that was sort of the focusing  point.

480
00:35:19.490 --> 00:35:20.960 
we used two very common

481
00:35:21.670 --> 00:35:27.350 
topics that happens in primary care confusion and infection.

482
00:35:28.570 --> 00:35:34.740 
we demonstrated advanced
clinical assessment also by nurses

483
00:35:35.100 --> 00:35:38.460 
and we demonstrated best practice communication with the nursing

484
00:35:38.460 --> 00:35:41.070 
home doctors that visit the nursing home from time to time.

485
00:35:42.140 --> 00:35:47.510 
so that there it opens up and
you have more transparency in them

486
00:35:47.780 --> 00:35:49.300 
in the set up and in the team.

487
00:35:52.430 --> 00:35:56.680 
the fourth mile and I was like a leading up
to the treyarch and

488
00:35:56.880 --> 00:36:00.660 
also deciding should be
handled this condition in

489
00:36:00.660 --> 00:36:06.210 
the nursing home or locally in primary care

490
00:36:07.290 --> 00:36:07.940 
or do we need to transfer to a hospital.

491
00:36:10.310 --> 00:36:15.830 
when going into more advanced parts of the mooc.

492
00:36:17.720 --> 00:36:24.750 
we included a videos, an explanations of

493
00:36:26.190 --> 00:36:29.470 
standard assessment and we used a traditional

494
00:36:29.880 --> 00:36:34.670 
top to toe assessment starting and then looking at the

495
00:36:35.760 --> 00:36:40.810 
different organ  systems and
demonstrating and explaining

496
00:36:41.290 --> 00:36:46.010 
the skills that's necessary in a clinical assessment and

497
00:36:46.990 --> 00:36:49.950 
demonstrating the skills and explaining

498
00:36:50.410 --> 00:36:55.440 
what you do and how you could interpret the different

499
00:36:55.440 --> 00:36:57.310 
findings that you get.

500
00:36:59.230 --> 00:37:02.390 
and then for the in-depth, we did something

501
00:37:02.850 --> 00:37:08.370 
that i think was extremely cool.
where you get both the opportunity

502
00:37:08.380 --> 00:37:09.770 
to practice and you get

503
00:37:11.030 --> 00:37:14.240 
authentic feedback.
so we had modules in this mooc,

504
00:37:14.840 --> 00:37:20.700 
where the learners are supposed to sort of
use a virtual stethoscope and then

505
00:37:20.860 --> 00:37:24.790 
either listen for the hot sounds
or listen for the long sounds

506
00:37:25.120 --> 00:37:29.760 
to orient themselves. and what they do
that is that they know where to listen,

507
00:37:30.320 --> 00:37:33.120 
where to place the stethoscope and they also have

508
00:37:33.650 --> 00:37:36.790 
immediate feedback of

509
00:37:37.670 --> 00:37:41.430 
what  the audio could be,
when you use the stethoscope.

510
00:37:41.710 --> 00:37:46.630 
so that you are taking advantage of
combining different modalities to

511
00:37:46.930 --> 00:37:51.250 
augment the learning experience and
actually give the learners the hands-on

512
00:37:51.680 --> 00:37:59.620 
experience to have both the skills
and the feedback an immediate

513
00:37:59.620 --> 00:38:01.070 
feedback rather than explanations.

514
00:38:01.860 --> 00:38:03.450 
It's a little bit like this resolutely

515
00:38:05.190 --> 00:38:10.270 
that's basically the mood.
I want to acknowledge the funding and the partners.

516
00:38:10.710 --> 00:38:12.740 
I would also like to mention that

517
00:38:13.710 --> 00:38:15.670 
we developed this MOOC

518
00:38:16.770 --> 00:38:21.310 
in twenty seventeen in a close collaboration with

519
00:38:22.260 --> 00:38:26.540 
the city of oslo with all municipality which is a more rural

520
00:38:26.540 --> 00:38:29.980 
community and university of southeast norway and

521
00:38:30.460 --> 00:38:35.390 
was copied struck and we were enforced for social distancing.

522
00:38:35.800 --> 00:38:41.350 
this MOOC becames part of a suit of digital tools to

523
00:38:42.210 --> 00:38:47.730 
give the learners the students a clinical skills opportunity,

524
00:38:47.730 --> 00:38:49.290 
so that they were better prepared.

525
00:38:50.180 --> 00:38:54.890 
before they went out into the clinical field
or into clinical practice.

526
00:38:55.170 --> 00:39:01.090 
and in that way also reduced
some of the risk of exposure for themselve.

527
00:39:01.600 --> 00:39:08.520 
But more importantly for the vulnerable patients.

528
00:39:10.070 --> 00:39:15.770 
so it also came in very handy
when the conditions for social meetings

529
00:39:15.770 --> 00:39:20.300 
or for practical education changed.
so on that note I would

530
00:39:20.300 --> 00:39:24.340 
like to thank you for the attention

531
00:39:25.190 --> 00:39:27.820 
and I stop sharing and
I give it back to the

532
00:39:29.220 --> 00:39:33.650 
moderators. Thanks a lot.
This was really interesting.

533
00:39:34.640 --> 00:39:35.710 
other questions?

534
00:39:39.900 --> 00:39:44.330 
I have a question regarding,

535
00:39:45.110 --> 00:39:50.120 
so who is doing the MOOCs,
are these mainly people from norway

536
00:39:50.120 --> 00:39:53.650 
also from other countries.
I think, it's in english?

537
00:39:55.110 --> 00:39:59.270 
well in my country we speak norwegian.
It's sometimes more close

538
00:39:59.270 --> 00:40:03.470 
to german than to english.
This was prepared. this was prepared

539
00:40:03.490 --> 00:40:06.300 
this specific MOOC was prepared in norwegian

540
00:40:08.230 --> 00:40:13.450 
for a life-long learning
in the municipalities for primary care.

541
00:40:13.740 --> 00:40:20.590 
from the assistance the norse aids,
The nurses other professionals

542
00:40:21.390 --> 00:40:25.010 
and the medical doctors to sort of create.

543
00:40:27.310 --> 00:40:32.010 
this shared understanding,
so that everyone were expected to

544
00:40:32.010 --> 00:40:36.870 
know a little bit more on how
and what to do to focus more

545
00:40:36.870 --> 00:40:40.760 
systematically and have a common frame of reference.
so rather than say

546
00:40:40.930 --> 00:40:42.710 
there is something with this person.

547
00:40:43.560 --> 00:40:46.880 
you would also say yes. There is something with this person

548
00:40:46.880 --> 00:40:50.570 
and I have done this assessment based on my

549
00:40:52.420 --> 00:40:59.620 
level of competence and my level of responsibility.

550
00:40:59.990 --> 00:41:02.720 
but it all helps towards the same goal for patient safety.

551
00:41:03.330 --> 00:41:09.060 
so it has been used for lifelong
learning in the collaborating municipalities.

552
00:41:09.450 --> 00:41:17.720 
It has been used also as introduction
to  practical in primary care for

553
00:41:18.230 --> 00:41:24.370 
medical students going to nursing homes.
the practice for advanced nurse practitioners

554
00:41:24.770 --> 00:41:27.900 
for nursing bachelor students in nursing

555
00:41:28.540 --> 00:41:32.590 
the corbett example were specifically

556
00:41:32.590 --> 00:41:34.590 
developed to bushes students in nursing.

557
00:41:35.470 --> 00:41:38.670 
But also it was used for medical students.

558
00:41:39.730 --> 00:41:40.770 
so I think a language,

559
00:41:43.340 --> 00:41:49.060 
we develop the videos in such a way that.

560
00:41:49.060 --> 00:41:52.020 
It's easy to translate we use a lot of voice over.

561
00:41:53.370 --> 00:41:57.680 
so some of the videos are in other learning

562
00:41:57.680 --> 00:42:01.940 
resources available with english was silver vs norwegian was

563
00:42:01.940 --> 00:42:03.100 
silver and I know that

564
00:42:03.810 --> 00:42:07.990 
for a period of time they were accessed

565
00:42:08.120 --> 00:42:10.820 
from collaborators in the US.
Because they were in english.

566
00:42:18.770 --> 00:42:24.090 
I have a small question.
A question of teaching practical skills with a mooc sounds

567
00:42:24.090 --> 00:42:28.700 
to me very challenging. I mean teaching theoretical skills.
I would say ok.

568
00:42:28.860 --> 00:42:33.620 
we can somehow make that possible. But practical skills when

569
00:42:33.620 --> 00:42:38.440 
it comes to nursing sounds to me quite challenging,

570
00:42:39.660 --> 00:42:44.510 
so you know what are your experiences with the people
who attended the courses,

571
00:42:45.260 --> 00:42:49.830 
I mean regarding we have an idea
what they learned maybe in comparison

572
00:42:50.440 --> 00:42:53.780 
to others that got a similar teaching.

573
00:42:54.780 --> 00:42:57.310 
so do we have forty any experiences on that?

574
00:42:57.730 --> 00:43:01.660 
we I agree with your teaching
practical skills in a mooc is

575
00:43:01.670 --> 00:43:04.920 
sort of a contradiction in terms
and that was the interesting

576
00:43:04.920 --> 00:43:08.430 
challenge also that we wanted to see,
if it's possible.

577
00:43:09.460 --> 00:43:12.740 
and if because if it is possible,
it comes with a huge potential

578
00:43:12.750 --> 00:43:14.120 
also for lifelong learning.

579
00:43:14.840 --> 00:43:19.890 
what we saw was empirically
and during the teaching?

580
00:43:19.890 --> 00:43:21.960 
we saw that it was used for orientation,

581
00:43:22.670 --> 00:43:24.390 
so that you had an idea

582
00:43:25.130 --> 00:43:30.470 
of how to practice the skills?
you could also use it to practice

583
00:43:30.490 --> 00:43:32.510 
on each other which is not uncommon.

584
00:43:33.500 --> 00:43:40.060 
so you started to get the look and
feel or the skills using and I think

585
00:43:41.060 --> 00:43:46.480 
the fact that we used a lot of videos of
course supported the practical skills.

586
00:43:48.370 --> 00:43:48.820 
then we saw that the persons

587
00:43:50.520 --> 00:43:57.930 
that had used these resources were better

588
00:43:58.250 --> 00:44:00.380 
equipped when they met patients.

589
00:44:01.520 --> 00:44:05.570 
also the audio feedback
that I showed you for heart and lung

590
00:44:06.260 --> 00:44:11.360 
came in quite handy,
because you would then know what you could expect to hear.

591
00:44:12.110 --> 00:44:17.270 
and you had a picture of that
before you met the person course there is always

592
00:44:17.540 --> 00:44:20.290 
nuances etcetera.

593
00:44:21.150 --> 00:44:26.130 
then the students  used the skills
when they met patients.

594
00:44:26.990 --> 00:44:31.230 
and then we also saw an increase use of the resources,

595
00:44:31.230 --> 00:44:32.730 
when they were preparing for practical.

596
00:44:34.120 --> 00:44:38.490 
so basically for introduction or demonstration

597
00:44:39.040 --> 00:44:42.630 
and for should I say repetition,

598
00:44:44.840 --> 00:44:46.880 
so I think that what

599
00:44:48.300 --> 00:44:52.650 
we learned from the mooc experience
and also from other days

600
00:44:52.670 --> 00:44:56.790 
and that's also supported by our experiences
in making other digital

601
00:44:57.080 --> 00:45:02.530 
resources is that we can use it to prepare for,

602
00:45:03.350 --> 00:45:08.480 
and I make sure that the students when they use it come better

603
00:45:08.480 --> 00:45:11.350 
equipped when they meet the patient.
so it's a little bit less

604
00:45:11.870 --> 00:45:15.130 
a trial error experience.

605
00:45:16.220 --> 00:45:20.540 
and I think that is in respect of the future active patients.

606
00:45:20.540 --> 00:45:24.350 
I think that is a good thing.
so from that perspective it worked and

607
00:45:24.780 --> 00:45:26.260 
it was interesting then.

608
00:45:27.030 --> 00:45:29.220 
the initial contradiction in terms didn't

609
00:45:29.830 --> 00:45:32.700 
hold out. Thanks a lot for answers.

610
00:45:38.590 --> 00:45:42.590 
no more questions?
Then I'm going to proceed with my presentation.

611
00:45:44.520 --> 00:45:45.570 
can you see my screen?

612
00:45:48.410 --> 00:45:52.840 
okay, so I'm showing you another concrete example of an

613
00:45:53.550 --> 00:45:58.520 
online learning program for medical students.
It's not a mooc,

614
00:45:59.180 --> 00:46:02.480 
but has been developed by me and the AI compose.

615
00:46:03.370 --> 00:46:08.580 
its on the one hand, it's a podcast,
so you can access it via spotify,

616
00:46:08.670 --> 00:46:12.610 
itunes and other providers. and on the other side

617
00:46:13.130 --> 00:46:18.290 
it's an online learning program
which can be assessed via the KI campus

618
00:46:18.680 --> 00:46:23.170 
that page. It's completely open source,
so everyone can enroll

619
00:46:23.170 --> 00:46:25.510 
into this program and learn with it

620
00:46:26.870 --> 00:46:30.380 
on the webpage. you have videos,
you have illustrations you have quizzes.

621
00:46:30.380 --> 00:46:33.540 
you have also tests and we will also want to certify

622
00:46:33.540 --> 00:46:37.950 
this as a micro degree and maybe later,
if you extend it more

623
00:46:37.950 --> 00:46:40.940 
and more it could also potentially.
Become a mooc,

624
00:46:41.390 --> 00:46:45.640 
I mainly developed this together
with Mike Bernd from the AI campus

625
00:46:46.620 --> 00:46:50.970 
with all the support of professor
Andrea Volkamer and Linos Ullmann

626
00:46:53.130 --> 00:46:57.650 
the topics of doctor med KI  is
when english you could say doctor AI

627
00:46:57.920 --> 00:47:01.950 
are first basic concepts of AI and machine learning.

628
00:47:02.590 --> 00:47:06.760 
the second season is about applications of AI and medicine,

629
00:47:06.760 --> 00:47:11.510 
so how can we apply AI and
medicine from very different perspectives.

630
00:47:12.240 --> 00:47:16.390 
these supposed seasons are almost finished and we are currently

631
00:47:16.390 --> 00:47:19.970 
at the planning stage for season three and for season three

632
00:47:19.970 --> 00:47:22.610 
is about ethics and translation.
so this is thought to give

633
00:47:22.610 --> 00:47:28.810 
a broader perspective on applications
of AI and medicine and the

634
00:47:28.930 --> 00:47:33.610 
fourth season is about coding.
so here we want to teach some basic

635
00:47:34.250 --> 00:47:36.310 
programming concepts.

636
00:47:37.860 --> 00:47:41.460 
so regarding the first season
you see here the different models

637
00:47:41.460 --> 00:47:45.480 
we have. it's also at the moment only in german,
ut there are

638
00:47:45.480 --> 00:47:47.770 
also plans to translate in to english.

639
00:47:49.030 --> 00:47:52.980 
but we have sessions on introduction of AI and medicine on

640
00:47:52.990 --> 00:47:57.480 
data sines and machine learning and deep learning.
and the idea is to

641
00:47:58.090 --> 00:48:01.010 
teach in a very simple and basic way

642
00:48:01.930 --> 00:48:06.030 
important concepts, such like how important is data?

643
00:48:06.030 --> 00:48:10.790 
what is good data? how to get data?
how to pre-process data? what is machine learning?

644
00:48:10.790 --> 00:48:13.880 
how to validate a machine learning model?

645
00:48:14.320 --> 00:48:18.950 
what are performance metrics? how do like on a very general

646
00:48:19.020 --> 00:48:22.110 
in a very general way different
machine learning algorithms works?

647
00:48:22.110 --> 00:48:25.330 
and then we really want to come to deep learning?

648
00:48:25.330 --> 00:48:29.240 
so what is a artificial neural network
and why are deep learning approaches.

649
00:48:29.240 --> 00:48:30.870 
so successful these days

650
00:48:32.640 --> 00:48:35.870 
the second season is about applications of ai medicine and

651
00:48:35.870 --> 00:48:39.810 
here I did interviews with a lot of
interesting people at charité and

652
00:48:39.970 --> 00:48:44.390 
teach médecine.
so we have applications in your imaging which

653
00:48:44.390 --> 00:48:49.830 
is also my research field and neuro neuro technology drug discovery

654
00:48:49.850 --> 00:48:53.940 
oncology and nephrology and we also have a new episode on

655
00:48:54.360 --> 00:49:00.300 
biology and we also plan two or three more
episodes on pathology and other topics.

656
00:49:02.090 --> 00:49:05.730 
the third season will be about ethics and translations,

657
00:49:05.730 --> 00:49:09.220 
as I said it's during the planning stage.
but here we want to really

658
00:49:09.790 --> 00:49:13.260 
broaden the perspective and we have some ideas here.

659
00:49:13.260 --> 00:49:16.230 
so we want to have a session on learning from data,
but how to get them

660
00:49:16.230 --> 00:49:21.760 
so this will also be about privacy concerns AI at bedside especially

661
00:49:21.760 --> 00:49:28.690 
from a ethics perspective care robots will
they replace human intimacy racist AI

662
00:49:28.840 --> 00:49:33.540 
a replace doctors shut thoughts ai a black box?
why do we need

663
00:49:33.540 --> 00:49:35.800 
regulation start-ups innovation.
so it's really like

664
00:49:36.580 --> 00:49:40.540 
like very different topics we consider important to talk about

665
00:49:40.560 --> 00:49:43.790 
with medical students.
and we also see that they're often quite

666
00:49:43.790 --> 00:49:46.350 
interested in this kind of topics.
and I'm happy that

667
00:49:46.880 --> 00:49:51.020 
I already found some people to support this mostly from charité.

668
00:49:52.340 --> 00:49:56.620 
and the first season will be about coding.
this will be mainly done by professor

669
00:49:56.860 --> 00:50:03.030 
falcom and our phd students.
so we developed programming tutorials

670
00:50:03.030 --> 00:50:06.460 
already for teaching at charité.
and it's ten minute scene

671
00:50:07.780 --> 00:50:13.350 
on a very basic level.
but we think it's good for also for medical students

672
00:50:13.650 --> 00:50:18.270 
to just see  how a code look like?
how this generally works

673
00:50:18.270 --> 00:50:19.730 
and what you can do there?

674
00:50:20.410 --> 00:50:22.250 
and the main challenge

675
00:50:22.970 --> 00:50:27.140 
here may be to how to integrate
this into the AI campus platform.

676
00:50:27.140 --> 00:50:31.290 
and then also to allow self-paced learning.
and because this is also quite

677
00:50:31.710 --> 00:50:32.980 
a practical work.

678
00:50:34.940 --> 00:50:39.250 
we developed the idea last year when the chrono pandemic starts.

679
00:50:39.250 --> 00:50:43.160 
and we just thought we maybe we just directly start recording

680
00:50:43.160 --> 00:50:47.970 
a podcast. and so I was outside of berlin and Mike Bernd and Linos were

681
00:50:48.410 --> 00:50:52.530 
in berlin from the AI campus and we started recording the podcast

682
00:50:53.000 --> 00:50:58.090 
and this was already a lot of fun. and then later in the summer

683
00:50:58.100 --> 00:51:02.770 
via um recorded some interviews with the experts mostly outside

684
00:51:03.270 --> 00:51:07.000 
and we started again to make some further interviews here.

685
00:51:07.420 --> 00:51:11.730 
and we plan to start with the ethics season

686
00:51:12.620 --> 00:51:13.560 
end of the year.

687
00:51:15.160 --> 00:51:20.040 
I'm also using the schools at charité on the Universitätsmedizin in

688
00:51:20.040 --> 00:51:23.870 
and the framework of the elective module AI. and medicine here

689
00:51:23.900 --> 00:51:27.850 
usually have about ten to fifteen students per semester.

690
00:51:27.850 --> 00:51:31.580 
This is a three weeks blockers with about sixty hours

691
00:51:32.260 --> 00:51:36.470 
and roughly the plan is to teach one or two seasons per week

692
00:51:36.740 --> 00:51:40.530 
and as already also mentioned by others.

693
00:51:41.240 --> 00:51:45.760 
we used to be an inverted classroom concept here.
so the students are expected

694
00:51:45.770 --> 00:51:50.830 
to work at home with the doctor meet KI
the online program and prepare stuff.

695
00:51:51.020 --> 00:51:56.250 
and then we can have in class at the moment.
it's mostly via sms teams

696
00:51:56.910 --> 00:52:00.830 
discussions and we can clarify questions and we really have more time

697
00:52:01.000 --> 00:52:02.080 
to talk about like the content.

698
00:52:03.480 --> 00:52:07.790 
and as you can see here also in the first

699
00:52:07.790 --> 00:52:11.690 
week we have the programming tutorial which at the moment is

700
00:52:11.690 --> 00:52:15.580 
still life, but later we want to make this also online.

701
00:52:15.580 --> 00:52:18.820 
maybe not for the medical students,
but we will see how how this is going.

702
00:52:20.090 --> 00:52:25.380 
yeah and that's already it.
I like to think especially Mike Bernd and

703
00:52:25.500 --> 00:52:32.910 
but also the others. I think AI campus is a really nice platform.
they have very

704
00:52:33.030 --> 00:52:37.570 
many different teaching programs mostly in an

705
00:52:38.290 --> 00:52:41.910 
AI which is of course the focus,
but there are also several

706
00:52:41.910 --> 00:52:46.680 
programs in AI medicine.
so for example asanas also we also

707
00:52:46.680 --> 00:52:49.050 
have a course on there and also a professor and him and he

708
00:52:49.300 --> 00:52:53.450 
was not here today will have a course there.
and I think it's really interesting

709
00:52:53.990 --> 00:52:57.350 
to check this out.
thank you very much for your attention.

710
00:53:00.790 --> 00:53:08.420 
and for your inspiring thought.
and very interesting to to see what you're

711
00:53:08.820 --> 00:53:13.520 
doing there. so any questions from the audience.

712
00:53:19.480 --> 00:53:22.510 
yeah, kasten thank you very much.
I think it's a really really

713
00:53:22.510 --> 00:53:27.310 
great example for online courses.
and I also kind of like how

714
00:53:27.310 --> 00:53:28.970 
you develop it

715
00:53:29.670 --> 00:53:34.710 
from the podcast and interviews to more
and more interdisciplinary

716
00:53:34.710 --> 00:53:38.110 
and then also active learning.
and I think this is

717
00:53:38.110 --> 00:53:42.140 
a will be a great learning journey.
I think already as it is, but I will

718
00:53:42.770 --> 00:53:46.330 
like the future perspective you showed with module three.
I think

719
00:53:46.720 --> 00:53:50.960 
we have quite a lot of similarities model two and three,
but also with coding.

720
00:53:51.190 --> 00:53:55.360 
I think is a really really great learning journey for students.

721
00:53:56.150 --> 00:54:01.370 
my question would be what was your biggest challenge on your way

722
00:54:01.750 --> 00:54:06.080 
through modules one and two,
what were the biggest obstacles you had to

723
00:54:06.260 --> 00:54:08.050 
go through to really make it work?

724
00:54:11.100 --> 00:54:17.660 
so it was relatively a very spontaneous idea,

725
00:54:17.660 --> 00:54:21.190 
and we really started very spontaneously to record

726
00:54:22.090 --> 00:54:25.750 
at the podcast. I had some difficulties with the interview format

727
00:54:25.750 --> 00:54:29.220 
at the beginning, because I thought it's less controlled and I like

728
00:54:29.560 --> 00:54:32.820 
i'll select giving lectures and have everything under control.

729
00:54:32.820 --> 00:54:36.460 
and i found it a bit difficult, if I have someone to talk to

730
00:54:36.460 --> 00:54:39.290 
and then like different kind of questions are coming.

731
00:54:39.730 --> 00:54:43.960 
so we scripted this a bit, because I had my lectures and I said

732
00:54:43.960 --> 00:54:48.400 
what i want to happen the podcast.
But then we also thought

733
00:54:48.400 --> 00:54:51.170 
it might be nice, if it's a bit more natural

734
00:54:51.740 --> 00:54:56.450 
like really like two people are talking about interesting stuff.

735
00:54:56.860 --> 00:55:02.010 
so this was some difficulty in the beginning,

736
00:55:05.360 --> 00:55:09.660 
and but also with the experts.
this is also like me, it's also

737
00:55:09.660 --> 00:55:13.110 
relatively spontaneous,
as I have some questions which I think

738
00:55:13.110 --> 00:55:16.130 
are important. so what's your research about what data are you

739
00:55:16.130 --> 00:55:19.010 
using what algorithms are using what difficulties

740
00:55:19.110 --> 00:55:21.850 
and is this used in clinical routine.

741
00:55:21.850 --> 00:55:26.870 
so these are my main questions,
but then of course every time you talk to someone and

742
00:55:27.180 --> 00:55:30.660 
he or she is interesting or telling you interesting stuff,

743
00:55:30.660 --> 00:55:33.180 
then you go in this direction and then in the other direction,

744
00:55:33.180 --> 00:55:36.730 
then you go back and I know that the people
from the AI campus really

745
00:55:36.740 --> 00:55:39.500 
developed the quizzes and all this stuff.

746
00:55:40.020 --> 00:55:45.190 
and trying to define certain learning maggots
have problems and making

747
00:55:45.610 --> 00:55:47.950 
a good program of it. so this I think.

748
00:55:54.360 --> 00:55:58.720 
I see a hand from a peal

749
00:55:59.380 --> 00:56:00.810 
please ask your question.

750
00:56:07.630 --> 00:56:09.910 
yes hello everyone

751
00:56:11.290 --> 00:56:13.660 
yes I am here

752
00:56:15.140 --> 00:56:20.790 
where we interested in some special MOOCs on else's.

753
00:56:22.440 --> 00:56:27.460 
and I am IT professional working here in germany and I am

754
00:56:28.050 --> 00:56:32.010 
originated from a small country in west africa.

755
00:56:33.490 --> 00:56:40.590 
so I would like to ask first, if it is

756
00:56:42.850 --> 00:56:48.030 
for example as moocs of the KI campus, if it will be

757
00:56:48.710 --> 00:56:53.770 
possible for students or researchers or

758
00:56:54.520 --> 00:56:57.140 
his scares workers in an

759
00:56:59.270 --> 00:57:04.020 
area like west africa to access anything and

760
00:57:05.240 --> 00:57:07.240 
I will have another question also.

761
00:57:11.310 --> 00:57:16.870 
I will participate to all the sessions

762
00:57:17.880 --> 00:57:21.890 
on openHPI,

763
00:57:23.860 --> 00:57:27.730 
because from my own experience I know from two

764
00:57:28.640 --> 00:57:30.580 
health workers

765
00:57:32.580 --> 00:57:37.940 
from the country where I am from burkina faso
and it is personal one is

766
00:57:38.220 --> 00:57:40.460 
my sister the other one is

767
00:57:42.440 --> 00:57:45.760 
another key daughter both

768
00:57:47.700 --> 00:57:52.390 
medicine doctors. my sister,

769
00:57:55.790 --> 00:58:00.980 
for example for many years,
she's a pediatric doctor

770
00:58:02.060 --> 00:58:05.560 
and to get, for example, qualified

771
00:58:06.690 --> 00:58:08.980 
a specialized in pediatrics.

772
00:58:10.100 --> 00:58:14.010 
she had to take more than seven years after

773
00:58:14.530 --> 00:58:16.540 
her  doctor degree to get qualified.

774
00:58:17.520 --> 00:58:21.960 
Because it was not possible to be specialized

775
00:58:22.740 --> 00:58:23.670 
when you are in africa,

776
00:58:25.120 --> 00:58:26.610 
for example,

777
00:58:29.060 --> 00:58:34.900 
so this to show that it is very important,

778
00:58:35.820 --> 00:58:39.570 
if  people from other countries can access to moocs.

779
00:58:40.470 --> 00:58:46.840 
and as the second person is my adoptive daughter and

780
00:58:47.660 --> 00:58:52.580 
she's since three years, she has a doctor degree

781
00:58:53.500 --> 00:58:56.760 
and she want to get specialized

782
00:58:57.410 --> 00:59:00.280 
and every year I have

783
00:59:01.370 --> 00:59:06.580 
this time two years, two times.
she tried to be qualified,

784
00:59:07.010 --> 00:59:12.040 
but she she cannot get qualified
and I think if she can access to some moocs.

785
00:59:12.490 --> 00:59:16.410 
and she can learn. and maybe she will

786
00:59:17.050 --> 00:59:24.400 
be better, she can get specialized quickly.

787
00:59:25.040 --> 00:59:28.800 
so it's on for my my question,

788
00:59:29.640 --> 00:59:35.170 
if the moocs that are developed,
for example for the KI campus,

789
00:59:35.890 --> 00:59:40.020 
if for example my adoptive daughter can register,

790
00:59:40.890 --> 00:59:42.690 
she can is she learned a little bit

791
00:59:43.640 --> 00:59:49.360 
english, when she learned a little bit german for years.

792
00:59:51.060 --> 00:59:53.590 
so yes. yes thanks for your question so the KI

793
00:59:54.010 --> 00:59:59.770 
campus page as open for everyone, so everyone can enroll there

794
01:00:00.220 --> 01:00:07.300 
most programs are in german. but it's also a plan to have english

795
01:00:07.300 --> 01:00:12.500 
programs there and at least for my program and also sebastian's

796
01:00:12.520 --> 01:00:17.060 
across it's plan to be translated at some point. so that also

797
01:00:17.560 --> 01:00:20.340 
that it's easier accessible also for others.

798
01:00:20.980 --> 01:00:26.180 
you also wanted to answer to this question with the moocs.

799
01:00:26.810 --> 01:00:32.980 
I don't want to answer or it's what's available through the

800
01:00:32.980 --> 01:00:35.020 
german sites. it's not my cup of tea

801
01:00:35.840 --> 01:00:39.520 
I could share some, if you're if you like

802
01:00:40.630 --> 01:00:44.610 
we put in some considerations in terms of access.

803
01:00:45.230 --> 01:00:49.760 
and maybe also we also put in a lot of considerations for that

804
01:00:49.760 --> 01:00:51.700 
in this other move that the

805
01:00:52.630 --> 01:00:58.570 
band was. so I was referring to on how to write the phd proposal.

806
01:00:58.880 --> 01:01:02.980 
because moocs is by its design open to everyone.

807
01:01:03.410 --> 01:01:05.630 
it's massive open online course.

808
01:01:06.450 --> 01:01:10.480 
so it is widely available, there are certain

809
01:01:10.970 --> 01:01:16.440 
issues, when it comes to institutions accepting a

810
01:01:17.210 --> 01:01:20.210 
the qualifications that you claim based on the moocs.

811
01:01:20.940 --> 01:01:26.040 
so the questions of exams
the questions of certification etcetera is

812
01:01:26.280 --> 01:01:31.500 
for my moocs an interesting one in the sense that

813
01:01:32.520 --> 01:01:37.790 
the common measures for if people sit an exam
or you can prove that you have

814
01:01:38.820 --> 01:01:43.080 
there is this certain, there is this uncertainty.

815
01:01:43.080 --> 01:01:46.810 
so was it kasten that actually did the work or was it her friend

816
01:01:47.440 --> 01:01:50.790 
to put it that way. so that's the troubling issue,

817
01:01:50.790 --> 01:01:55.450 
when it comes to moocs, but of course the knowledge the material

818
01:01:55.500 --> 01:01:59.380 
using the material is there. so for this

819
01:02:00.100 --> 01:02:05.240 
framework we used it as an add-on, as an introduction and

820
01:02:05.700 --> 01:02:09.270 
as an offering to strengthen and professionalize the clinical

821
01:02:09.590 --> 01:02:11.920 
a systematic clinical assessment for all

822
01:02:12.640 --> 01:02:17.710 
for them the other books that we have developed and offered

823
01:02:17.830 --> 01:02:19.870 
globally on how to write a

824
01:02:20.510 --> 01:02:24.640 
phd proposal which is more of a theoretical and conceptual issue.

825
01:02:24.870 --> 01:02:27.680 
we also said that the goal there is to make sure that you are

826
01:02:27.680 --> 01:02:31.510 
better prepared to meet a potential supervisor

827
01:02:32.220 --> 01:02:36.870 
or to join a research group. so it depends a little bit on how you

828
01:02:37.410 --> 01:02:41.880 
present it and how you what your goals are, but the last thing

829
01:02:41.880 --> 01:02:43.520 
that I wanted to point out is that

830
01:02:45.360 --> 01:02:49.190 
for the move with global reach we took special measures to

831
01:02:49.190 --> 01:02:53.350 
make sure that it was sensitive and relevant in many settings.

832
01:02:53.770 --> 01:02:58.350 
It's not sure that active inter-disciplinary

833
01:02:58.860 --> 01:03:05.170 
collaboration in primary care in norway,
goes for germany or goes for france

834
01:03:05.510 --> 01:03:10.830 
or goes for sub-saharan africa.
so that's kind of a that's kind

835
01:03:10.830 --> 01:03:13.350 
of a disclaimer there.
but in principle they're open.

836
01:03:17.900 --> 01:03:21.420 
so I think we should proceed with the talk by Jenny.

837
01:03:22.500 --> 01:03:27.830 
yes thank you very much.
I will start screen sharing in three or four minutes,

838
01:03:28.850 --> 01:03:29.980 
but at first,

839
01:03:31.330 --> 01:03:35.670 
kasten later asked me to contribute the student perspective

840
01:03:35.890 --> 01:03:38.880 
to the topic and I'm really glad to do so today.

841
01:03:39.590 --> 01:03:42.340 
so to begin i'll briefly tell you about

842
01:03:43.030 --> 01:03:46.300 
my past that brought me to digital medicine.

843
01:03:47.350 --> 01:03:52.030 
and also to this panel today. and in the second part,

844
01:03:53.020 --> 01:03:57.690 
then I screen sharing
and I would very briefly report on a study,

845
01:03:58.090 --> 01:04:03.180 
on the topic of AI in medical education
that I am currently working on together

846
01:04:03.620 --> 01:04:06.330 
with a research group from charité Berlin.

847
01:04:07.140 --> 01:04:12.840 
So before I went to medical school,
I worked in a completely

848
01:04:12.840 --> 01:04:15.670 
different field and that was finance.

849
01:04:16.940 --> 01:04:21.440 
I worked in a company that launched AI powered investment funds.

850
01:04:21.490 --> 01:04:25.070 
and that's where I was confronted with the

851
01:04:25.830 --> 01:04:31.230 
with the chances and also the limitations of AI for the first time.

852
01:04:32.600 --> 01:04:36.790 
at some point in my life I made a conscious decision to

853
01:04:37.390 --> 01:04:42.610 
study medicine and it was the best decision of my life,

854
01:04:42.610 --> 01:04:47.790 
but at the same time it was a journey
into the past with fax machines and

855
01:04:48.210 --> 01:04:50.060 
paper files and watts.

856
01:04:51.390 --> 01:04:57.260 
so I could hardly believe it,
but eventually came to terms with it,

857
01:04:57.660 --> 01:05:03.000 
and then in my semester eight
I had an elective course with the interesting

858
01:05:03.000 --> 01:05:04.870 
name medicine in the digital age.

859
01:05:05.480 --> 01:05:09.060 
you have to budgets to lecture and develop this course professors

860
01:05:09.060 --> 01:05:12.420 
invest in who happens to be here today as well. and it really

861
01:05:13.060 --> 01:05:15.590 
is a coincidence that we are both here.

862
01:05:16.850 --> 01:05:22.610 
and that's where I first came into contact with the future

863
01:05:22.610 --> 01:05:29.940 
of medicine or rather to present.
I heard about apps on prescription

864
01:05:30.670 --> 01:05:34.730 
patient monitoring via AI in diagnostics

865
01:05:35.620 --> 01:05:41.970 
robotics in surgery and so on.
and I couldn't believe that all these developments

866
01:05:43.220 --> 01:05:49.660 
have passed me by so far.
and that I heard about all this for the first time.

867
01:05:51.120 --> 01:05:56.330 
so I went home every day after the course,
and wanting to read

868
01:05:56.340 --> 01:06:01.580 
more about it and I researched on
the internet and read papers and I

869
01:06:01.960 --> 01:06:05.090 
was looking for workshops and courses and

870
01:06:06.220 --> 01:06:12.350 
I realized how little we are prepared in medical school
for all these developments.

871
01:06:13.420 --> 01:06:15.750 
so within the federal representation of medical students

872
01:06:16.570 --> 01:06:21.500 
which we called before MD in germany.

873
01:06:22.230 --> 01:06:27.420 
I met some students who are also following this

874
01:06:27.420 --> 01:06:32.030 
topic with great interest and the big thing we had in common was

875
01:06:32.570 --> 01:06:38.020 
that at some point we happened to have
an elective course on the subjec,

876
01:06:38.470 --> 01:06:44.810 
and thus came into contact with the topic of
digitalisation in medicine.

877
01:06:45.690 --> 01:06:51.480 
so we founded the project digital medicine
at the end of two thousand and twenty.

878
01:06:52.220 --> 01:06:59.840 
and we formulated clear goals
which made it easier to approach faculties and teachers

879
01:07:00.290 --> 01:07:06.230 
who had already integrated the topic of digital medicine
into the curriculum.

880
01:07:06.520 --> 01:07:12.210 
so there was an incredible openness to exchange

881
01:07:12.210 --> 01:07:17.050 
and a real joy that students had organized
themselves to push this issue forward.

882
01:07:17.830 --> 01:07:23.000 
and this this has shown me that I am on the right track.

883
01:07:23.470 --> 01:07:28.390 
I also had some detours and dead ends

884
01:07:29.280 --> 01:07:33.690 
on my way, but the direction was right.
so I joined a few working

885
01:07:33.690 --> 01:07:36.990 
groups that were doing research on the topic and

886
01:07:37.420 --> 01:07:43.100 
in this context I would like to particularly
highlight the team from

887
01:07:43.520 --> 01:07:46.940 
charité berlin and the expert lead from AI campus

888
01:07:48.060 --> 01:07:53.160 
which brought me into contact with a really interesting persons,

889
01:07:53.580 --> 01:07:58.690 
for  example Felix Mensa, Martin Hills or Casting Rita.

890
01:07:59.250 --> 01:08:02.260 
and this closest desert could

891
01:08:03.120 --> 01:08:08.640 
do the study and now I will start sharing my screen.

892
01:08:09.230 --> 01:08:13.140 
so the results of the study will be published

893
01:08:13.140 --> 01:08:19.030 
in august or september.
so I may only give a very brief insight today

894
01:08:20.040 --> 01:08:23.620 
and here below you can see all
the organizations that were

895
01:08:23.620 --> 01:08:26.120 
involved in the conduction of the study.

896
01:08:26.670 --> 01:08:31.590 
It was a really my project in a very short time

897
01:08:32.220 --> 01:08:36.500 
the study was commissioned by AI campus
and conducted by charity berlin

898
01:08:36.720 --> 01:08:42.690 
most specifically the institute for medical informatics.

899
01:08:43.170 --> 01:08:48.190 
and the aim of the study was to compare the current status

900
01:08:48.190 --> 01:08:54.750 
and demand for teaching programs
on the topic of artificial intelligence in medicine,

901
01:08:55.170 --> 01:09:00.860 
as well as to identify gaps in demand and implementation hurdles

902
01:09:01.180 --> 01:09:05.450 
in education and training and also solution strategies.

903
01:09:05.910 --> 01:09:11.300 
and for this purpose the study was divided into two parts.

904
01:09:11.650 --> 01:09:15.400 
first we did an inventory of existing and planned learning office.

905
01:09:15.910 --> 01:09:21.080 
we wrote to the deans of all medical faculties

906
01:09:21.080 --> 01:09:24.870 
in germany and asked them to fill out a survey about the learning

907
01:09:24.870 --> 01:09:26.970 
opportunities on the topic of AI.

908
01:09:27.740 --> 01:09:31.170 
In addition we have researched medical residency

909
01:09:31.880 --> 01:09:35.670 
master's degree programs which are specifically aimed

910
01:09:36.460 --> 01:09:41.770 
at medical professionals and other offerings like moocs or

911
01:09:41.770 --> 01:09:45.900 
digital knowledge bases like ambos for example.

912
01:09:46.860 --> 01:09:52.990 
then in the second part we did a needs  analysis.

913
01:09:53.810 --> 01:09:57.540 
we interviewed twenty four experts from the fields of

914
01:09:58.030 --> 01:10:04.720 
medicine informatics, business, politics and education
using a semi-structured guide

915
01:10:05.300 --> 01:10:11.580 
and after that the interviews were
transcribed and qualitatively analyzed.

916
01:10:11.990 --> 01:10:19.510 
so we we asked the experts for their opinion
on ai core competencies for physicians and

917
01:10:20.340 --> 01:10:24.250 
their implementation in medical education,
but also for obstacles

918
01:10:24.250 --> 01:10:29.620 
and problems as well as recommendation for action.

919
01:10:29.630 --> 01:10:34.210 
and two of these experts are present today
namely kirsten ritter and sebastian kühn.

920
01:10:35.080 --> 01:10:40.900 
so we have worked intensively on the content as

921
01:10:40.900 --> 01:10:45.330 
you can see here on our online whiteboards and

922
01:10:45.330 --> 01:10:48.240 
the analysis of the interviews revealed

923
01:10:48.850 --> 01:10:53.950 
the following clusters which should be considered more in detail.

924
01:10:54.640 --> 01:10:58.260 
first changes through AI changes

925
01:10:58.260 --> 01:11:02.730 
in health care and for the medical profession that medical

926
01:11:02.730 --> 01:11:07.990 
students and physicians should be prepared for in their training

927
01:11:08.520 --> 01:11:10.610 
secondly paradigm shifts and

928
01:11:11.440 --> 01:11:14.260 
a challenges.

929
01:11:15.570 --> 01:11:21.970 
then contents and competencies,
so what should be included in inter curricular the implementation.

930
01:11:22.690 --> 01:11:28.120 
so how should it be incorporated into the curricula

931
01:11:28.540 --> 01:11:33.410 
and also responsibilities. who is responsible

932
01:11:33.940 --> 01:11:37.130 
politics universities and so on?

933
01:11:37.840 --> 01:11:43.400 
and of course the problems that we are facing regarding this topic.

934
01:11:44.380 --> 01:11:48.500 
so these were very interesting discussions in which there was

935
01:11:48.500 --> 01:11:52.820 
a lot of consensus, but also different opinions

936
01:11:53.300 --> 01:11:56.810 
and each expert shed light on another point

937
01:11:57.230 --> 01:12:02.560 
that not had been mentioned before. so as I said the results

938
01:12:02.560 --> 01:12:05.750 
will be published in august or september and unfortunately

939
01:12:05.750 --> 01:12:10.620 
I have to put off until then and
I am not allowed to talk about the study.

940
01:12:13.330 --> 01:12:17.550 
but I want to invite everybody who is interested to the AI

941
01:12:17.550 --> 01:12:23.280 
campus kickoff event in september where the study results will be

942
01:12:23.750 --> 01:12:29.320 
presented so safe to date. and thank you very much for your attention.

943
01:12:30.020 --> 01:12:33.590 
and I'm looking forward for your questions. thanks a lot Jenny.

944
01:12:34.300 --> 01:12:37.700 
It's really interesting to hear from students how difficult it

945
01:12:38.020 --> 01:12:42.610 
is to get like digital knowledge here
and like walking around and see where

946
01:12:42.930 --> 01:12:46.620 
there's something else to see.
this is my medical students some of them are

947
01:12:46.830 --> 01:12:50.800 
visiting courses on data camp or other platforms.

948
01:12:52.420 --> 01:12:55.320 
but it's yeah it's so other questions.

949
01:12:59.410 --> 01:13:02.880 
thank you very much Jenni for one nothing pointing out your

950
01:13:02.880 --> 01:13:06.820 
personal story which really is I think at some points especially

951
01:13:06.820 --> 01:13:09.010 
kind of like the realisation

952
01:13:09.920 --> 01:13:13.710 
that there's like a whole new world out
there which is developing and she

953
01:13:13.810 --> 01:13:18.300 
not something relevant in twenty forty two or something like

954
01:13:18.300 --> 01:13:20.470 
that. but actually in two thousand and twenty one

955
01:13:21.150 --> 01:13:27.450 
or at least before most medical students
will treat the first patients themselves.

956
01:13:27.690 --> 01:13:30.110 
so a lot of the clinical students to have two three years still

957
01:13:30.110 --> 01:13:34.580 
ahead of them and in 2023, 2024.

958
01:13:35.980 --> 01:13:38.110 
this will be in quite a different areas of

959
01:13:39.390 --> 01:13:40.790 
the status quo and

960
01:13:42.190 --> 01:13:47.170 
very good pointed out how this is not like a story of one person

961
01:13:47.170 --> 01:13:50.700 
at maybe one medical school in germany, but can like the students

962
01:13:50.700 --> 01:13:53.610 
get together and different students at the same experience.

963
01:13:53.610 --> 01:13:57.160 
I think that's great and I think even though we would like

964
01:13:57.160 --> 01:14:02.080 
to already do a deep dive into needs analysis. I think I'm already

965
01:14:02.080 --> 01:14:06.720 
by can like the the topics you mentioned. I think a lot of people

966
01:14:06.720 --> 01:14:08.310 
who are i think involved in

967
01:14:09.560 --> 01:14:13.370 
from a different perspective. I think they really realize those

968
01:14:13.370 --> 01:14:15.570 
are really the challenges the questions.

969
01:14:16.210 --> 01:14:21.730 
and I think the question about using digital medicine
or especially AI in medicine

970
01:14:22.090 --> 01:14:24.960 
goes beyond just a purely technical problem.

971
01:14:25.820 --> 01:14:29.260 
I think an understand of technology is quite important and

972
01:14:29.260 --> 01:14:33.430 
also i think we touch a little bit on that. and i think maybe cast in

973
01:14:33.540 --> 01:14:38.230 
goes into more into that maybe the first module or probably

974
01:14:38.230 --> 01:14:43.190 
a bit also does this in the mass course very thoroughly,

975
01:14:43.190 --> 01:14:46.340 
but also it's very very interdisciplinary and I heard that from

976
01:14:46.670 --> 01:14:49.950 
all of you and also from my own experience.

977
01:14:49.950 --> 01:14:54.340 
It is a very interdisciplinary into professional question and goes

978
01:14:54.520 --> 01:14:59.410 
into all different areas of medicine and and science in general and

979
01:14:59.680 --> 01:15:02.900 
and also the social and ethical points are very important.

980
01:15:03.280 --> 01:15:05.400 
so I think we look very much forward to that.

981
01:15:05.850 --> 01:15:08.290 
and I think we all together have a long

982
01:15:08.740 --> 01:15:09.810 
path ahead of us.

983
01:15:13.940 --> 01:15:19.290 
yes so I can say something about. because this

984
01:15:19.290 --> 01:15:23.750 
is really really important because for example what we are

985
01:15:23.750 --> 01:15:29.160 
on the subject of AI.
and we often have the such situation that people

986
01:15:29.440 --> 01:15:34.940 
oscillate between euphoria and fear when it comes to the topic of AI.

987
01:15:35.410 --> 01:15:40.670 
and at first the topic is associated with a great deal of mystification,

988
01:15:41.380 --> 01:15:45.540 
when there's no knowledge of it and then when the topic is dealt with

989
01:15:45.980 --> 01:15:49.420 
for the first time the whole thing can tip over into the opposite.

990
01:15:49.670 --> 01:15:54.430 
then i have seen as a solution for all questions. and i think

991
01:15:54.440 --> 01:16:00.570 
the only thing that helps to create an equilibrium is an accompanied

992
01:16:00.570 --> 01:16:06.340 
discussion and confrontation with
the opportunities and limitations of AI.

993
01:16:06.630 --> 01:16:12.820 
and only then can a reflected position be developed. but as students

994
01:16:12.940 --> 01:16:16.900 
we need guidance and i really want to emphasize this.

995
01:16:17.880 --> 01:16:25.500 
we need guidance to understand the basics of
AI to gain a discriminatory power to differentiate

996
01:16:25.830 --> 01:16:29.530 
the basic concepts.
but we also need an understanding as user

997
01:16:30.190 --> 01:16:34.370 
of the legal ethical and legal implications and very quickly

998
01:16:34.370 --> 01:16:39.060 
it becomes a matter of our own self image as physicians and

999
01:16:39.820 --> 01:16:44.490 
discussions often focus on whether AI will replace the physician

1000
01:16:44.860 --> 01:16:49.220 
especially to radiologist or pathologist in the future.

1001
01:16:49.750 --> 01:16:54.520 
But this shows that there is not even
a basic understanding of AI and

1002
01:16:54.810 --> 01:16:58.010 
usually the normal medical student is not going to sit down.

1003
01:16:58.010 --> 01:17:02.970 
in the afternoon or evening and watch moocs
on the basics of machine learning,

1004
01:17:03.260 --> 01:17:08.190 
because he or she is not not even sensitized to what this has

1005
01:17:08.190 --> 01:17:12.110 
to do with the professional a work as a physician.

1006
01:17:16.790 --> 01:17:19.440 
Anna you  wanted to say something?

1007
01:17:20.420 --> 01:17:25.500 
while i was applauding, but i also wanted to say something,
because i think that

1008
01:17:26.960 --> 01:17:33.930 
there is a and if you have looked back into
the field of health informatics, medical informatics

1009
01:17:35.300 --> 01:17:41.770 
what i received in the discussions of AI today
is sort of a reiteration or

1010
01:17:42.180 --> 01:17:46.930 
the 3.0 of decision support that was

1011
01:17:46.930 --> 01:17:50.060 
as very promising in the field in the seventies and eighties.

1012
01:17:51.470 --> 01:17:55.300 
and it could be maybe a good idea to look back and see what

1013
01:17:55.410 --> 01:18:00.330 
did they do that were everyone got so excited about and why did that

1014
01:18:02.160 --> 01:18:05.860 
why  did that not be or why were it not

1015
01:18:08.090 --> 01:18:13.190 
sustained. because there are certain aspects and I think what we see

1016
01:18:13.420 --> 01:18:18.490 
when it comes to AI in healthcare, where you do have clear cost

1017
01:18:20.660 --> 01:18:27.510 
structure data you see more AI then when you deal
with a a more fussy clinical

1018
01:18:28.400 --> 01:18:31.580 
assessments of the clinical evaluations.

1019
01:18:32.070 --> 01:18:37.530 
so I part of understanding the limitations of

1020
01:18:37.530 --> 01:18:41.420 
AI is sorts of a discussion of how do we express

1021
01:18:42.010 --> 01:18:45.430 
health, how do we express health concerns?

1022
01:18:46.930 --> 01:18:51.830 
it is we see that there is more progress
when it comes to medical imaging.

1023
01:18:52.900 --> 01:18:58.660 
and the biological markers?
then when it comes to

1024
01:18:59.140 --> 01:19:02.900 
big data and for example watson and

1025
01:19:04.900 --> 01:19:08.620 
those are those aspects where you are totally dependent on

1026
01:19:08.870 --> 01:19:12.780 
a rich curated data that you can do something about.
so I think

1027
01:19:12.780 --> 01:19:15.230 
there is a lot to do is a lot to discuss say it

1028
01:19:15.710 --> 01:19:19.140 
must be very exciting to be starting a career in healthcare

1029
01:19:19.720 --> 01:19:24.450 
these days with all the opportunities.
and i look forward to the continuous discussion.

1030
01:19:24.990 --> 01:19:27.690 
i'm also going to leave you in a couple of minutes,
because i

1031
01:19:28.110 --> 01:19:32.210 
have to continue in another meeting, but thank you!

1032
01:19:39.240 --> 01:19:43.070 
sebastian, yeah i've maybe one question.
I think it would

1033
01:19:43.070 --> 01:19:46.010 
be maybe for both maybe for Keston and for Jenny.

1034
01:19:47.160 --> 01:19:48.860 
i think today we have a big focus on

1035
01:19:49.490 --> 01:19:51.550 
the position of the area also

1036
01:19:52.370 --> 01:19:56.390 
what is the right spot for moocs.
but maybe what is also the limitation of it

1037
01:19:56.710 --> 01:20:00.430 
and I would like to maybe hear your personal opinion on

1038
01:20:01.900 --> 01:20:06.500 
moose an AI education.
I think what do you think the position

1039
01:20:06.500 --> 01:20:09.840 
that maybe the strength of the mooc is,

1040
01:20:10.480 --> 01:20:13.430 
but also where do we have to go maybe also

1041
01:20:14.350 --> 01:20:17.780 
into a direct inter- interaction with each other maybe.

1042
01:20:17.780 --> 01:20:23.740 
it's in the superlattice online format
or an on-site format come like

1043
01:20:23.950 --> 01:20:28.840 
would you see the come like the line between which would you

1044
01:20:28.840 --> 01:20:33.670 
put in the mooc and which would you put maybe
in synchronous or outside education?

1045
01:20:34.380 --> 01:20:38.250 
in addition to that so i think you all planning
the winter semester right?

1046
01:20:39.560 --> 01:20:42.990 
these are exactly the questions we have right now online,

1047
01:20:43.800 --> 01:20:49.560 
on the place or some hybrid formats,
so your answers are very important.

1048
01:20:51.060 --> 01:20:53.230 
I think we both want our problem solved.

1049
01:20:54.340 --> 01:20:56.550 
I think you have our problem.

1050
01:20:58.120 --> 01:21:00.660 
so two minutes please two problems.

1051
01:21:04.030 --> 01:21:08.810 
so shall i answer, I do want to okay.

1052
01:21:09.550 --> 01:21:10.870 
well I think that

1053
01:21:12.400 --> 01:21:14.620 
moocs are,

1054
01:21:15.260 --> 01:21:17.090 
so online learning platforms

1055
01:21:18.020 --> 01:21:23.020 
can I believe these causes can form an important pitch

1056
01:21:23.130 --> 01:21:26.810 
to compensate for the current lack of offerings.

1057
01:21:26.810 --> 01:21:30.370 
so I think it is very important and
desirable that these causes become

1058
01:21:30.370 --> 01:21:36.240 
part maybe of the medical education
and can also be credited as an achievement.

1059
01:21:36.900 --> 01:21:39.630 
but it has its limitations and

1060
01:21:40.840 --> 01:21:45.990 
we need also these hands on experience
and when i think about

1061
01:21:46.930 --> 01:21:49.110 
our our course in mind's.

1062
01:21:50.260 --> 01:21:55.880 
I had this section in my mind when we had

1063
01:21:56.330 --> 01:22:02.950 
a section we were three of us and one was a patient with certain

1064
01:22:02.950 --> 01:22:07.760 
symptoms and one person was supposed to
take a history like we just learned

1065
01:22:08.550 --> 01:22:13.090 
and the other person used the app ada to take the history.

1066
01:22:13.100 --> 01:22:16.630 
and that was an interesting experience,

1067
01:22:16.640 --> 01:22:22.900 
because it became clear that ada had much
more knowledge to draw on and as a result

1068
01:22:23.060 --> 01:22:29.390 
also very rare diseases were considered.
but the elp had failed to ask about

1069
01:22:29.590 --> 01:22:33.670 
pre existing conditions or previous surgeries.

1070
01:22:34.240 --> 01:22:38.380 
so which we learn in the first clinical semester at the

1071
01:22:38.380 --> 01:22:43.670 
latest to ask about these things so that clarified something

1072
01:22:43.670 --> 01:22:48.360 
very quickly, namely that such an app does not replace the doctor.

1073
01:22:49.750 --> 01:22:54.550 
and that it has limitations, but it also has a great strength

1074
01:22:54.550 --> 01:22:59.930 
namely in the area of cognitive knowledge and control on thousands

1075
01:22:59.930 --> 01:23:04.070 
of clinical pictures and symptom constellations. and such learning

1076
01:23:04.070 --> 01:23:08.340 
experience are incredibly important.

1077
01:23:08.790 --> 01:23:12.400 
so moocs are really an important.

1078
01:23:13.850 --> 01:23:18.510 
i found that is really important. but we need his hands-on experience.

1079
01:23:21.050 --> 01:23:25.350 
I totally agree and also anna said that like a hybrid version is good

1080
01:23:25.620 --> 01:23:28.990 
and I think like some parts like the technical stuff or some

1081
01:23:28.990 --> 01:23:33.490 
explanations they usually just do like teaching on a board.

1082
01:23:33.540 --> 01:23:36.290 
it is really helpful to have this online and also supported by

1083
01:23:36.290 --> 01:23:40.680 
illustration videos and so on and then some other parts such

1084
01:23:40.680 --> 01:23:43.660 
as discussions should be live and also. it's very important

1085
01:23:43.660 --> 01:23:47.080 
for the group atmosphere that the people discuss  with each other

1086
01:23:47.330 --> 01:23:51.390 
that they together make a like making presentations
they usually have some

1087
01:23:51.610 --> 01:23:55.210 
creativity task where they can match different teams.

1088
01:23:55.210 --> 01:24:00.130 
and then they're going to plan their own AI application.
and I think this is really

1089
01:24:00.590 --> 01:24:05.590 
useful and very important for students
also to keep interested in this topic.

1090
01:24:05.880 --> 01:24:09.780 
otherwise it would be to like to be too separated. I know that

1091
01:24:09.780 --> 01:24:12.500 
the AI company is also very interested in social learning.

1092
01:24:13.320 --> 01:24:17.180 
and how to achieve that they also have social learning on the

1093
01:24:17.180 --> 01:24:21.820 
AI platform so that they discuss certain things and forums like

1094
01:24:22.020 --> 01:24:26.150 
and check places or so on,
but I think it's at the moment, it's not

1095
01:24:26.780 --> 01:24:31.620 
working so nicely. but maybe they have some ideas.
I don't know

1096
01:24:31.620 --> 01:24:34.270 
how to engage people to keep track on this.

1097
01:24:36.490 --> 01:24:42.440 
okay I think unfortunately we already at the end of our panel.

1098
01:24:44.100 --> 01:24:47.090 
thanks a lot, especially to the speakers was really

1099
01:24:47.090 --> 01:24:50.470 
interesting to listen to you and I already left thank you very

1100
01:24:50.470 --> 01:24:53.890 
much for supporting us thank you also for the audience for

1101
01:24:53.890 --> 01:24:57.810 
listening and asking interesting questions and yeah

1102
01:24:58.960 --> 01:25:01.600 
thanks a lot and bye to everyone.
