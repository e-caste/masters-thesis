WEBVTT

1
00:00:01.040 --> 00:00:03.129 
Hello and welcome. Today

2
00:00:03.129 --> 00:00:06.759 
we will be briefly talking about the history of neural networks.

3
00:00:08.839 --> 00:00:16.370 
Why learn history? As mentioned by Harari, it is not about to predict the future but free

4
00:00:16.370 --> 00:00:22.949 
our mind avoid being shaped by the past and imagine alternative destinies.

5
00:00:23.940 --> 00:00:31.050 
We know that artificial neural networks have experienced several rises and falls in its not so long history.

6
00:00:32.240 --> 00:00:41.469 
In addition to analyzing the technical evolution of neural networks, we might also try to learn how to be objectively calmly

7
00:00:41.469 --> 00:00:44.859 
facing the ups and downs of the technology waves.

8
00:00:47.840 --> 00:00:56.060 
This diagram briefly shows the subjective popularity of some mainstream machine learning methods in different periods.

9
00:00:56.939 --> 00:01:05.349 
We can figure out that there are two enlightenment periods of neural networks in which perception and back propagation play

10
00:01:05.349 --> 00:01:06.560 
the core role.

11
00:01:07.239 --> 00:01:13.730 
But unfortunately after a short period of poor prosperity, they all suffered from

12
00:01:13.730 --> 00:01:14.560 
AI Winters.

13
00:01:17.340 --> 00:01:25.340 
Meanwhile, other machine learning methods such as random forests, AdaBoost and support vector machine won the favor of more

14
00:01:25.340 --> 00:01:27.260 
people in the last two decades.

15
00:01:28.840 --> 00:01:40.709 
So starting from the year 2006, neural networks blow some in a way that nobody expected. Connectionism assured in another

16
00:01:40.709 --> 00:01:43.060 
Ai revolution which is deep learning.

17
00:01:43.640 --> 00:01:52.500 
Three pioneers, Geoffrey Hinton, LeCun and Bengio recognized as the father of deep learning won the Turing award

18
00:01:52.510 --> 00:02:03.049 
of 2018 and this is probably the best compliment to their many efforts in the AI winters of many years.

19
00:02:06.739 --> 00:02:12.460 
First let's step into biological neuron and check how they work at a high level.

20
00:02:13.439 --> 00:02:22.349 
The brain has billions of neurons, each of them with thousands of connections and that's what it takes to create human mind.

21
00:02:23.219 --> 00:02:28.300 
The whole field of AI is based on an understanding of how our brain works.

22
00:02:28.939 --> 00:02:37.870 
So, you know, over millions of years of evolution, nature has come up with a way to make us think and we just reverse engineer

23
00:02:37.870 --> 00:02:44.789 
the way that the brain works. So that we can get insights on how to make the machines.

24
00:02:44.800 --> 00:02:54.009 
So within our brain specifically our cerebral cortex, which is where all of our thinking happens.

25
00:02:54.020 --> 00:02:56.270 
We have a branch of neurons.

26
00:02:56.280 --> 00:03:02.960 
They are in the rural nerve cells and they are connected to each other via axons and dendrites.

27
00:03:04.439 --> 00:03:13.550 
Now where enough of a simple signals are activated, an individual neuron will be fired and send a signal to all the neurons

28
00:03:13.550 --> 00:03:15.360 
that it's connected to.

29
00:03:15.939 --> 00:03:20.759 
So as we see the individual level neuron is very simple mechanism.

30
00:03:21.539 --> 00:03:29.129 
But when we start to have many of these neurons connected together in many, many different ways with different strengths

31
00:03:29.139 --> 00:03:30.539 
between the connections.

32
00:03:30.550 --> 00:03:32.960 
Things get very complicated.

33
00:03:33.539 --> 00:03:41.610 
You have a very simple concept, very simple model, but when you start enough of them together you will have very complex

34
00:03:41.610 --> 00:03:45.259 
behavior and this can become a learning behavior.

35
00:03:45.740 --> 00:03:51.860 
It actually works not only our brain, it works you know also in our computer as well.

36
00:03:53.939 --> 00:04:02.250 
Furthermore, if we look deeper into the biology of our brain, we can see that within our cortex neurons,

37
00:04:02.740 --> 00:04:08.870 
this seems to be arranged into Starks or cortical columns that process information

38
00:04:08.879 --> 00:04:09.759 
in parallel.

39
00:04:12.139 --> 00:04:17.449 
It is believed that mammalian neural cortex is hierarchical.

40
00:04:18.139 --> 00:04:21.459 
The recognition pathway has multiple stages.

41
00:04:21.470 --> 00:04:25.060 
There are a lot of intermedia representations there.

42
00:04:25.540 --> 00:04:36.959 
For example the light signal comes from retina in the area we want we see the basic forms like edges and corners and

43
00:04:36.970 --> 00:04:42.019 
in the area before we can see feature groups and in the AI

44
00:04:42.019 --> 00:04:42.120 


45
00:04:42.120 --> 00:04:42.439 


46
00:04:42.439 --> 00:04:47.360 
finally we see the high level object objects like faces.

47
00:04:51.639 --> 00:05:00.550 
More interesting is that each of these columns is in turn made of mini columns of around 100 neurons per mini column.

48
00:05:01.319 --> 00:05:10.120 
There are about 100 million mini columns in the Cortex which showing significant statistical meaning but we still don't

49
00:05:10.120 --> 00:05:11.579 
know the functional meaning.

50
00:05:12.740 --> 00:05:17.699 
And coincidentally, this is similar architecture to how GPU works.

51
00:05:17.709 --> 00:05:20.360 
The graphic processing unit of our computer.

52
00:05:21.040 --> 00:05:27.160 
It has a bunch of very simple and very small processing unit is normally called a ALU.

53
00:05:27.160 --> 00:05:27.579 


54
00:05:27.589 --> 00:05:32.660 
Arithmatic logic unit which are responsible for computing.

55
00:05:33.339 --> 00:05:39.160 
So at some point someone could say the way we think neurons work is pretty simple.

56
00:05:39.170 --> 00:05:45.250 
It wouldn't be too hard to replicate that ourselves and maybe try to build our own brain.

57
00:05:47.740 --> 00:05:55.680 
And this idea goes all the way back to 1943 using the same basic concept as the biological neurons.

58
00:05:55.689 --> 00:06:05.269 
McCulloch and Pitts proposed the first artificial neuron is called MP neuron. According to the diagram for a neural note

59
00:06:05.269 --> 00:06:08.959 
yj, it may receive many input signals, xi.

60
00:06:08.970 --> 00:06:09.449 


61
00:06:10.040 --> 00:06:19.720 
Positive and negative ways are used to express the synaptic excitement and inhibition in the biological neurons and the weight

62
00:06:19.720 --> 00:06:22.250 
value represents the connection strength.

63
00:06:23.040 --> 00:06:30.649 
That is when only when the sum of its inputs exceeds some threshold, the neuron are activated.

64
00:06:30.660 --> 00:06:33.750 
Otherwise the neuron will be suppressed.

65
00:06:34.139 --> 00:06:39.430 
By doing this MP model, MP neuron can create logical expressions.

66
00:06:39.439 --> 00:06:42.769 
For example, the logical all operator.

67
00:06:42.779 --> 00:06:52.610 
If the sum from the inputs greater or equal a threshold value one, it is also possible to implement the logical AND and logic

68
00:06:52.610 --> 00:06:54.750 
NOT in a similar means.

69
00:06:58.339 --> 00:06:59.790 
Building upon that -

70
00:06:59.800 --> 00:07:05.750 
Psychologists Rosenblatt created the Perceptron in 1958.

71
00:07:06.740 --> 00:07:11.750 
A perceptron has one hidden layer made of multiple MP neurons.

72
00:07:12.240 --> 00:07:17.759 
We normally use a sine function as the activation function for a single layer perception.

73
00:07:18.339 --> 00:07:25.050 
So basically, there are two state, 1 and 0 for each neuron.

74
00:07:26.339 --> 00:07:31.120 
He applies the Hebbian theory, babies rules to train the neuron ways.

75
00:07:35.240 --> 00:07:44.139 
So now we are starting to get into things that can actually learn by re forcing ways between these neurons nodes, we can create

76
00:07:44.139 --> 00:07:48.949 
a system that learns over time how to produce desired output.

77
00:07:49.339 --> 00:07:57.579 
And again, this is working more towards our growing understanding of how our brain works. Perceptron.

78
00:07:57.589 --> 00:08:01.149 
first time demonstrate the learning ability of a machine.

79
00:08:01.639 --> 00:08:10.189 
This made the perceptron all the rage setting up the first wave of artificial intelligence. While the single layer perceptron

80
00:08:10.199 --> 00:08:11.959 
is simple and elegant.

81
00:08:12.740 --> 00:08:22.959 
But it is clearly not smart enough it has the ability to classify linear problems, simply the data points in the graphics that

82
00:08:22.959 --> 00:08:25.360 
can be divided by a straight line.

83
00:08:26.540 --> 00:08:31.850 
For example, logic AND and logical OR are linear separable problem.

84
00:08:32.240 --> 00:08:37.059 
We can use a single line to separate the 0 and 1 in this figure.

85
00:08:39.139 --> 00:08:45.929 
However, dealing with the nonlinear problems, the single layer perceptron cannot do anything about it.

86
00:08:45.940 --> 00:08:55.909 
For example, the following XOR cannot be separated by a straight line. So the single layer perception cannot solve a simple XOR

87
00:08:56.289 --> 00:08:57.220 
problem.

88
00:08:57.230 --> 00:09:02.460 
This drawback has been first time pointed out by Miss Key in their book.

89
00:09:03.340 --> 00:09:09.960 
The misunderstanding of the arguments in the book directly resulted in the 1st AI winter.

90
00:09:14.139 --> 00:09:24.049 
The development of artificial neuron networks field has been long years of stagnation and low tide at the result, almost

91
00:09:24.049 --> 00:09:32.759 
our research funding is provided for artificial neural networks have been dried up and experts in many fields have abandoned

92
00:09:32.759 --> 00:09:34.659 
their research on this topic.

93
00:09:35.440 --> 00:09:41.029 
But this is just the limitation of cognition of human in a certain period of time.

94
00:09:41.039 --> 00:09:47.259 
Just like heliocentric overthrowing the longstanding dominance of the geocentric.

95
00:09:50.240 --> 00:09:51.350 
Thank you for watching.
