WEBVTT

1
00:00:00.610 --> 00:00:05.180 
Welcome to knowledge graphs lecture six
"Advance Knowledge Graph Applications".

2
00:00:05.520 --> 00:00:09.100 
So in this lecture we are going to talk
about knowledge graph embeddings.

3
00:00:11.250 --> 00:00:16.680 
So what do we use, what is the
semantic similarity? So for search

4
00:00:16.680 --> 00:00:21.000 
and retrieval system semantic similarity
of entity is very important.

5
00:00:21.560 --> 00:00:25.850 
How do you do that? So given an
entity you've tried to find most

6
00:00:25.850 --> 00:00:30.790 
similar entities around it or
given an entity you try to find

7
00:00:30.790 --> 00:00:35.020 
more similar documents or
given a document you try

8
00:00:35.020 --> 00:00:39.850 
to find the similarity between the documents
and get the most similar documents.

9
00:00:40.160 --> 00:00:44.260 
So what does it actually
mean? So let's see

10
00:00:45.230 --> 00:00:50.700 
this what are the two entities
which are semantically similar.

11
00:00:51.130 --> 00:00:56.210 
So for example let us consider an
example where carbon dioxide

12
00:00:56.730 --> 00:01:02.160 
is a greenhouse gas, water vapor
is also a greenhouse gas. Then

13
00:01:02.160 --> 00:01:08.080 
we have another set of facts that is Albert
Einstein is a physicist, Stephen Hawking

14
00:01:08.310 --> 00:01:13.060 
is a physicist. So given these
two set of information,

15
00:01:13.840 --> 00:01:18.320 
is Stephen Hawking most
similar to Albert Einstein

16
00:01:18.330 --> 00:01:23.590 
or to the carbon dioxide. I mean of
course you having the background

17
00:01:23.590 --> 00:01:27.170 
knowledge you can answer this
question more easily but how

18
00:01:27.180 --> 00:01:31.580 
would a machine answer such kind of questions
with the help of knowledge graphs?

19
00:01:32.400 --> 00:01:37.650 
So here you see two knowledge graphs- on
your left, on your right. So you have

20
00:01:37.840 --> 00:01:41.480 
on your left you have carbon
dioxide which is a gas and then

21
00:01:41.480 --> 00:01:45.500 
water vapor which is also a gas,
both are chemical compound,

22
00:01:45.500 --> 00:01:49.330 
both are greenhouse gas. And so
you have this kind of contextual

23
00:01:49.330 --> 00:01:53.250 
information about the entity
carbon dioxide here.

24
00:01:53.530 --> 00:01:57.530 
And on your right you see the
other subgraph where you see

25
00:01:57.530 --> 00:02:02.380 
Stephen Hawking an entity which
is linked to physicists. So

26
00:02:02.480 --> 00:02:07.020 
Stephen Hawking is a physicist,
Albert Einstein is also a physicist

27
00:02:07.020 --> 00:02:10.730 
and then you have the information
about both the scientist

28
00:02:10.730 --> 00:02:15.880 
or one of the scientists also like
albert einstein was born in Ulm

29
00:02:16.440 --> 00:02:18.600 
and other information.

30
00:02:20.500 --> 00:02:26.450 
So here you can see carbon dioxide and
water vapor share similar kind of

31
00:02:27.110 --> 00:02:32.470 
neighboring nodes. So this is what
we mean by the context of a graph.

32
00:02:34.690 --> 00:02:39.660 
Then we have Stephen Hawking and
Albert Einstein. They also

33
00:02:39.670 --> 00:02:43.580 
share similar neighbouring
nodes, similar neighborhood.

34
00:02:43.910 --> 00:02:49.150 
So this means they have this
structural context in the graph.

35
00:02:54.060 --> 00:02:57.680 
How do you find these kind of
similarities? So you should know

36
00:02:57.680 --> 00:03:02.370 
you shall know a node by the
company it keeps. A similar node

37
00:03:02.520 --> 00:03:06.980 
can be identified by having
the similar environment.

38
00:03:08.230 --> 00:03:12.850 
So the point is how do you measure
these kind of similarities

39
00:03:12.850 --> 00:03:15.180 
that you will see in the
course of this lecture.

40
00:03:15.620 --> 00:03:20.250 
So similar entities are represented
by nodes that are connected

41
00:03:20.250 --> 00:03:23.490 
to similar facts that
we saw previously

42
00:03:24.120 --> 00:03:29.220 
and that is that they that are connected
to similar graph structures.

43
00:03:29.910 --> 00:03:35.100 
In order to identify similar entities
we have to identify similar

44
00:03:35.100 --> 00:03:39.330 
graph structures, the neighborhood
of that and of that entity.

45
00:03:39.610 --> 00:03:45.100 
So for doing so there are many
graph algorithms, graph mining

46
00:03:45.260 --> 00:03:50.480 
algorithms which perform the
semantic similarity but

47
00:03:50.480 --> 00:03:52.030 
they are of high
complexity.

48
00:03:53.420 --> 00:03:59.270 
So in the end of the
last lecture you saw

49
00:03:59.280 --> 00:04:03.920 
that when the knowledge
graphs are bigger then this

50
00:04:03.930 --> 00:04:07.470 
semantic matching gets
really difficult. So

51
00:04:08.660 --> 00:04:13.410 
in this case if we have wikidata it
doesn't really work efficiently

52
00:04:13.410 --> 00:04:19.660 
because wikidata keeps a lot of information
a lot of a a huge number of triples.

53
00:04:20.420 --> 00:04:25.530 
So how do we approximate these kind
of these kind of algorithms? So

54
00:04:25.870 --> 00:04:31.510 
one of the ways to perform the
approximation is by transferring

55
00:04:31.650 --> 00:04:36.220 
from graph structures to the vector
spaces which are more easier to handle.

56
00:04:37.750 --> 00:04:42.410 
So here this is how you go from the
graph structure to the vector space.

57
00:04:42.590 --> 00:04:46.100 
So on the left you have a graph
which is not directed but it

58
00:04:46.100 --> 00:04:50.510 
is labelled only on the nodes and
then on the right side this

59
00:04:50.510 --> 00:04:55.810 
is how the vector space of this
of this graph looks like.

60
00:04:56.000 --> 00:05:00.560 
Sso here you can see on the right side
that the vector space contains the

61
00:05:01.030 --> 00:05:05.790 
contains the vectors of the nodes
here and the similar nodes

62
00:05:05.790 --> 00:05:09.500 
are appearing closer to each
other in this vector space as

63
00:05:09.500 --> 00:05:12.180 
compared to the ones
which are dissimilar.

64
00:05:13.980 --> 00:05:18.320 
So now we are going to just take
a little excursion in the

65
00:05:18.320 --> 00:05:22.230 
middle of this lecture which
would be about word embeddings.

66
00:05:22.400 --> 00:05:27.700 
So the similar kind of vector spaces have
been created for the word embeddings

67
00:05:27.970 --> 00:05:33.560 
where the natural language words are
mapped to a dense vector representation.

68
00:05:33.780 --> 00:05:37.170 
So the assumption here is that
the similar words would occur

69
00:05:37.170 --> 00:05:41.730 
together. So for example you have
carbon dioxide, water vapors, methane

70
00:05:42.110 --> 00:05:46.910 
they would occur together
because these are

71
00:05:46.910 --> 00:05:49.440 
the driving agents
of climate change.

72
00:05:50.380 --> 00:05:55.450 
So the basic idea here is that
instead of counting co-occurrences

73
00:05:55.450 --> 00:05:59.380 
of the words we predict the
likelihood of the appearance of

74
00:05:59.380 --> 00:06:01.720 
the words in the
neighborhood of others.

75
00:06:02.670 --> 00:06:08.320 
So in this case we train a
predictor which can predict

76
00:06:08.330 --> 00:06:13.680 
a word from its context. So this is one
of the ways to train the predictor

77
00:06:13.950 --> 00:06:17.860 
which is called continuous bag
of words. The other way given

78
00:06:17.870 --> 00:06:22.110 
a context it actually predicts the
words. So this is what we call

79
00:06:22.390 --> 00:06:23.140 
skip gram.

80
00:06:24.560 --> 00:06:30.100 
So in skip gram what happens you train a
neural network with one hidden layer

81
00:06:30.210 --> 00:06:34.240 
and use the output hidden layer
as vector representation.

82
00:06:35.960 --> 00:06:41.520 
So here for example carbon dioxide
water vapor and methane will

83
00:06:41.680 --> 00:06:46.740 
activate similar kind of context
words, so which means that

84
00:06:46.740 --> 00:06:51.440 
their output weights at the
projection layer should be similar

85
00:06:51.440 --> 00:06:52.140 
to each other.

86
00:06:54.800 --> 00:07:00.050 
So now let's see how the word embeddings
look like. Here you can actually see

87
00:07:00.230 --> 00:07:05.170 
you are on the left hand side you
have the word embeddings for man,

88
00:07:05.500 --> 00:07:10.830 
woman, king and queen. So here man
and woman are closer to each

89
00:07:10.830 --> 00:07:14.570 
other but we are going
to see some examples

90
00:07:14.570 --> 00:07:21.020 
with the analogies also. Then you have
where verbs as in the vector spaces

91
00:07:21.150 --> 00:07:24.050 
and then in the third one you
have the countries and capitals

92
00:07:24.050 --> 00:07:27.950 
which are there. So for
example you have China

93
00:07:27.950 --> 00:07:31.610 
and Vietnam which are close to each
other because they are both countries

94
00:07:32.050 --> 00:07:36.610 
and Hanoi and Beijing they are both
close because they are the capitals.

95
00:07:36.940 --> 00:07:41.820 
However so Beijing is
more close to China as

96
00:07:41.820 --> 00:07:44.760 
compared to Hanoi is much
more close to China.

97
00:07:45.270 --> 00:07:51.150 
So here in this analogies how
would it work? So for example

98
00:07:51.510 --> 00:07:58.240 
if we remove if we do the minus
operator for the vector of king

99
00:07:59.020 --> 00:08:04.350 
and man, so king minus man would be
somebody who actually rules the country.

100
00:08:04.900 --> 00:08:10.420 
This would be equivalent to queen
minus woman that would also be

101
00:08:10.530 --> 00:08:15.920 
somebody who rules the country.
However if you do king minus man

102
00:08:16.240 --> 00:08:20.320 
plus woman so it would be
somebody who rules the country

103
00:08:21.180 --> 00:08:25.630 
and woman then it would
be equivalent to queen.

104
00:08:26.890 --> 00:08:31.400 
So this kind of analogies we can actually
perform over the word embeddings.

105
00:08:31.960 --> 00:08:35.660 
So now we move from the word
embeddings to the graph embeddings.

106
00:08:36.030 --> 00:08:40.210 
So here what happens you have the
nodes and you have the edges

107
00:08:40.330 --> 00:08:45.790 
that you have already seen. So
you can encode this node into

108
00:08:45.960 --> 00:08:50.850 
into vector space. So you have
for each of these nodes

109
00:08:51.420 --> 00:08:58.040 
one vector. And then in the next step you
can define another function which is

110
00:08:58.250 --> 00:09:03.470 
decode. This, in the decoding
you can have you can

111
00:09:03.470 --> 00:09:08.940 
decode the neighborhood of one
node or one vertex or the

112
00:09:09.610 --> 00:09:12.050 
or the label of
that node.

113
00:09:13.850 --> 00:09:18.610 
So here you can see it more clearly
the goal is to encode the nodes

114
00:09:18.830 --> 00:09:23.190 
of the graph in a way that the
similarity in the embedding space

115
00:09:23.720 --> 00:09:25.990 
which is actually given
by the dot product

116
00:09:26.740 --> 00:09:33.070 
and we have to approximate this
similarity in the original network.

117
00:09:33.450 --> 00:09:37.680 
So how do we do that? So we have
the encoder function as I said

118
00:09:37.750 --> 00:09:42.890 
explained a little bit before which
encodes the nodes to its vectors

119
00:09:43.370 --> 00:09:48.700 
and then we have this decode
function with actually decodes

120
00:09:48.710 --> 00:09:53.360 
the similarity of the edges, oh
sorry, of the nodes in the vector

121
00:09:53.360 --> 00:09:55.320 
space to the
original graph.

122
00:09:58.180 --> 00:10:01.890 
So here we define, actually
this is what is saying

123
00:10:01.890 --> 00:10:04.260 
the same thing that
define an encoder

124
00:10:04.900 --> 00:10:09.210 
which is a mapping from nodes to
the embeddings and then define

125
00:10:09.220 --> 00:10:12.970 
a node similarity function that
specifies how relationships

126
00:10:12.970 --> 00:10:18.390 
in the vector space map to the
relationships in the original network.

127
00:10:18.560 --> 00:10:20.170 
And the optimization

128
00:10:21.030 --> 00:10:24.170 
and then we optimize the
parameters of the encoder

129
00:10:24.600 --> 00:10:26.770 
for the similarity
function

130
00:10:27.960 --> 00:10:29.530 
as we explained before.

131
00:10:30.300 --> 00:10:34.290 
Now up until now what you have
seen are the graph embeddings.

132
00:10:34.500 --> 00:10:38.730 
So now we are moving from the graph embeddings
to the knowledge graph embeddings.

133
00:10:39.410 --> 00:10:43.580 
So there are so far many algorithms
proposed for that. There

134
00:10:43.580 --> 00:10:46.670 
are many ways to generate knowledge
graph embeddings. I am going

135
00:10:46.670 --> 00:10:52.430 
to discuss only some of them. So here you
have for example translational methods

136
00:10:52.530 --> 00:10:56.550 
which are transE, transH,
transR, transEdge, there

137
00:10:56.560 --> 00:11:00.620 
are a lot of them. There is a
whole family of this algorithms.

138
00:11:01.170 --> 00:11:05.760 
Then we have rotation based methods which
actually rotate the vector spaces.

139
00:11:05.980 --> 00:11:08.540 
So it is rotatE.

140
00:11:09.170 --> 00:11:13.670 
Then we have graph convolutional
networks which is relational

141
00:11:13.670 --> 00:11:17.610 
graph convolutional networks
and transGCN. So i'm going to

142
00:11:17.610 --> 00:11:22.370 
discuss one of them and then we
have walk-based methods which

143
00:11:22.480 --> 00:11:28.060 
use random walks for generating as
a first step for generating the

144
00:11:28.450 --> 00:11:32.020 
knowledge graph embedding such
as DeepWalk and RDF2Vec.

145
00:11:33.450 --> 00:11:37.890 
So let's give you an overview of the
translational distance models.

146
00:11:38.090 --> 00:11:44.150 
So what happens here is that it uses
distance based scoring function?

147
00:11:44.540 --> 00:11:49.600 
What is that? It means that it
measures the plausibility of a fact

148
00:11:49.750 --> 00:11:53.380 
as the distance between the two
entities. You will see a little

149
00:11:53.380 --> 00:11:58.000 
bit more explanation into it in the
next slide. And the translation

150
00:11:58.000 --> 00:12:01.890 
there is carried out with
the help of a relation. So

151
00:12:01.960 --> 00:12:07.230 
you can see here there
are many models which

152
00:12:07.230 --> 00:12:10.720 
are using these translational
based scoring functions and I

153
00:12:10.720 --> 00:12:12.300 
am going to try
to discuss

154
00:12:12.900 --> 00:12:14.310 
one or two of them.

155
00:12:15.160 --> 00:12:19.910 
So here for example this is the
first algorithm that was proposed

156
00:12:19.910 --> 00:12:24.890 
for knowledge graph embeddings where
the entities and relations are

157
00:12:25.160 --> 00:12:29.540 
embedded in the same vector space.
So now here I am going to

158
00:12:29.670 --> 00:12:33.640 
introduce the terminology which
is usually used for them. So

159
00:12:33.640 --> 00:12:39.620 
you know that you have entities and
then relations. So here you have then

160
00:12:39.970 --> 00:12:44.980 
subject, property, object. The
subject is called the head

161
00:12:45.840 --> 00:12:50.530 
and then object is called the
tail and the property is the

162
00:12:50.530 --> 00:12:55.980 
relationship. So for head we
use h, for tail we use t, for

163
00:12:55.980 --> 00:12:57.540 
relation we use r.

164
00:12:58.860 --> 00:13:04.230 
So as we said before also that
relation r is considered

165
00:13:04.230 --> 00:13:09.680 
as translation from h two t,
so if you add the vector of

166
00:13:09.890 --> 00:13:15.170 
h to the vector of r, you will
obtain t. This way you get the

167
00:13:15.450 --> 00:13:19.220 
value of the t and you can also
use this for knowledge graphs,

168
00:13:19.240 --> 00:13:23.610 
knowledge graph completion. Now
the problem with this kind

169
00:13:23.610 --> 00:13:28.660 
of this algorithm it always considers
one to one relationships.

170
00:13:29.110 --> 00:13:32.640 
So it does not take into an account
symmetric functions, 1-N,

171
00:13:32.640 --> 00:13:35.440 
N-1 or N-N
relationships.

172
00:13:37.530 --> 00:13:42.230 
So there is an enhancement
over this algorithm

173
00:13:42.550 --> 00:13:46.730 
is that what it does this is
transH and what it does from

174
00:13:46.760 --> 00:13:51.360 
it, what it does is from original
space it goes to the hyperplane.

175
00:13:51.870 --> 00:13:56.760 
So here hyperplane is actually a
subspace here. I will not go

176
00:13:56.760 --> 00:14:00.470 
into the detail. I think this
much is enough to understand

177
00:14:00.470 --> 00:14:05.810 
the idea behind transH. So it
enables different roles of

178
00:14:05.810 --> 00:14:11.080 
an entity for different relationships. So
of course each entity is connected to

179
00:14:11.590 --> 00:14:16.220 
some other entity and each entity
has different kind of properties.

180
00:14:16.270 --> 00:14:22.920 
So to consider that it uses it helps in
capturing this kind of information.

181
00:14:23.730 --> 00:14:29.740 
So what happens is that entities h
and t are projected into specific

182
00:14:29.870 --> 00:14:33.680 
hyperplane of the relation which
is so here in the dotted line

183
00:14:33.680 --> 00:14:36.510 
you see this hyperplane which
is just a subspace here

184
00:14:36.960 --> 00:14:41.420 
in the vector space. And then
it is used for predicting new

185
00:14:41.420 --> 00:14:45.200 
links based on this translation
on the hyperplane.

186
00:14:48.260 --> 00:14:52.110 
So now what you saw up until now
they were considered these

187
00:14:52.310 --> 00:14:58.290 
two algorithms these algorithms were considered
only these triple level information.

188
00:14:58.530 --> 00:15:04.230 
So you have the head tail and the
relation. However what about

189
00:15:04.230 --> 00:15:07.060 
the contextual information that
we were talking about in the

190
00:15:07.060 --> 00:15:10.950 
beginning? That is actually
considering the semantics of the

191
00:15:10.950 --> 00:15:12.870 
neighborhoods
of one node.

192
00:15:13.530 --> 00:15:19.310 
So for doing so we have a graph
convolutional networks which actually

193
00:15:19.690 --> 00:15:23.550 
so it considers the contextual
information around an entity

194
00:15:24.190 --> 00:15:28.650 
but graph convolutional
networks actually considers

195
00:15:28.720 --> 00:15:33.700 
unlabeled and undirected graphs.
So we have seen already that

196
00:15:34.460 --> 00:15:39.430 
we are most awe are always
considering knowledge graphs as

197
00:15:39.580 --> 00:15:43.880 
the labeled multi
diagraph.

198
00:15:44.970 --> 00:15:51.250 
So here for this reason
there was an evolution of

199
00:15:51.260 --> 00:15:56.170 
graph convolutional networks which
was introduced which is relational

200
00:15:56.500 --> 00:16:01.280 
draft convolutional networks.
So it considers the relation

201
00:16:01.280 --> 00:16:05.070 
information also for generating
the graph embeddings

202
00:16:05.810 --> 00:16:09.820 
which can then further be used
in link prediction or a

203
00:16:09.830 --> 00:16:13.540 
triple classification. We are going
to talk more in detail about

204
00:16:13.540 --> 00:16:17.620 
in the next lecture. So for
the link prediction what

205
00:16:17.620 --> 00:16:22.830 
happens is that as an encoder
the embedding the embeddings

206
00:16:22.830 --> 00:16:27.110 
of the entities the latent features
of the entities are actually

207
00:16:27.110 --> 00:16:33.280 
actually generated, and then in
the decoder these are used

208
00:16:33.280 --> 00:16:37.020 
for predicting the labelled edges.
So you are actually predicting

209
00:16:37.020 --> 00:16:40.960 
the link between the two entities,
so which means you are predicting

210
00:16:40.980 --> 00:16:43.190 
the relation between
the two entities.

211
00:16:45.810 --> 00:16:49.790 
And then we have word based
methods out of which I am going

212
00:16:49.790 --> 00:16:53.910 
to explain a little bit about one
of them. So here I am going

213
00:16:53.910 --> 00:16:58.150 
to talk about RDF2Vec. So what
happens in word2vec that

214
00:16:58.150 --> 00:17:03.580 
you saw in the small excursion that
we did before was we generated

215
00:17:03.810 --> 00:17:08.430 
one embedding space for
the words. In rdf2vec

216
00:17:08.600 --> 00:17:14.420 
it generates embedding spaces from the
rdf data and represents the nodes and

217
00:17:14.720 --> 00:17:20.710 
edges there. So in order to do
that what it does is generates

218
00:17:20.710 --> 00:17:24.040 
the sentences the sequences
from the knowledge graphs

219
00:17:24.470 --> 00:17:29.060 
from the rdf triples and of
course it considers multiple

220
00:17:29.380 --> 00:17:33.300 
it considers the neighbourhood
of the graph and this can be

221
00:17:33.300 --> 00:17:37.290 
done with the help of several
selection strategies. So you

222
00:17:37.290 --> 00:17:42.680 
can use depth-first search, breadth-first
search, random walk, rdf graph kernels.

223
00:17:43.270 --> 00:17:47.840 
So here you can actually see it
with the help of one example

224
00:17:47.840 --> 00:17:51.140 
where we explain a little
bit about the rdf walks.

225
00:17:51.550 --> 00:17:57.150 
In rdf2vec. So for example if I
want to generate the sequences

226
00:17:57.160 --> 00:17:59.780 
out of the graph that you
RDF graph that you are

227
00:17:59.780 --> 00:18:04.400 
seeing in front of you. And we are
doing it only for the depth three.

228
00:18:05.230 --> 00:18:10.760 
So how does it happen is that let's
start with the entity carbon dioxide.

229
00:18:11.130 --> 00:18:18.630 
Then we want to go to the depth of three,
so you take carbon dioxide and then

230
00:18:18.900 --> 00:18:23.560 
you read the edge that is rdf
type, you see it in blue in the

231
00:18:23.560 --> 00:18:28.160 
bottom of the slide and then you have
the greenhouse gas. Now we have this

232
00:18:28.370 --> 00:18:33.040 
one hop here that, we
have depth of one.

233
00:18:33.830 --> 00:18:40.250 
Then we take contributing factor of which
actually connects to greenhouse effect.

234
00:18:40.380 --> 00:18:45.130 
Then here we have depth 2 and
then we go toward discovered

235
00:18:45.130 --> 00:18:50.490 
by and Joseph Fourier that
will be the depth three.

236
00:18:50.750 --> 00:18:54.960 
So after these sequences are
obtained these are fit to word2vec

237
00:18:54.960 --> 00:19:00.200 
that we have seen before in this
lecture and then this way you obtain

238
00:19:00.560 --> 00:19:02.710 
these knowledge graph

239
00:19:02.710 --> 00:19:05.110 
embeddings with the
help of rdf2vec.

240
00:19:06.700 --> 00:19:12.120 
So finally these are the libraries, so
if you want to play around with them

241
00:19:12.230 --> 00:19:17.090 
these are the libraries which provide the
code for knowledge graph embeddings.

242
00:19:17.380 --> 00:19:21.510 
So the first one is actually
provided by PyTorch, the second

243
00:19:21.510 --> 00:19:26.350 
one is by AmpliGraph, the
third one is Pykeen which is

244
00:19:26.530 --> 00:19:31.650 
provided by the smart data analytics
and then the third one is openKE.

245
00:19:33.940 --> 00:19:37.070 
So in the next lecture we are
going to talk about knowledge

246
00:19:37.070 --> 00:19:41.870 
graph completion which are
actually used for evaluating

247
00:19:41.870 --> 00:19:43.990 
the methods for knowledge
graph embeddings.
