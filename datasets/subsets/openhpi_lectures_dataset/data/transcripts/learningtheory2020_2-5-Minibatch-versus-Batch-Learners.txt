WEBVTT

1
00:00:01.570 --> 00:00:06.410 
Welcome. While we have considered so
far full batch learners and iterative

2
00:00:06.410 --> 00:00:07.650 
or mini bridge
learners,

3
00:00:08.320 --> 00:00:13.680 
we have proved that the half spaces
are learnable by full batch learners

4
00:00:13.840 --> 00:00:18.090 
and by mini batch learners.
But do mini batch learners

5
00:00:18.210 --> 00:00:21.680 
have the same learning power as
full batch learners, meaning

6
00:00:21.820 --> 00:00:25.460 
can they also learn every
hypothesis space?

7
00:00:26.140 --> 00:00:29.300 
We will answer this question in
this video and for this we

8
00:00:29.300 --> 00:00:33.590 
introduce a very important notion namely
the notion of a locking sequence.

9
00:00:33.750 --> 00:00:37.790 
Formally this is defined as
follows: so we have a learner, we

10
00:00:37.790 --> 00:00:42.820 
have a prediction model and we have
a finite sequence 𝜏 of data

11
00:00:43.110 --> 00:00:47.020 
labelled data and we call this
sequence a locking sequence

12
00:00:47.020 --> 00:00:53.390 
for this learner M on the predicrtion
model f if first of all

13
00:00:53.580 --> 00:00:59.060 
𝜏 fits f so it is consistent with
f, so all the labels the tao

14
00:00:59.160 --> 00:01:03.560 
assigns to natural numbers are
the same labels that f assigns

15
00:01:03.560 --> 00:01:04.700 
to the natural numbers.

16
00:01:07.270 --> 00:01:12.670 
The prediction model that M outputs after
having seen 𝜏 is indeed correct.

17
00:01:13.090 --> 00:01:16.830 
So this is the same prediction
model that we want to learn.

18
00:01:18.090 --> 00:01:24.360 
And last of all no matter which finite
sequence that is consistent with f,

19
00:01:24.510 --> 00:01:26.360 
so the labels
are the same,

20
00:01:27.060 --> 00:01:31.580 
we pick and we add it to 𝜏.
So we have the sequence

21
00:01:31.580 --> 00:01:36.030 
𝜏 and afterwards we just add
the labelled numbers in σ,

22
00:01:36.320 --> 00:01:42.860 
then M will just repeat its guess. So
if a consistent sequence is added

23
00:01:43.330 --> 00:01:47.770 
then the learner will not change its
mind anymore, and in this sense

24
00:01:47.950 --> 00:01:51.330 
the learner is locked
after it has seen 𝜏.

25
00:01:52.730 --> 00:01:54.410 
Well let's illustrate
this a bit.

26
00:01:55.280 --> 00:02:00.190 
So we have the learning process, the
learner maybe at first after three images

27
00:02:00.410 --> 00:02:04.300 
gets this half space after four
images it gets this one and

28
00:02:04.300 --> 00:02:09.720 
after five images it sticks to
this one then because we assumed

29
00:02:09.870 --> 00:02:14.050 
that this half-space is the
correct one the locking

30
00:02:14.050 --> 00:02:18.340 
sequence we can talk about
here is already the first

31
00:02:18.350 --> 00:02:25.520 
four labeled images because no matter
which image that is labelled

32
00:02:25.760 --> 00:02:29.260 
correctly according to the question
is there a butterfly on it or not,

33
00:02:29.580 --> 00:02:33.310 
the learner will see it will
not change its mind anymore

34
00:02:33.770 --> 00:02:37.670 
if we assume that this was a for example
learning the enumeration strategy.

35
00:02:39.780 --> 00:02:43.810 
Ok let's look at another
example. So maybe the learner

36
00:02:44.150 --> 00:02:47.760 
after three images gets this one,c
after four images gets this

37
00:02:47.760 --> 00:02:53.210 
half space and after five images it was
correct. Then now the locking sequence

38
00:02:53.400 --> 00:02:58.460 
will be the sequence of these five images.
This is an example for locking sequence

39
00:02:58.670 --> 00:03:02.170 
because no matter which images
it will see afterwards,

40
00:03:02.690 --> 00:03:07.030 
it will not change its mind
anymore and it is correct

41
00:03:07.210 --> 00:03:12.150 
and this sequence of images is
like also correctly labeled.

42
00:03:12.330 --> 00:03:16.040 
This is the first requirement in the
definition of a locking sequence.

43
00:03:17.660 --> 00:03:20.440 
Ok so now we have definition
but how does it help?

44
00:03:21.150 --> 00:03:25.750 
Well we want to prove this
is a very important result

45
00:03:26.070 --> 00:03:29.980 
that if M learns f then there
is indeed a locking sequence

46
00:03:29.980 --> 00:03:33.650 
from M on f. So
what do we do?

47
00:03:34.360 --> 00:03:38.750 
Well this is a nice proved technique
that is often used in mathematics.

48
00:03:38.990 --> 00:03:42.240 
So we assume the contrary
namely that there

49
00:03:42.800 --> 00:03:50.020 
is no such sequence. So there is an
f and there is an M and M learns f

50
00:03:50.240 --> 00:03:52.490 
but there is no
locking sequence.

51
00:03:53.280 --> 00:03:57.240 
Ok then we look at the following
informant. So it's just the

52
00:03:57.250 --> 00:04:01.060 
informant where each natural
number comes in the right order.

53
00:04:01.070 --> 00:04:04.910 
So for zero the label of
zero, one the label of one,

54
00:04:05.370 --> 00:04:07.340 
two the label of
two and so on.

55
00:04:09.090 --> 00:04:14.400 
Then because we don't have a
locking sequence for every i

56
00:04:15.420 --> 00:04:21.530 
with f of i, we can find a sequence
that if we inserted afterwards

57
00:04:21.880 --> 00:04:26.040 
it will make like the learner
will make a mind change

58
00:04:26.700 --> 00:04:30.030 
because we can like pick the
sequence to be consistent so that

59
00:04:30.030 --> 00:04:33.890 
the first and the second
requirement are fulfilled.

60
00:04:35.280 --> 00:04:40.300 
But yeah we know by our
assumption that M has to

61
00:04:40.300 --> 00:04:43.410 
make a mind change because otherwise it
would have been a locking sequence.

62
00:04:45.760 --> 00:04:48.960 
Ok but then if we look at the
informant where we still have

63
00:04:48.960 --> 00:04:52.210 
like zero f of zero but then we
insert a σ zero so there

64
00:04:52.210 --> 00:04:56.110 
is a mind change then we have
one, f of one we insert σ

65
00:04:56.110 --> 00:04:59.880 
one, so there is a mind change, we
insert two, f of two and then

66
00:04:59.880 --> 00:05:04.780 
σ two the mind change and this
informant still is consistent

67
00:05:04.780 --> 00:05:07.620 
with f, so still all the
labels in this informant

68
00:05:08.220 --> 00:05:12.430 
are the same labels that f
assigns but on this informant

69
00:05:12.660 --> 00:05:16.370 
M will make infinitely many
mind changes. And so

70
00:05:16.370 --> 00:05:19.890 
the assumption that there was no
locking sequence was wrong.

71
00:05:20.540 --> 00:05:22.360 
We have proven
the theorem.

72
00:05:27.840 --> 00:05:32.970 
In order to compare the full batch
and the iterative learners

73
00:05:33.310 --> 00:05:36.580 
we will look at several
or we will find

74
00:05:37.640 --> 00:05:43.360 
maybe or I can show you already, we
will find some other hypothesis space

75
00:05:43.590 --> 00:05:47.830 
that is learnable in full batch
manner but is not learnable

76
00:05:47.830 --> 00:05:52.620 
in an iterative manner. And we
will write sets. So I want

77
00:05:52.620 --> 00:05:57.300 
to talk about in which sense sets and
prediction models correspond to each other.

78
00:05:57.720 --> 00:06:01.410 
So for example if I have the full
natural numbers, so everything

79
00:06:01.410 --> 00:06:05.960 
is labeled by one, this corresponds
to exactly this function; every n

80
00:06:06.140 --> 00:06:10.740 
is mapped to one. If we have the empty
set so there are no elements in it,

81
00:06:11.000 --> 00:06:14.370 
then well every natural
number gets the label zero.

82
00:06:14.900 --> 00:06:19.130 
So we have that the sets correspond
to their characteristic functions

83
00:06:19.400 --> 00:06:24.040 
in just the way I told you. Ok so if
I have the set that only contains

84
00:06:24.040 --> 00:06:28.520 
a single natural number then
the function that assigns one

85
00:06:28.630 --> 00:06:34.860 
only if the input is this and zero
and assigns zero in all other cases

86
00:06:35.140 --> 00:06:38.700 
is again a prediction model and
directly corresponds to this

87
00:06:38.700 --> 00:06:42.360 
set only containing this one
element and the other way around

88
00:06:42.370 --> 00:06:46.160 
for the complement, so for the
set that contains all numbers

89
00:06:46.160 --> 00:06:51.740 
but not n0. So the characteristic
function the prediction model

90
00:06:52.160 --> 00:06:56.910 
for this set will assign to all
natural numbers the value one

91
00:06:57.040 --> 00:06:58.540 
that are not n0.

92
00:07:01.520 --> 00:07:06.310 
Ok we can do that too for the even
numbers. So the value of the

93
00:07:06.310 --> 00:07:10.940 
prediction model is one if the
number is even and for the

94
00:07:10.990 --> 00:07:15.640 
odd numbers just the same
way and what we will use

95
00:07:16.090 --> 00:07:20.810 
in the following is that we can
also do the same for finite sets.

96
00:07:20.980 --> 00:07:25.730 
So if we have a finite set that
contains the numbers n0, n1

97
00:07:26.010 --> 00:07:29.520 
until N - size
of the set

98
00:07:30.900 --> 00:07:35.300 
then the function, the
characteristic function,

99
00:07:35.300 --> 00:07:38.090 
or the prediction model that
corresponds to it is just the

100
00:07:38.090 --> 00:07:42.260 
function that assigns one to every
natural number in this finite set

101
00:07:42.360 --> 00:07:44.580 
n zero to all
other numbers.

102
00:07:46.510 --> 00:07:51.580 
Ok now we are talking about hypothesis
spaces, so about collections of sets.

103
00:07:51.790 --> 00:07:55.790 
So examples of collections
of sets that you might be

104
00:07:55.790 --> 00:07:59.410 
interested to think about whether
they are hypothesis spaces

105
00:07:59.580 --> 00:08:02.230 
are for example all
singletons collected

106
00:08:03.000 --> 00:08:06.620 
or all complements of
singletons collected

107
00:08:08.110 --> 00:08:13.180 
or all sets that are multiples
of a pre-specified k

108
00:08:13.480 --> 00:08:18.060 
collected, you might want to include
or exclude k equals zero here.

109
00:08:19.810 --> 00:08:24.030 
And the yeah the set that
collects all finite subsets,

110
00:08:25.030 --> 00:08:30.820 
so for every finite subset we have that
it is in this said collection Fin,

111
00:08:32.410 --> 00:08:35.620 
or we can just add one more
element like that of the whole

112
00:08:35.620 --> 00:08:39.090 
set which is not finite and you
can think of a lot of other

113
00:08:39.090 --> 00:08:40.220 
things to come up with.

114
00:08:41.270 --> 00:08:45.990 
Ok so what we will use in the
following is that for example

115
00:08:46.090 --> 00:08:51.730 
fin and also are the finite
images end one in finite image

116
00:08:51.740 --> 00:08:54.840 
namely whole natural numbers
are hypothesis spaces.

117
00:08:55.760 --> 00:09:00.940 
Or just a short word on that,
well if we have an input n

118
00:09:01.290 --> 00:09:07.740 
and we have an i, then from i we
can unmerge two natural numbers

119
00:09:08.190 --> 00:09:11.200 
and the first one we interpret
as the size of the finite set

120
00:09:11.550 --> 00:09:15.670 
so that from the second one we can
emerge all elements of the finite set.

121
00:09:16.350 --> 00:09:20.180 
And then we can check whether the N
that was our input in the beginning

122
00:09:20.530 --> 00:09:23.740 
equals one of these elements
of the finite set and then

123
00:09:23.740 --> 00:09:27.700 
we output one if it does and zero
if it does not and this is

124
00:09:27.700 --> 00:09:31.490 
already a uniform
decision procedure for

125
00:09:31.980 --> 00:09:36.640 
for the finite sets and if you
now have an infinite set

126
00:09:36.650 --> 00:09:40.000 
or any other set to it that is
not already contained you

127
00:09:40.000 --> 00:09:45.630 
can shift this whole thing by one, so that
zero stands for the whole natural numbers

128
00:09:45.740 --> 00:09:51.810 
and for all other i you do the yeah
the unmerge method that I already

129
00:09:51.980 --> 00:09:53.400 
talked about a
minute ago.

130
00:09:55.230 --> 00:09:58.790 
So I hope you believe me that
these are hypothesis spaces if

131
00:09:58.790 --> 00:10:02.020 
you want to see a formal proof you
can find it in the hand out.

132
00:10:04.180 --> 00:10:07.050 
Now the theorem is the
following: so indeed

133
00:10:07.680 --> 00:10:11.520 
this collection of all finite
sets and the infinite

134
00:10:11.520 --> 00:10:15.360 
set of natural numbers cannot be
learned by an iterative learner.

135
00:10:15.730 --> 00:10:19.260 
Because it's a hypothesis space we
already know that it can be learned

136
00:10:19.390 --> 00:10:21.110 
by a full information
learner.

137
00:10:23.800 --> 00:10:29.240 
How does the proof work? Well again it's
a contradictory argument. So we assume

138
00:10:29.340 --> 00:10:33.490 
that we have an iterative learner and
it learns this hypothesis space.

139
00:10:34.730 --> 00:10:38.210 
Then we already proved then we
have a locking sequence. So

140
00:10:38.220 --> 00:10:41.720 
locking sequences are very important
in these kinds of arguments.

141
00:10:41.720 --> 00:10:46.030 
We have a locking sequence for the whole
natural numbers. So we have a sequence

142
00:10:46.150 --> 00:10:50.830 
of positively labeled natural
numbers because all natural

143
00:10:50.830 --> 00:10:53.390 
numbers are positively labeled
with regard to the whole set

144
00:10:54.250 --> 00:11:00.510 
and we know that when the learner sees
a positively labeled natural number

145
00:11:00.610 --> 00:11:05.190 
after 𝜏 then it will not
change its mind because a 𝜏

146
00:11:05.190 --> 00:11:06.440 
is a locking sequence.

147
00:11:08.900 --> 00:11:13.640 
So we look at all the natural
numbers that are positively

148
00:11:13.640 --> 00:11:17.540 
labeled by 𝜏. 𝜏 is a finite
sequence, so this is a finite set.

149
00:11:19.460 --> 00:11:23.290 
And then 𝜏 is consistent
with this finite set and

150
00:11:25.560 --> 00:11:30.350 
we can find a locking sequence
for this finite set D

151
00:11:30.880 --> 00:11:33.360 
which extends 𝜏.

152
00:11:34.040 --> 00:11:40.270 
So we just have more labelled positive,
more labeled natural numbers

153
00:11:41.050 --> 00:11:44.410 
but now there might also be
negatively labeled natural numbers

154
00:11:44.410 --> 00:11:48.200 
in σ because D is finite. So
there are also natural numbers

155
00:11:48.200 --> 00:11:49.200 
that are not in D

156
00:11:51.790 --> 00:11:57.120 
and we know that as long as the learner
sees only data that is consistent

157
00:11:57.300 --> 00:12:01.080 
with this set D, this finite
set, then it will not change

158
00:12:01.090 --> 00:12:02.200 
its mind anymore.

159
00:12:05.380 --> 00:12:11.780 
Okay but 𝜏 with σ is again finite.
So we can find a natural number

160
00:12:11.990 --> 00:12:16.620 
that is not labelled neither positively
nor negatively by 𝜏 σ.

161
00:12:16.960 --> 00:12:18.010 
We call this number m.

162
00:12:19.930 --> 00:12:23.840 
And then, well, then now the magic
with the locking sequences happens

163
00:12:24.190 --> 00:12:26.960 
but also we need that
m is iterative.

164
00:12:27.530 --> 00:12:32.530 
So m after 𝜏 and σ
outputs a prediction model

165
00:12:32.530 --> 00:12:36.040 
for capital D for this
finite set and as

166
00:12:37.060 --> 00:12:42.690 
m and zero is something that
has not been labelled by 𝜏,

167
00:12:43.320 --> 00:12:46.430 
we know that m
is not in D

168
00:12:47.090 --> 00:12:50.570 
because it only contains the elements
that were positively labeled by

169
00:12:50.850 --> 00:12:56.960 
𝜏. So if we add m negatively labeled
afterwards this is consistent

170
00:12:57.080 --> 00:13:00.940 
with D and so the learner
will not change its mind

171
00:13:02.870 --> 00:13:04.340 
from 𝜏 σ

172
00:13:05.180 --> 00:13:09.240 
but on the other hand
side if we inserted m,1

173
00:13:09.660 --> 00:13:15.450 
after 𝜏 already because 𝜏 is a locking
sequence for the natural numbers

174
00:13:16.020 --> 00:13:19.330 
m will also not change its
mind after having seen it.

175
00:13:20.000 --> 00:13:23.550 
And then it's σ so it will
output the same as if it has

176
00:13:23.550 --> 00:13:27.610 
seen only 𝜏 σ. And
then it also outputs

177
00:13:27.610 --> 00:13:33.140 
the same when we see m zero
afterwards as we have argued ago.

178
00:13:35.430 --> 00:13:38.660 
Ok but now we have two
finite sets, namely D

179
00:13:39.250 --> 00:13:42.280 
and we have D with
this m in addition

180
00:13:42.900 --> 00:13:49.230 
and learner, well, it outputs the
same hypothesis after having seen

181
00:13:49.340 --> 00:13:53.960 
sequences that are only consistent
with one of them and if we continue

182
00:13:54.150 --> 00:13:58.400 
with only information that
does not contain this m

183
00:13:59.100 --> 00:14:02.280 
then it will not change its mind
anymore. So it cannot learn

184
00:14:02.280 --> 00:14:06.120 
both, it cannot learn
D and D with this m.

185
00:14:07.170 --> 00:14:10.690 
Ok but this is a contradiction because
we assume that the learner learns

186
00:14:10.980 --> 00:14:12.900 
all finite sets.

187
00:14:13.560 --> 00:14:17.870 
So our assumption was wrong end there
can't be an iterative learner

188
00:14:18.090 --> 00:14:22.870 
learning this collection. And
so what we proved is we

189
00:14:22.870 --> 00:14:26.400 
proved that mini batch learners
have less learning power than

190
00:14:26.400 --> 00:14:27.360 
full batch learners.

191
00:14:30.480 --> 00:14:34.010 
We used for this the concept
of a locking sequence

192
00:14:37.340 --> 00:14:42.410 
and we proved also that every successful
learning process is always witnessed

193
00:14:42.520 --> 00:14:46.490 
by such a locking sequence and
this is a very helpful tool

194
00:14:47.050 --> 00:14:51.280 
and in the next week we will talk
about whether we really want

195
00:14:51.280 --> 00:14:54.850 
to allow all iterative computable
learners or whether we want

196
00:14:54.850 --> 00:14:59.100 
to put maybe other restrictions
or requirements on the learner

197
00:14:59.390 --> 00:15:02.160 
and there also locking
sequences will prove helpful.
