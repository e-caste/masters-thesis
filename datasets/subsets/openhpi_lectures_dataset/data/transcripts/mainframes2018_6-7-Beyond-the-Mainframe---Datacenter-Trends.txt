WEBVTT

1
00:00:00.910 --> 00:00:06.560 
Okay so welcome at our final presentation talking about the

2
00:00:06.560 --> 00:00:09.390 
mainframe and data center trends, and the future.

3
00:00:10.340 --> 00:00:13.990 
Title of this presentation is Beyond the Mainframe- Datacenter Trends.

4
00:00:15.550 --> 00:00:19.680 
First we want to look back and identify some trends and if

5
00:00:19.680 --> 00:00:24.380 
you look at this old classification, you see the SISD, SIMD

6
00:00:24.380 --> 00:00:31.200 
MISD and MIMD systems classification by Flynn from 1966, but

7
00:00:31.500 --> 00:00:36.520 
basically we distinguish between unit processors the classical for normal computer

8
00:00:36.790 --> 00:00:39.650 
and vector and area processors which are

9
00:00:40.090 --> 00:00:46.590 
all computers, but today we have the entire world of multiprocessor distributed systems

10
00:00:46.740 --> 00:00:50.170 
and we have the pipelining as a concept that we see in all

11
00:00:50.170 --> 00:00:53.250 
the instruction sets today as well. So basically

12
00:00:54.110 --> 00:00:56.750 
we can notice that all the systems that we

13
00:00:57.250 --> 00:01:00.590 
see today are either SIMD or MIMD systems.

14
00:01:01.450 --> 00:01:06.830 
And if you look at MIMD systems, then you see similarities to standard

15
00:01:07.190 --> 00:01:11.380 
multicore, manycore systems. Today you may have shared memory

16
00:01:11.380 --> 00:01:12.970 
with and close coupling,

17
00:01:13.830 --> 00:01:19.750 
you may have shared disks with RAID systems, or you may have

18
00:01:19.770 --> 00:01:23.320 
so called shared-nothing systems. So most systems today are

19
00:01:23.320 --> 00:01:25.890 
still closely coupled but there's a clear trend

20
00:01:26.380 --> 00:01:30.510 
towards distributed systems with fewer coupling

21
00:01:31.370 --> 00:01:35.770 
facilities. The mainframe belongs in the category of closely coupled systems.

22
00:01:37.360 --> 00:01:41.680 
There's another aspect and this is often called a NUMA

23
00:01:42.220 --> 00:01:47.150 
problem so the non uniform memory access problem that systems

24
00:01:47.390 --> 00:01:52.130 
are built out of components, and while for a long time the hardware kept the

25
00:01:52.230 --> 00:01:54.610 
impression that the components are all

26
00:01:56.310 --> 00:02:00.300 
ordered and accessed in a quite regular fashion,

27
00:02:01.420 --> 00:02:05.550 
this abstraction is no longer maintained by today's hardware.

28
00:02:05.900 --> 00:02:08.670 
So we have NUMA characteristics, which means that

29
00:02:09.230 --> 00:02:11.060 
some parts of the memory are

30
00:02:12.560 --> 00:02:17.550 
quicker accessible from a processor than other parts of the memory

31
00:02:18.500 --> 00:02:20.650 
that are closer to other processes.

32
00:02:21.330 --> 00:02:24.810 
Sp memory and processing units are getting co-located,

33
00:02:25.300 --> 00:02:32.110 
and this has implication on the speed of execution if you organize your programs

34
00:02:33.130 --> 00:02:34.380 
not correctly. One more

35
00:02:36.570 --> 00:02:42.040 
look into the history and into the textbook basically,

36
00:02:42.610 --> 00:02:46.460 
the sequence symmetry which was a classical example for a system

37
00:02:46.460 --> 00:02:51.290 
that is well balanced that is symmetric in the sense that all

38
00:02:51.290 --> 00:02:56.840 
the processors have the same access path to memory. You have a bus infrastructure

39
00:02:57.210 --> 00:02:59.890 
and so forth. So a well balanced system

40
00:03:00.790 --> 00:03:03.280 
classical MIMD computer

41
00:03:03.910 --> 00:03:08.380 
and then we see another example that became quite

42
00:03:08.380 --> 00:03:12.280 
famous, which is still pro-gun. It was not terribly successful

43
00:03:12.280 --> 00:03:16.110 
in the commercial sense however it was a milestone in the sense that it

44
00:03:16.410 --> 00:03:21.560 
consisted of many nodes and each node was running its own operating system's

45
00:03:21.790 --> 00:03:26.240 
instance. In that case that was the MACH operating system but

46
00:03:26.750 --> 00:03:31.950 
this is an indication that systems getting closer to clusters

47
00:03:31.950 --> 00:03:36.140 
or distributed systems are in today's world

48
00:03:36.990 --> 00:03:39.230 
closer then to classical

49
00:03:40.220 --> 00:03:43.990 
SIMD or MIMD systems. And there's actually an entire trend

50
00:03:43.990 --> 00:03:47.210 
which is called red scale computing which indicates that future

51
00:03:47.210 --> 00:03:50.160 
architectures are going to be built in that fashion.

52
00:03:53.970 --> 00:03:58.250 
So we are talking about scaling and scaling happens in scale

53
00:03:58.250 --> 00:04:03.620 
out scenarios, basically addressing more than one computer, addressing an entire

54
00:04:03.810 --> 00:04:09.670 
rack, but scaling also happens in scale up scenarios where computers getting

55
00:04:09.880 --> 00:04:14.480 
more compac,  more dense. If you still look at standard architectures

56
00:04:14.820 --> 00:04:18.750 
where multi-core and multi sweating are key features and we

57
00:04:18.750 --> 00:04:24.180 
look at standard operating systems with socialization and trustworthiness and security clustering

58
00:04:24.430 --> 00:04:29.270 
as key attributes. Turns out these systems

59
00:04:31.370 --> 00:04:36.060 
need new programming models,  we look at software architecture,

60
00:04:36.060 --> 00:04:38.090 
we look at services. So

61
00:04:39.410 --> 00:04:44.640 
there's a trend that architecture principles that have been

62
00:04:44.640 --> 00:04:48.940 
exercised in the mainframe world for a long time, that they arrive at standard

63
00:04:49.280 --> 00:04:53.150 
architectures at commodity service today and need to be addressed

64
00:04:53.150 --> 00:04:54.260 
by the software as well.

65
00:04:55.360 --> 00:04:58.500 
And then there is one one other problem that is

66
00:04:59.270 --> 00:05:03.130 
well known to the mainframe people, but also largely ignored

67
00:05:03.130 --> 00:05:07.810 
by the mainframe people because these systems have one objective,

68
00:05:07.810 --> 00:05:09.110 
and this is zero downtime.

69
00:05:10.330 --> 00:05:15.860 
If you look at other optimization criteria then energy consumption comes

70
00:05:15.970 --> 00:05:20.280 
into play, and what we see here is a screenshot actually from

71
00:05:20.280 --> 00:05:26.460 
an on-board administration console and blade system where we are able to

72
00:05:26.660 --> 00:05:28.570 
put power caps on the

73
00:05:31.550 --> 00:05:32.600 
power units and

74
00:05:34.030 --> 00:05:37.560 
restricting the energy consumption of the entire computer. So

75
00:05:37.560 --> 00:05:41.050 
this is actually an open research question- how should you structure your programs

76
00:05:41.270 --> 00:05:46.340 
how should you place your workload so that the energy consumption

77
00:05:47.360 --> 00:05:48.580 
is minimized?

78
00:05:50.060 --> 00:05:55.870 
And along that line comes the term of green IT, where we look at

79
00:05:55.870 --> 00:06:00.470 
consolidation and we try to increase the efficiency of program execution.

80
00:06:00.680 --> 00:06:05.000 
And you can only optimize what you can measure. So the first initial question is,

81
00:06:05.280 --> 00:06:08.480 
how would you measure energy consumption of your programs? So

82
00:06:08.480 --> 00:06:11.660 
if you look at your programming language, then you

83
00:06:12.290 --> 00:06:17.340 
will figure out that there's no such keyword like power consumption or energy

84
00:06:17.610 --> 00:06:21.240 
consumption in the language. So it has to be somewhere in between.

85
00:06:21.240 --> 00:06:24.630 
It's a non functional property, but it needs to be added to

86
00:06:24.820 --> 00:06:25.820 
systems in the future.

87
00:06:27.730 --> 00:06:32.910 
Next aspect, I mean going from zero downtime what the mainframe stands for, to

88
00:06:33.230 --> 00:06:41.010 
more commodity commoditized service systems, we still talk about dependability.

89
00:06:41.240 --> 00:06:44.510 
And dependability means that we have certain trustworthiness

90
00:06:45.300 --> 00:06:47.610 
that the computer system

91
00:06:48.460 --> 00:06:53.930 
works reliable and the question always is how can we deal with unexpected events

92
00:06:54.350 --> 00:07:00.050 
and these events they impact the cost and performance of a

93
00:07:00.050 --> 00:07:02.160 
system. We need maybe to replicate

94
00:07:02.820 --> 00:07:09.790 
units, we need to take measures for fault prevention and and fault removal.

95
00:07:10.220 --> 00:07:12.720 
We need to look at the source code and

96
00:07:13.820 --> 00:07:17.820 
put efforts into software engineering, but also

97
00:07:18.430 --> 00:07:20.510 
it's a question of system architecture.

98
00:07:22.050 --> 00:07:26.380 
And there are examples like the left hand side where we have

99
00:07:26.380 --> 00:07:29.990 
to have a solution, we look at mission-critical systems, and

100
00:07:31.070 --> 00:07:33.640 
if it comes to human life no price is too high. This is the

101
00:07:33.640 --> 00:07:35.970 
place for the mainframe and still will

102
00:07:36.930 --> 00:07:40.450 
be the place for the mainframe for a long, long time. But

103
00:07:40.450 --> 00:07:43.760 
then we have also large scale clusters and distributed systems,

104
00:07:43.760 --> 00:07:45.230 
and these many core service

105
00:07:45.950 --> 00:07:50.840 
we want to optimize for both- for performance and cost but also

106
00:07:50.980 --> 00:07:53.790 
to some extent at least for dependability. So what is the

107
00:07:54.350 --> 00:07:58.030 
unit of failure problem basically.

108
00:08:00.510 --> 00:08:03.670 
Another aspect if you look at commodity service, this is the

109
00:08:03.670 --> 00:08:07.340 
introduction of accelerators of additional functional units

110
00:08:07.340 --> 00:08:13.390 
that are no longer CPUs but maybe GPUs or FPJS that bring additional

111
00:08:13.390 --> 00:08:16.740 
compute capacity to the table, but somehow need to be

112
00:08:18.080 --> 00:08:22.210 
added into the infrastructure. And this can be done via

113
00:08:22.840 --> 00:08:26.390 
interfaces like the PCI Express bus for instance or can be done

114
00:08:27.490 --> 00:08:33.120 
in a cash coherent manner. So the seamless integration of accelerators

115
00:08:33.120 --> 00:08:36.260 
is another trend that we see in the data center.

116
00:08:37.030 --> 00:08:41.560 
And the mainframe is actually not a very good example for integrating

117
00:08:41.560 --> 00:08:42.490 
accelerators

118
00:08:44.590 --> 00:08:48.540 
if it comes to additional functional units. If it comes to

119
00:08:49.820 --> 00:08:54.120 
enriching the instruction set and building additional functionality on a processor,

120
00:08:54.480 --> 00:08:58.310 
then the mainframe is the prime example for implementing java

121
00:08:58.470 --> 00:09:02.320 
application processes for implementing crypto units and having

122
00:09:02.320 --> 00:09:05.860 
extensions to the instruction set. So the message here while

123
00:09:05.860 --> 00:09:09.260 
mainframe tries to integrate everything into a single box,

124
00:09:09.900 --> 00:09:13.590 
the trend in a commodity service is rather that we keep adding

125
00:09:14.610 --> 00:09:15.950 
additional functional units.

126
00:09:17.210 --> 00:09:20.770 
And this goes quite a way what we see here is a screenshot

127
00:09:20.770 --> 00:09:24.880 
of the structure of a silicon graphics systems that we actually

128
00:09:24.880 --> 00:09:30.790 
have in the building, where we see that besides the quickpath interconnect which is

129
00:09:31.560 --> 00:09:32.910 
the prime link inter-

130
00:09:34.220 --> 00:09:37.220 
connecting two structures of four

131
00:09:37.860 --> 00:09:45.320 
basically blades four sockets to a more dense interconnection network and a

132
00:09:47.750 --> 00:09:50.710 
directory based protocol in place to

133
00:09:51.820 --> 00:09:53.360 
set party interconnection

134
00:09:54.160 --> 00:09:59.040 
technologies in place. So Rackscale computing- a future trend in the data center.

135
00:10:00.650 --> 00:10:03.500 
And besides the mainframe

136
00:10:05.380 --> 00:10:09.050 
there's another system this is worth mentioning which will be IBM power.

137
00:10:10.470 --> 00:10:14.760 
Also a little exotic in comparison to the ubiquitous intel architecture

138
00:10:15.120 --> 00:10:18.790 
however a system that inherits many attributes from the

139
00:10:18.790 --> 00:10:24.780 
the mainframe in terms of high availability, in terms of maintainability and serviceability.

140
00:10:25.090 --> 00:10:30.340 
And these are also great scale up systems like depicted here

141
00:10:30.790 --> 00:10:31.550 
on the screen.

142
00:10:34.090 --> 00:10:39.140 
The power architecture is interesting because they have a special approach towards integrating

143
00:10:39.380 --> 00:10:44.020 
accelerators with the computer. As I mentioned, nothing gets

144
00:10:44.680 --> 00:10:49.380 
close to the mainframe where we just integrate or keep integrating an instruction set

145
00:10:49.830 --> 00:10:54.640 
additional capabilities. But with power is CAPI and CAPI is to

146
00:10:54.640 --> 00:10:58.070 
Coherent Accelerator Processor Interface very idea is that we

147
00:10:58.070 --> 00:11:02.110 
have a cache coherent integration of accelerator units like GPU or FPGA

148
00:11:03.330 --> 00:11:04.370 
is the processor.

149
00:11:05.990 --> 00:11:09.930 
And then you figure that problems occur

150
00:11:10.340 --> 00:11:12.150 
in software more and more

151
00:11:13.110 --> 00:11:18.320 
and that we have additional fault classes, that we have less error containment

152
00:11:18.570 --> 00:11:19.920 
if it comes to software.

153
00:11:20.670 --> 00:11:24.010 
If you look from an HPC High-Performance Computing Perspective

154
00:11:24.670 --> 00:11:29.340 
it might even be as bad that as entire system, entire computation

155
00:11:29.590 --> 00:11:31.820 
breaks down if a single node breaks

156
00:11:33.390 --> 00:11:36.550 
in more throughput, oriented workloads.

157
00:11:37.240 --> 00:11:43.080 
We oftentimes have little better error containment, but typically

158
00:11:43.390 --> 00:11:47.390 
if a system feels quite a significant amount of the system

159
00:11:47.600 --> 00:11:52.710 
is going to fail. So the traditional harware models need an update.

160
00:11:53.120 --> 00:11:57.290 
We have memory with increased density and higher data rates instead of

161
00:11:57.760 --> 00:12:01.390 
cores. We use many cores instead of a simple

162
00:12:01.860 --> 00:12:03.530 
monolithic processor.

163
00:12:04.130 --> 00:12:06.240 
We have to interconnect which actually might be

164
00:12:07.280 --> 00:12:11.430 
a source for contention and a source for problems, which is

165
00:12:11.430 --> 00:12:13.450 
crucial if it comes to fault isolation.

166
00:12:14.820 --> 00:12:16.830 
If you look at reactive fault-tolerance

167
00:12:17.240 --> 00:12:21.430 
that might get in appropriate. Also an area where we can learn

168
00:12:21.430 --> 00:12:25.040 
from the mainframe operating systems if you look for instance at z/OS

169
00:12:25.440 --> 00:12:27.340 
that you have predictive

170
00:12:28.320 --> 00:12:30.220 
fault tolerance measures.

171
00:12:31.770 --> 00:12:35.160 
And also systems do not really scale very well.

172
00:12:36.500 --> 00:12:38.920 
We have additional software layers like the virtualization

173
00:12:38.920 --> 00:12:42.260 
layer which has been there for a long time in the mainframe world

174
00:12:42.610 --> 00:12:48.380 
but is relatively new to the table in the classical interbase data center.

175
00:12:48.810 --> 00:12:55.360 
And we have relatively weak tool sets if it comes to reliability research.

176
00:12:57.840 --> 00:13:00.950 
Here at the Hasso Plattner Institut, we have the FutureSOC Lab which

177
00:13:00.950 --> 00:13:03.450 
is collaboration with industry partners

178
00:13:04.150 --> 00:13:06.580 
for looking at next generation

179
00:13:07.330 --> 00:13:10.730 
X86, X64 hardware. It

180
00:13:11.820 --> 00:13:15.470 
is active research work where we want to understand new fault classes,

181
00:13:15.470 --> 00:13:19.020 
we want to do prediction, where we want to look at active,

182
00:13:20.070 --> 00:13:21.920 
pro-active virtual machine integration.

183
00:13:23.360 --> 00:13:27.670 
This is actually lab that hosts many of these research projects,

184
00:13:28.510 --> 00:13:33.660 
not only from our institute but it's open to the entire world basically.

185
00:13:36.690 --> 00:13:41.830 
And I just want to mention a few details from one project which looks at

186
00:13:42.480 --> 00:13:46.840 
failure prediction on several layers. On the CPU level, we want

187
00:13:46.840 --> 00:13:50.880 
to look at online hardware failure prediction which means performance counters.

188
00:13:51.990 --> 00:13:54.390 
These are available in all the major architectures.

189
00:13:55.250 --> 00:13:58.240 
You can also look at performance counters if it comes to memory

190
00:13:58.410 --> 00:14:02.900 
and this is actually an interesting incident here where we see that

191
00:14:03.230 --> 00:14:06.490 
a memory module failure was predicted a

192
00:14:08.130 --> 00:14:10.170 
couple of years ago already, but

193
00:14:10.900 --> 00:14:12.490 
twenty minutes basically

194
00:14:13.400 --> 00:14:18.740 
before the actual failure happened. And talking about problem size,

195
00:14:20.020 --> 00:14:22.490 
this unit was

196
00:14:23.810 --> 00:14:28.510 
sixteen gigabyte of memory so it's not just a few memory

197
00:14:28.520 --> 00:14:32.040 
cells or a few bits but huge amount of memory that fails.

198
00:14:33.840 --> 00:14:37.950 
I mentioned z/OS as an example where we have Predictive Failure Analysis

199
00:14:38.090 --> 00:14:41.610 
and Runtime Diagnostics built into the z/OS.

200
00:14:42.070 --> 00:14:46.150 
So operating systems need to be extended and

201
00:14:46.630 --> 00:14:49.890 
I mentioned power where we have AIX with a couple of these features

202
00:14:49.890 --> 00:14:52.720 
but we also seeing these features showing up in

203
00:14:53.650 --> 00:14:54.990 
the linux operating system.

204
00:14:56.030 --> 00:15:00.640 
And then there's an entire new field that receives much attention

205
00:15:00.640 --> 00:15:05.420 
today which is machine learning and data science and probably

206
00:15:05.870 --> 00:15:10.330 
the vision is that we could do fault prediction by applying machine

207
00:15:10.330 --> 00:15:13.460 
learning to all the aspects like to the hardware where we have

208
00:15:13.460 --> 00:15:16.840 
failure predictors, to the virtual machine monitor, to the operating system,

209
00:15:17.010 --> 00:15:18.540 
but also to the application. That

210
00:15:20.470 --> 00:15:24.610 
way we can factor in domain-specific knowledge. It will be pluggable

211
00:15:24.610 --> 00:15:27.480 
so that it's adaptable to different use cases and so forth.

212
00:15:27.870 --> 00:15:29.110 
And división then is

213
00:15:29.990 --> 00:15:35.240 
building such an system health indicator as outcome, as result

214
00:15:35.240 --> 00:15:38.730 
of a multi-level failure prediction and a system health indicator

215
00:15:39.120 --> 00:15:44.690 
would trigger a virtual machine migration, so that we can migrate

216
00:15:44.700 --> 00:15:49.460 
away from a service system before problems manifest in the application.

217
00:15:52.260 --> 00:15:53.430 
System health indicator.

218
00:15:55.270 --> 00:15:57.320 
And with that approach

219
00:15:59.020 --> 00:16:02.250 
we can draw a couple of conclusions. So we see that there are

220
00:16:02.260 --> 00:16:04.080 
advances in server technology.

221
00:16:04.940 --> 00:16:08.630 
We have an ever growing number of CPU cores, systems are getting

222
00:16:08.630 --> 00:16:13.620 
more and more dense. We have tremendous amounts of memory today

223
00:16:14.160 --> 00:16:18.490 
and there's non-volatile memory showing up on the horizon,

224
00:16:18.930 --> 00:16:22.650 
which actually might bring new problems to the table because

225
00:16:22.990 --> 00:16:27.980 
the typical approach to solve a problem by just rebooting and redoing the thing

226
00:16:28.810 --> 00:16:31.390 
will no longer work if you have non-volatile memory.

227
00:16:32.090 --> 00:16:36.530 
Also we notice that reliability becomes a key quality attribute.

228
00:16:36.810 --> 00:16:39.870 
This is something we take away from the mainframe, however the

229
00:16:40.210 --> 00:16:44.190 
high level of integration and the shrinking structure sizes like 22nm

230
00:16:44.380 --> 00:16:48.390 
or 14 nanometers or even 7 nanometers in the future.

231
00:16:49.500 --> 00:16:53.500 
We will see multi bit errors instead of just a single bit flip.

232
00:16:53.500 --> 00:16:55.510 
So we need new measures for

233
00:16:56.450 --> 00:16:58.510 
fault runs and error correction.

234
00:16:59.860 --> 00:17:03.580 
And as I suggested for prediction dynamic reconfiguration where

235
00:17:03.580 --> 00:17:05.740 
we see examples in all the major systems,

236
00:17:06.410 --> 00:17:11.690 
they may provide new solutions. And isolation against fault propagation,

237
00:17:12.430 --> 00:17:16.390 
we know the dLPAR as the ultimate means of isolation, the mainframe in

238
00:17:17.480 --> 00:17:21.540 
commodities server would correspond or would translate into blades. So you just

239
00:17:21.800 --> 00:17:24.380 
place your workload across several nodes

240
00:17:24.810 --> 00:17:28.080 
and then there's also new programming models showing up like

241
00:17:28.080 --> 00:17:31.370 
the micro service architecture which became famous with netflix

242
00:17:31.370 --> 00:17:33.530 
and the cosmic chaos monkeys where

243
00:17:34.740 --> 00:17:36.420 
while operating just

244
00:17:37.290 --> 00:17:42.010 
kill service instances to show and to see and to be sure that the system

245
00:17:42.240 --> 00:17:45.480 
has the desired self-healing capabilities.

246
00:17:45.990 --> 00:17:50.830 
So in the end we see again that the computer architecture drives

247
00:17:50.830 --> 00:17:54.100 
changes in system software on several levels and it

248
00:17:54.520 --> 00:17:57.670 
starts on the left where we see that the servers have evolved

249
00:17:57.670 --> 00:18:01.190 
two new form factors, higher density. We see advances in the

250
00:18:01.190 --> 00:18:05.930 
operating system like virtualization and trustworthiness, security like clustering.

251
00:18:06.700 --> 00:18:09.770 
We see new problems coming up with virtualization, by the way,

252
00:18:09.780 --> 00:18:13.730 
like the so called blue pill attack. So how can you basically tell

253
00:18:14.010 --> 00:18:18.230 
as somebody has placed the virtualization layer between you and your hardware

254
00:18:18.580 --> 00:18:22.130 
and makes changes to the operation of the hardware.

255
00:18:23.070 --> 00:18:25.790 
We also see at that started with cloud computing

256
00:18:26.630 --> 00:18:31.370 
that people don't want to buy big systems anymore but they want to

257
00:18:31.560 --> 00:18:36.680 
follow the pay as you go scheme, so we need to provide additional hardware

258
00:18:37.350 --> 00:18:41.310 
and have a model for pricing,

259
00:18:42.350 --> 00:18:45.950 
to have the customer pay only if data is being used.

260
00:18:47.180 --> 00:18:50.910 
And then I mentioned already the GPU and FPGA.

261
00:18:51.970 --> 00:18:54.910 
So we see an entire world of hybrid computing where there is

262
00:18:54.910 --> 00:18:59.580 
more than just the cpu as a processing model and OpenCL is

263
00:18:59.580 --> 00:19:04.460 
one of the new programming models CUDA and as a model but there's

264
00:19:04.460 --> 00:19:08.510 
more to come, and I also mentioned already the CAPI and opencapi

265
00:19:08.920 --> 00:19:12.990 
which got introduced with power.The idea is that everything gets

266
00:19:14.720 --> 00:19:20.110 
in a homogeneous fashion again by implementing cache coherent interfaces.

267
00:19:20.470 --> 00:19:26.610 
So an entire world of changes, some of these changes are visible in

268
00:19:26.910 --> 00:19:32.270 
mainframe today already but some of them also are driven by

269
00:19:33.230 --> 00:19:39.560 
price optimization (PCO) goals and they will not be visible

270
00:19:39.940 --> 00:19:45.190 
in the mainframe dead march but more important in the commodity data center today.

271
00:19:45.850 --> 00:19:49.960 
In the end the mainframe still defines the gold standard for the datacenter.

272
00:19:51.700 --> 00:19:54.800 
Thank you for your attention. That concludes the course.
