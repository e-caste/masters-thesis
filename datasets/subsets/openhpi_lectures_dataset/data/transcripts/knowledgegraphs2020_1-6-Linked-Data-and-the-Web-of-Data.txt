WEBVTT

1
00:00:01.040 --> 00:00:04.420 
Welcome to knowledge graphs. This
is lecture one - knowledge graphs

2
00:00:04.430 --> 00:00:09.160 
in the web of data. In the final
part of this lecture we want

3
00:00:09.160 --> 00:00:13.210 
to talk about linked data
and the web of data.

4
00:00:14.500 --> 00:00:19.310 
In order to do so we want to
recapitulate a bit of what is

5
00:00:19.310 --> 00:00:23.120 
the web and what's the basic
architecture of the traditional web.

6
00:00:23.440 --> 00:00:26.770 
As you all know in the traditional
web if I want to identify

7
00:00:26.770 --> 00:00:30.890 
something I'm using URLs. So
these kind of web addresses

8
00:00:30.890 --> 00:00:32.680 
that everybody of us
has already see.

9
00:00:33.810 --> 00:00:38.220 
To access then and to get content
into your browser between

10
00:00:38.220 --> 00:00:41.210 
your browser and the web server
there is a specific protocol

11
00:00:41.210 --> 00:00:44.730 
going on and this protocol is
referred to as the hypertext

12
00:00:44.730 --> 00:00:47.160 
transfer protocol,
so this is http.

13
00:00:48.190 --> 00:00:53.860 
And via an http request you get
in the end some kind of web

14
00:00:53.860 --> 00:00:57.890 
page which is encoded in html.
So this is the representation

15
00:00:57.890 --> 00:01:03.080 
language of the web. All three parts
of it URL for identification,

16
00:01:03.080 --> 00:01:08.870 
http for transport and communication
and html for representation. They

17
00:01:09.470 --> 00:01:13.180 
form and make up the
entire web as we know it.

18
00:01:14.780 --> 00:01:17.780 
So now we are talking
about the semantic web.

19
00:01:18.930 --> 00:01:21.890 
Already in the last part of the
lecture you have seen the single

20
00:01:21.890 --> 00:01:26.120 
layers of which the semantic web
technology stack is constructed of.

21
00:01:26.270 --> 00:01:30.970 
So let's now put again the focus
on the web which means we

22
00:01:30.970 --> 00:01:36.190 
want to put semantic technologies to the
web to result in something like a web

23
00:01:36.400 --> 00:01:39.050 
of data that we can
access later on.

24
00:01:39.890 --> 00:01:45.550 
Again of course we have to identify
stuff but this time not only web pages,

25
00:01:45.800 --> 00:01:49.230 
we want to identify anything and
want to make assumptions and

26
00:01:49.230 --> 00:01:52.850 
represent facts about anything
you can find. So therefore

27
00:01:53.320 --> 00:01:57.350 
the original concept of URLs
uniform resource locators

28
00:01:57.350 --> 00:02:02.120 
has been extended to URIs, which
are uniform resource identifiers.

29
00:02:04.170 --> 00:02:08.250 
Next the access which means
communication again is done in

30
00:02:08.250 --> 00:02:13.920 
the same way like in the traditional web. So we
use simply the hypertext transfer protocol.

31
00:02:14.910 --> 00:02:18.340 
And then instead of html
of course we need

32
00:02:18.750 --> 00:02:23.880 
a new representation language and
exchange format and for that

33
00:02:23.890 --> 00:02:27.490 
there is this blue layer in the
semantic web technology stack

34
00:02:27.490 --> 00:02:30.630 
there is the resource
description format rdf

35
00:02:31.060 --> 00:02:36.990 
and all three of them which means resources
that are identified via URI,

36
00:02:37.220 --> 00:02:42.660 
accessible via http
and encoded as rdf,

37
00:02:43.170 --> 00:02:48.370 
they together form the web of data.
So we come up with the definition.

38
00:02:49.320 --> 00:02:54.240 
The web of data is nothing else but an
upgrade of the traditional web of documents

39
00:02:54.360 --> 00:03:00.400 
as we have seen and it's a web and you can see
it as a huge decentralized database because

40
00:03:00.620 --> 00:03:03.680 
what we are looking for is data
that is represented on the web

41
00:03:04.100 --> 00:03:09.820 
and this is machine-accessible data.
Moreover it's a knowledge base and

42
00:03:10.000 --> 00:03:14.850 
the machine-accessible data is not only accessible
it's really also machine understandable.

43
00:03:15.830 --> 00:03:21.090 
This already has been found out by Tim
Berners-Lee who is you might know that

44
00:03:21.200 --> 00:03:24.380 
the inventor of the traditional
web as well as of the semantic

45
00:03:24.380 --> 00:03:26.680 
web and he wrote already
in nineteen eighty nine

46
00:03:27.230 --> 00:03:31.020 
the web of human readable document
is being merged with the

47
00:03:31.020 --> 00:03:35.080 
web of machine understandable data.
So here is the term machine

48
00:03:35.080 --> 00:03:38.960 
understandable not only readable,
it's understandable data and

49
00:03:38.960 --> 00:03:44.220 
the potential of the mixture of humans and
machines working together and communication

50
00:03:44.530 --> 00:03:49.740 
through the web could be immense.
And you will see that in the

51
00:03:49.740 --> 00:03:51.170 
following course
of the lecture.

52
00:03:52.100 --> 00:03:55.660 
So if we are now looking at the
web of data or the semantic web

53
00:03:56.030 --> 00:03:59.720 
what is changing you know if we
want to access the data now?

54
00:04:00.870 --> 00:04:05.670 
So in this web of data there are
no pre formed of fixed html

55
00:04:05.670 --> 00:04:07.020 
pages as you might know.

56
00:04:07.840 --> 00:04:13.960 
This kind of let's say displaying
technology this goes from the web server

57
00:04:14.210 --> 00:04:17.470 
then to your personal computer
or to your browser because you

58
00:04:17.470 --> 00:04:20.560 
only access the data that is
there in the web and the data

59
00:04:20.560 --> 00:04:26.160 
will be aggregated and collected and filtered
via so called intelligent infrastructure services

60
00:04:26.780 --> 00:04:31.100 
and then your personal assistant on
your computer or in your browser

61
00:04:31.390 --> 00:04:36.480 
will collect, aggregate, filter
and put all the information

62
00:04:36.480 --> 00:04:40.190 
together you need in something
like let's say a personalized

63
00:04:40.190 --> 00:04:44.910 
template that you yourself defined.
So it's kind of a virtual

64
00:04:45.350 --> 00:04:48.760 
web page that has been created
on your computer with the data

65
00:04:48.760 --> 00:04:54.620 
that is out in the web. So this is the way
how information transfer will change.

66
00:04:56.430 --> 00:05:00.290 
Of course the web of data already
exists, so there is the huge

67
00:05:00.290 --> 00:05:04.720 
so called linked open data cloud
that you see depicted here

68
00:05:04.890 --> 00:05:09.130 
and this is all of the publicly
available rdf data in the web

69
00:05:09.130 --> 00:05:13.070 
which can be identified via
URIs and accessible via

70
00:05:13.180 --> 00:05:18.450 
http and all of these datasets they are
linked one to another via URIs.

71
00:05:19.570 --> 00:05:23.680 
There are no current you know
a measures of the current

72
00:05:23.680 --> 00:05:27.030 
size of the web, so the latest
one that I found were from two

73
00:05:27.030 --> 00:05:30.010 
thousand and seventeen and by
that we had already about ten

74
00:05:30.010 --> 00:05:33.180 
zero hundred huge data sets with
roughly one hundred and fifty

75
00:05:33.180 --> 00:05:35.920 
billion facts and more than
eight hundred million links.

76
00:05:36.880 --> 00:05:40.310 
She knowledge or the data that
is out there is really immense

77
00:05:40.310 --> 00:05:43.330 
and you can of course have a look
at it simply follow the link

78
00:05:43.330 --> 00:05:45.630 
that is given here
in the slide.

79
00:05:47.340 --> 00:05:51.420 
One of the very earliest websites
that made use of the web

80
00:05:51.420 --> 00:05:56.200 
of data interestingly is the BBC. So
this is now for more than ten years.

81
00:05:56.570 --> 00:06:00.760 
You must imagine BBC programme of
course there's a lot of change

82
00:06:01.080 --> 00:06:04.110 
or lots of changes every day
because the programme is changing

83
00:06:04.110 --> 00:06:07.300 
and of course BBC not only has
one programme, it does not only

84
00:06:07.300 --> 00:06:10.760 
have TV programmes, there are also
radio programmes and stuff like that

85
00:06:10.920 --> 00:06:14.680 
and of course for each of the
programmes they want to display

86
00:06:14.840 --> 00:06:19.180 
the most recent information, let's say,
about the artists or the songs or

87
00:06:19.380 --> 00:06:23.070 
let's say, where do these artists
performing and stuff like that.

88
00:06:23.450 --> 00:06:27.110 
And if you have to change this on
a daily basis of course this

89
00:06:27.110 --> 00:06:30.260 
is a huge effort if you want to
do this manually. So they came

90
00:06:30.260 --> 00:06:35.590 
up with the idea why not simply
connect our web pages to kind of

91
00:06:35.980 --> 00:06:39.720 
always updated databases that
are out there on the web, like

92
00:06:39.720 --> 00:06:44.740 
for example, data that is available in
wikipedia, data that is available

93
00:06:44.880 --> 00:06:48.660 
in musicbrainz, data that is
available for example in last fm

94
00:06:48.660 --> 00:06:51.570 
or other kind of services. And
this is exactly what happens.

95
00:06:51.840 --> 00:06:57.070 
So when you see today somebody
playing in a BBC program and

96
00:06:57.070 --> 00:07:00.700 
you want to know exactly you
know what was the latest album

97
00:07:00.700 --> 00:07:04.020 
of that artist and where does he
perform next and stuff like

98
00:07:04.020 --> 00:07:08.320 
that, this kind of information has
been collected and directly

99
00:07:08.320 --> 00:07:13.100 
from the web and will be put there. So
this is one of the advantages because

100
00:07:13.310 --> 00:07:18.490 
of course you can save a lot of
effort while of course displaying

101
00:07:18.760 --> 00:07:20.090 
most recent information.

102
00:07:22.720 --> 00:07:26.550 
So going one step back again
traditionally if you want to access

103
00:07:26.550 --> 00:07:29.660 
data on the traditional web you
couldn't do that directly.

104
00:07:30.380 --> 00:07:33.220 
From your browser you only
had access to a web server

105
00:07:34.020 --> 00:07:38.110 
and to get access to a database that
is somewhere behind the web server,

106
00:07:38.400 --> 00:07:42.060 
you had to engage another protocol.
So usually you communicate

107
00:07:42.060 --> 00:07:46.430 
with the web server by http and then you
need some other protocol that accesses

108
00:07:46.620 --> 00:07:50.600 
the database that is somewhere
else outside. For that you need

109
00:07:50.600 --> 00:07:54.860 
so called web APIs, application
programming interfaces.

110
00:07:55.690 --> 00:07:58.590 
And each of these databases that
might be out there on the

111
00:07:58.590 --> 00:08:03.080 
web they might require a proprietary,
a special API, so no

112
00:08:03.190 --> 00:08:08.420 
API looks like the other. And what is
even worse they are often changing.

113
00:08:09.290 --> 00:08:14.320 
However if an API is available, so
if one of the data providers

114
00:08:14.560 --> 00:08:18.730 
provides a specialized
proprietary you can use it and

115
00:08:18.730 --> 00:08:23.220 
you can build so called mashup applications
on top of that combining, you know,

116
00:08:23.540 --> 00:08:25.880 
each of these databases
in a specific way.

117
00:08:26.750 --> 00:08:30.320 
The only drawback you will face
there is then that usually

118
00:08:30.320 --> 00:08:34.820 
the data keeps of course staying
in their data sites and all

119
00:08:34.820 --> 00:08:39.550 
of the data is locked up in this
so called small data islands,

120
00:08:39.710 --> 00:08:43.720 
which means other applications usually
cannot access this data directly. So

121
00:08:43.970 --> 00:08:50.310 
you cannot use let's say you're google
search API to access the amazon e-commerce

122
00:08:50.580 --> 00:08:54.380 
data for example. So this is
not possible. You need always

123
00:08:54.380 --> 00:08:59.280 
proprietary special APIs and what
is even worse they are keep

124
00:08:59.280 --> 00:09:01.940 
changing which means if you
have a mashup application

125
00:09:02.580 --> 00:09:06.580 
you have to keep track of all of these
changes and this is lots of effort.

126
00:09:07.440 --> 00:09:12.720 
So the solution to that situation is
why not simply engage another layer

127
00:09:12.890 --> 00:09:17.420 
here to refer to as the linked
data layer and provide a generic

128
00:09:17.420 --> 00:09:21.370 
interface that is always the
same and that is based here on

129
00:09:21.370 --> 00:09:26.940 
linked data, which means we encode
simply all our data based on rdf, the

130
00:09:27.170 --> 00:09:31.650 
resource description framework which is
one of the basic constituents of the

131
00:09:31.900 --> 00:09:34.110 
semantic web
technology stack

132
00:09:34.760 --> 00:09:40.240 
and of course the access then works via
http, a standard port and everything

133
00:09:40.390 --> 00:09:42.280 
and we are fine because

134
00:09:42.890 --> 00:09:46.300 
each single database that engages
the link data layer then

135
00:09:46.300 --> 00:09:48.540 
might be accessible
in exactly that way.

136
00:09:49.430 --> 00:09:53.220 
This again then helps of course
most of these applications

137
00:09:53.220 --> 00:09:58.490 
that try to access the data because
changes are less frequently required.

138
00:09:58.640 --> 00:10:02.190 
You can simply use the same API. So
the API isn't changing because

139
00:10:02.190 --> 00:10:04.670 
it's standardized and
because it's generic.

140
00:10:06.210 --> 00:10:11.730 
Now putting all of that together is summed
up in the so called linked data principles.

141
00:10:11.920 --> 00:10:14.740 
So what are these principles?
It's quite easy for principles,

142
00:10:14.740 --> 00:10:16.130 
you have to keep
in mind. First,

143
00:10:17.470 --> 00:10:21.180 
we use URIs as names for
things, which means we access

144
00:10:21.180 --> 00:10:23.900 
every kind of data point that we
want to access with the help

145
00:10:23.900 --> 00:10:29.780 
of a URI. Second, each kind of
access to these URIs we provide

146
00:10:30.010 --> 00:10:34.540 
via http. So we use http URIs
so that people can look

147
00:10:34.540 --> 00:10:36.750 
up those names. So there
are lookup services.

148
00:10:37.580 --> 00:10:42.650 
And of course when someone looks up a URI,
we should provide useful information

149
00:10:43.150 --> 00:10:46.420 
using the available standards.
They are standardized by the

150
00:10:46.420 --> 00:10:50.290 
W3C which is the world wide
web consortium which standardizes

151
00:10:50.290 --> 00:10:54.660 
everything which is out there on
the web. Like for example also rdf

152
00:10:55.210 --> 00:10:58.770 
as the resource description
format and SPARQL which is the

153
00:10:58.990 --> 00:11:03.280 
generalized query language that
you can use to query rdf data.

154
00:11:04.190 --> 00:11:07.990 
And of course number four is also
very much important to, you know,

155
00:11:08.470 --> 00:11:13.140 
come over or let's say abandon
our data islands that we had

156
00:11:13.310 --> 00:11:17.860 
simply link to other URIs,
link to other datasets, so that

157
00:11:18.450 --> 00:11:23.680 
you can discover more things on the web.
Therefore it's named linked data. So

158
00:11:23.870 --> 00:11:28.590 
these things all put together form a
huge web huge web of knowledge graphs

159
00:11:28.770 --> 00:11:32.720 
and this is of course the main
subject of this lecture. So simply

160
00:11:32.720 --> 00:11:35.270 
follow these four principles and
you put your data on the web

161
00:11:35.270 --> 00:11:38.330 
and it will be immediately
really useful.

162
00:11:39.750 --> 00:11:42.960 
So if you take this into account
what were the benefits of

163
00:11:43.230 --> 00:11:48.220 
for example the BBC website using
linked data instead of traditional

164
00:11:48.400 --> 00:11:49.820 
methods? So first of all,

165
00:11:50.560 --> 00:11:55.830 
information is dynamically aggregated
from external publicly available data.

166
00:11:56.070 --> 00:11:59.530 
So they are taking into account
wikidata, musicbrainz last fm

167
00:11:59.580 --> 00:12:00.760 
and so on and so on.

168
00:12:01.970 --> 00:12:07.170 
Accessing these services no special
screen scraping process was

169
00:12:07.170 --> 00:12:10.530 
necessary. You also know if you
have defined screen scraping

170
00:12:10.530 --> 00:12:14.620 
to get data from an html page in
a website if somebody changes

171
00:12:14.620 --> 00:12:18.100 
the design of the web page, your
screen scraper is lost and

172
00:12:18.100 --> 00:12:19.400 
you have to do
it once again.

173
00:12:20.260 --> 00:12:23.490 
Also they were not using a
specialized or many specialized

174
00:12:23.490 --> 00:12:27.360 
APIs to use these services. What they
did was simply using the generic

175
00:12:27.880 --> 00:12:32.890 
rdf based linked data API and
all of the data that they

176
00:12:32.890 --> 00:12:38.090 
are using as is available as linked open
data and it's accessible via simple

177
00:12:38.460 --> 00:12:41.850 
http request. And what is of course
most important, this data

178
00:12:41.850 --> 00:12:46.080 
is always up to date because it's taken
from the original sources without

179
00:12:46.590 --> 00:12:52.420 
any manual interaction. And this of
course is worthwhile considering. So

180
00:12:54.110 --> 00:12:58.260 
imagine you put your data up on the
web. When will your data become fully

181
00:12:58.460 --> 00:13:02.340 
linked open data? For that Tim
Berners-Lee, creator of the web,

182
00:13:02.350 --> 00:13:05.990 
created a so called five star
criteria for link to open data.

183
00:13:06.470 --> 00:13:09.150 
Let's have a look at that.
So you get one star,

184
00:13:10.440 --> 00:13:13.190 
already when everything is open
so you put something on the

185
00:13:13.190 --> 00:13:15.440 
web it's open you have
already one star.

186
00:13:16.750 --> 00:13:20.750 
Moreover if it's machine readable
so instead for example taking

187
00:13:20.770 --> 00:13:22.780 
as an image scan
of a table

188
00:13:23.510 --> 00:13:27.370 
to the web you take an excel sheet
then it's machine-readable,

189
00:13:27.370 --> 00:13:29.640 
but however it's
a proprietary format.

190
00:13:30.140 --> 00:13:33.660 
However it's accessible and
machine-readable on the web which means

191
00:13:33.920 --> 00:13:40.050 
two stars. If you consider not to take
a proprietary format but let's

192
00:13:40.050 --> 00:13:44.380 
say an open format, let's say
csv for example instead of excel,

193
00:13:44.670 --> 00:13:47.210 
then you get an additional star,
you end up with three stars.

194
00:13:48.780 --> 00:13:52.630 
Then if all of the information
contained in your document you

195
00:13:52.630 --> 00:13:57.780 
are using the W3C standards like
rdf and SPARQL you get star number four

196
00:13:58.890 --> 00:14:03.180 
and then of course if you also
link to other data sets then

197
00:14:03.180 --> 00:14:08.040 
you fulfill all the criteria that are
necessary for a five star linked

198
00:14:08.160 --> 00:14:13.560 
open data dataset. So let's all
aim for five stars when going

199
00:14:13.570 --> 00:14:16.920 
public, when going to
linked open data. So

200
00:14:17.910 --> 00:14:21.940 
this was everything about linked
open data and also this is

201
00:14:21.940 --> 00:14:25.660 
the end of the very first lecture.
In the second lecture of

202
00:14:25.670 --> 00:14:29.650 
the course we will talk about
basic semantic technologies.
