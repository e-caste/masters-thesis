WEBVTT

1
00:00:00.540 --> 00:00:04.880 
Welcome to an introduction of the mainframe as a big data analytics platform,

2
00:00:05.420 --> 00:00:09.910 
especially the Apache Spark solution. My name is Philipp Brune

3
00:00:10.060 --> 00:00:12.480 
from the Neu-Ulm University of Applied Sciences.

4
00:00:15.500 --> 00:00:22.040 
Apache Spark is a framework for building enterprise grade, enterprise wide

5
00:00:22.810 --> 00:00:26.550 
data analytics, and big data processing applications.

6
00:00:27.600 --> 00:00:32.720 
It has been released as an open source framework by the Apache foundation

7
00:00:33.770 --> 00:00:38.200 
which hosts a vast setup of enterprise related

8
00:00:41.870 --> 00:00:46.980 
open source projects that are dealing with different aspects of enterprise

9
00:00:47.990 --> 00:00:48.760 
computing. The most well known

10
00:00:50.320 --> 00:00:53.960 
one of course is the original apache web server but nowadays we have

11
00:00:54.440 --> 00:01:00.310 
dozens of other projects. So Apache Spark nicely fits into this portfolio of projects.

12
00:01:01.590 --> 00:01:07.160 
Apache Spark has become increasingly popular as a tool for building data processing applications.

13
00:01:08.180 --> 00:01:12.150 
We must imagine that Apache Spark is not mainly an interactive platform

14
00:01:12.150 --> 00:01:14.250 
that data scientists will use to

15
00:01:15.150 --> 00:01:19.120 
work with the data, to analyze the data, and to get ideas on how

16
00:01:19.380 --> 00:01:23.760 
data might be used for applications. But it is then the

17
00:01:23.760 --> 00:01:26.860 
platform to implement this on an enterprise level with

18
00:01:27.470 --> 00:01:29.390 
the necessary performance and

19
00:01:30.900 --> 00:01:32.980 
possibilities to to run it really

20
00:01:34.510 --> 00:01:37.870 
as a processing platform. So it does not have

21
00:01:38.420 --> 00:01:42.800 
a graphical user interface or an interactive component. It's mainly a command-line

22
00:01:43.040 --> 00:01:47.140 
environment but it is of course well-suited for

23
00:01:47.720 --> 00:01:49.530 
writing applications and running them

24
00:01:50.990 --> 00:01:52.050 
on a server environment.

25
00:01:53.750 --> 00:02:00.550 
The Spark framework, the main purpose of this framework is to improve data processing

26
00:02:00.750 --> 00:02:03.390 
speed and throughput by in-memory processing.

27
00:02:03.910 --> 00:02:09.330 
And therefore it provides a core functionality called the Resilient Distributed Datasets,

28
00:02:09.770 --> 00:02:14.460 
RDD which is sort of the core of the spark framework.

29
00:02:15.260 --> 00:02:19.230 
And these RDDS are in-memory

30
00:02:20.010 --> 00:02:24.730 
representations objects of data that can be loaded from

31
00:02:25.760 --> 00:02:27.870 
various data sources and

32
00:02:28.790 --> 00:02:32.390 
persistent storage, maybe a Hadoop file system underlying a

33
00:02:32.390 --> 00:02:35.350 
Apache Spark. So a spark is built on top of the hadoop

34
00:02:36.250 --> 00:02:39.970 
file system or it can be any other kind of data. It can be NoSQL

35
00:02:39.970 --> 00:02:43.780 
databases, it can be classical relational databases like

36
00:02:43.780 --> 00:02:47.910 
for example DB2, it can be files, flat files, it can be unstructured

37
00:02:47.910 --> 00:02:51.920 
data, it can be data from the internet stream of data and so

38
00:02:51.920 --> 00:02:54.790 
on. Everything that can be loaded by a

39
00:02:56.150 --> 00:03:00.420 
respective adapters into into the RDDs, and then processed with

40
00:03:00.460 --> 00:03:03.990 
the support of in-memory. Second thing that RDDs do-

41
00:03:04.350 --> 00:03:07.780 
resilient distributed data sets, is they split the processing

42
00:03:08.410 --> 00:03:11.780 
in a cluster on a grid on multiple computation nodes.

43
00:03:12.850 --> 00:03:18.620 
And on top of this Spark provides different libraries for implementing

44
00:03:19.180 --> 00:03:22.450 
data processing on top of the RDDS, like for example

45
00:03:22.980 --> 00:03:28.590 
an SQL layer that allows to access RDDs in a way that is similar to classical

46
00:03:28.720 --> 00:03:33.040 
relational databases. Streaming layer are very important, called the MLiB

47
00:03:33.430 --> 00:03:35.760 
the machine learning library, which implements the

48
00:03:36.670 --> 00:03:39.410 
common algorithms for machine learning, like

49
00:03:39.860 --> 00:03:42.820 
artificial neural networks and support vector machines and

50
00:03:42.820 --> 00:03:44.850 
regression and all these things. So it's a

51
00:03:45.460 --> 00:03:48.370 
tool set for implementing most standard

52
00:03:48.940 --> 00:03:52.410 
machine learning and artificial intelligence algorithms.

53
00:03:53.690 --> 00:03:57.580 
Using the RDD for improved performance. And then we have

54
00:03:57.590 --> 00:04:01.150 
a similar thing for graph data processing which is not so

55
00:04:02.090 --> 00:04:06.610 
common maybe and then we have on top a more recent API called

56
00:04:06.610 --> 00:04:10.860 
the Data Frames API, which is sort of an abstraction on top of the other

57
00:04:11.240 --> 00:04:13.330 
concepts that simplifies programming.

58
00:04:14.190 --> 00:04:16.820 
And the programming itself,

59
00:04:17.450 --> 00:04:20.200 
as I said, it's an interactive tool in a way that you

60
00:04:20.200 --> 00:04:24.330 
have a GOI for working with the data, but you programme

61
00:04:25.070 --> 00:04:28.520 
your data processing using normal programming languages and

62
00:04:28.840 --> 00:04:34.830 
Apache spark provides support for Scala, Java, Python and the famous

63
00:04:34.850 --> 00:04:38.470 
R language which is used very frequently by data scientists. So

64
00:04:40.130 --> 00:04:43.190 
you can work in this typical languages that are popular at the moment.

65
00:04:44.650 --> 00:04:48.380 
Sparc framework itself is written in Scala. Scala is a language, a

66
00:04:49.260 --> 00:04:51.960 
functional language that compiles to the

67
00:04:52.420 --> 00:04:55.660 
java virtual machine as well. So you can mix it also as java.

68
00:04:55.970 --> 00:05:00.460 
So java and Scala are sort of natural languages for it to be run

69
00:05:00.460 --> 00:05:05.780 
on this framework but the others are supported as well. And

70
00:05:09.290 --> 00:05:11.890 
this framework, as I said, is open source so it can be basically

71
00:05:11.890 --> 00:05:15.920 
run on any platform that has JBM support. But we already learned

72
00:05:15.920 --> 00:05:19.830 
that the mainframe has a very, very high-performance

73
00:05:20.300 --> 00:05:23.480 
java virtual machine, so it is very well-suited to run this

74
00:05:23.480 --> 00:05:25.060 
kind of applications. And

75
00:05:27.030 --> 00:05:28.520 
Apache Spark on the

76
00:05:29.170 --> 00:05:33.090 
mainframe runs on Linux, of course and as well

77
00:05:33.550 --> 00:05:37.300 
on z/OS in the unix system services environment, using the

78
00:05:38.010 --> 00:05:39.260 
mainframe JBM.

79
00:05:42.760 --> 00:05:47.990 
So  basic feature of this Spark core is, as I said the implementation

80
00:05:47.990 --> 00:05:51.070 
of the resilient distributed data sets, which provide not only

81
00:05:51.070 --> 00:05:56.000 
in memory calculation, but also to split large data sets on multiple

82
00:05:56.410 --> 00:05:59.920 
nodes and computation nodes which can be servers in a grid,

83
00:05:59.920 --> 00:06:01.020 
or computers in a grid,

84
00:06:01.840 --> 00:06:07.190 
and the architecture is always in that way that you have a Central Master Node

85
00:06:07.310 --> 00:06:08.810 
which is called the Spark Driver.

86
00:06:09.480 --> 00:06:12.570 
This is the node that the programmer actually interacts with or that

87
00:06:13.260 --> 00:06:16.870 
actually executes the programs that the developer

88
00:06:16.870 --> 00:06:19.710 
provides to the system. These programs are

89
00:06:20.640 --> 00:06:25.190 
run here and then the Spark Driver are executed here and internally all the

90
00:06:25.900 --> 00:06:29.750 
operations from the spark libraries are then split up on the Worker Nodes

91
00:06:30.050 --> 00:06:33.960 
and the data is also split among the Worker Notes as you can see.

92
00:06:34.210 --> 00:06:38.750 
A single RDD here for example, RDD1 can reside on multiple

93
00:06:38.750 --> 00:06:41.220 
nodes to make it scalable for large

94
00:06:41.930 --> 00:06:44.580 
amounts of data. The processing then is also split

95
00:06:45.410 --> 00:06:49.080 
among the nodes. So Spark provides transparent functionality

96
00:06:49.080 --> 00:06:52.480 
for splitting the calculation on a cluster without the need of

97
00:06:52.900 --> 00:06:56.970 
defining that by the developer and of doing that in the

98
00:06:58.480 --> 00:07:01.860 
background. And the data may be split as well so that a single

99
00:07:01.860 --> 00:07:05.610 
RDD resides on multiple nodes and also different RDDs

100
00:07:05.610 --> 00:07:08.730 
can reside on the same node as you see here. For example, we have here

101
00:07:08.920 --> 00:07:11.690 
parts of RDD2 and 1 on one Worker Node.

102
00:07:13.570 --> 00:07:17.750 
After the calculation the data is collected again and the spark driver sort of

103
00:07:18.500 --> 00:07:21.340 
gets back the results and combines them to,

104
00:07:21.760 --> 00:07:22.830 
for example the output.

105
00:07:25.670 --> 00:07:30.910 
So in this slide you see a simple example how such a spark

106
00:07:30.910 --> 00:07:35.860 
program would look like. I used a program that basically uses a

107
00:07:37.340 --> 00:07:41.890 
modular perceptron, an artificial neural network to analyze some data.

108
00:07:42.500 --> 00:07:45.960 
This is a java program but the program in Scala or

109
00:07:45.980 --> 00:07:48.990 
Python would look the same because the classes and the API

110
00:07:49.010 --> 00:07:53.410 
are of course the same, just with a different style for the respective programming language.

111
00:07:54.460 --> 00:07:57.780 
Every Spark program needs to first initiate a so called Spark

112
00:07:57.780 --> 00:08:00.330 
Session- which is here started the variable spark.

113
00:08:00.910 --> 00:08:03.590 
This spark session is basically like a connection to the spark

114
00:08:03.590 --> 00:08:06.980 
system. It represents the spark system and then you can

115
00:08:06.980 --> 00:08:09.530 
use that for making API calls.

116
00:08:10.370 --> 00:08:14.970 
And the next set here, the data is loaded into dataFrames.

117
00:08:15.380 --> 00:08:18.380 
And you can see here the dataFrames and then

118
00:08:18.870 --> 00:08:21.490 
the data is split up into a part that is

119
00:08:22.020 --> 00:08:25.460 
training data and test data. So of course in reality you would

120
00:08:25.460 --> 00:08:27.420 
have different data for training and testing.

121
00:08:28.100 --> 00:08:33.100 
But this is just a test program - a demo program and then the

122
00:08:33.310 --> 00:08:36.960 
network is defined with its layers. This area here defines the

123
00:08:36.960 --> 00:08:39.690 
number of perceptrons per layer and then

124
00:08:40.780 --> 00:08:44.440 
the network is trained with training data part and tested with

125
00:08:44.440 --> 00:08:49.380 
test data part, and at the end security accuracy is calculated and

126
00:08:49.490 --> 00:08:52.910 
put out of the prediction that test data would

127
00:08:52.920 --> 00:08:57.080 
achieve with the trained model. So that is basically the idea.

128
00:08:57.300 --> 00:09:00.550 
I think when you read it you see the different steps and

129
00:09:01.190 --> 00:09:04.140 
this is more to show how such a spark program would look like.

130
00:09:04.140 --> 00:09:07.130 
You see it's really a program. It's not something interactive it's really

131
00:09:07.920 --> 00:09:11.870 
a processing that has to be defined and that actually can be used to build

132
00:09:12.310 --> 00:09:16.240 
data science and AI solutions for large-scale

133
00:09:16.660 --> 00:09:18.970 
sets of data. This of course is a typical

134
00:09:20.060 --> 00:09:23.400 
task that mainframes, a typical workload that mainframes are made for.

135
00:09:23.520 --> 00:09:27.250 
And this is why of course spark is an interesting technology on the mainframe.

136
00:09:28.040 --> 00:09:33.270 
So IBM is strongly supporting spark on z/OS and linux on the mainframes,

137
00:09:33.270 --> 00:09:35.500 
and they also provide special

138
00:09:36.110 --> 00:09:40.370 
add-ons that are commercial add-ons. As I said Spark itself is

139
00:09:40.590 --> 00:09:44.720 
open source but IBM of course provides a commercial version for z/OS, which

140
00:09:44.820 --> 00:09:50.790 
has support and enterprise support and also adds different components

141
00:09:51.060 --> 00:09:54.200 
that are used, for example to access the mainframe and the z/OS

142
00:09:54.830 --> 00:09:59.860 
style data for, example DB2. You can see it here, it is

143
00:10:00.540 --> 00:10:04.450 
this access layer here. It's just added access component

144
00:10:04.890 --> 00:10:09.060 
and it allows for example to access different mainframe style data sets like

145
00:10:09.600 --> 00:10:13.920 
VSAM data, data sets on z/OS, DB2 and

146
00:10:14.990 --> 00:10:18.070 
transaction processing data from CICS and so on. So this is

147
00:10:18.810 --> 00:10:22.090 
tightly integrated into this extended version of spark that

148
00:10:22.090 --> 00:10:26.260 
is especially useful for processing on z/OS.

149
00:10:26.470 --> 00:10:29.410 
And of course it has connectors for accessing other kinds of

150
00:10:29.410 --> 00:10:32.470 
data- internet data and social media and the

151
00:10:33.360 --> 00:10:38.010 
RESTful API and so on what we have today. But it has special features for accessing

152
00:10:38.520 --> 00:10:42.780 
classical mainframe data like data sets, partitioned data sets, and

153
00:10:43.850 --> 00:10:49.950 
VSAM data and so on. And this solution is packaged as Apache Spark

154
00:10:49.950 --> 00:10:54.490 
on z/OS and this is something that is unique for the mainframe platform.

155
00:10:54.690 --> 00:10:58.490 
And of course you could ask why this is special or why is it meaningful to run

156
00:10:58.610 --> 00:11:05.290 
spark on the mainframe. And the answer to this is, there are different answers.

157
00:11:06.870 --> 00:11:11.210 
Originally Spark was developed to scale horizontally on a cluster. So it is

158
00:11:11.730 --> 00:11:14.180 
by its heart, it's an approach to split

159
00:11:14.950 --> 00:11:18.360 
processing on multiple nodes and collect the data afterwards again

160
00:11:18.570 --> 00:11:21.840 
using in-memory techniques. So of course the question is why

161
00:11:21.840 --> 00:11:22.910 
this technique from an

162
00:11:23.840 --> 00:11:28.670 
approach where you have a horizontal scaling scale out

163
00:11:29.340 --> 00:11:32.610 
to move it to a classic mainframe environment where you have a vertical scaling.

164
00:11:32.870 --> 00:11:37.260 
Of course it is something that needs to be discussed, and the reason

165
00:11:37.260 --> 00:11:41.920 
for that is that in a vertically scalable environment like mainframes,

166
00:11:43.120 --> 00:11:45.640 
you reduce the admin overhead for setting up the

167
00:11:45.640 --> 00:11:48.390 
cluster, of course because you can basically run everything

168
00:11:49.040 --> 00:11:53.740 
in one big node making it much easier to maintain, and achieving

169
00:11:53.980 --> 00:11:55.620 
a similar computational power.

170
00:11:56.490 --> 00:12:01.140 
And of course you reduce also the overhead for communication

171
00:12:01.140 --> 00:12:05.230 
that is necessary in a cluster because you can run it inside a single box.

172
00:12:05.570 --> 00:12:06.960 
And this of course is

173
00:12:08.150 --> 00:12:12.770 
an improvement in some scenarios and

174
00:12:13.920 --> 00:12:17.090 
security is also improved because the data does not have to

175
00:12:17.100 --> 00:12:21.390 
sort of moved over a network, and it remains inside a box. So it has

176
00:12:21.630 --> 00:12:23.250 
some advantages as well.

177
00:12:23.920 --> 00:12:27.630 
And of course the hardware itself is optimized for such kind

178
00:12:27.630 --> 00:12:33.790 
of highly vertically scalable data oriented workloads with IO sub-system, channels and

179
00:12:33.990 --> 00:12:38.240 
and the high number of CPUs that can simultaneously

180
00:12:38.240 --> 00:12:40.580 
access them the main memory and so on. So

181
00:12:41.170 --> 00:12:44.320 
the mainframe is very well suited for data processing and of course

182
00:12:45.450 --> 00:12:49.450 
if you look at analytical applications in an enterprise context,

183
00:12:49.840 --> 00:12:53.830 
it is very interesting to have the analytical processing there,

184
00:12:54.030 --> 00:12:56.860 
where the data already is and the data typically the core data

185
00:12:56.860 --> 00:13:00.630 
of an enterprise resides on the mainframe for large scale enterprises.

186
00:13:01.080 --> 00:13:03.560 
So there you have the DB2 instances, there you have

187
00:13:03.560 --> 00:13:06.120 
all the traditional workloads, you have all the structured data,

188
00:13:06.570 --> 00:13:09.650 
the core business data is residing on the mainframe and if

189
00:13:09.650 --> 00:13:12.780 
you do the analytics there, you don't have to move around and copy the data.

190
00:13:12.900 --> 00:13:14.360 
But you can directly access it.

191
00:13:15.790 --> 00:13:19.500 
And of course other types of data that may originate from from

192
00:13:19.500 --> 00:13:22.250 
other kinds of applications from the internet, from streaming,

193
00:13:22.270 --> 00:13:26.370 
ustructure data, videos and graphics data and so on, they might be

194
00:13:26.530 --> 00:13:28.670 
loaded to the mainframe platform, but

195
00:13:29.310 --> 00:13:34.070 
the core business data is already there. So that is an advantage

196
00:13:34.070 --> 00:13:36.230 
as well because it avoids copying

197
00:13:37.010 --> 00:13:41.870 
duplicating data that is not necessary and it allows very quick access because

198
00:13:42.210 --> 00:13:44.790 
these additional components have been an optimized access,

199
00:13:44.790 --> 00:13:48.010 
for example to a DB2, because they run inside the same same

200
00:13:48.010 --> 00:13:51.340 
box and don't need to use a network

201
00:13:52.050 --> 00:13:55.700 
connection. They can use in memory techniques to do that.

202
00:13:56.150 --> 00:14:01.530 
So spark is a very promising and interesting platform to be used on z/OS,

203
00:14:01.780 --> 00:14:07.130 
especially and I guess we will see many interesting

204
00:14:07.790 --> 00:14:10.760 
applications regarding data science and data analytics

205
00:14:11.290 --> 00:14:14.530 
in the next years that make use of this mainframe capabilities.

206
00:14:14.960 --> 00:14:15.710 
Thank you very much.
