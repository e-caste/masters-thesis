WEBVTT

1
00:00:00.390 --> 00:00:04.510 
Hi everyone. I am GonÃ§alo and I am
a PhD student at the chair of

2
00:00:04.630 --> 00:00:08.380 
Internet Technologies and Systems
led by professor Meinel at Hasso Plattner

3
00:00:08.690 --> 00:00:12.260 
Institute. In this talk, I will
talk about the compression

4
00:00:12.260 --> 00:00:16.040 
of deep generative models which
is very important because in

5
00:00:16.040 --> 00:00:19.180 
our current AI group, we train
very big models and we would

6
00:00:19.180 --> 00:00:22.630 
like to make these models smaller
while keeping their performance.

7
00:00:26.040 --> 00:00:31.860 
So, in this talk, I will talk specifically
about generative models which usually

8
00:00:32.110 --> 00:00:34.480 
needs a lot of memory
and computational

9
00:00:35.370 --> 00:00:39.410 
resources.So, this might be due
to the model size itself or

10
00:00:39.410 --> 00:00:41.410 
overall training and
inference costs.

11
00:00:42.820 --> 00:00:47.610 
The deployment of such devices
on the edge is very

12
00:00:47.610 --> 00:00:50.880 
difficult because we have very
big models that would like to

13
00:00:50.880 --> 00:00:54.860 
fit on smart phones, smart watches
and so on and these devices

14
00:00:54.860 --> 00:00:56.570 
are memory and
power constrained.

15
00:00:57.860 --> 00:01:02.350 
So, applying compression techniques to further
reduce the model size and complexity

16
00:01:02.570 --> 00:01:06.010 
can be a very good thing. However,
there is a tradeoff between

17
00:01:06.010 --> 00:01:08.750 
the compression levels and
model performance at the end.

18
00:01:10.880 --> 00:01:15.550 
So, generative models simply learn the data
distribution to generate fake samples.

19
00:01:15.970 --> 00:01:19.670 
And these fake samples are different
but resemble real samples.

20
00:01:20.640 --> 00:01:25.480 
Such models are very important in a lot
of use cases, such as customer service

21
00:01:25.680 --> 00:01:28.820 
where we have chat bots or
conversational AI systems,

22
00:01:29.360 --> 00:01:32.030 
data augmentation to
augmented data sets,

23
00:01:32.690 --> 00:01:36.870 
data manipulation to perform some
style transfer on images, for example,

24
00:01:37.310 --> 00:01:41.360 
and data simulation which is can be
very important for self-driving cars.

25
00:01:43.650 --> 00:01:46.620 
To give an example of a generative
model, we have this framework

26
00:01:46.620 --> 00:01:49.290 
that is called Generative
Adversarial Networks or GANs.

27
00:01:50.170 --> 00:01:54.170 
In this framework we have two models -
one generator (G) and one discriminator (D).

28
00:01:54.920 --> 00:01:58.750 
The first model learns the real data
distribution to generate fake samples.

29
00:01:59.310 --> 00:02:00.900 
On the other end
the discriminator

30
00:02:01.570 --> 00:02:06.020 
attribute a probability of confidence
that each sample is real or not.

31
00:02:06.860 --> 00:02:11.540 
So, down below here we can see an example of
a framework and we have the generic

32
00:02:11.670 --> 00:02:14.810 
the generator and the discriminator.
The generator receives

33
00:02:14.810 --> 00:02:18.250 
noise to produce a fake sample
and the discriminator receives

34
00:02:18.250 --> 00:02:22.210 
a real sample from the training data
and a fake sample from the generator

35
00:02:22.460 --> 00:02:26.760 
and its job is to attribute a
probability p of the sample that

36
00:02:26.760 --> 00:02:28.440 
it just saw is real or not.

37
00:02:30.980 --> 00:02:34.320 
So both of these models are trained
together in a minimax game.

38
00:02:35.120 --> 00:02:38.240 
So the generator's goal is to
increase the probability of the

39
00:02:38.240 --> 00:02:39.820 
discriminator to
make mistakes.

40
00:02:40.810 --> 00:02:43.900 
On the other hand the discriminators
goal is to classify real

41
00:02:43.900 --> 00:02:45.620 
samples with greater
confidence.

42
00:02:46.710 --> 00:02:49.010 
So after we train these
models for a while,

43
00:02:49.460 --> 00:02:53.240 
we can see that the generator will
slightly change the generated data

44
00:02:53.890 --> 00:02:55.770 
based on the
discriminator feedback.

45
00:02:57.080 --> 00:03:01.930 
So in the end, we have very realistic looking
samples that are actually not real.

46
00:03:02.360 --> 00:03:05.900 
So we can see here in these
images, all these cars actually

47
00:03:05.900 --> 00:03:09.450 
do not exist and they're all
generated by the generator network.

48
00:03:13.860 --> 00:03:16.920 
So in terms of compression,
we have several techniques

49
00:03:17.380 --> 00:03:20.860 
and two very typical techniques
are quantization and pruning.

50
00:03:21.530 --> 00:03:24.940 
So, in quantization we have
continuous values and we transform

51
00:03:24.940 --> 00:03:26.670 
those to discrete
values.

52
00:03:27.340 --> 00:03:30.290 
So we have floating-point numbers
and at the end we want to

53
00:03:30.290 --> 00:03:31.690 
convert them
to integers.

54
00:03:32.660 --> 00:03:36.640 
So this overall reduces the number
of bits that the model would need.

55
00:03:37.130 --> 00:03:42.510 
So in this case we could, for example,
given a set of discrete values, we could

56
00:03:43.780 --> 00:03:46.870 
approximate that to the
closest integer for example.

57
00:03:48.470 --> 00:03:51.170 
On the other hand, we also have
pruning techniques which set

58
00:03:51.170 --> 00:03:55.440 
some of these values to zero
and this, in the end, will save

59
00:03:55.440 --> 00:04:00.490 
us some operations since we can skip
zero value operations. So

60
00:04:00.870 --> 00:04:04.100 
in this example values that are
close to zero will actually

61
00:04:04.100 --> 00:04:07.970 
be zero at the end. So of course
we could combine both of these

62
00:04:07.970 --> 00:04:10.900 
approaches to get combined
benefits. So we could

63
00:04:11.690 --> 00:04:17.190 
at the same time apply this discrete
value approximation to the values

64
00:04:17.390 --> 00:04:21.120 
as well as making them sparse, meaning
some of them will be set to zero.

65
00:04:23.930 --> 00:04:26.620 
Another question is when to
actually compress these models.

66
00:04:27.040 --> 00:04:30.420 
So, if you're looking at compression
during training, we could

67
00:04:30.490 --> 00:04:33.460 
achieve higher levels of compression,
up to thirty two times

68
00:04:33.460 --> 00:04:37.790 
for example, if we compress a thirty
two bit model to a one bit model.

69
00:04:38.350 --> 00:04:42.720 
So in this case, it's good but
generative models in general

70
00:04:42.870 --> 00:04:44.910 
tend to suffer from
training instability.

71
00:04:45.420 --> 00:04:47.970 
So the compression techniques
that we just discussed

72
00:04:48.660 --> 00:04:52.910 
might increase such instability
and make training very difficult.

73
00:04:53.760 --> 00:04:55.610 
So on top of this,
if you have

74
00:04:56.310 --> 00:04:59.520 
a framework with several models
like GANs, we would need to

75
00:04:59.520 --> 00:05:01.630 
compress several models
at training time.

76
00:05:02.650 --> 00:05:05.760 
On the other hand, we could focus
on compression after training

77
00:05:06.060 --> 00:05:08.320 
which means that we would
probably achieve less

78
00:05:09.320 --> 00:05:11.280 
compression levels
so instead of

79
00:05:12.310 --> 00:05:15.820 
compressing the model thirty two times, we
might get away with only compressing it

80
00:05:15.920 --> 00:05:21.880 
eight times of its original size. But
the goal here is to only compress the

81
00:05:22.180 --> 00:05:23.650 
defined alternative model.

82
00:05:24.290 --> 00:05:28.310 
So this would improve performance both
in terms of memory and compute only at

83
00:05:28.610 --> 00:05:29.430 
inference time.

84
00:05:30.640 --> 00:05:33.790 
So, a realistic example would be
to train the generative model

85
00:05:33.790 --> 00:05:36.870 
on the cloud and then compress
this pre- trained model and

86
00:05:36.870 --> 00:05:38.220 
run it on an
edge device.

87
00:05:42.810 --> 00:05:46.560 
So, another question is how to
evaluate the generated samples?

88
00:05:47.070 --> 00:05:50.390 
So, it's important that each
generated sample is pretty much

89
00:05:50.390 --> 00:05:53.740 
indistinguishable from a real sample.
So we are looking at quality.

90
00:05:55.650 --> 00:05:57.420 
However, it is also
important that

91
00:05:58.170 --> 00:06:01.420 
the set of generated samples
also differ from each other. So,

92
00:06:01.420 --> 00:06:03.920 
there should be some diversity
on the generated set as well.

93
00:06:06.470 --> 00:06:09.480 
So, it's very important
to evaluate

94
00:06:10.520 --> 00:06:14.730 
these models post compression. So
we have much tinier models and

95
00:06:14.920 --> 00:06:19.350 
we should be able to evaluate the
changes that the models went through.

96
00:06:19.680 --> 00:06:23.320 
So this evaluation is usually performed
using another pre-trained model

97
00:06:23.580 --> 00:06:26.160 
since human evaluation
is often too expensive.

98
00:06:27.040 --> 00:06:29.840 
Now, it's important to look at
both quality and diversity to

99
00:06:29.840 --> 00:06:32.670 
see if they are affected the
same and this would require a

100
00:06:32.670 --> 00:06:35.330 
separate assessment of both
quality and diversity.

101
00:06:36.840 --> 00:06:40.810 
Another thing to look at is the
quality of individual samples.

102
00:06:41.160 --> 00:06:45.010 
So, we know that the overall generated
set may be affected by compression,

103
00:06:45.380 --> 00:06:50.410 
especially at high levels, but
some samples may remain intact

104
00:06:50.410 --> 00:06:52.750 
or at least be less
affected than others.

105
00:06:53.930 --> 00:06:55.110 
So this proves that this post-

106
00:06:56.450 --> 00:07:00.020 
compression of generative models
can also be very sensitive

107
00:07:00.020 --> 00:07:04.410 
to different data domains. So
compressing a generative image

108
00:07:04.410 --> 00:07:08.090 
model could be easier than compressing
a generative text model for example.

109
00:07:10.350 --> 00:07:13.170 
So, to conclude - compression
may enable these

110
00:07:13.650 --> 00:07:17.150 
big generative models to run on
edge devices since it could

111
00:07:17.150 --> 00:07:20.340 
compress them up to thirty two
times, for example. And this would

112
00:07:20.340 --> 00:07:23.080 
be possible applying
quantization and also pruning.

113
00:07:24.810 --> 00:07:29.040 
However, genetic models are also
very often unstable during

114
00:07:29.040 --> 00:07:31.860 
training. So applying this compression
post training is probably

115
00:07:31.860 --> 00:07:32.660 
a good thing to do.

116
00:07:34.870 --> 00:07:38.560 
To conclude, evaluation of these
compressed models is also necessary

117
00:07:38.560 --> 00:07:42.300 
for real world applications and
more specifically it's important

118
00:07:42.300 --> 00:07:46.050 
to look at all the quality and
diversity was affected in the

119
00:07:46.050 --> 00:07:47.220 
end, in the final set.

120
00:07:50.180 --> 00:07:50.620 
Thank you.
