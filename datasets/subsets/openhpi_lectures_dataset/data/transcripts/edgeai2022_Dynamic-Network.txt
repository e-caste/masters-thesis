WEBVTT

1
00:00:00.740 --> 00:00:09.259 
Hello and welcome. This video will briefly discuss how we could apply a dynamic neural network to improve efficiency.

2
00:00:11.039 --> 00:00:15.150 
Why dynamic network? To better understand the motivation,

3
00:00:15.369 --> 00:00:24.710 
let's first look at how the static network works. In the training phase we have a fixed predefined network architecture. If

4
00:00:24.710 --> 00:00:30.920 
you randomly initialize the weights and start optimization using the training data for several epochs.

5
00:00:31.839 --> 00:00:37.890 
Once the training is finished, we then save the weights. In the inference phase,

6
00:00:37.899 --> 00:00:45.060 
we will use the same network architecture as training and the saved weights for arbitrary input examples.

7
00:00:47.329 --> 00:00:55.899 
Now let's look at the dynamic networks workflow. In the training phase it will learn a specific network structure for each

8
00:00:55.899 --> 00:01:04.230 
training example, then apply joint optimization of the neural network and then decision making mechanism.

9
00:01:04.239 --> 00:01:14.620 
So you need this decision making algorithm for a specific network choice. In the inference stage we will use an instance wise

10
00:01:14.620 --> 00:01:17.650 
dynamic network structure for each input.

11
00:01:18.540 --> 00:01:26.840 
The decision making mechanism predicts the structure based on each input example to achieve the score and different from

12
00:01:26.840 --> 00:01:28.319 
the static network,

13
00:01:28.329 --> 00:01:37.629 
the dynamic one can dynamically adjust its structure or the parameters when processing different test examples, thereby showing

14
00:01:37.629 --> 00:01:45.310 
excellent advantage in reasoning, efficiency, expression, ability and adaptability.

15
00:01:46.540 --> 00:01:57.540 
We can roughly categorize this network into dynamic width, depths and the path.

16
00:01:57.540 --> 00:02:00.260 
Regarding the advantages of dynamic network,

17
00:02:00.640 --> 00:02:11.300 
first, the efficiency. One of the most notable advantages is that they are able to strategically allocate computations on

18
00:02:11.300 --> 00:02:20.449 
demand at the test time by selectively activating model components like layers, channels. Less computation

19
00:02:20.460 --> 00:02:26.560 
is spent on canonical examples that are relatively easy to recognize.

20
00:02:27.539 --> 00:02:37.919 
Second, the representation power. Due to the data dependent network architecture and parameters, dynamic network have significantly

21
00:02:37.919 --> 00:02:42.150 
enlarged parameter space and improve the representation power.

22
00:02:43.039 --> 00:02:50.750 
It is worth noting that the popular soft attention mechanism could also be unified into the framework of dynamic network.

23
00:02:53.039 --> 00:03:02.039 
Adaptiveness - dynamic models are able to achieve a desired trade off between accuracy and efficiency for dealing with the

24
00:03:02.039 --> 00:03:04.659 
varying a computational budgets on the fly.

25
00:03:05.340 --> 00:03:14.180 
Therefore they are adaptable to different hardware platforms and changing environments compared to the static

26
00:03:14.180 --> 00:03:14.810 
model

27
00:03:14.819 --> 00:03:18.840 
with a fixed computation costs.

28
00:03:18.840 --> 00:03:20.020 
Compatibility -

29
00:03:20.030 --> 00:03:25.860 
dynamic networks are compatible with most advanced techniques in the deep learning era.

30
00:03:26.819 --> 00:03:31.300 
It can also apply similarly to the wide range of applications.

31
00:03:33.139 --> 00:03:43.750 
It may potentially bridge the gap between the underlying mechanism of deep models and brains, as it is believed that the brains

32
00:03:43.759 --> 00:03:48.509 
process information in a dynamic way. With the dynamic neural networks,

33
00:03:48.520 --> 00:03:57.060 
it is possible to analyze which components of deep learning models are activated when processing input example

34
00:03:57.069 --> 00:04:03.449 
and to observe which parts of the input are accountable for the certain predictions.

35
00:04:07.139 --> 00:04:16.660 
This is an example network using dynamic width. We can see that after the input sample gets through the network it will be

36
00:04:16.660 --> 00:04:21.250 
fed into a gating layer which predicts a subsequent network width.

37
00:04:21.639 --> 00:04:27.050 
So we can see the factors including 1.0 and 0.75

38
00:04:27.370 --> 00:04:28.810 
0.5 and 0.25.

39
00:04:28.819 --> 00:04:32.860 
It means that the width prediction result depends on the input examples.

40
00:04:34.339 --> 00:04:44.779 
Another work, autoslim, in this work it will first prepare a model pool by using a slimmable network method. Then

41
00:04:44.790 --> 00:04:46.269 
to ensure accuracy,

42
00:04:46.279 --> 00:04:51.759 
a greedy approach is adapted to reduce the channels layer by layer.

43
00:04:52.540 --> 00:04:59.310 
No training is required when the greatest slimming is performed here and the large batch sizes used for influence.

44
00:04:59.319 --> 00:05:10.339 
So this autoslim is a way of one shot architecture search. Finally get the selected optimal model and train from scratch

45
00:05:10.339 --> 00:05:14.660 
again, it shows promising results on the four different networks.

46
00:05:17.439 --> 00:05:22.459 
I use a simple example to introduce dynamic depths, called early exit.

47
00:05:23.040 --> 00:05:25.759 
Take the VGG image classification model.

48
00:05:27.240 --> 00:05:35.980 
In addition to the final classification output, we can add a multiple classification heads which are placed in the

49
00:05:35.980 --> 00:05:39.350 
middle of the network are closer to the input.

50
00:05:40.639 --> 00:05:41.579 
In doing so,

51
00:05:41.589 --> 00:05:46.589 
we finally get four classification results. In the training phase,

52
00:05:46.600 --> 00:05:55.829 
we can use the way that a linear combination of four loss functions to perform joint optimization. In the inference stage

53
00:05:55.839 --> 00:06:05.589 
when we reach the nearest exit which will measure the confidence of the prediction for example, we can use the entropy to measure

54
00:06:05.589 --> 00:06:08.649 
the confidence of the classification results.

55
00:06:09.040 --> 00:06:14.759 
If they're confident enough, execute an early exit, return the result directly.

56
00:06:15.639 --> 00:06:23.560 
Therefore we can obtain different inference rate according to the difficulty of classifying the input examples.

57
00:06:24.240 --> 00:06:32.910 
We use the deeper network results for more challenging examples and for simple examples we use early exit. In this way we

58
00:06:32.910 --> 00:06:35.060 
can obtain great flexibility.

59
00:06:42.939 --> 00:06:46.389 
This slide briefly explains how the gating layer works.

60
00:06:47.230 --> 00:06:56.319 
Suppose that a probability distribution is calculated in the middle of the model and then we need a specific example to continue

61
00:06:56.319 --> 00:06:58.959 
the calculation of the subsequent model.

62
00:06:59.639 --> 00:07:07.879 
For example, the distribution like p equals 0.1, 0.7 and 0.2.

63
00:07:07.889 --> 00:07:15.980 
You might want to ask why don't just ask a large value of 0.7 as a result, this is incorrect.

64
00:07:15.990 --> 00:07:19.930 
The reason is that model has not been well trained yet.

65
00:07:19.939 --> 00:07:28.759 
If the parameters are still random and the output distribution of this parameter p is obviously sub optimal.

66
00:07:29.199 --> 00:07:38.050 
It makes no sense to choose a larger one, the meaning of the sampling or in other words the meaning of the adding some randomness

67
00:07:38.050 --> 00:07:48.639 
here is encouraging the exploration, allowing the model to try various possible choices and then adjust the parameter according

68
00:07:48.639 --> 00:07:51.019 
to the gradient in the backward path

69
00:07:51.029 --> 00:07:54.240 
to accomplish the training.

70
00:07:54.240 --> 00:08:01.000 
In order to now destroy the gradient propagation of the computation graph while doing the sampling,

71
00:08:01.009 --> 00:08:11.579 
we do not directly perform random operation on p but introduce Gumbel distribution which provide is randomness we need.

72
00:08:11.629 --> 00:08:21.600 
So it commonly used the method is Gumbel softmax function, a classical way to optimize neural networks with argmax by

73
00:08:21.600 --> 00:08:26.149 
relaxing it to differentiatable softmax.

74
00:08:27.839 --> 00:08:32.559 
So from the formulation, the parameter g

75
00:08:32.559 --> 00:08:36.549 
example from Gumbel distribution which offers randomness.

76
00:08:36.549 --> 00:08:46.549 
We need argmax is a non differentiable but we can use the softmax function with temperature parameter to approximate it.

77
00:08:47.340 --> 00:08:53.990 
So, for every i equals from 1 to x temperature parameter

78
00:08:53.990 --> 00:09:04.490 
Tau controls how closely the new samples approximate is created and one-hot vectors. As tau close to zero,

79
00:09:04.500 --> 00:09:14.759 
the softmax computation smoothly approaches the argmax and this sample vectors approach one-hot as tau

80
00:09:14.769 --> 00:09:18.549 
close to infinity the sample vectors become uniform.

81
00:09:19.340 --> 00:09:29.149 
Now that continuous vector are used during training but sample vector are discrete to one-hot vector during the inference

82
00:09:29.159 --> 00:09:30.259 
for the testing.

83
00:09:30.840 --> 00:09:37.230 
Here we can see whenever we have to deal with the discrete variables in the neural network,

84
00:09:37.240 --> 00:09:44.049 
we can use Gumbel softmax distributions to approximate the sampling process of the discrete data.

85
00:09:45.029 --> 00:09:52.860 
The network can then be trained into using back propagation, where the performance of the network will depend on the choice

86
00:09:52.860 --> 00:09:54.750 
of the temperature parameter tau.

87
00:09:57.240 --> 00:10:06.279 
This paper presents a kind of dynamic BERT implementation which adaptively adjust with and depths of BERT architecture

88
00:10:06.360 --> 00:10:14.649 
at the inference stage. The model consists of 12 sub networks with various width and depth ratio.

89
00:10:14.659 --> 00:10:21.950 
It applied the gating module with intermediate classifiers to implement the early exit.

90
00:10:23.039 --> 00:10:28.090 
This figure on the right hand side shows the exemplary inference analysis

91
00:10:28.100 --> 00:10:29.379 
On the SST-2

92
00:10:29.379 --> 00:10:37.470 
dataset where dynamic BERT is about two times faster on GPU and five times faster on CPU

93
00:10:37.480 --> 00:10:42.360 
without any accuracy loss, compared to the BERT base model.

94
00:10:44.840 --> 00:10:54.370 
As a summary, in this video, we first introduced differences between static and dynamic networks so that we can highlight

95
00:10:54.370 --> 00:10:57.509 
the benefit of a dynamic network.

96
00:10:57.519 --> 00:11:01.309 
We use a case study to present dynamic width and depth.

97
00:11:01.320 --> 00:11:04.889 
We also talk about how the gating algorithm works.

98
00:11:04.899 --> 00:11:06.210 
In the next video,

99
00:11:06.220 --> 00:11:09.460 
we will learn Super net and sub net architectures.

100
00:11:12.039 --> 00:11:16.659 
Here's some related literature for your interest for your extended reading.

101
00:11:18.139 --> 00:11:19.250 
Thank you for watching.
