WEBVTT

1
00:00:00.540 --> 00:00:01.760 
Hello and welcome.

2
00:00:02.240 --> 00:00:07.849 
This video will introduce the pruning technology for neural network compression.

3
00:00:09.740 --> 00:00:15.439 
Pruning is a method for model compression but when we say synaptic pruning

4
00:00:15.449 --> 00:00:21.649 
it's a natural process in the brain between early childhood and adulthood.

5
00:00:22.440 --> 00:00:27.000 
During synaptic pruning the brain eliminates extra synapses.

6
00:00:28.039 --> 00:00:34.659 
Synapses allow for the transmission of an electrical or chemical signal between neurons.

7
00:00:34.670 --> 00:00:44.659 
Synaptic pruning is thought to be the brain's way of removing connections in the brain that are no longer needed.

8
00:00:45.740 --> 00:00:54.259 
So it is our body's way of maintaining more efficient brain function as we age and learn new complex information.

9
00:00:55.439 --> 00:01:01.189 
But during infancy, the brain experiences a large amount of growth.

10
00:01:01.200 --> 00:01:09.560 
There is an explosion of synapse formation between the neurons during the early brain development.

11
00:01:10.439 --> 00:01:17.459 
This rapid period plays a vital role in learning memory formation and adaption

12
00:01:17.469 --> 00:01:22.540 
early in life. At about 2-3 years of age,

13
00:01:22.620 --> 00:01:31.810 
the number of synapses, hits peak level. During the second year of life, the number of synapses drops dramatically.

14
00:01:31.819 --> 00:01:37.750 
Synaptic pruning happens very quickly between age 2 and 10.

15
00:01:38.540 --> 00:01:51.450 
During this time, about 50% of the extra synapses are eliminated. In the visual cortex, pruning continues until about 6 years old.

16
00:01:53.340 --> 00:02:02.680 
In the previous video we have seen that a small model like SqueezeNet can achieve similar accuracy compared to AlexNet. But

17
00:02:02.680 --> 00:02:05.510 
reduced 98% of the model size.

18
00:02:06.239 --> 00:02:09.110 
You can say that 98% of the parameters.

19
00:02:09.120 --> 00:02:18.680 
It shows that the deep learning model are often overparameterised and the pruning method is as applicable to getting a more

20
00:02:18.680 --> 00:02:22.840 
efficient model.

21
00:02:22.840 --> 00:02:23.419 


22
00:02:23.780 --> 00:02:28.199 
Well, do you still remember how drop out works?

23
00:02:28.210 --> 00:02:32.759 
It randomly sets some neurons to zero during the forward propagation.

24
00:02:32.770 --> 00:02:38.050 
And it uses a parameter drop out rate to set the probability of dropping.

25
00:02:38.740 --> 00:02:43.860 
For example, 50% of neurons will be deactivated.

26
00:02:45.030 --> 00:02:49.650 
This behavior of dropout is very similar to the pruning method.

27
00:02:52.340 --> 00:02:58.360 
So we started with the unstructured pruning method which prune individual parameters.

28
00:02:59.139 --> 00:03:07.960 
Doing so produces a sparse neural network model which although smaller in terms of parameter number, may not be arranged

29
00:03:07.969 --> 00:03:14.050 
in a fashion conductive to speed enhancements using existing frameworks and hardware.

30
00:03:15.139 --> 00:03:17.599 
This is also called Weight Pruning.

31
00:03:17.650 --> 00:03:22.159 
As we set individual weights in the weight matrix to zero.

32
00:03:22.939 --> 00:03:26.550 
Now, as mentioned it in neural network computing,

33
00:03:26.560 --> 00:03:33.159 
we usually apply general matrix multiplication for convolution and fully connected layers.

34
00:03:33.840 --> 00:03:36.960 
Here I use the weight metrics as an example.

35
00:03:37.439 --> 00:03:41.550 
So the weight pruning workflow is as follows.

36
00:03:44.639 --> 00:03:49.949 
We first rank the weight using L1 norm as an important score.

37
00:03:50.439 --> 00:04:00.699 
Then we set the certain portion of weights with smaller score to zero for instance, 50%. Based on the specific implementation

38
00:04:00.710 --> 00:04:01.680 
and hardware,

39
00:04:01.689 --> 00:04:11.039 
we can skip zero ways during influence for latency improvement.

40
00:04:11.039 --> 00:04:13.069 
Structured pruning methods

41
00:04:13.080 --> 00:04:22.389 
consider parameters in groups, removing entire neurons, filters or channels to exploit hardware and software optimized for

42
00:04:22.389 --> 00:04:23.550 
dense computation.

43
00:04:24.139 --> 00:04:35.290 
This is also called unit or neuron pruning. As we set entire columns in the weight matrix to zero deleting the corresponding

44
00:04:35.290 --> 00:04:36.160 
output neuron.

45
00:04:37.139 --> 00:04:42.019 
Let's just take the same example as a last slide for the neuron pruning,

46
00:04:42.029 --> 00:04:44.759 
we will do the following on the weight metrics.

47
00:04:46.439 --> 00:04:54.699 
We first rank the weight columns or filters using the L2 norm as an important score. Here

48
00:04:54.709 --> 00:04:57.860 
each column is a future in the weight matrix.

49
00:04:58.240 --> 00:05:03.459 
Then we set the certain portion of the new columns with smaller scores to zero.

50
00:05:03.839 --> 00:05:08.759 
For instance, here we set 50% columns completely to zero.

51
00:05:11.240 --> 00:05:16.569 
In effect, it deletes the corresponding columns or the output neurons.

52
00:05:16.579 --> 00:05:26.610 
This way, we can achieve a smaller model smaller weight futures and for speed up using the current hardware

53
00:05:26.610 --> 00:05:28.060 
and software frameworks.

54
00:05:31.439 --> 00:05:40.009 
Weight on neuron pruning will cause performance degradation because we change the ways or the network structure but the

55
00:05:40.009 --> 00:05:43.980 
degradation can be somehow recovered to a certain degree.

56
00:05:43.990 --> 00:05:52.759 
Using fine tuning on training data. In practice iteratively pruning can effectively preserve accuracy.

57
00:05:53.279 --> 00:05:55.449 
Let's take a closer look at the workflow.

58
00:05:57.040 --> 00:06:02.970 
So for each layer in the given network we will start with a relatively small pruning step.

59
00:06:02.980 --> 00:06:07.259 
For example, just 5% weight will be reduced.

60
00:06:08.240 --> 00:06:15.569 
Then we calculate the important score of the weights or each weight column. According to the score,

61
00:06:15.579 --> 00:06:22.100 
we can prove the least essential 5% items. In our case,

62
00:06:22.110 --> 00:06:22.990 
the weight value.

63
00:06:23.000 --> 00:06:23.699 
Or

64
00:06:23.709 --> 00:06:24.860 
weight columns.

65
00:06:26.939 --> 00:06:37.230 
We do the network fine tuning for accuracy recovery if the final pruning rate is achieved a range and then

66
00:06:37.230 --> 00:06:38.120 
we stop.

67
00:06:38.129 --> 00:06:40.560 
Otherwise we will go to the step one.

68
00:06:43.339 --> 00:06:46.430 
This is a case study on pruning technique.

69
00:06:46.439 --> 00:06:54.689 
The authors developed a YOLOv3 model for hand detection and the corresponding prune version of the model. The applied

70
00:06:54.689 --> 00:06:56.910 
data set is called the VGG Hand.

71
00:06:57.129 --> 00:07:03.439 
It is a small dataset and the author only fine tune the neural model on it.

72
00:07:03.439 --> 00:07:04.939 
According to the table,

73
00:07:05.160 --> 00:07:12.970 
the original model has a mean average precision of 0.769. Besides,

74
00:07:12.970 --> 00:07:22.519 
we also showing the other four evaluation metrics, a number of parameters, model size, computing complexity in terms of loops

75
00:07:22.529 --> 00:07:24.379 
and the influence speed.

76
00:07:25.540 --> 00:07:27.550 
Now let's look at the prune version.

77
00:07:28.240 --> 00:07:38.860 
It achieved 82% of parameters and model size reduction and 71% of loops reduction and 49% faster inference speeds.

78
00:07:39.439 --> 00:07:40.529 
While the mAP

79
00:07:40.939 --> 00:07:43.959 
mean average precision value even slightly increased.

80
00:07:44.839 --> 00:07:49.790 
So after fine tuning the same reduction with the same reduction rate

81
00:07:49.800 --> 00:07:56.019 
The mAP is slightly improved.

82
00:07:56.019 --> 00:08:03.769 
In these two pictures we show the layer index, its corresponding channel count and the remaining channel numbers

83
00:08:03.769 --> 00:08:13.649 
after pruning. We can see that in the example the pruning rate distribution of the channels is not uniform and it is not easy

84
00:08:13.649 --> 00:08:15.459 
to find any regularity.

85
00:08:15.930 --> 00:08:20.959 
Some layers have extremely high reduction rates like over 95%.

86
00:08:24.639 --> 00:08:32.559 
Some works try to preserve the ability of the pruned part and expand the ability during the model compression process.

87
00:08:32.940 --> 00:08:42.399 
For example this work introduces the soft filter pruning by which the pruned filter can still be updated during the training,

88
00:08:42.409 --> 00:08:47.759 
so that it still has a chance to be restored in the later stage of the training.

89
00:08:49.139 --> 00:08:52.220 
The lottery ticket hypothesis is proposed.

90
00:08:52.230 --> 00:08:55.049 
This is a random initialization.

91
00:08:55.059 --> 00:08:57.990 
The dense network contains the sub network.

92
00:08:58.000 --> 00:09:07.330 
If this sub network is initialized with the original networks weight. The original networks tests accuracy can be obtained after

93
00:09:07.330 --> 00:09:12.090 
training at the same number of iterations. At the same time,

94
00:09:12.100 --> 00:09:16.059 
it also introduced the method to find the sub network structure.

95
00:09:16.740 --> 00:09:27.570 
This paper believed that the sub structure and its initial value are essential to training effectiveness and they are called

96
00:09:27.649 --> 00:09:29.110 
winning lottery tickets.

97
00:09:30.940 --> 00:09:39.840 
It is proposed that over parameterization is not essential for training but reusing its weights from the original network

98
00:09:39.850 --> 00:09:41.389 
may not be a good choice.

99
00:09:41.399 --> 00:09:50.929 
It may make the tailored model fail into a local minimum. If the weight of the original network or its initial value is not

100
00:09:50.929 --> 00:09:51.659 
essential,

101
00:09:52.039 --> 00:09:56.090 
the most crucial things left is the network architecture

102
00:09:56.090 --> 00:09:56.960 
of the pruning.

103
00:09:58.139 --> 00:10:06.480 
In other words, in some sense, pruning is like neural architecture search but because it only involves the layer dimension,

104
00:10:06.500 --> 00:10:09.750 
the search space is relatively smaller.

105
00:10:12.840 --> 00:10:17.759 
ProxylessNAS use pruning to employment neural architecture search.

106
00:10:18.139 --> 00:10:26.940 
There are some, there are some related works you could take a closer look if you are interested in this topic.

107
00:10:26.940 --> 00:10:28.450 
In this video,

108
00:10:28.460 --> 00:10:31.950 
we studied the pruning method for deep model compression.

109
00:10:32.639 --> 00:10:38.509 
We investigated weight pruning and neural pruning method and further check the performance improvement

110
00:10:38.509 --> 00:10:48.570 
using a hand detection example. A possible disadvantage of the pruning method is that the iterative pruning and fine tuning

111
00:10:48.570 --> 00:10:50.860 
pipeline might be time consuming.

112
00:10:53.240 --> 00:10:54.649 
Thank you for watching the video.
