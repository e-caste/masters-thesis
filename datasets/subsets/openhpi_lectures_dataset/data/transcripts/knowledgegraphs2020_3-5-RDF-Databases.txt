WEBVTT

1
00:00:01.000 --> 00:00:05.250 
This is knowledge graphs lecture
number 2, Querying RDFS with SPARQL.

2
00:00:06.140 --> 00:00:10.720 
In this section of the lecture, we are
going to talk about RDF databases.

3
00:00:11.110 --> 00:00:16.610 
So far you know a lot about RDF and
RDFS and you also know about SPARQL.

4
00:00:17.110 --> 00:00:19.230 
But we haven't
talked so far

5
00:00:20.150 --> 00:00:24.950 
on how to store that RDF
data. How is that done?

6
00:00:25.390 --> 00:00:30.120 
One basic idea would be probably
simply to use already existing

7
00:00:30.120 --> 00:00:34.540 
relational database management system, so
simply put your SPARQL stuff all the

8
00:00:34.770 --> 00:00:39.330 
triples that you have into a database,
into a relational database.

9
00:00:39.840 --> 00:00:45.750 
Let's see this, so an RDF graph can be represented
by a set of triples consisting of subject,

10
00:00:45.960 --> 00:00:47.550 
property and object.

11
00:00:48.930 --> 00:00:51.660 
We know that and of course, all
of these triples can simply

12
00:00:51.660 --> 00:00:53.170 
be stored in an RDBMS.

13
00:00:53.800 --> 00:00:55.890 
Why not, the
motivation is clear.

14
00:00:56.850 --> 00:01:01.620 
Databases are already around for
quite a while. So we use specific

15
00:01:01.620 --> 00:01:04.870 
relational schemata for RDF data
and benefit simply from the

16
00:01:04.880 --> 00:01:09.010 
more than 40 years of experience of
research within the database community.

17
00:01:09.850 --> 00:01:13.520 
The only thing we have to do for
that is of course we have to wrap

18
00:01:13.740 --> 00:01:18.770 
our SPARQL query into a sequel
query for that, which means we

19
00:01:18.770 --> 00:01:22.050 
have to abide the following
three steps. So first of all we

20
00:01:22.050 --> 00:01:25.580 
convert our SPARQL query into
a sequel query, of course with

21
00:01:25.580 --> 00:01:30.560 
respect to the according schema that we are
using, then we use the database system

22
00:01:30.690 --> 00:01:34.520 
to answer the sequel query and
in the end of course I have

23
00:01:34.520 --> 00:01:39.060 
to generate a SPARQL query
result from the sequel query

24
00:01:39.060 --> 00:01:43.680 
site. So this is always the same if I'm using
a relational database management system.

25
00:01:44.160 --> 00:01:49.420 
But now let's have a look how I
really store the triples efficiently

26
00:01:49.600 --> 00:01:51.970 
within a database
management system.

27
00:01:52.810 --> 00:01:56.050 
In principle we have four
alternatives here for you.

28
00:01:56.570 --> 00:02:01.700 
One would be so called monolithic triple
storage or we can use property tables,

29
00:02:02.230 --> 00:02:06.330 
vertically partitioned tables also
referred to as binary tables as well

30
00:02:06.630 --> 00:02:08.390 
as the hexastore
principle.

31
00:02:09.190 --> 00:02:10.170 
Let's have a look at them.

32
00:02:11.290 --> 00:02:14.570 
First of all we have the monolithic
triple storage. So this

33
00:02:14.570 --> 00:02:18.130 
is the most basic one. I simply
create one large table

34
00:02:18.560 --> 00:02:22.550 
consisting of course of three columns
subject, property and object

35
00:02:22.850 --> 00:02:27.120 
and I fill all of my rdf
triples simply into the table.

36
00:02:28.210 --> 00:02:30.800 
Now to carry out
a SPARQL query,

37
00:02:31.830 --> 00:02:36.120 
my performance that I achieve only
depends on really efficient indexing

38
00:02:36.360 --> 00:02:39.510 
on the table. You might remember
that in our SPARQL queries

39
00:02:39.650 --> 00:02:44.110 
we often used to know the same
variable at different places

40
00:02:44.110 --> 00:02:48.020 
on the subject place, then later on
on the object place and so on. So

41
00:02:48.330 --> 00:02:52.500 
you might see here lots of so
called self joints that might

42
00:02:52.510 --> 00:02:57.840 
be necessary. Also of course this variant
is rather easy to implement and

43
00:02:57.970 --> 00:03:01.920 
it works for a huge number of
properties. However what you have

44
00:03:01.920 --> 00:03:07.360 
to do that it works quickly or
efficiently choose your indices rather

45
00:03:07.640 --> 00:03:12.050 
wisely and with care. However self
joints are necessary, which

46
00:03:12.050 --> 00:03:18.100 
would see here for example, when we try
to map a SPARQL query for that table

47
00:03:18.380 --> 00:03:23.150 
to a sequel query. So that table is
about university and university

48
00:03:23.150 --> 00:03:26.510 
stuff with professors and great
students and we have a query

49
00:03:26.510 --> 00:03:30.850 
that simply wants to find out at
which university did my grad

50
00:03:30.850 --> 00:03:32.350 
students do
their bachelor.

51
00:03:33.660 --> 00:03:39.890 
So you have here in blue the SPARQL query asking
for an output which is the university

52
00:03:40.040 --> 00:03:44.140 
of a variable v which is of type
grad student and it should

53
00:03:44.140 --> 00:03:48.090 
have the bachelor from the variable which
is university I want to see there.

54
00:03:48.750 --> 00:03:51.080 
Let's translate
this to sequel.

55
00:03:52.440 --> 00:03:56.410 
What you see already in the SPARQL
query is that the variable v

56
00:03:56.740 --> 00:04:00.220 
occurs twice, in the first ripple
and in the second triple,

57
00:04:00.940 --> 00:04:04.050 
which means if we are referring
to both of them what we have

58
00:04:04.050 --> 00:04:08.730 
to do in sequel is we have to
do a self join. So we have to

59
00:04:08.730 --> 00:04:11.350 
join the table that
we have with itself.

60
00:04:12.570 --> 00:04:15.930 
The output in the end that we need
of course is the university,

61
00:04:15.930 --> 00:04:19.650 
so this is the object that we
take here from our second table

62
00:04:19.660 --> 00:04:22.920 
because the first triple we will
choose from the first table

63
00:04:23.050 --> 00:04:26.110 
and the second triple we will
choose from the second table.

64
00:04:26.550 --> 00:04:29.260 
So what I do is I select
here from the second table

65
00:04:29.670 --> 00:04:34.140 
my variable university which is
in t2, the second table the

66
00:04:34.140 --> 00:04:38.670 
object of the system third column
I have here in my table and

67
00:04:38.670 --> 00:04:42.740 
then I say okay, from which tables
from table t1 which is

68
00:04:42.740 --> 00:04:46.710 
triples and of course the same table
now referred to as t2, so I

69
00:04:46.810 --> 00:04:49.570 
combine the table with
itself, I do a join.

70
00:04:51.060 --> 00:04:54.920 
Then I say okay, from the first
triple. So from table one

71
00:04:55.370 --> 00:05:00.200 
property type should be chosen and
object grad student. So this means

72
00:05:00.560 --> 00:05:04.940 
I choose all variables which have the
type grad students and from table two

73
00:05:05.260 --> 00:05:10.590 
I say okay, I want to know bachelors
from, so where is the bachelor from

74
00:05:10.950 --> 00:05:14.880 
and of course then I have my join
conditions, so both should have

75
00:05:15.180 --> 00:05:18.500 
in table one in table tool which
means a triple one and triple two,

76
00:05:18.650 --> 00:05:22.480 
I have the same subject so t1
subject equals t2

77
00:05:22.480 --> 00:05:26.650 
subject. That would be a rather
easy translation for exactly

78
00:05:26.650 --> 00:05:30.880 
that kind of query and as you
might imagine we had previously

79
00:05:30.880 --> 00:05:35.560 
already queries with lots of
either conjunctively or

80
00:05:35.560 --> 00:05:40.410 
disjunctively connected triples
and also sub selects and stuff

81
00:05:40.410 --> 00:05:43.400 
like that. If you'd have been a
complex SPARQL query like that

82
00:05:43.400 --> 00:05:47.300 
and want to translate it later
on here to a sequel query you

83
00:05:47.300 --> 00:05:52.910 
might end up really in lots of self joins and
then of course inefficiency and complexity

84
00:05:53.120 --> 00:05:54.170 
related problems.

85
00:05:55.610 --> 00:06:00.470 
For that reason people came up with
alternatives - how to store the stuff. Not

86
00:06:00.570 --> 00:06:04.860 
again in one monolithic table
but to use several tables.

87
00:06:06.180 --> 00:06:09.210 
The very first trick that was
implemented was simply using

88
00:06:09.210 --> 00:06:14.070 
a so called resource table. So this
is uh one of the fundamental

89
00:06:14.070 --> 00:06:18.440 
things I use in the so called ID
based tripled storage. So there

90
00:06:18.440 --> 00:06:22.720 
I have a resource table and
sorry this resource

91
00:06:22.720 --> 00:06:26.820 
table is used for indexing URIs
and literals simply with a numerical

92
00:06:26.980 --> 00:06:31.470 
identifier. I do this simply
because it's much shorter and I

93
00:06:31.470 --> 00:06:34.580 
avoid lots of redundancies, you
know sometimes if I use you

94
00:06:34.580 --> 00:06:38.410 
arise they might be rather long
and if they occur again

95
00:06:38.410 --> 00:06:41.260 
and again and again and again of
course I use a lot of space

96
00:06:41.260 --> 00:06:46.460 
so therefore I simply do an index
and use some key value for that.

97
00:06:47.070 --> 00:06:50.750 
And then of course I have my RDF
triple table, again a monolithic table

98
00:06:51.050 --> 00:06:54.650 
but instead of putting in there
the URIs and the literals

99
00:06:54.650 --> 00:06:58.300 
I simply have them the numerical
identifiers in there and this

100
00:06:58.470 --> 00:07:02.270 
saves, of course, space and
enhances greatly efficiency.

101
00:07:03.810 --> 00:07:07.040 
And what I can also do instead of
you know using a three column

102
00:07:07.040 --> 00:07:10.270 
table, I can use a four column
table and then I can use the

103
00:07:10.270 --> 00:07:14.600 
fourth column to indicate to
which graph to which rdf graph

104
00:07:14.600 --> 00:07:19.750 
this triple belongs to, which means
I can put several graphs into

105
00:07:19.990 --> 00:07:24.870 
one single table. So again this
is rather interesting.

106
00:07:26.780 --> 00:07:30.200 
However that's not basically
different from storing everything

107
00:07:30.200 --> 00:07:33.240 
in a monolithic table. You again
have the same problems with

108
00:07:33.240 --> 00:07:35.540 
self joins for
exactly that variant.

109
00:07:37.150 --> 00:07:40.770 
So what people came up to, they
thought we can make this

110
00:07:40.770 --> 00:07:45.410 
much more let's say efficient if
we try to combine either all

111
00:07:45.410 --> 00:07:50.350 
or some of the properties of similar
subjects in, now not binary, but

112
00:07:50.540 --> 00:07:54.740 
n-ary tables. So for example what
we did with our example is

113
00:07:54.740 --> 00:07:57.430 
we created a table for the
professors because we said

114
00:07:57.910 --> 00:08:01.700 
no matter whether it's a full professor
or an associate professor, all of them

115
00:08:01.840 --> 00:08:05.430 
they teach something, so they need
the property teacher of all

116
00:08:05.430 --> 00:08:07.800 
of them have their bachelors, all
of them have the master, the

117
00:08:07.800 --> 00:08:11.400 
phd and they work for somebody
and of course we have a table

118
00:08:11.400 --> 00:08:14.830 
then for the grad students who
are associated in the table

119
00:08:14.830 --> 00:08:18.030 
with her supervisor, of course
they also have a bachelor, they

120
00:08:18.030 --> 00:08:21.840 
also have a master, sometimes
they are teaching assistants to

121
00:08:21.840 --> 00:08:24.080 
a course and so on
and so on and so on.

122
00:08:24.730 --> 00:08:28.160 
So this means we have simply
created two tables. So this is

123
00:08:28.160 --> 00:08:32.570 
a kind of clustering that we did.
And of course you see them there

124
00:08:32.700 --> 00:08:36.290 
things are stored much more
efficiently. Especially you avoid

125
00:08:36.640 --> 00:08:38.650 
many of these so
called self joins.

126
00:08:39.420 --> 00:08:43.920 
One of the drawbacks that you have
of course potentially not for

127
00:08:44.050 --> 00:08:47.520 
all then of the fields you might have
a value and then you end up with

128
00:08:47.710 --> 00:08:49.820 
many so called
null values.

129
00:08:50.600 --> 00:08:54.480 
Another thing is of course as
you see the clustering that we

130
00:08:54.480 --> 00:08:59.060 
do here heavily depends on the
content that we are trying here

131
00:08:59.060 --> 00:09:02.160 
to manage. So this means this
kind of clustering is anything

132
00:09:02.160 --> 00:09:06.550 
else but trivial. So this is
really and a complex task that

133
00:09:06.550 --> 00:09:07.610 
has to be solved there.

134
00:09:08.690 --> 00:09:13.670 
And especially if you want to
try to represent multi-valued

135
00:09:13.670 --> 00:09:17.010 
properties then it becomes complicated.
Just imagine what do you do

136
00:09:17.130 --> 00:09:20.510 
when you have a full professor
lets say with double affiliation

137
00:09:20.520 --> 00:09:24.590 
or if he has two masters or
whatever. Then you need of course

138
00:09:24.590 --> 00:09:30.060 
two rows and one of them contains lots of
nulls or everything is redundant. So

139
00:09:30.420 --> 00:09:34.890 
this is some critical thing here
to consider if you talk about

140
00:09:34.890 --> 00:09:39.240 
property tables in going right to
the horizontal partitioning here.

141
00:09:40.560 --> 00:09:45.430 
Therefore people came up with a new idea which
are so called vertically partitioned tables.

142
00:09:45.570 --> 00:09:48.240 
What's a vertically partitioned
table? So this is rather easy.

143
00:09:48.880 --> 00:09:53.830 
For each unique property that you have
in your rdf statements we create a

144
00:09:53.930 --> 00:09:55.540 
simple two column table.

145
00:09:56.420 --> 00:10:01.480 
And we use of course this ID based
encoding just for case of efficiency.

146
00:10:02.120 --> 00:10:05.440 
As you see here we have a property
for type, we have a property

147
00:10:05.440 --> 00:10:09.480 
for a teacher of, we have a
property for phd from and so on

148
00:10:09.480 --> 00:10:11.840 
and so on. So for each
property we have a table.

149
00:10:13.070 --> 00:10:18.590 
The nice thing is now this of course
easily supports multi-valued properties

150
00:10:18.930 --> 00:10:22.070 
simply because of the fact, so if
a property have multiple values

151
00:10:22.070 --> 00:10:25.240 
I simply add another row and that's
it and this is only two fields. So

152
00:10:25.860 --> 00:10:29.280 
no problem. So I also don't
have no ull, so if thing

153
00:10:29.280 --> 00:10:33.280 
doesn't exist then of course this
he has no row, or the triple

154
00:10:33.280 --> 00:10:37.220 
has no row in for that property.
So we have no null values to

155
00:10:37.220 --> 00:10:38.000 
to deal with.

156
00:10:40.040 --> 00:10:44.040 
Then on the other hand of course you
read only the attributes that you need,

157
00:10:44.230 --> 00:10:47.840 
so you create much less I/O. So
for example if you only

158
00:10:47.840 --> 00:10:50.850 
want to access lets say to a few
of the properties, you access

159
00:10:50.850 --> 00:10:54.090 
only to a few of the tables in
your query and you don't access

160
00:10:54.090 --> 00:10:58.210 
the others, so you create less
I/O. You don't need any that

161
00:10:58.210 --> 00:11:01.950 
say sophisticated clustering
and it has often an excellent

162
00:11:01.950 --> 00:11:04.840 
performance especially if the
number of properties is small,

163
00:11:05.070 --> 00:11:10.580 
and the queries are of course taken
out or carried out with bound

164
00:11:10.680 --> 00:11:13.680 
properties. This is important if
the property isn't bound, so

165
00:11:13.680 --> 00:11:17.930 
if you want to query over all properties,
if properties itself is a variable,

166
00:11:18.180 --> 00:11:21.520 
then of course you have to extend
or join all of the tables

167
00:11:21.520 --> 00:11:23.850 
together and this again
will be expensive.

168
00:11:24.740 --> 00:11:27.760 
The same is sometimes with insert,
so sometimes you have to

169
00:11:27.760 --> 00:11:30.570 
create if there is a new triple
you have to create a new table

170
00:11:30.570 --> 00:11:34.790 
because it has a new property. So this
of course is also rather complex

171
00:11:34.940 --> 00:11:38.350 
and it has a better performance
if it comes to a large number

172
00:11:38.350 --> 00:11:41.790 
of properties, especially if you
have queries to answer that

173
00:11:41.790 --> 00:11:45.250 
have unbound properties as we
have said before. So there we

174
00:11:45.250 --> 00:11:47.110 
have efficiency issues.

175
00:11:49.130 --> 00:11:53.140 
So we come to the very last
possibility, how to implement rdf

176
00:11:53.140 --> 00:11:56.410 
databases and this is the so
called hexastores. What's

177
00:11:56.410 --> 00:11:58.240 
a hexastore?
For a hexastore

178
00:11:58.850 --> 00:12:03.330 
you create an index for
each possible combination

179
00:12:03.770 --> 00:12:09.640 
of subject, property, object to enable
efficient processing.

180
00:12:10.120 --> 00:12:13.150 
So what you are going to do here,
you create an index for the

181
00:12:13.150 --> 00:12:18.100 
sequence spo, subject property
object, then also pos,

182
00:12:18.110 --> 00:12:21.250 
osp, sop, pso, ops.

183
00:12:23.010 --> 00:12:25.920 
An index looks in the way like
that, so you have a subject key

184
00:12:26.180 --> 00:12:29.370 
that points to a sorted
vector of property keys

185
00:12:29.860 --> 00:12:33.120 
and each of these property keys
then is linked to a sorted

186
00:12:33.120 --> 00:12:36.750 
list of object keys. So this
database consists of these kind

187
00:12:36.750 --> 00:12:39.460 
of sorted lists of subject
property and objects.

188
00:12:40.400 --> 00:12:43.960 
The drawback here is or let's
first talk about the benefit

189
00:12:43.960 --> 00:12:50.430 
so if I do an spo index
I can of course reuse

190
00:12:50.620 --> 00:12:54.540 
these sorted lists of old and
also for my pso index. So

191
00:12:54.540 --> 00:12:59.850 
there is at least some redundancy
that I can make use of. However,

192
00:13:01.550 --> 00:13:06.720 
however means although if I
then do a join on these

193
00:13:06.720 --> 00:13:10.350 
kind of data structures they are
fast. This is true, but on the

194
00:13:10.350 --> 00:13:15.250 
other hand I need up to five times
more storage than I have triplets

195
00:13:15.940 --> 00:13:20.410 
and for huge knowledge bases like
for example if you take the

196
00:13:20.410 --> 00:13:25.440 
entire dbpdia or entire wiki data
going that way would be critical

197
00:13:25.440 --> 00:13:28.650 
because the performance gets
really really weak if you can't

198
00:13:28.650 --> 00:13:31.300 
do that stuff these kind of
joints that you have to do

199
00:13:31.780 --> 00:13:36.030 
not anymore within your main memory.
If you have to use disk access,

200
00:13:36.420 --> 00:13:40.110 
then everything slows down
dramatically and you're busted

201
00:13:40.920 --> 00:13:44.300 
So that's the problem with
so called hexastores.

202
00:13:46.230 --> 00:13:51.250 
Never mind. So we have some of the
existing and freely available

203
00:13:51.250 --> 00:13:56.000 
triple stores here copied on
that slide simply tried out, so

204
00:13:56.000 --> 00:14:00.860 
from Apache for example. There is also
open version of the Apache Jena

205
00:14:01.000 --> 00:14:05.970 
a triple database, AllegroGraph, Blaze
Graph. So there are lots of possibilities

206
00:14:06.080 --> 00:14:10.710 
that you might try out then and
see how your SPARQL queries

207
00:14:10.710 --> 00:14:14.860 
will perform also on large rdf
graphs that you might store there.

208
00:14:16.450 --> 00:14:21.360 
So, that was it all on rdf graphs
and in the next lecture which

209
00:14:21.360 --> 00:14:25.860 
is also the last lecture of that
or the last part of that lecture,

210
00:14:26.080 --> 00:14:30.700 
we will talk about SPARQL and that
SPARQL is indeed much more than only

211
00:14:30.860 --> 00:14:33.180 
a query language.
