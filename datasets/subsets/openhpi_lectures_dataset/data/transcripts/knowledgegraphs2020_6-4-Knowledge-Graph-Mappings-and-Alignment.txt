WEBVTT

1
00:00:00.850 --> 00:00:05.680 
This is knowledge graphs. Welcome to lecture
six, advanced knowledge graph applications.

2
00:00:06.150 --> 00:00:10.840 
In this lecture we are going to talk about
knowledge graph mappings and alignment.

3
00:00:11.670 --> 00:00:17.260 
So what are the actual challenges
that we encounter while building

4
00:00:17.260 --> 00:00:20.940 
knowledge graph, while we have
knowledge graphs? So when

5
00:00:20.940 --> 00:00:24.610 
you build a small knowledge
graph, it is much easier. But

6
00:00:24.850 --> 00:00:27.920 
when you are building something
huge like google knowledge

7
00:00:27.920 --> 00:00:32.010 
graph then there are huge
challenges that you face. So here

8
00:00:32.020 --> 00:00:36.420 
you see three different kind of
challenges that you will see that

9
00:00:36.910 --> 00:00:42.000 
what they are and how they actually
are the challenges. So here

10
00:00:42.000 --> 00:00:46.210 
we have for example the coverage.
Coverage is the information if

11
00:00:46.310 --> 00:00:49.480 
the information in the
knowledge graph is complete.

12
00:00:49.960 --> 00:00:54.660 
Then we have correctness which
tells that how much accurate

13
00:00:54.670 --> 00:00:59.010 
is the information in the knowledge
graph. Then we have freshness

14
00:00:59.010 --> 00:01:03.450 
which tells us how much this
knowledge graph is updated

15
00:01:03.450 --> 00:01:09.000 
with the new information. Now these three
parameters, these three properties

16
00:01:09.300 --> 00:01:13.170 
for the knowledge graphs are
actually dependent on each other.

17
00:01:14.130 --> 00:01:20.150 
So for example when you try to increase
freshness and coverage then it is

18
00:01:20.410 --> 00:01:25.060 
hard to ensure that the knowledge
inside is correct or not.

19
00:01:25.980 --> 00:01:29.720 
So then when you try to
increase the correctness

20
00:01:29.790 --> 00:01:34.720 
then it is harder to ensure if the
coverage or and the freshness

21
00:01:34.730 --> 00:01:37.330 
of this knowledge
is good or not.

22
00:01:38.670 --> 00:01:43.510 
So correctness is always a hard task
what is true and what is correct.

23
00:01:44.710 --> 00:01:49.660 
So in order to automatically
manage the knowledge graph so

24
00:01:49.750 --> 00:01:53.180 
when we want to do the automatic
knowledge graph management

25
00:01:53.460 --> 00:01:59.470 
we can follow this workflow,
so you have data extraction,

26
00:01:59.890 --> 00:02:05.430 
then there is entity linking, on top
of which you perform knowledge

27
00:02:05.430 --> 00:02:10.050 
inferences and then you publish
this knowledge. And then

28
00:02:10.050 --> 00:02:15.320 
when you have the feedback from the user
or any feedback, then you refine your

29
00:02:15.610 --> 00:02:18.480 
knowledge and you go back to
the first step and then you

30
00:02:19.260 --> 00:02:23.910 
go through the similar steps.
So for example the knowledge

31
00:02:23.910 --> 00:02:29.050 
extraction can be unsupervised
from unstructured data,

32
00:02:29.430 --> 00:02:34.170 
it can be unstructured data
for example text, or it

33
00:02:34.170 --> 00:02:37.890 
could be domain independent.
And then we have

34
00:02:38.350 --> 00:02:43.460 
how do we embed the semantics
in this? We do this

35
00:02:43.470 --> 00:02:46.890 
semantic embedding with
the help of ontology.

36
00:02:47.340 --> 00:02:52.280 
Now the point is when you do this
automated knowledge graph extraction

37
00:02:52.550 --> 00:02:57.870 
then you come across huge
knowledge graph where you

38
00:02:57.870 --> 00:03:01.890 
have a huge amount of entities,
huge number of entities. And

39
00:03:01.890 --> 00:03:06.750 
then you have to perform large scale
entity linking and disambiguation.

40
00:03:07.450 --> 00:03:12.150 
And then you can perform inferences
and verification: also one

41
00:03:12.150 --> 00:03:16.420 
of the things that should be taken care of
is the knowledge graph versioning because

42
00:03:16.700 --> 00:03:20.210 
whenever you are trying to add
freshness to the knowledge graph

43
00:03:20.210 --> 00:03:25.100 
so when you are adding new
a new information so the

44
00:03:25.100 --> 00:03:28.120 
so the knowledge graph is actually
evolving from one version

45
00:03:28.120 --> 00:03:32.240 
to another. So you have to
keep track of the knowledge

46
00:03:32.240 --> 00:03:36.790 
graph versioning and archiving. And
the last one is knowledge precision

47
00:03:36.930 --> 00:03:38.830 
and comprehensiveness.

48
00:03:39.930 --> 00:03:44.820 
So now the question is how to automate
knowledge graph construction.

49
00:03:45.440 --> 00:03:48.870 
So knowledge graph actually
heavily the knowledge graph

50
00:03:48.870 --> 00:03:52.580 
construction heavily relies on
the ontologies and ontology

51
00:03:52.580 --> 00:03:58.430 
design is a very expensive task with
respect to time and resources.

52
00:03:58.860 --> 00:04:03.650 
So it can be learned automatically.
So the ontology learning

53
00:04:03.650 --> 00:04:07.400 
actually defines a set of methods
and techniques. So there is

54
00:04:07.400 --> 00:04:10.660 
the fundamental development
which means you are developing

55
00:04:11.070 --> 00:04:16.180 
new ontologies from the scratch. Then
we have extension or adaptation.

56
00:04:16.420 --> 00:04:22.120 
So we reuse the existing
ontologies. And then the

57
00:04:22.550 --> 00:04:28.800 
it is, ontology learning is
actually done in partly automated

58
00:04:28.800 --> 00:04:30.580 
way for various
resources.

59
00:04:32.600 --> 00:04:37.370 
So there are some fundamental
types of ontology learning. So

60
00:04:37.370 --> 00:04:41.480 
first is ontology learning from text
which we have talked about. So

61
00:04:41.830 --> 00:04:46.790 
it's like automatic or semi
automatic generation of ontologies

62
00:04:47.060 --> 00:04:52.230 
with the help of natural language
processing or information extraction.

63
00:04:52.850 --> 00:04:58.070 
Then we have linked data mining
where the meaningful patterns

64
00:04:58.070 --> 00:05:02.360 
are detected in the RDF graphs,
with the help of statistical

65
00:05:02.360 --> 00:05:05.730 
schema induction or statistical
relational learning.

66
00:05:06.410 --> 00:05:10.060 
Then we have concept learning
and description logics and OWL

67
00:05:10.450 --> 00:05:17.240 
in which you mostly learn schema axioms from
existing ontologies and instance data

68
00:05:17.830 --> 00:05:20.720 
based on an inductive
logic programming.

69
00:05:21.120 --> 00:05:25.380 
Then you have crowdsourcing
ontologies where you use, where

70
00:05:25.380 --> 00:05:30.290 
you use the crowds to learn the
ontologies, where you the advantage

71
00:05:30.290 --> 00:05:34.960 
of that is that you get the
accuracy of humans and one of

72
00:05:34.960 --> 00:05:39.260 
the examples of this place one
of the platforms is amazon

73
00:05:39.260 --> 00:05:44.540 
turk where you can try to do
this crowdsourcing task.

74
00:05:46.620 --> 00:05:50.990 
So ontology learning from text
what it usually does it is the

75
00:05:50.990 --> 00:05:57.660 
process which identifies terms,
concepts, relations and some axioms

76
00:05:57.820 --> 00:06:03.050 
from the textual information. What
it uses for doing so it uses

77
00:06:03.200 --> 00:06:06.910 
natural language processing
techniques, data mining, machine

78
00:06:06.910 --> 00:06:10.470 
learning techniques or information
retrieval techniques.

79
00:06:11.200 --> 00:06:15.900 
So the whole overall architecture
of ontology learning from

80
00:06:15.900 --> 00:06:19.740 
text a very basic way
of explaining is

81
00:06:20.350 --> 00:06:24.740 
that you have a document corpus from
which the terms are extracted

82
00:06:24.750 --> 00:06:29.100 
by identifying the nouns
phrases and their internal

83
00:06:29.610 --> 00:06:34.860 
semantic structure. This way you obtain
the terminology from the textual data.

84
00:06:35.230 --> 00:06:39.930 
And then you move towards the
conceptualisation of this ontology

85
00:06:39.930 --> 00:06:44.990 
with the help of core reference resolution
or any other such kind of algorithms.

86
00:06:45.420 --> 00:06:49.510 
This way you obtain an ontology.
So here for example from

87
00:06:49.510 --> 00:06:52.800 
the document you obtain the
terms, dog, dogs cat,

88
00:06:53.250 --> 00:06:57.920 
siamese cat. Here in the
ontology you define pet,

89
00:06:58.680 --> 00:07:04.840 
dog, cat and then siamese cat is
more specific class than the cat.

90
00:07:05.360 --> 00:07:09.120 
Then the evaluation and adaptation
is performed and the cycle

91
00:07:09.120 --> 00:07:09.820 
starts again.

92
00:07:12.060 --> 00:07:16.990 
So here is the whole layer
cake for ontology learning.

93
00:07:17.210 --> 00:07:22.960 
So as we said we start with the terms.
So we have the terms here for example,

94
00:07:23.230 --> 00:07:27.630 
so we are starting at the bottom
of this image. So we have river,

95
00:07:27.840 --> 00:07:31.920 
country, nation, city, capital.
And then you have multi-lingual

96
00:07:31.920 --> 00:07:37.190 
synonyms of these terms. Then
you go one level above and you

97
00:07:37.200 --> 00:07:41.380 
describe the concepts. The descriptions
can be textual descriptions

98
00:07:41.920 --> 00:07:48.090 
and then you have concept hierarchies.
So once you have defined

99
00:07:48.090 --> 00:07:52.750 
the concepts now you can add more complicated
relations which are hierarchies

100
00:07:53.060 --> 00:07:57.240 
between the concept. So capital
is more is a subset of city,

101
00:07:57.240 --> 00:08:00.570 
city is subset of
inhabited geoentity.

102
00:08:01.530 --> 00:08:06.450 
Then we define the relationship.
So relations you have flow

103
00:08:06.450 --> 00:08:10.890 
through, so the river flows
through a geoentity.

104
00:08:11.390 --> 00:08:15.680 
So here the domain of the relation
flow-through is river and

105
00:08:15.680 --> 00:08:19.450 
the range is geoentity.
Then we have similar

106
00:08:19.980 --> 00:08:22.980 
hierarchies over the
relationships. So

107
00:08:23.610 --> 00:08:28.390 
capital of is subset
of located in.

108
00:08:29.220 --> 00:08:35.450 
Now we go a bit more a more general
where we can define axiomatic

109
00:08:35.460 --> 00:08:40.500 
schemata, for example, given the two
sets we have river and mountain.

110
00:08:40.810 --> 00:08:45.380 
We can define the disjointness
between these two

111
00:08:45.380 --> 00:08:49.980 
sets which means that a river cannot
be a mountain and vice versa.

112
00:08:50.620 --> 00:08:56.590 
Similarly when we go towards the general
axioms here we can say a country

113
00:08:56.790 --> 00:08:58.620 
has at least
one capital.

114
00:09:01.710 --> 00:09:06.110 
Ontologies are not the reality.
So ontologies are a context

115
00:09:06.120 --> 00:09:11.340 
dependent projection of the reality. So
different ontology might represent

116
00:09:11.840 --> 00:09:17.160 
similar knowledge as for example
it can reflect different tasks

117
00:09:17.160 --> 00:09:21.290 
or requirement of the applications,
so they can be bent according to

118
00:09:21.770 --> 00:09:26.160 
different applications for
which the user needs them or

119
00:09:26.160 --> 00:09:29.650 
they could follow different
conventions or restrictions.

120
00:09:30.610 --> 00:09:35.050 
So how can these ontologies be
different? Let's make it a bit more

121
00:09:35.350 --> 00:09:40.430 
precise. So the same term can
describe more than one concept.

122
00:09:41.940 --> 00:09:46.980 
So for example author can be
writer of a book or author can

123
00:09:46.980 --> 00:09:48.590 
be the creator
of a document.

124
00:09:49.610 --> 00:09:52.510 
Then different terms
describe the same concept.

125
00:09:52.940 --> 00:09:57.230 
For example author and writer are two
different terms for the same thing.

126
00:09:58.180 --> 00:10:01.860 
Then we have different modeling
conventions and paradigms. For

127
00:10:01.860 --> 00:10:07.750 
example we have intervals and points. So
if for example when you are defining

128
00:10:08.030 --> 00:10:13.520 
temporal aspects like time, you can
define one distinct point in time

129
00:10:13.620 --> 00:10:18.780 
or you can define the time
between this this like between

130
00:10:18.780 --> 00:10:23.720 
two o'clock to three o'clock. So this way
this is what we call the time interval.

131
00:10:25.420 --> 00:10:28.070 
Then we have different
levels of granularity.

132
00:10:28.540 --> 00:10:34.600 
So fiction and then we have political
fiction, science fiction, romantic fiction.

133
00:10:34.820 --> 00:10:38.160 
So fiction is more general than
the rest of the three, so we

134
00:10:38.160 --> 00:10:42.990 
have two levels of granularity
here in the literary genre.

135
00:10:44.180 --> 00:10:48.510 
And then there is a difference
in the coverage or

136
00:10:48.510 --> 00:10:53.760 
different point of views for two different
ontology, two or more different ontologies.

137
00:10:54.350 --> 00:10:58.540 
So how can we perform the alignment
between these ontologies?

138
00:10:59.030 --> 00:11:03.430 
So ontology alignment and ontology
matching is the process

139
00:11:03.430 --> 00:11:08.420 
of determining the correspondences
between ontological concepts.

140
00:11:08.840 --> 00:11:13.620 
So for example you have
two ontologies O1 and O2

141
00:11:13.760 --> 00:11:17.670 
and you give that as an input
to the matching method

142
00:11:18.080 --> 00:11:22.970 
where you have thesauri, common
knowledge rules and you have several

143
00:11:22.970 --> 00:11:27.110 
parameters such as weights and
thresholds. And at the end you get

144
00:11:27.330 --> 00:11:29.320 
an alignment as
an output.

145
00:11:32.190 --> 00:11:37.380 
So the correspondence or the mapping
between the two ontologies can be

146
00:11:37.730 --> 00:11:41.410 
between, so when you have two
ontologies and we want to get

147
00:11:41.410 --> 00:11:45.530 
the correspondence on the level of
entities, how do you define that?

148
00:11:45.860 --> 00:11:49.900 
So for example you have O1
and O2, two ontologies

149
00:11:49.900 --> 00:11:54.620 
and e1 and e2 are the two
entities coming fromO1

150
00:11:54.620 --> 00:11:58.840 
and O2 respectively. So two
different resources. So here

151
00:11:58.850 --> 00:12:03.860 
it can be given like this. So you
have the id of the correspondence,

152
00:12:04.210 --> 00:12:07.830 
two entities, relation and
then the confidence measure.

153
00:12:08.300 --> 00:12:14.850 
So ID is the identifier, relation can be
any relation like equivalence relation or

154
00:12:15.090 --> 00:12:18.860 
more general relation less
general so on and so forth.

155
00:12:19.290 --> 00:12:24.130 
Then you have confidence measure which
usually lies between zero and one.

156
00:12:24.530 --> 00:12:29.280 
So it tells you how what is the
probability that the correspondence

157
00:12:29.280 --> 00:12:33.150 
holds between the two
entities e1 and e2.

158
00:12:36.690 --> 00:12:40.320 
Let's look at the simple
correspondences here. So for example

159
00:12:40.320 --> 00:12:45.090 
the entity in DBpedia Joseph
Fourier is equal to the entity in

160
00:12:45.480 --> 00:12:50.080 
wikidata Q8772. Then we
have author which is

161
00:12:50.430 --> 00:12:55.170 
equal to the writer, then we have
gas which is more general than

162
00:12:55.420 --> 00:12:58.770 
the greenhouse gas and the
probability of this is one.

163
00:12:59.310 --> 00:13:04.630 
Then we haverdfs label which
is more general than dc

164
00:13:05.040 --> 00:13:08.930 
title with the confidence
of zero point nine.

165
00:13:09.930 --> 00:13:13.780 
There can be more complex
correspondences such as conversion

166
00:13:13.780 --> 00:13:18.360 
between the speed and the velocity
and also for example you have

167
00:13:18.850 --> 00:13:23.920 
a book x, a book x has an
author y and the writer y

168
00:13:24.520 --> 00:13:28.790 
which implies that the
book x is written by

169
00:13:29.480 --> 00:13:33.210 
the concatenation of the
first name and the last

170
00:13:33.210 --> 00:13:38.740 
name of the author y. And the confidence
of this is zero point eight five.

171
00:13:39.260 --> 00:13:44.620 
So now here is an example of
alignment. So here you can on the

172
00:13:44.620 --> 00:13:48.220 
right side you can, each of
these lines you can see

173
00:13:48.220 --> 00:13:52.730 
these are the alignments which are defined
between two different ontologies.

174
00:13:53.140 --> 00:13:57.650 
So here book is equal to volume
and this is the confidence

175
00:13:57.730 --> 00:14:02.240 
with which it has been made, this
alignment has been defined.

176
00:14:02.900 --> 00:14:08.440 
Then we have ID which is more general
than ISBD and the confidence

177
00:14:08.450 --> 00:14:13.560 
is zero point nine, person is equal
to human. Now here the confidence

178
00:14:13.560 --> 00:14:16.420 
is zero point nine.
Similarly there are other

179
00:14:16.970 --> 00:14:20.220 
alignments which are defined
between these two ontologies.

180
00:14:21.420 --> 00:14:26.020 
So now the ontology matching
techniques can be on different

181
00:14:26.020 --> 00:14:29.870 
levels. So you have the element level
ontology matching techniques,

182
00:14:30.370 --> 00:14:34.990 
so in this element level ontology
matching techniques consider

183
00:14:34.990 --> 00:14:39.990 
entities and their instances in
isolation from their relations

184
00:14:40.530 --> 00:14:45.260 
with other entities. So it only considers
the entities and the instances.

185
00:14:45.530 --> 00:14:49.960 
So what happens in this kind of
alignment is that string based

186
00:14:49.970 --> 00:14:53.760 
string based similarities
or matching is performed.

187
00:14:54.210 --> 00:14:57.390 
Then it could be linguistic
based where you can have

188
00:14:57.840 --> 00:15:02.080 
other kind of linguistic relations,
for example homonymy, synonymy

189
00:15:02.750 --> 00:15:08.520 
etc. Then you have constraint-based
alignments where you can have

190
00:15:08.700 --> 00:15:12.740 
types, cardinality of properties,
so it is actually considering

191
00:15:12.740 --> 00:15:17.230 
the internal constraints applied to
the definitions of the entities.

192
00:15:17.820 --> 00:15:22.880 
And finally it is the extensional
based as extensional

193
00:15:22.890 --> 00:15:26.190 
based techniques where
you have where you use

194
00:15:26.600 --> 00:15:30.890 
individual representation of the
classes, that is classes are

195
00:15:30.890 --> 00:15:35.630 
considered similar if they share this
similar or many of the similar

196
00:15:35.990 --> 00:15:42.120 
instances. Then we have structure
level ontology alignment techniques.

197
00:15:42.920 --> 00:15:46.950 
So here what happens these ontologies
are considered as graphs.

198
00:15:47.290 --> 00:15:52.660 
So graph there are graph based
ontology alignment techniques where

199
00:15:53.140 --> 00:15:57.940 
the assumption is if nodes are similar
then their neighbors must be similar.

200
00:15:58.390 --> 00:16:02.140 
Then we have taxonomy based where
taxonomy is considered as

201
00:16:02.140 --> 00:16:07.820 
graph and then we consider only specialization
and generalization relationship.

202
00:16:08.380 --> 00:16:12.450 
Then we have method based which
takes into account the semantic

203
00:16:12.470 --> 00:16:17.410 
interpretation of the ontologies
if two entities are the

204
00:16:17.410 --> 00:16:20.060 
same then they share the
same interpretation.

205
00:16:20.720 --> 00:16:25.390 
Then we have data analysis and
statistics. So what it does is

206
00:16:25.390 --> 00:16:30.010 
take a large sample, try to find
regularities and discrepancies

207
00:16:30.480 --> 00:16:34.600 
which allows grouping of
grouping or it helps

208
00:16:34.600 --> 00:16:37.140 
in determining also the
distance matrices.

209
00:16:39.390 --> 00:16:43.980 
So now here you can actually see
its various methods for ontology

210
00:16:44.270 --> 00:16:48.340 
ontology matching. So you have string
based matching which is syntax

211
00:16:48.600 --> 00:16:52.690 
and this is language based matching
which is also syntactic.

212
00:16:53.070 --> 00:16:57.950 
So you can actually read this
this whole picture this way. So

213
00:16:57.950 --> 00:17:03.780 
it talks about the basic techniques and the
granularity input of the interpretation.

214
00:17:04.200 --> 00:17:09.790 
So I would refer you to the link
to the book on the bottom

215
00:17:10.050 --> 00:17:13.790 
which is about ontology matching.
You can have a look at that.

216
00:17:15.950 --> 00:17:20.870 
And then in the next lecture we are
going to talk about semantic search
