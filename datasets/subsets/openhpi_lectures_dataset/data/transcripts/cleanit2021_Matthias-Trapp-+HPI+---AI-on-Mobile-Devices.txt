WEBVTT

1
00:00:00.660 --> 00:00:05.580 
Hi, some of you might know that cloud
services are quite energy efficient,

2
00:00:06.090 --> 00:00:10.550 
but at the same time require a
significant amount of energy to operate

3
00:00:11.260 --> 00:00:15.630 
and the cloud services are used to
train and run neural networks.

4
00:00:16.810 --> 00:00:21.650 
Today, I would like to talk about how to optimize
neural networks for the application

5
00:00:21.810 --> 00:00:27.620 
on mobile devices directly and how
this can reduce energy consumption

6
00:00:27.890 --> 00:00:31.790 
by avoiding cloud services for the
inference of neural networks.

7
00:00:34.990 --> 00:00:38.880 
In our visual media analysis and
processing group at the HPI,

8
00:00:39.250 --> 00:00:43.990 
we apply neural networks for the
processing, interpretation, stylization,

9
00:00:44.190 --> 00:00:48.440 
and abstraction of visual media
such as digital images and videos.

10
00:00:49.690 --> 00:00:53.550 
Our main objective is the research
and development of innovative

11
00:00:53.550 --> 00:00:58.340 
tools to transform abstract and
stylize such media, for example,

12
00:01:00.520 --> 00:01:04.270 
we use machine learning to derive
additional information from

13
00:01:04.270 --> 00:01:05.880 
a single colored
input image.

14
00:01:08.140 --> 00:01:13.050 
Information such as depth, normal,
saliency, albedo, and shading

15
00:01:13.280 --> 00:01:17.320 
can be used to control the
stylization of an image into

16
00:01:17.750 --> 00:01:20.120 
for example, a
3D photo

17
00:01:21.190 --> 00:01:25.350 
here using the neuro style transfer
technique that is quite popular.

18
00:01:25.950 --> 00:01:30.400 
But which hardware is used to
process these neural networks?

19
00:01:33.490 --> 00:01:37.490 
While the energy and resource intense
process of training a neural network

20
00:01:37.870 --> 00:01:42.370 
is a perfect task for server-side
processing in cloud-based data centers,

21
00:01:42.850 --> 00:01:47.700 
the inference meaning the user application
of an already trained neural network

22
00:01:47.860 --> 00:01:50.600 
can nowadays be
performed on device.

23
00:01:52.850 --> 00:01:56.760 
This can reduce operating costs and
the amount of energy required.

24
00:01:58.400 --> 00:02:02.480 
Recent developments and mobile
device technology enable the

25
00:02:02.480 --> 00:02:06.410 
processing of neural networks using
specialized hardware, so called

26
00:02:06.540 --> 00:02:07.960 
Neural Processing Units.

27
00:02:08.740 --> 00:02:12.720 
These enable the application of
neural networks directly on device

28
00:02:12.890 --> 00:02:15.250 
without requiring
server-based processing.

29
00:02:17.130 --> 00:02:20.690 
However, mobile hardware
capabilities are limited,

30
00:02:21.460 --> 00:02:24.630 
and require optimised neural
networks in order to operate in

31
00:02:24.630 --> 00:02:25.760 
a power efficient way.

32
00:02:26.460 --> 00:02:28.440 
So how can this
be achieved?

33
00:02:29.170 --> 00:02:34.560 
I'd like to give you a brief
overview of an optimization

34
00:02:34.560 --> 00:02:36.060 
process for
neural networks.

35
00:02:37.280 --> 00:02:43.300 
After first network architecture analysis,
pre-training optimization can be performed

36
00:02:43.530 --> 00:02:46.550 
to reduce the
amount of learned parameters.

37
00:02:47.420 --> 00:02:53.210 
This includes tasks like adapting an architecture
towards mobile friendly operations

38
00:02:53.550 --> 00:02:59.390 
such as, using separate convolutions or
testing specific network operations

39
00:02:59.640 --> 00:03:04.030 
can be executed in hardware for
instance using Apple's neural engine.

40
00:03:05.140 --> 00:03:07.010 
After the network
training,

41
00:03:09.250 --> 00:03:13.370 
post training optimization techniques
such as pruning or quantization,

42
00:03:13.590 --> 00:03:17.490 
or even more sophisticated approaches
like knowledge distillation

43
00:03:17.810 --> 00:03:23.450 
can be applied to compress the network size
before the actual deployment and integration

44
00:03:23.660 --> 00:03:25.030 
and to the target
platform.

45
00:03:28.740 --> 00:03:33.250 
Let me illustrate a post training
optimization that reduces the network size

46
00:03:33.580 --> 00:03:36.510 
to fit the memory constraints
of current mobile devices.

47
00:03:37.270 --> 00:03:41.050 
In this example the floating point
position of the weight parameters

48
00:03:41.050 --> 00:03:45.210 
in a depth inference network is
reduced by quantizing their values,

49
00:03:46.010 --> 00:03:49.380 
and the top row you can see the
impact of the quantization

50
00:03:49.630 --> 00:03:54.520 
to the resulting output quality,
compared to the reduced network size

51
00:03:54.700 --> 00:03:55.960 
shown on the
bottom table.

52
00:03:59.310 --> 00:04:03.890 
You can see that the network with
16 or even 8 bit precision

53
00:04:04.090 --> 00:04:08.410 
delivers sufficient quality by resulting
in a much smaller memory footprint

54
00:04:08.610 --> 00:04:14.320 
of the network. Thus using quantization can
easily facilitate the network inference

55
00:04:14.550 --> 00:04:15.720 
on mobile devices.

56
00:04:19.120 --> 00:04:22.360 
Now I'd like to complement this
presentation with applications

57
00:04:22.360 --> 00:04:26.110 
we developed together with our project
partner digital masterpieces.

58
00:04:27.040 --> 00:04:31.150 
This android app automatically
transfers different artistic styles

59
00:04:31.380 --> 00:04:34.370 
to photos using the newest
dye transfer technique

60
00:04:35.100 --> 00:04:39.090 
Besides different levels of control,
this mobile app also enables

61
00:04:39.090 --> 00:04:42.450 
the combination with classical
stylization techniques such

62
00:04:42.450 --> 00:04:43.720 
as water color
rendering.

63
00:04:47.540 --> 00:04:52.640 
This ios app shows how different artistic
styles can be interactively combined

64
00:04:52.810 --> 00:04:56.260 
in a single image using the
style transfer orchestration.

65
00:04:57.120 --> 00:05:02.060 
This real time orchestration can
easily be controlled using brushing

66
00:05:02.380 --> 00:05:04.830 
and produce various
output results.

67
00:05:10.000 --> 00:05:19.130 
This example shows how
fast optic flow computation

68
00:05:19.420 --> 00:05:23.950 
obtained by a neural network is used
to depict the dynamics or movement

69
00:05:24.310 --> 00:05:28.110 
in a live photo or short video
within a single output image.

70
00:05:29.710 --> 00:05:33.940 
Here the optic flow is used to
extract and visualize motion

71
00:05:34.150 --> 00:05:38.690 
using different techniques such as
motion lines, halos, or ghosting.

72
00:05:43.030 --> 00:05:48.370 
And finally, this recent app uses a variety
of neural networks for different tasks,

73
00:05:48.810 --> 00:05:54.030 
for example, the color and contrast of
input images automatically adjusted,

74
00:05:54.710 --> 00:05:58.100 
and the drawing styles can be
orchestrated with the newest

75
00:05:58.100 --> 00:05:59.960 
dye transfer techniques
interactively.

76
00:06:02.180 --> 00:06:07.260 
All these examples use neural networks
that are inferred on device.

77
00:06:10.070 --> 00:06:13.640 
So after these examples, I would like
to conclude this presentation.

78
00:06:14.570 --> 00:06:18.360 
I've shown that enabling on-device
inference has the potential

79
00:06:18.360 --> 00:06:23.750 
to reduce the need for energy consuming cloud
computing when applying neural networks.

80
00:06:24.510 --> 00:06:28.780 
I further sketched an optimization
process that facilitates the usage

81
00:06:29.020 --> 00:06:33.570 
of new networks on mobile devices
with limited hardware capabilities.

82
00:06:35.340 --> 00:06:38.750 
With future development on mobile
hardware and software, this

83
00:06:38.750 --> 00:06:42.250 
can increase the potential for future
machine learning applications.

84
00:06:43.950 --> 00:06:49.060 
Finally, I thank my colleagues in our visual
media analysis and application group

85
00:06:49.290 --> 00:06:51.650 
and digital masterpieces
for their support.

86
00:06:54.460 --> 00:06:55.940 
Thank you very much
for your attention.
