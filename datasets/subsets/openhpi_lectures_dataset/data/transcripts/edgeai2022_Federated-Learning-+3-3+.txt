WEBVTT

1
00:00:00.740 --> 00:00:05.660 
Hello and welcome. This video is the last part for Federated Learning.

2
00:00:06.339 --> 00:00:10.939 
We will discuss its categories

3
00:00:10.939 --> 00:00:14.050 
according to the distribution of all parties.

4
00:00:14.060 --> 00:00:15.199 
Data sources.

5
00:00:15.490 --> 00:00:19.250 
Federated Learning can be divided into three categories.

6
00:00:20.140 --> 00:00:27.960 
1st is horizontal Federated learning where the participants have different examples but share similar features.

7
00:00:29.239 --> 00:00:34.689 
Second, vertical Federated Learning while similar examples with various features.

8
00:00:34.700 --> 00:00:41.810 
The last one is Federated Transfer Learning with very little overlapping of examples and features.

9
00:00:41.820 --> 00:00:49.539 
Now let's try to explain it with some examples

10
00:00:49.539 --> 00:00:51.799 
in traditional machine learning modeling.

11
00:00:52.539 --> 00:01:01.049 
The data required for model training is collected in the data center and then train it at the data center.

12
00:01:02.539 --> 00:01:08.260 
The essence of our recent a Federated Learning is the union of samples.

13
00:01:08.739 --> 00:01:13.930 
It is suitable for scenarios where participants have the same business form

14
00:01:13.939 --> 00:01:16.810 
still it reaches different customers,

15
00:01:16.819 --> 00:01:23.650 
that is scenarios where there is a more overlap of features but less overlap of users.

16
00:01:24.239 --> 00:01:35.099 
For example, hospitals in other regions have similar business representing similar functional features but different customers

17
00:01:35.109 --> 00:01:37.159 
denoting different samples.

18
00:01:38.019 --> 00:01:40.250 
The training process is as follows.

19
00:01:41.040 --> 00:01:46.560 
First each participant downloads the latest model from the central server.

20
00:01:47.040 --> 00:01:50.319 
Then the server is traded with local data,

21
00:01:50.329 --> 00:01:59.659 
the encrypted gradient is upload to the central server and the server aggregators gradients to update the model parameters.

22
00:02:00.340 --> 00:02:04.950 
After that the server returns the latest model to each participant.

23
00:02:04.959 --> 00:02:08.159 
And over there it updates the local model.

24
00:02:09.340 --> 00:02:17.909 
Each participant had the same complete model in the whole process and there's no communication and no dependence between

25
00:02:17.909 --> 00:02:18.759 
the parties.

26
00:02:19.340 --> 00:02:23.169 
Each party can also make independent predictions

27
00:02:23.180 --> 00:02:25.289 
after the training is finished.

28
00:02:26.639 --> 00:02:35.819 
In our previous example, google initially used original Federation to solve the problem of local model updates

29
00:02:35.830 --> 00:02:37.240 
for their android phone

30
00:02:37.240 --> 00:02:42.939 
users.

31
00:02:42.939 --> 00:02:49.360 
The essence of vertical Federated learning combines the features from different parties.

32
00:02:49.939 --> 00:02:57.360 
It is suitable for scenarios with a lot of overlap between users, but less overlap of features.

33
00:02:57.840 --> 00:03:08.759 
For example, drugstores or hospitals in the same region and the users, they are the rich are a citizens in the same area.

34
00:03:09.639 --> 00:03:17.250 
Thus the sample is the same, but the business is different, which means that the future are different.

35
00:03:18.340 --> 00:03:21.300 
So the train processes is as follows.

36
00:03:21.740 --> 00:03:27.560 
The first step is alignment of encrypted samples from each participant.

37
00:03:28.039 --> 00:03:39.560 
This is done at the system level and so non intersecting users will not be exposed at the enterprise precision level.

38
00:03:41.840 --> 00:03:46.460 
The second step is to align the samples for the encrypted model training.

39
00:03:46.840 --> 00:03:50.460 
So this process will need a collaborator C,

40
00:03:50.469 --> 00:03:53.650 
to handle the home graphic encryption.

41
00:03:56.439 --> 00:03:58.430 
First the third party C

42
00:03:58.439 --> 00:04:05.050 
will send the public key to the party A and B to encrypt the data to be transmitted.

43
00:04:05.740 --> 00:04:13.560 
Second, A and B respectively, calculate the intermediate features and perform encrypted interaction.

44
00:04:14.139 --> 00:04:19.860 
In short, they can somehow combine encrypted features from both parties.

45
00:04:20.439 --> 00:04:22.259 
Then A and B

46
00:04:22.269 --> 00:04:32.110 
will calculate their encrypted gradients and add a mask, then send them to C. Adding a mask is to prevent the leakage

47
00:04:32.110 --> 00:04:32.860 
from C.

48
00:04:33.339 --> 00:04:39.459 
Finally C decrypts the summarize gradient and sends it back to A and B.

49
00:04:40.040 --> 00:04:40.720 
A and B

50
00:04:40.720 --> 00:04:45.649 
will remove the corresponding mask and update the model locally.

51
00:04:47.139 --> 00:04:56.420 
An essential feature of vertical Federated learning in that during the whole process the participant does not know the data

52
00:04:56.430 --> 00:05:05.399 
and feature of the other party. After the training is complete and the participant obtained the model parameters of its own

53
00:05:05.399 --> 00:05:08.649 
side and semi model.

54
00:05:11.139 --> 00:05:20.009 
Since each participant can only get the model parameters related to itself, both parties need to collaborate to complete

55
00:05:20.009 --> 00:05:20.949 
the prediction.

56
00:05:21.839 --> 00:05:24.269 
The inference step has follows.

57
00:05:25.040 --> 00:05:27.139 
First Coordinator C

58
00:05:27.149 --> 00:05:32.959 
will create an encryption key pair and send the public key to party A and B.

59
00:05:33.639 --> 00:05:34.339 
Then A

60
00:05:34.339 --> 00:05:38.449 
And B will compute the intermedia output you UA and UB

61
00:05:38.449 --> 00:05:39.029 


62
00:05:39.040 --> 00:05:40.149 
respectively.

63
00:05:40.839 --> 00:05:48.149 
Then they will encrypt the feature vectors using the public key. In step three

64
00:05:48.540 --> 00:05:50.389 
the encrypted version of UA

65
00:05:50.389 --> 00:05:50.600 


66
00:05:50.600 --> 00:05:53.110 
And UB will be sent to C,

67
00:05:53.120 --> 00:05:54.259 
And at C,

68
00:05:54.740 --> 00:06:04.189 
the partial result from both parties will be integrated. Subsequently, C creates the final output,

69
00:06:04.259 --> 00:06:05.069 
Y

70
00:06:05.110 --> 00:06:15.560 
which is the inference result. At present, machine learning models such as logistic regression or decision trees are all built

71
00:06:15.569 --> 00:06:19.759 
under the framework of the vertical Federated learning system.

72
00:06:20.240 --> 00:06:21.040 
In summary,

73
00:06:21.189 --> 00:06:25.550 
such kind of collaborative modeling has the following advantages.

74
00:06:26.139 --> 00:06:35.259 
Both parties can obtain data protection and jointly optimize the future interaction without losing the model performance.

75
00:06:39.439 --> 00:06:48.470 
So the purpose of Federated transfer learning is to use transfer learning to overcome the problem of insufficient data or

76
00:06:48.470 --> 00:06:53.810 
leveling under the premise of protecting privacy.

77
00:06:53.819 --> 00:07:03.529 
So Federated transfer learning can be considered when only a small portion of feature space from both parties overlaps, such

78
00:07:03.529 --> 00:07:07.500 
as the combination of hospitals and drug stores

79
00:07:07.509 --> 00:07:14.870 
in different regions. It is mainly suitable for applications using deep learning methods.

80
00:07:17.939 --> 00:07:25.959 
The definition of transfer learning is that the trended model in the source domain is applied to a learning process in the

81
00:07:25.959 --> 00:07:27.449 
large targeted domain

82
00:07:27.459 --> 00:07:32.060 
based on the similarity of data task or architectures.

83
00:07:32.639 --> 00:07:37.949 
In fact, humans are born with the ability to transfer learning.

84
00:07:38.439 --> 00:07:48.379 
For example, if we already know how to play table tennis, we can learn to play tennis by analogy because this activity already

85
00:07:48.379 --> 00:07:50.209 
have high similarities.

86
00:07:51.240 --> 00:07:57.569 
The core of transfer learning is to find the similarity between the source domain and the target domain.

87
00:07:58.240 --> 00:08:06.949 
The idea of applying transfer learning to Federated Learning is that a common representation between two feature space is

88
00:08:06.949 --> 00:08:16.560 
learned using the limited common sample sets and later applied to obtain predictions for examples with only one side features.

89
00:08:18.339 --> 00:08:26.990 
Therefore, Federated transfer learning is an important extension to the existing Federated learning system, because it deals

90
00:08:26.990 --> 00:08:36.929 
with the problem exceeding the scope of existing Federated Learning algorithms. Basically, the steps of the Federated

91
00:08:36.929 --> 00:08:45.809 
transfer learning are similar to that of vertical Federated learning, but more specifically, transfer learning typically

92
00:08:45.809 --> 00:08:51.860 
involves in learning a common representation between the features of party A and B.

93
00:08:52.139 --> 00:08:57.409 
And minimizing the errors in predicting the labels for the target domain

94
00:08:57.419 --> 00:09:01.860 
party by leveraging the labels in the source domain party.

95
00:09:03.240 --> 00:09:12.149 
Therefore the gradient computation for party A and party B are different from that in a vertical Federated learning scenario.

96
00:09:12.940 --> 00:09:18.850 
At the inference time it still requires both parties to compute the prediction results.

97
00:09:19.539 --> 00:09:26.659 
Federated Transfer learning is an active research field and has many overlaps with transfer learning.

98
00:09:26.669 --> 00:09:33.820 
Therefore, for more details, please refer to the relevant papers enlisted here regarding the transfer learning.

99
00:09:33.830 --> 00:09:40.250 
We will also have another videos to introduce its application in edgeAI scenarios.

100
00:09:45.139 --> 00:09:51.659 
Next, I would like to briefly introduce Sedna, an open source framework designed for edge AI.

101
00:09:52.320 --> 00:10:01.039 
Sedna is an edge cloud synergy AI project incubated in the KubeEdge AI group benefiting from the edge

102
00:10:01.039 --> 00:10:04.850 
Cloud synergy capabilities provided by KubeEdge.

103
00:10:05.429 --> 00:10:08.330 
Sedna can implement across edge

104
00:10:08.330 --> 00:10:12.750 
cloud collaborative training and inference capabilities.

105
00:10:13.440 --> 00:10:17.529 
We will use it in our practical session.Sedna and KubeEdge

106
00:10:17.529 --> 00:10:28.389 
have very active community and continuously introduce new features and use cases. Currently, Sedna supports the mainstream

107
00:10:28.389 --> 00:10:38.940 
AI frameworks such as tensorflow, pytorch, MindSpore, it can enable collaborative capability to your existing

108
00:10:38.940 --> 00:10:41.960 
code without modifying the code structure.

109
00:10:44.899 --> 00:10:50.059 
This figure shows the relationship between Sedna and other related components.

110
00:10:50.840 --> 00:11:00.710 
First, Kubernetes is an open source container system for automating computation, computer application

111
00:11:00.710 --> 00:11:03.759 
deployment, scaling and management.

112
00:11:04.539 --> 00:11:15.850 
Many cloud services of offer a Kubernetes based platform or infrastructure as a service. KubeEdge is for extending

113
00:11:15.850 --> 00:11:26.509 
native containerized application of situation capability to host at edge. It is built upon the Kubernetes and provides

114
00:11:26.509 --> 00:11:36.460 
fundamental infrastructure, support for networks and applications, deployment and meta data synchronization between cloud

115
00:11:36.460 --> 00:11:39.809 
and edge. Sedna as already mentioned

116
00:11:39.809 --> 00:11:48.759 
is on top of KubeEdge utilizing the edge, clouds energy capability and focusing on AI applications.

117
00:11:48.769 --> 00:11:59.559 
It provided extended interface for developers to quickly integrate their code and present several interesting use cases. We

118
00:11:59.559 --> 00:12:09.460 
can see that some applications have been shown at the top level of the figure in this course we will experience Federated

119
00:12:09.460 --> 00:12:15.240 
Learning and joint inference examples.

120
00:12:15.240 --> 00:12:17.830 
Sedna consists of the following components.

121
00:12:17.840 --> 00:12:21.899 
Let me give you a brief introduction on the cloud side.

122
00:12:22.019 --> 00:12:31.159 
The global Manager is responsible for unified task management and central configuration management for datasets and models.

123
00:12:32.039 --> 00:12:35.970 
The local controller can either be on the cloud or at edge.

124
00:12:35.970 --> 00:12:45.710 
Note it connects to the global Manager performs local process control and is responsible for local management of datasets

125
00:12:45.720 --> 00:12:49.549 
and models and for status synchronization.

126
00:12:50.340 --> 00:12:56.379 
The worker will execute actual machine learning jobs like training and Inference.

127
00:12:56.629 --> 00:12:58.950 
It will be launched on demand.

128
00:12:59.440 --> 00:13:05.860 
They are actually Dockers the worker could run on both cloud and edge.

129
00:13:06.340 --> 00:13:11.860 
Finally, lib will expose the edgeAI features to applications.

130
00:13:15.440 --> 00:13:19.789 
Sedna also supports a more flexible and asynchronous

131
00:13:19.799 --> 00:13:27.659 
cloud client collaboration method, for example, we will use this approach in the Federation learning assignment.

132
00:13:28.139 --> 00:13:31.710 
We will deploy General manager and worker on the clouds

133
00:13:31.710 --> 00:13:40.559 
note for aggregation. And the Sedna worker will use as node coupled to the local controller on the edge side.

134
00:13:41.440 --> 00:13:51.159 
The advantage is that, we don't need to install the cube age environment which allow us to use Sedna and cloud nodes for collaborative

135
00:13:51.159 --> 00:13:54.360 
training on the clients such as cargo notebooks.

136
00:13:54.940 --> 00:13:59.879 
And can asynchronously update the ways submitted by each edge node.

137
00:13:59.879 --> 00:14:10.679 
The disadvantages is that the current age note cannot benefit from management function for datasets and models brought

138
00:14:10.690 --> 00:14:17.649 
by the local controller and the functions such as secure state communication based on Kubedge.

139
00:14:18.139 --> 00:14:28.220 
Therefore, if you are developing professional projects on the cloud, I recommend using the complete KubeEdge based Sedna

140
00:14:28.269 --> 00:14:31.250 
models for the Federated Learning task.

141
00:14:35.029 --> 00:14:44.340 
In this week's practical session we will use Federated Learning to train a model together. So it may include every participant

142
00:14:44.340 --> 00:14:53.840 
in this course, which is amazing. We will distribute the prepared dataset to all the participants and use Sedna framework

143
00:14:53.840 --> 00:15:02.860 
for the training. We use a cargo notebook at the client and the central server will be set up for the model aggregation.

144
00:15:04.139 --> 00:15:11.850 
The coding time is about 3-6 hours and the period of Federated learning is about a few days.

145
00:15:12.340 --> 00:15:15.980 
I hope you will have fun and have great success.

146
00:15:19.039 --> 00:15:20.450 
Thank you for watching the video.
