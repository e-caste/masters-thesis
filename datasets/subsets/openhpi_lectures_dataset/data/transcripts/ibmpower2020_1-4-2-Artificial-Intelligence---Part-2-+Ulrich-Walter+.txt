WEBVTT

1
00:00:01.460 --> 00:00:04.970 
Hello. I'm Ulrich Walter again. Thanks
for having you with me today

2
00:00:04.970 --> 00:00:08.950 
and I am very happy to give you
the second part of our speech

3
00:00:08.950 --> 00:00:12.520 
about power 9 and beyond
for artificial intelligence.

4
00:00:13.030 --> 00:00:16.440 
This is a follow on our part we
have discussed already about

5
00:00:16.440 --> 00:00:21.390 
the economy and possibilities of
artificial intelligence and

6
00:00:21.400 --> 00:00:25.950 
the systems we are going to see
the future in our indust ries.

7
00:00:26.460 --> 00:00:29.980 
In that part we will
discuss more in detail about

8
00:00:29.980 --> 00:00:33.420 
the technology but from the hardware
perspective but also from

9
00:00:33.420 --> 00:00:37.790 
the data line perspective and last
not least also of the orchestration

10
00:00:37.790 --> 00:00:41.440 
and the software stack, what is
necessary to build an AI system

11
00:00:41.500 --> 00:00:42.850 
and the efficiency
of that.

12
00:00:44.430 --> 00:00:48.740 
So when we talk about AI in general,
we have three very important

13
00:00:48.740 --> 00:00:53.200 
part to consider. Which is one,
scale; how does a system scale?

14
00:00:53.750 --> 00:00:57.540 
The timeline to actually get
something finished and last not

15
00:00:57.540 --> 00:01:00.780 
least and this is I think most
important, is accuracy, what

16
00:01:00.780 --> 00:01:04.360 
you want to achieve with your
AI system in general, because

17
00:01:04.360 --> 00:01:07.640 
an AI system in comparison
to traditional data systems

18
00:01:07.890 --> 00:01:13.000 
has never ever one hundred percent
accuracy because everything is built on

19
00:01:13.360 --> 00:01:19.180 
dependencies, on evaluation
of the data in combination of

20
00:01:19.180 --> 00:01:22.410 
correlations of, let's say,
mathematical functions in here

21
00:01:22.410 --> 00:01:26.640 
and weights and states within the
data and the algorithms. Therefore

22
00:01:26.740 --> 00:01:32.180 
accuracy is actually a very much
important item for getting

23
00:01:32.180 --> 00:01:37.490 
things done to a very high level. Especially
in automotive driving, autonomous driving

24
00:01:37.700 --> 00:01:41.310 
you probably won't have a car
with a curiously ratio

25
00:01:41.500 --> 00:01:45.600 
of less than ninety per cent,
otherwise or even higher rate,

26
00:01:45.820 --> 00:01:51.730 
otherwise of course you actually are heading
to accident or whatever else. Therefore

27
00:01:51.850 --> 00:01:56.490 
accuracy is a very very high important
part of our endeavour here.

28
00:01:57.370 --> 00:02:01.540 
And how is it actually established?
So the scale of our system

29
00:02:01.540 --> 00:02:06.230 
is driven by data and storage. So data
is one of the most important things

30
00:02:06.360 --> 00:02:10.350 
what we have here to consider
in our AI system. Without data

31
00:02:10.350 --> 00:02:14.730 
we can't do anything and the more data
we have the better the system might be.

32
00:02:16.310 --> 00:02:21.220 
But the timeline also is important. So
the timeline we are talking about here

33
00:02:21.380 --> 00:02:25.240 
is a compute power to achieve a
certain job and the bandwidth

34
00:02:25.240 --> 00:02:28.820 
for transporting the data to the
computer itself. So this

35
00:02:28.820 --> 00:02:32.910 
can take a long long time on
traditional systems but here of

36
00:02:32.910 --> 00:02:36.850 
course we actually have a new
system to come, new technologies

37
00:02:37.080 --> 00:02:41.770 
that save a lot of time especially
for long lasting training runs.

38
00:02:42.170 --> 00:02:46.010 
And last not least accuracy as
I said before it's very much

39
00:02:46.010 --> 00:02:49.690 
depending on the quality of your
data and the model of your

40
00:02:49.690 --> 00:02:53.410 
implementation you are going to use.
These are the three important

41
00:02:53.410 --> 00:02:57.720 
factors we have always to consider
when we talk about an AI system

42
00:02:57.760 --> 00:03:00.050 
and the dependencies and
the benefits of it.

43
00:03:01.360 --> 00:03:05.490 
So what do we need for building
up an ecosystem? As I said before,

44
00:03:05.730 --> 00:03:09.630 
we need a large set of data and
of course the more data we have

45
00:03:09.810 --> 00:03:14.790 
the better it is but the quality of the data
needs to match also our requirements.

46
00:03:15.290 --> 00:03:18.890 
Then we need to have a neural
network. There are a couple of them.

47
00:03:19.160 --> 00:03:22.680 
I don't go in that session in
detail in the neural networks,

48
00:03:22.680 --> 00:03:26.400 
it will definitely become a separate
session here, but therefore

49
00:03:26.400 --> 00:03:29.320 
of course it's very important
to understand the concept of

50
00:03:29.320 --> 00:03:33.400 
neural networks, how they act, how
they can be implemented and

51
00:03:33.400 --> 00:03:37.080 
of course they are the foundation
of all the inference models

52
00:03:37.100 --> 00:03:41.490 
what we see in the artificial
intelligence in productions.

53
00:03:41.710 --> 00:03:47.250 
And last but not least we need a system
that is very much designed

54
00:03:47.260 --> 00:03:52.860 
for the usage of such models and
also compute power to get the data

55
00:03:52.990 --> 00:03:57.940 
transported to an inference
model quite shortly.

56
00:04:00.590 --> 00:04:03.630 
So how do humans learn, how do
systems learn? It's actually

57
00:04:03.630 --> 00:04:08.140 
very similar. Humans learn by
teachers and teachers learn

58
00:04:08.140 --> 00:04:12.040 
by books and of course at the end
of the day a student or pupil

59
00:04:12.040 --> 00:04:16.050 
can write his exam can write his
task by the learning he has

60
00:04:16.050 --> 00:04:19.910 
done. The more he learned of course the
better maybe his results will be.

61
00:04:20.260 --> 00:04:24.000 
And this is very true also for
a system and as I said before

62
00:04:24.300 --> 00:04:27.920 
we will have in the system we have
our data which is our library,

63
00:04:28.400 --> 00:04:33.250 
to be honest here and we have our
neural network for recognition,

64
00:04:33.250 --> 00:04:37.010 
learning and education and last
not least we have our inference

65
00:04:37.010 --> 00:04:40.560 
model who actually will action
on that inference model what

66
00:04:40.560 --> 00:04:44.170 
he has learned off. So these are
the three important parts what

67
00:04:44.180 --> 00:04:48.190 
we have in our data pipeline down
to the inference model. But

68
00:04:48.190 --> 00:04:51.940 
this is not just only that. We need
a lot of technology actually

69
00:04:51.940 --> 00:04:55.670 
to comply on that. And the most
important part as I said before

70
00:04:55.950 --> 00:05:01.380 
is data. Data is one of the most
critical parts of our road map here.

71
00:05:02.020 --> 00:05:06.410 
Imagine that in two thousand and
nineteen, on a single day, the

72
00:05:06.410 --> 00:05:10.200 
whole mankind creates more data
than the whole mankind in the

73
00:05:10.200 --> 00:05:15.060 
last five thousand years before until
the year two thousand and three.

74
00:05:15.430 --> 00:05:18.920 
This is just happening because
of all of our mobile phones,

75
00:05:18.920 --> 00:05:23.660 
of all the sensors, of all the
cameras we have deployed. We expect

76
00:05:23.670 --> 00:05:26.870 
to see the year two thousand and
twenty about fifty billion

77
00:05:27.050 --> 00:05:31.010 
of sensors and devices which are
attached to the internet and

78
00:05:31.020 --> 00:05:35.310 
transporting data. Every one of
us is moving data around from

79
00:05:35.310 --> 00:05:39.670 
the tv at home, coffee makers,
sensors at homes, whatever it takes

80
00:05:39.910 --> 00:05:44.160 
autonomous systems, trash bins,
which are with intelligence.

81
00:05:44.330 --> 00:05:48.370 
So there's a lot of data flowing
around everywhere you go, on

82
00:05:48.370 --> 00:05:52.210 
acres, on fields actually. This is
definitely something we will

83
00:05:52.210 --> 00:05:55.830 
see in the future much more to
come and this is also the source

84
00:05:55.830 --> 00:06:00.240 
then for building up AI systems. So
this is a very important thing.

85
00:06:00.490 --> 00:06:04.820 
On the other hand, that data needs to be
stored and the storage for that

86
00:06:04.980 --> 00:06:09.500 
is a highly critical thing also
in our data pipeline, how to

87
00:06:09.500 --> 00:06:14.640 
take the data, how to proceed the data
and how to store the data wisely

88
00:06:14.890 --> 00:06:17.460 
in the usage of
the AI system.

89
00:06:18.950 --> 00:06:22.850 
In AI systems you have usually
also two phases which is one

90
00:06:22.850 --> 00:06:26.120 
is a learning phase what we
discussed before, and you have the

91
00:06:26.120 --> 00:06:29.010 
inference phase which is actually
the actual phase when the

92
00:06:29.010 --> 00:06:31.690 
system is taking action
on a certain learning.

93
00:06:32.170 --> 00:06:37.230 
The learning phase of course is
most, is a phase where it

94
00:06:37.230 --> 00:06:41.000 
takes most computing power and
also of course a lot of storage

95
00:06:41.000 --> 00:06:44.780 
and a lot of technology to be
completed. We have in the learning

96
00:06:44.780 --> 00:06:48.550 
phase a collection phase for the
data that we need to clean

97
00:06:48.550 --> 00:06:52.690 
and analyze the data, prepare the
data, analyze it again.

98
00:06:52.960 --> 00:06:56.180 
Then we have to learn from the
data and last not least we have

99
00:06:56.180 --> 00:06:59.960 
to bring it to inference model and
the inference model will feedback

100
00:06:59.960 --> 00:07:03.790 
the data to the system, so we have
a continuous learning here

101
00:07:03.980 --> 00:07:08.540 
in our training cycle. On the
inference part, you have a sensor

102
00:07:08.540 --> 00:07:12.840 
or camera or some detector that
actually understands an image

103
00:07:12.840 --> 00:07:17.620 
or can read something from a
sign. He's doing the perception

104
00:07:17.850 --> 00:07:23.280 
cognition and last, not least also the action
because he has to do something,

105
00:07:23.510 --> 00:07:28.230 
either sending a signal or closing
a door or doing something

106
00:07:28.230 --> 00:07:31.640 
of course but this is on the
inference side and this is actually

107
00:07:31.640 --> 00:07:35.680 
what our system has been trained
for. So we always have to separate

108
00:07:35.680 --> 00:07:39.460 
the learning phase from the inference
phase in here. Both phases

109
00:07:39.460 --> 00:07:43.260 
are very relevant in questions
of compute power but of course

110
00:07:43.270 --> 00:07:46.790 
most important parts for storage
is happening always in the

111
00:07:46.790 --> 00:07:47.640 
learning phase.

112
00:07:49.750 --> 00:07:55.470 
Coming to the point of our learning
phase here in questions of storage,

113
00:07:55.610 --> 00:07:59.560 
in storage we have to understand
there is a need for data,

114
00:07:59.580 --> 00:08:03.280 
yes, that's right, but of course
we always have to understand

115
00:08:03.280 --> 00:08:06.630 
also the volume of data we are
creating. Are we talking about

116
00:08:06.640 --> 00:08:10.980 
gigabyte, terabyte, petabyte or
even exabyte of data that

117
00:08:10.990 --> 00:08:14.780 
needs to be stored? What is the
velocity of that data to be

118
00:08:14.780 --> 00:08:20.260 
flown into such a system? Are we
talking about millions of IOs

119
00:08:20.260 --> 00:08:25.450 
per second or trillions of IOs per second?
This is really a very important question,

120
00:08:25.760 --> 00:08:29.440 
how to actually understand the
performance needs for the data.

121
00:08:30.040 --> 00:08:35.260 
The most important part for training
itself is trust the veracity of the data

122
00:08:35.370 --> 00:08:40.020 
because if I cannot trust my data then my
inference model and my training model

123
00:08:40.280 --> 00:08:43.550 
would probably not actually work
as I was expecting it to be,

124
00:08:43.710 --> 00:08:48.770 
because trust in the ground
truth of the data, these are some

125
00:08:48.770 --> 00:08:52.400 
of the most fundamental requirements
what we have for AI.

126
00:08:53.180 --> 00:08:56.690 
Then we have to understand the
variability of data. Not every

127
00:08:56.810 --> 00:09:00.730 
record actually is actually the
same format We have data coming

128
00:09:00.730 --> 00:09:05.840 
from sensors, from databases, from social
networks, from wherever, from mobile phones.

129
00:09:06.150 --> 00:09:09.970 
That type of variability we need
to understand and then bring

130
00:09:09.970 --> 00:09:13.180 
in context of our learning
model what we want to achieve.

131
00:09:14.040 --> 00:09:18.150 
Then if we actually have created
such a record or such a data

132
00:09:18.160 --> 00:09:22.680 
system where we have ground truth
and we have a proven record here

133
00:09:22.910 --> 00:09:26.660 
then the data has also value for
us ,because this is something

134
00:09:26.660 --> 00:09:30.550 
of course, this is also in many
cases an intellectual property

135
00:09:30.550 --> 00:09:34.320 
of many companies which is not
given away quite easily. So

136
00:09:34.320 --> 00:09:38.380 
you have to pay a lot of money
for some ground proven certified

137
00:09:38.380 --> 00:09:42.150 
data sets to actually for
doing your training rather

138
00:09:42.150 --> 00:09:45.650 
than doing just, let's say, online
images with cats and dogs.

139
00:09:45.830 --> 00:09:49.790 
This is a very important point here
to get the value out of the data.

140
00:09:50.770 --> 00:09:56.410 
Last not least we have to understand how
frequently is a training cycle running

141
00:09:56.540 --> 00:10:00.920 
through, that means are we taking
this only one time, are we taking

142
00:10:00.920 --> 00:10:04.720 
it on a daily basis or even maybe
on an hourly basis? So what

143
00:10:04.720 --> 00:10:08.110 
is actually the performance
requirements on that and that all

144
00:10:08.110 --> 00:10:11.680 
dictates and the special
requirements for storage but also

145
00:10:11.680 --> 00:10:14.420 
for our technology and the
infrastructure appeal.

146
00:10:16.270 --> 00:10:19.900 
Look at that. That's our CORAL
infrastructure in Oak Ridge

147
00:10:19.900 --> 00:10:21.140 
and Lawrence Livermore.

148
00:10:21.990 --> 00:10:27.110 
Last year actually IBM has deployed the
largest and fastest computer here

149
00:10:27.490 --> 00:10:32.430 
on the planet today and this is
really an outstanding machine

150
00:10:32.690 --> 00:10:36.040 
and this is actually designed
especially for the performance

151
00:10:36.040 --> 00:10:40.410 
needs they have in questions of
the data flow but also of course

152
00:10:40.410 --> 00:10:44.050 
with the compute needs
for performance in the CPU

153
00:10:44.050 --> 00:10:46.050 
and the GPUs. I come
to that later.

154
00:10:46.780 --> 00:10:52.210 
An important point this was the very first
time and that's very astonishing was

155
00:10:52.410 --> 00:10:57.730 
that not the compute power alone was
a driver for that infrastructure,

156
00:10:57.900 --> 00:11:01.490 
it was a bandwith because
the researchers and all people

157
00:11:01.490 --> 00:11:06.310 
who are actually driving AI projects
say we need to have a system

158
00:11:06.410 --> 00:11:10.680 
that is capable to transport the
data also at a very very high

159
00:11:10.680 --> 00:11:16.450 
speed. So as I said before data is
the source but it needs actually

160
00:11:16.450 --> 00:11:21.010 
be transported very very fast to
the  CPU or the compute time

161
00:11:21.190 --> 00:11:25.940 
in order to not waste actually
IO cycles here or  CPU cycles

162
00:11:25.940 --> 00:11:30.430 
and waiting for data. So bandwidth is
one of the most critical parts in our

163
00:11:30.620 --> 00:11:36.280 
technology. And just to
give you another example how

164
00:11:37.190 --> 00:11:41.000 
and this is I think a very important
one, if you want to create

165
00:11:41.010 --> 00:11:45.480 
accuracy in an autonomous car
driving about twenty percent

166
00:11:45.480 --> 00:11:49.550 
higher accuracy like an average
human, it would take a fleet

167
00:11:49.550 --> 00:11:54.450 
of one hundred cars, seven twenty
four hours a day and the speed

168
00:11:54.450 --> 00:11:57.720 
of fifty kilometres per hour
would require three hundred and

169
00:11:57.720 --> 00:12:01.930 
eighty years eight years to come
or to complete a data set

170
00:12:02.120 --> 00:12:06.230 
that's necessary to train such a
system today. That means there's

171
00:12:06.230 --> 00:12:09.970 
a hell lot of things to be done
here and considered to

172
00:12:09.970 --> 00:12:13.410 
get the right data in
time and in numbers.

173
00:12:14.710 --> 00:12:18.810 
So the big data we talked about,
we have also the requirements

174
00:12:18.810 --> 00:12:23.020 
of the neural networks, CPUs, but
now of course a new thing comes

175
00:12:23.020 --> 00:12:27.010 
to the block. It's called the
accelerators and accelerators have

176
00:12:27.010 --> 00:12:30.580 
a special function, because they
are the drivers for getting

177
00:12:30.580 --> 00:12:34.650 
these neural networks performing
really really fast and this

178
00:12:34.650 --> 00:12:39.600 
is something of course we could not see in
the past because actually technology needs

179
00:12:39.780 --> 00:12:43.760 
did not allow that before. One
of the reasons is so called

180
00:12:43.760 --> 00:12:47.330 
von Neumann bottleneck and if
every one of you who knows about

181
00:12:47.330 --> 00:12:51.640 
the design of  CPU knows that we
actually have the problem always

182
00:12:51.760 --> 00:12:56.570 
between the  CPU and the memory
and the RAM, how to access that

183
00:12:56.570 --> 00:13:01.090 
memory quite fast and also
the number of, let's say,

184
00:13:01.100 --> 00:13:06.160 
the ALUs in our CPU and
the components

185
00:13:06.160 --> 00:13:08.930 
we have in here to
have parallelization, or

186
00:13:09.670 --> 00:13:14.630 
for example matrix
multiplication and things like that.

187
00:13:14.810 --> 00:13:18.430 
This is not something a
traditional CPU was designed for.

188
00:13:18.750 --> 00:13:22.060 
Therefore new kids are now coming
on the block and some of

189
00:13:22.060 --> 00:13:28.700 
them like GPUs or IPUs
especially designed is exactly for

190
00:13:28.700 --> 00:13:33.520 
running such kind of the technologies
and such kind of algorithms

191
00:13:33.670 --> 00:13:37.820 
because they have a much much
higher ratio of course and are

192
00:13:37.820 --> 00:13:41.770 
really driven to the right to the
range where these operations

193
00:13:41.770 --> 00:13:45.240 
can be done much more efficiently.
On the other side we see

194
00:13:45.240 --> 00:13:48.960 
a lot of new technologies rising
up like quantum computing.

195
00:13:48.970 --> 00:13:53.030 
IBM has also a lot of knowledge
in quantum computing here

196
00:13:53.130 --> 00:13:58.250 
which can be also utilized for AI
in certain cases. IBM TrueNorth

197
00:13:58.250 --> 00:14:02.250 
vhip which has a very low footprint
energy will also have, let's say,

198
00:14:02.250 --> 00:14:05.720 
an impact here on AI. So there's
a lot of development going on

199
00:14:05.930 --> 00:14:10.420 
around the globe and you can rest
assured that in the future

200
00:14:10.420 --> 00:14:13.640 
this is definitely something
what we definitely see to come

201
00:14:13.640 --> 00:14:17.320 
with new technologies and very
very fast technologies that

202
00:14:17.330 --> 00:14:21.040 
we haven't seen before. Just another
number here at that point

203
00:14:21.040 --> 00:14:25.220 
about six years ago the google
brain consisted of approximately

204
00:14:25.220 --> 00:14:30.340 
sixteen thousand systems running at
fifty Teraflops. A single GPU today

205
00:14:30.450 --> 00:14:33.600 
has about one hundred and twenty
Teraflops, that means you

206
00:14:33.600 --> 00:14:38.570 
have actually the power of three times
google brain, of course six years ago

207
00:14:38.770 --> 00:14:43.710 
in the size of, let's say, a stamp
in your pocket. This is really

208
00:14:43.710 --> 00:14:48.280 
some very astonishing technology
shift and therefore of course

209
00:14:48.440 --> 00:14:52.850 
that means AI is becoming also
commodity and commoditized for

210
00:14:52.850 --> 00:14:57.210 
everyone and can be integrated and
can be utilized quite easily

211
00:14:57.320 --> 00:15:00.290 
rather than running in
large large data files.

212
00:15:01.600 --> 00:15:04.850 
So what are the differences
between the  CPUs and the GPUs?

213
00:15:04.860 --> 00:15:08.340 
So on the left hand side is
the CPU as a set, we have a

214
00:15:08.340 --> 00:15:11.660 
limited number of cores, we are
also limited in threads and

215
00:15:11.660 --> 00:15:15.040 
we are limited in throughput and
calculations per second and this

216
00:15:15.040 --> 00:15:18.050 
is of course the major difference
what we see the GPUs where

217
00:15:18.050 --> 00:15:21.820 
we have thousands of cores and
thousands of threads going in parallel.

218
00:15:21.950 --> 00:15:25.880 
That means everything that can
be parallelized, a GPU might be

219
00:15:25.880 --> 00:15:28.100 
a very well fit
for doing that.

220
00:15:28.790 --> 00:15:30.990 
On the other side we
also have to see

221
00:15:31.740 --> 00:15:35.940 
power 9 technology drives
exactly the bandwidth demand what

222
00:15:35.940 --> 00:15:40.390 
I have seen before in our CORAL
set up here in Oak Ridge but

223
00:15:40.390 --> 00:15:44.490 
also of course we see a lot of other
things between the bandwidth,

224
00:15:44.490 --> 00:15:49.890 
between the  CPU and the GPU.
In the traditional PCI system

225
00:15:50.180 --> 00:15:56.670 
the bandwidth between the  CPU and the GPU
is becoming moreand more bottleneck because

226
00:15:56.880 --> 00:16:01.240 
the data has always to bypass through
that small bandwidth from our

227
00:16:01.530 --> 00:16:07.210 
PCI bus and this is of course
not sufficient to satisfy that

228
00:16:07.320 --> 00:16:11.720 
data hungry GPUs with
sufficient number of data.

229
00:16:12.480 --> 00:16:18.430 
It means our systems here are really
designed especially for this AI era

230
00:16:18.600 --> 00:16:22.520 
and with that system of course
we can bring up quite nicely

231
00:16:22.720 --> 00:16:26.150 
a building block where you can
actually build up on your own,

232
00:16:26.150 --> 00:16:31.400 
lets say, summit cluster if you wish so but
on a very much smaller footprint as well.

233
00:16:32.620 --> 00:16:37.740 
As I said the GPU is one of the
most important components in

234
00:16:37.740 --> 00:16:43.020 
such an AI endeavor. That means in a
traditional system, we have, let's say, our

235
00:16:43.180 --> 00:16:48.170 
PCI bus and the PCI bus is our
bottleneck in our power9

236
00:16:48.170 --> 00:16:52.480 
system. On the other side we have a
technology called NVLink 2

237
00:16:52.580 --> 00:16:57.140 
where we have one hundred and
fifty gigabytes in the free for GPU

238
00:16:57.400 --> 00:17:02.460 
system setup that gives you the flexibility
and performed and the throughput

239
00:17:02.570 --> 00:17:05.990 
to operate much much better. That
gives you a throughput of

240
00:17:06.000 --> 00:17:10.630 
more than five times against the
traditional PCI based machine only.

241
00:17:10.810 --> 00:17:15.110 
Five times as much means also
five times the time saving for

242
00:17:15.110 --> 00:17:18.740 
your training runs, for your actually
development runs for your

243
00:17:18.740 --> 00:17:22.590 
AI systems in here and you can
do much more than that because

244
00:17:22.770 --> 00:17:26.880 
the memory, as you can see here,
becomes also common shared memory

245
00:17:26.880 --> 00:17:32.560 
because of NVLink 2 that
means GPU RAM and CPU RAM can

246
00:17:32.560 --> 00:17:36.810 
actually be in a in one ratio,
in one range and therefore of

247
00:17:36.810 --> 00:17:40.260 
course you have a global area
of memory on that as well.

248
00:17:42.000 --> 00:17:46.970 
On the data pipeline as I said
we have five phases in here and

249
00:17:46.970 --> 00:17:50.630 
the most important phase actually
in that is not just only

250
00:17:50.630 --> 00:17:54.940 
the learning path, it's important,
but the most time consuming

251
00:17:54.940 --> 00:17:58.680 
path is actually the collection
and the preparation of the data.

252
00:17:58.920 --> 00:18:03.380 
For that we have to consider that
takes about eighty percent

253
00:18:03.380 --> 00:18:05.340 
of the development
efforts today.

254
00:18:06.460 --> 00:18:09.840 
With IBM technology and what we
have actually all the software

255
00:18:09.840 --> 00:18:14.490 
parts as well, we have a lot of
things that can help you to

256
00:18:14.500 --> 00:18:20.090 
get that data phases much more
quickly defined, and quickly developed.

257
00:18:20.970 --> 00:18:24.430 
That means actually when we start
from the data creation to

258
00:18:24.430 --> 00:18:28.360 
the data storage to the
transformation and validation path,

259
00:18:28.650 --> 00:18:33.390 
there are a lot of possibilities what
IBM can bring in on technology here

260
00:18:33.550 --> 00:18:38.200 
with IBM AI vision where
we for example have automated

261
00:18:38.200 --> 00:18:42.440 
labeling system that can help you
to label that data automatically,

262
00:18:42.650 --> 00:18:46.360 
get you a better accuracy and therefore
of course a higher throughput

263
00:18:46.490 --> 00:18:50.340 
in your training runs. In those phases
you always have to understand

264
00:18:50.340 --> 00:18:54.420 
also about what are the requirements
in each individual step

265
00:18:54.420 --> 00:18:58.470 
in the phase about. Data validation
for example or the volume

266
00:18:58.470 --> 00:19:02.530 
of the data that needs to be stored.
What is actually my storage

267
00:19:02.540 --> 00:19:07.060 
or my underlying storage? IBM has
a system called spectrum scale

268
00:19:07.110 --> 00:19:10.600 
which can give you actions of
flexibility of a tiered storage

269
00:19:10.600 --> 00:19:16.400 
concept where you can use NVME
but also SAS disks for example

270
00:19:16.400 --> 00:19:20.520 
or SATA disk or even cloud storage
if you wish so and can run

271
00:19:20.520 --> 00:19:25.530 
it as one single instance and one single
view here for your data management.

272
00:19:26.090 --> 00:19:29.740 
Copies and other feature what we
build in our system that helps

273
00:19:29.740 --> 00:19:35.430 
you actually eliminate the
necessity of having data access

274
00:19:35.640 --> 00:19:41.750 
to the  CPU by reducing CPU
cycles from twenty thousand

275
00:19:41.760 --> 00:19:44.960 
four to about three
hundred for a single IO.

276
00:19:45.830 --> 00:19:49.170 
But IBM is also working closely
with a lot of partners in the

277
00:19:49.180 --> 00:19:52.810 
open power world, so as you see
we have open power partners

278
00:19:52.810 --> 00:19:55.470 
here like Mellanox, NVIDIA
but also XILINX

279
00:19:56.000 --> 00:20:01.550 
and others and on the software side
we see a lot of things to rise here

280
00:20:01.710 --> 00:20:06.670 
like Spark, mongoDB, H2O
but also here in Germany

281
00:20:06.670 --> 00:20:10.980 
we have a lot of partners here
building up new workloads based on AI,

282
00:20:11.240 --> 00:20:16.180 
based on training systems here where
you can use also on IBMpower.

283
00:20:16.730 --> 00:20:19.380 
Open frameworks we have
a lot of them today.

284
00:20:20.260 --> 00:20:26.310 
There is still continuing of
course a new rise of frameworks

285
00:20:26.550 --> 00:20:30.930 
like Caffe2, Pytorch and others
to come Tensorflow

286
00:20:31.100 --> 00:20:35.610 
you all know them but of course
you can rest assured

287
00:20:35.620 --> 00:20:39.170 
that those frameworks are always
running performance also on

288
00:20:39.170 --> 00:20:44.470 
the IBM power9 systems. OpenCAPI
also as you see our competitors

289
00:20:44.470 --> 00:20:48.080 
are taking place on here. They
actually want to implement also

290
00:20:48.090 --> 00:20:51.950 
openCAPI in their own system. That
means this is an open world

291
00:20:51.990 --> 00:20:56.480 
and IBM actually opens such
technology possibilities on here.

292
00:20:57.800 --> 00:21:01.630 
On the software stack side, we
actually have, as I said before,

293
00:21:02.420 --> 00:21:07.000 
a whole bundle of software that
can help to drive especially

294
00:21:07.000 --> 00:21:10.810 
the demand here for building up such
a system in a more robust way.

295
00:21:11.050 --> 00:21:14.260 
You can do it in two flavors. You
can either build up your own

296
00:21:14.260 --> 00:21:18.490 
stack, build your own, bring your
own with open source libraries,

297
00:21:18.490 --> 00:21:23.090 
with open source components, that's
fine, but in many cases actually

298
00:21:23.090 --> 00:21:27.050 
customers have the need to have, let's
say, a more robust development

299
00:21:27.050 --> 00:21:30.380 
environment and a trusted environment
where they can build on

300
00:21:30.610 --> 00:21:35.160 
their experience. And IBM
Watson ML Accelerator and the

301
00:21:35.200 --> 00:21:38.980 
spectrum conductor
integrated is actually one

302
00:21:38.980 --> 00:21:43.960 
of those tools where you have, let's say,
a building block in your life cycle

303
00:21:44.180 --> 00:21:49.210 
pipeline to build up an AI system.
In combination with Watson studio

304
00:21:49.370 --> 00:21:54.650 
you actually can really reach out to
your end users, to your developers,

305
00:21:54.850 --> 00:21:58.730 
that means you have the whole
process under control and not

306
00:21:58.730 --> 00:22:01.500 
the hassle and doing all
your things on your own.

307
00:22:03.330 --> 00:22:07.090 
At the, to finalize the presentation
here I will just give you some

308
00:22:07.090 --> 00:22:09.440 
examples of what is
possible to do today

309
00:22:10.120 --> 00:22:16.160 
with tools like AI Vision or
AI Watson ML Accelerator here

310
00:22:16.370 --> 00:22:20.440 
and the first image we see, for
example, some very simple thing

311
00:22:20.440 --> 00:22:23.720 
but it's called mask detection
here. For example if you have

312
00:22:23.720 --> 00:22:27.340 
a bank robber or some sitting there
in the bank, so it could actually

313
00:22:27.340 --> 00:22:31.090 
see there's a guy coming in with
a mask on its face and maybe

314
00:22:31.090 --> 00:22:35.710 
then automatic alert would start
automatically, or somebody would actually get

315
00:22:35.970 --> 00:22:39.480 
notice about that. This is of
course something which can be

316
00:22:39.480 --> 00:22:43.690 
done quite quickly and easily and
this can be done also implemented

317
00:22:43.700 --> 00:22:47.490 
on cameras or on sensors, however
you would like to do it.

318
00:22:48.770 --> 00:22:52.000 
On the other hand we have also
possibilities for drone surveillance.

319
00:22:52.340 --> 00:22:56.080 
Not just only for parking lots
or people movement in crowds,

320
00:22:56.090 --> 00:23:00.420 
but imagine everything where a
drone can be of help here in

321
00:23:00.420 --> 00:23:04.320 
controlling, let's say, trash on
the beaches of the oceans or

322
00:23:04.330 --> 00:23:08.310 
for example what we have seen
getting rid of poachers

323
00:23:08.310 --> 00:23:12.430 
in Africa who are running for
Rhinos or something like that,

324
00:23:12.490 --> 00:23:17.300 
wherever drones might actually
be of help. So as an AI system

325
00:23:17.450 --> 00:23:20.940 
definitely can be of help in here
and the inference model can

326
00:23:20.940 --> 00:23:24.950 
be transported then from our systems
from our power9 system

327
00:23:24.950 --> 00:23:25.580 
on the drone.

328
00:23:28.120 --> 00:23:33.670 
A car smarter and safer cities. For
example, traffic control in the cities,

329
00:23:33.870 --> 00:23:38.770 
how people are behaving,
travel control in any dimension

330
00:23:38.770 --> 00:23:41.580 
that's actually something that
can be what we can see here

331
00:23:41.830 --> 00:23:46.300 
for example parking regulations.
This is all very much depending

332
00:23:46.300 --> 00:23:49.400 
actually on the use case but
just to show you about

333
00:23:49.400 --> 00:23:52.790 
the possibilities, what can
be established today.

334
00:23:53.810 --> 00:23:56.870 
A very important part here, this
is one of my, let's say, hobbies

335
00:23:56.870 --> 00:24:01.670 
here in Germany is actually document
and archive digtalization

336
00:24:01.900 --> 00:24:04.860 
and probably you know Germany is
not one of the countries with

337
00:24:04.860 --> 00:24:08.400 
the best letter digitalisation
ratio here, especially in public

338
00:24:08.400 --> 00:24:11.750 
services, so therefore of course
we have a high demand here

339
00:24:11.870 --> 00:24:19.020 
for getting that old archives and files away
on paper to, let's say, automated systems.

340
00:24:19.420 --> 00:24:23.360 
So that means there are a lot of
data sitting there and that

341
00:24:23.360 --> 00:24:28.640 
cost millions of working hours spent on
search before actionable results every year.

342
00:24:29.590 --> 00:24:33.530 
Just this is a software we have
built together with a software

343
00:24:33.530 --> 00:24:38.410 
company here in the next in
Germany and the software company

344
00:24:38.410 --> 00:24:43.030 
actually have created a
system that is able to read and

345
00:24:43.030 --> 00:24:47.310 
understand and transcript
handwritten forms in any kind of

346
00:24:47.310 --> 00:24:52.570 
language, in any kind of writing.
That means that's a good thing

347
00:24:52.570 --> 00:24:56.800 
actually if you want to take that
part of the software automatically

348
00:24:56.800 --> 00:25:00.150 
in your business process. In our
little example here we are

349
00:25:00.150 --> 00:25:04.160 
just looking for words, but imagine
if you can really take out

350
00:25:04.170 --> 00:25:08.540 
such information from forms and can
process it in the SAP process

351
00:25:08.540 --> 00:25:11.870 
by the way or something like
that where you actually have a

352
00:25:11.880 --> 00:25:16.000 
description of a doctor or
something, you can put it to your

353
00:25:16.000 --> 00:25:20.120 
insurance company. That all can
be done automatically with an

354
00:25:20.130 --> 00:25:24.560 
accuracy of, let's say, higher than
ninety percent. With that particular

355
00:25:24.560 --> 00:25:28.810 
software we can run about sixty
thousand pages of handwriting

356
00:25:28.910 --> 00:25:32.330 
per hour on a single system, on
a single power9 system

357
00:25:32.520 --> 00:25:35.730 
that gives you the flexibility
and the performance indication

358
00:25:35.790 --> 00:25:40.070 
where we are heading to. And this is
of course approximately the size

359
00:25:40.240 --> 00:25:43.900 
if the same thing would be done
by humans, you would take about

360
00:25:43.900 --> 00:25:47.510 
one thousand four hundred people
eight hours working a day

361
00:25:47.600 --> 00:25:51.470 
for completing that single task.
That really is astonishing, isn't it?

362
00:25:53.030 --> 00:25:58.240 
But I think before we
conclude on that session, I think

363
00:25:58.240 --> 00:26:02.640 
the future and people usually say
why don't we do it on cloud

364
00:26:02.640 --> 00:26:06.020 
and I think I'm pretty sure
that the future is a very hybrid

365
00:26:06.020 --> 00:26:09.780 
future. We will have data arresting
on side in data centers,

366
00:26:10.020 --> 00:26:13.670 
you will see a lot of data is flowing
from the cloud in the cloud

367
00:26:13.810 --> 00:26:17.010 
and you can see that there is
definitely a demand to do some

368
00:26:17.010 --> 00:26:21.550 
AI systems within the cloud because
the data should stay there,

369
00:26:21.550 --> 00:26:25.660 
where is actually the data
to be stored too. So the cloud

370
00:26:25.670 --> 00:26:29.850 
might be an option on that. And the
question is how do we integrate this

371
00:26:29.850 --> 00:26:33.330 
with our own private system and
the answer for that is we call

372
00:26:33.330 --> 00:26:35.490 
it a hybrid system
in a hybrid world.

373
00:26:36.870 --> 00:26:41.050 
That will actually finally bring
me to that chart where we

374
00:26:41.050 --> 00:26:45.810 
can see definitely a combination
of multiple data sources

375
00:26:45.970 --> 00:26:50.260 
and this closes our little circle
here also, where we see that

376
00:26:50.270 --> 00:26:54.220 
data is flowing from any kind
of data source and at the end

377
00:26:54.220 --> 00:26:56.780 
of the day this is really
something that we can see in the

378
00:26:56.780 --> 00:27:00.860 
future of general AI, where
systems actually can reason and

379
00:27:00.860 --> 00:27:05.340 
conclude on various inputs, on
various parameters from around

380
00:27:05.340 --> 00:27:09.330 
the globe, from around other systems
here and therefore of course

381
00:27:09.450 --> 00:27:13.770 
that's a little universe
where we call the digital

382
00:27:13.770 --> 00:27:18.220 
universe. By the way this is
going to happen very soon.

383
00:27:19.170 --> 00:27:23.400 
Last not least there are some
considerations. Always you

384
00:27:23.400 --> 00:27:28.990 
have to consider because AI is not
a self sufficient implementation,

385
00:27:29.260 --> 00:27:35.840 
you always have to consider how do I
orchestrate my AI system in my organization,

386
00:27:36.020 --> 00:27:39.910 
because presently they just only
five percent of all developments

387
00:27:39.910 --> 00:27:43.260 
are really ranging up to the
production level. That means a

388
00:27:43.260 --> 00:27:47.440 
lot of time is really is in
research and is in development,

389
00:27:47.580 --> 00:27:51.650 
but of course we have to focus also on the
orchestration level in the production.

390
00:27:52.020 --> 00:27:56.030 
Ethics, a very important point
if we didn't have today. Data

391
00:27:56.030 --> 00:28:00.500 
government, who is accessing the data,
how long does it take time to market,

392
00:28:00.700 --> 00:28:05.020 
how do I integrate legacy systems
here, do I have to consider

393
00:28:05.020 --> 00:28:09.900 
some legal requirements. In many countries
we have different legal requirements,

394
00:28:10.120 --> 00:28:12.610 
the ground truth of the
data as I said before,

395
00:28:13.070 --> 00:28:16.710 
do we need actually to consider
security or even patterns what

396
00:28:16.720 --> 00:28:20.830 
we have achieved with our development,
compatibility with other

397
00:28:20.830 --> 00:28:23.320 
systems how to be
integrated and interact,

398
00:28:23.990 --> 00:28:27.370 
availability concepts have a high
concern because if you have

399
00:28:27.370 --> 00:28:30.440 
to trust such a system you have
to also trust actually the

400
00:28:30.440 --> 00:28:35.010 
technology to make that system
happen all the time. Compliance

401
00:28:35.020 --> 00:28:39.620 
is used, if you actually have some systems
that make decisions on their own

402
00:28:39.790 --> 00:28:43.280 
do we have to consider compliance
and a very important point

403
00:28:43.280 --> 00:28:47.980 
is last at least is also privacy. So
this is a couple of considerations

404
00:28:47.980 --> 00:28:51.500 
and each of those that fill a
full session here probably, but

405
00:28:51.500 --> 00:28:54.970 
of course at the end of my
presentation this is just one thing

406
00:28:54.970 --> 00:28:59.160 
I say that we always should keep
in mind that these are something

407
00:28:59.160 --> 00:29:02.970 
we need to consider when building
up an AI infrastructure and

408
00:29:02.970 --> 00:29:07.650 
an AI architecture. And remember
there is no AI without IA.

409
00:29:07.660 --> 00:29:08.930 
Thank you very much.
