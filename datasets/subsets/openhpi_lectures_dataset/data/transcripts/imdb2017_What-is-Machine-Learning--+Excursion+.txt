WEBVTT

1
00:00:00.000 --> 00:00:03.300 
Pleasure to be here again, I'm going
to talk a bit about machine learning.

2
00:00:03.300 --> 00:00:07.700 
I've had the honor to
talk about the topic last

3
00:00:07.700 --> 00:00:10.100 
year as well and a lot has
happened. ML is evolving

4
00:00:10.100 --> 00:00:15.150 
so fast that it seems new research
results are available everyday,

5
00:00:15.150 --> 00:00:18.180 
new companies are being
founded everyday, new offerings

6
00:00:18.180 --> 00:00:21.210 
from the logic corporates
are coming out every day,

7
00:00:21.210 --> 00:00:25.250 
and events are just
chasing each other. What

8
00:00:25.250 --> 00:00:28.280 
I brought forward today is a
bit of an overview and recap

9
00:00:28.280 --> 00:00:32.320 
of what ML is about.
A look at recent

10
00:00:32.320 --> 00:00:35.350 
research and technology
highlights and a view

11
00:00:35.350 --> 00:00:38.380 
of what SAP has introduced to the
market and where we are headed

12
00:00:39.390 --> 00:00:41.410 
with making all enterprise
applications intelligent.

13
00:00:42.420 --> 00:00:47.470 
Quick recap, what is
machine learning about?

14
00:00:47.470 --> 00:00:50.500 
There's a lot of talk about
artificial intelligence and

15
00:00:50.500 --> 00:00:53.530 
robots taking over the planet and
what is the future of humanity.

16
00:00:54.540 --> 00:00:57.570 
I want to ground this in a very
simple statement of machine

17
00:00:57.570 --> 00:01:00.600 
learning is about machines and
computers learning from data

18
00:01:01.610 --> 00:01:03.630 
rather than being
explicitly programmed.

19
00:01:03.630 --> 00:01:07.670 
That's by and large what we
and everybody else who claims

20
00:01:07.670 --> 00:01:11.710 
to be doing A.I. is doing.
We're learning from data, we're

21
00:01:11.710 --> 00:01:16.760 
inferring from patterns and that
gives us the incredible potential

22
00:01:16.760 --> 00:01:18.780 
to look at all the
unstructured data out there

23
00:01:18.780 --> 00:01:21.810 
where rules are hard
to program or to give.

24
00:01:21.810 --> 00:01:24.840 
I'm talking about natural
language text, I'm talking about

25
00:01:24.840 --> 00:01:28.880 
the spoken word, I'm
talking about images, videos

26
00:01:28.880 --> 00:01:31.910 
IOT sensor device,
times series data.

27
00:01:31.910 --> 00:01:34.940 
All these types of data are
incredibly hard to deal with if

28
00:01:34.940 --> 00:01:37.970 
you have to give hard
grammatical rules for them

29
00:01:38.980 --> 00:01:40.100 
but it turns out that
they are very tractable

30
00:01:41.101 --> 00:01:43.103 
when you learn from the data
and you infer the patterns.

31
00:01:43.103 --> 00:01:47.107 
Opening up this universe
matters tremendously because

32
00:01:47.107 --> 00:01:50.110 
90%+ of the digital
universe of data out there

33
00:01:51.111 --> 00:01:55.115 
is un structured so by unlocking
that we can deliver tremendous

34
00:01:55.115 --> 00:01:59.119 
value to the enterprise. Why is
this happening now, on these three

35
00:01:59.119 --> 00:02:03.123 
main factors coming together.
Number one, we have the data.

36
00:02:03.123 --> 00:02:07.127 
A couple years ago having
a thousand images for a

37
00:02:07.127 --> 00:02:10.130 
computer vision challenge was
considered a big data set.

38
00:02:10.130 --> 00:02:14.134 
Today big data sets are internet
curated tens of millions.

39
00:02:15.135 --> 00:02:18.138 
The privately held data sets
like that company that has

40
00:02:18.138 --> 00:02:21.141 
two billion users all
of whom are uploading

41
00:02:21.141 --> 00:02:24.144 
pictures of people and
basically labeling who they are

42
00:02:24.144 --> 00:02:27.147 
they are getting two hundred
million labelled images a day

43
00:02:27.147 --> 00:02:30.150 
that they can use to train face
detectors, face recognizers,

44
00:02:30.150 --> 00:02:33.153 
and other things that
relate to people in images.

45
00:02:33.153 --> 00:02:36.156 
That's a pretty great
data set and having a two

46
00:02:36.156 --> 00:02:39.159 
billion volunteer force
that does that un-paid

47
00:02:39.159 --> 00:02:42.162 
is tremendous. Other
companies have to pay for

48
00:02:42.162 --> 00:02:45.165 
that kind of labeling effort,
so think about that when next

49
00:02:45.165 --> 00:02:48.168 
you upload something on facebook
or snapchat or instagram.

50
00:02:49.169 --> 00:02:51.171 
The second one is the
improvements in hardware

51
00:02:52.172 --> 00:02:55.175 
because those huge data sets
only mean something if we can

52
00:02:55.175 --> 00:02:57.177 
actually do something
with them before the sun

53
00:02:57.177 --> 00:03:01.181 
stops shining and sort of the
universe dies a slow heat death.

54
00:03:01.181 --> 00:03:04.184 
Last but not least deep
learning algorithms

55
00:03:04.184 --> 00:03:07.187 
are behind many
of these advances.

56
00:03:07.187 --> 00:03:11.191 
The improvements in these
algorithms are fairly subtle.

57
00:03:11.191 --> 00:03:15.195 
Some of the breakthroughs that
led to computer vision jumping

58
00:03:15.195 --> 00:03:19.199 
onto the scene with deep learning
are really amazingly simple.

59
00:03:19.199 --> 00:03:22.202 
If you look at them in
retrospect and the key element

60
00:03:22.202 --> 00:03:25.205 
of deep learning which is
why I find that so exciting

61
00:03:25.205 --> 00:03:29.209 
is that almost every problem
considered hard before

62
00:03:29.209 --> 00:03:33.213 
is suddenly within the reach
of a few determined people

63
00:03:33.213 --> 00:03:37.217 
writing a few hundreds to a few
thousand lines of code and having

64
00:03:37.217 --> 00:03:40.220 
the right data set. It's not a
massive engineering discipline

65
00:03:40.220 --> 00:03:44.224 
where we are thrashing in the pit
of requirements documents and sort

66
00:03:44.224 --> 00:03:47.227 
of hundreds of thousands of lines
of code to get anything done.

67
00:03:48.228 --> 00:03:52.232 
Guy with a laptop can actually
solve many of these problems

68
00:03:52.232 --> 00:03:54.234 
if the right data is there and
if the modern approaches are

69
00:03:54.234 --> 00:03:57.237 
there. This is a
huge democratizer

70
00:03:57.237 --> 00:04:01.241 
and it also negates the advantage
that others who invested into

71
00:04:01.241 --> 00:04:04.244 
traditional learning
approaches five ten years ago

72
00:04:04.244 --> 00:04:07.247 
and put a lot of effort in
writing rules and creating sort

73
00:04:07.247 --> 00:04:11.251 
of expert systems. It negates
their advantage because with data

74
00:04:11.251 --> 00:04:17.257 
sets we can simply leap frog and
go to the most modern solution

75
00:04:17.257 --> 00:04:20.260 
that in most cases beats
traditional approaches by 10 to 15

76
00:04:20.260 --> 00:04:26.266 
percentage points. What does
that look like? Very grossly

77
00:04:26.266 --> 00:04:30.270 
simplifying, this is the cycle
of doing machine learning and

78
00:04:30.270 --> 00:04:33.273 
bringing machine intelligence
into an application today.

79
00:04:33.273 --> 00:04:37.277 
It all starts with the data
and once you have secured first

80
00:04:37.277 --> 00:04:42.282 
data you enter a cycle of data
preparation and extraction

81
00:04:42.282 --> 00:04:46.286 
where you focus on the relevant
features and aspects. This

82
00:04:46.286 --> 00:04:48.288 
for example is
something where S4HANA

83
00:04:48.288 --> 00:04:52.292 
is of tremendous help because
it simplifies the enterprise

84
00:04:52.292 --> 00:04:54.294 
data models and it makes the
features that we're looking for

85
00:04:54.294 --> 00:04:58.298 
machine learning much more
accessible. The second step is a

86
00:04:58.298 --> 00:05:01.301 
round of iterations of
training models to ever

87
00:05:01.301 --> 00:05:04.304 
increase accuracy and make them
more relevant for the context.

88
00:05:04.304 --> 00:05:07.307 
Then we can deploy them into
applications or services

89
00:05:09.309 --> 00:05:12.312 
and generate first value for
users but it doesn't stop there

90
00:05:12.312 --> 00:05:15.315 
because every time the user
interacts with the model outcome

91
00:05:15.315 --> 00:05:20.000 
or a service they are generating
a clickstream, a swipe stream,

92
00:05:20.000 --> 00:05:23.323 
a feedback stream that constitutes
additional data for us to

93
00:05:23.323 --> 00:05:27.327 
learn from. Have they agreed
with the recommendation? Are they

94
00:05:27.327 --> 00:05:29.329 
going into an entirely different
direction? Have the ignored

95
00:05:29.329 --> 00:05:33.333 
things? This is the data points
that we can use to make the

96
00:05:33.333 --> 00:05:37.337 
next model even better. For
the first time ML offers the

97
00:05:37.337 --> 00:05:40.340 
promise of software
that can improve itself

98
00:05:40.340 --> 00:05:45.345 
over time, just as you use it, no
heavy consulting and customizing

99
00:05:45.345 --> 00:05:50.000 
required that's the promise. Lets have
a look at some of the technologies

100
00:05:50.350 --> 00:05:53.353 
that we are using
in order to go there

101
00:05:53.353 --> 00:05:58.358 
and this can only be the briefest
of overviews of the field of

102
00:05:58.358 --> 00:06:02.362 
deep learning. This forming
flavors of deep learning

103
00:06:02.362 --> 00:06:06.366 
that are being used today,
there are convolutional networks

104
00:06:06.366 --> 00:06:09.369 
or CNNs, the recurrent
networks or RNNs,

105
00:06:09.369 --> 00:06:13.373 
both of these or in an applied
engineering stage where many

106
00:06:13.373 --> 00:06:16.376 
many groups are putting them into
concrete products and shipping

107
00:06:16.376 --> 00:06:19.379 
them. There are two more
approaches which are

108
00:06:19.379 --> 00:06:23.383 
let's say in an applied research
stage for reinforcement learning

109
00:06:24.384 --> 00:06:27.387 
and in a more basic
research stage for memory

110
00:06:27.387 --> 00:06:31.391 
networks. The way to think about
them is two simple questions.

111
00:06:32.392 --> 00:06:37.397 
Number one, do I have a lot of data?
If I have a lot of data probably

112
00:06:37.397 --> 00:06:40.400 
convolutional networks and
recurrent networks are the way to

113
00:06:40.400 --> 00:06:43.403 
go. If they don't have a lot
of data, I need to look for the

114
00:06:43.403 --> 00:06:47.407 
other things. The second
thing that I need to ask is do

115
00:06:47.407 --> 00:06:51.411 
I have some form of locality
in space that I can exploit?

116
00:06:52.412 --> 00:06:55.415 
If I have that I can
make a decision which of

117
00:06:55.415 --> 00:06:58.418 
the approaches to use.
If I have a locality

118
00:06:58.418 --> 00:07:02.422 
in time or temporal flow, I want
to go for the other approach.

119
00:07:02.422 --> 00:07:06.426 
Lets look at these
in a bit more depth.

120
00:07:06.426 --> 00:07:11.431 
If you have abundant data and
it is local in space because

121
00:07:11.431 --> 00:07:14.434 
it's sort of something that
is perhaps tied to vision

122
00:07:14.434 --> 00:07:17.437 
or video frames or
encoded text input.

123
00:07:18.438 --> 00:07:21.441 
You always find data locality,
the value of a pixel is related

124
00:07:21.441 --> 00:07:24.444 
to the pixel surrounding it in
the image but not necessarily

125
00:07:24.444 --> 00:07:26.446 
the pixels at the
other end of the thing.

126
00:07:26.446 --> 00:07:29.449 
In building ML models
we can exploit that

127
00:07:29.449 --> 00:07:33.453 
locality and the way to do this
with convolutional networks

128
00:07:33.453 --> 00:07:37.457 
is to create very very small
filters that we can drag across

129
00:07:37.457 --> 00:07:40.460 
the image and the features are
the same everywhere but we're

130
00:07:40.460 --> 00:07:43.463 
creating many many
layers of them.

131
00:07:43.463 --> 00:07:46.466 
Current state of the
art approaches use a 150

132
00:07:46.466 --> 00:07:49.469 
or even 200 layers
of these filters.

133
00:07:49.469 --> 00:07:52.472 
Each of them is a very
simple abstraction step,

134
00:07:52.472 --> 00:07:55.475 
it's basically a 3 x 3
convolution, a very small

135
00:07:55.475 --> 00:07:59.479 
matrix multiply that we're doing
but by having that many layers

136
00:07:59.479 --> 00:08:03.483 
we can achieve super human
computer vision performance with

137
00:08:03.483 --> 00:08:07.487 
this. This is one of the most
active areas of research and

138
00:08:07.487 --> 00:08:10.490 
application right now.
There are many advances

139
00:08:10.490 --> 00:08:14.494 
being made in how to train
these with adversarial training,

140
00:08:14.494 --> 00:08:16.496 
how to generate
data sets because

141
00:08:16.496 --> 00:08:19.499 
sometimes you don't quite have
abundant data but you want to

142
00:08:19.499 --> 00:08:21.501 
use this great tool so you
could figure out what to do

143
00:08:21.501 --> 00:08:25.505 
and recent advances in
network geometry that allow

144
00:08:25.505 --> 00:08:28.508 
you to tackle problems
even with higher accuracy.

145
00:08:29.509 --> 00:08:31.511 
The second part of
this is recurrent

146
00:08:31.511 --> 00:08:35.515 
networks and when you don't have
locality in space you typically

147
00:08:35.515 --> 00:08:38.518 
have some kind of temporal
flow or locality in space.

148
00:08:38.518 --> 00:08:41.521 
For things like the
written word and text

149
00:08:41.521 --> 00:08:44.524 
you can see that as a
sequence of characters

150
00:08:44.524 --> 00:08:48.528 
which is flowing over time. For
the spoken word in audio files

151
00:08:48.528 --> 00:08:51.531 
you have an audio signal or
maybe the fourier transform of

152
00:08:51.531 --> 00:08:55.535 
audio signal also flowing
over time and the way to deal

153
00:08:55.535 --> 00:08:58.538 
with them is to have
a very small networks

154
00:08:58.538 --> 00:09:02.542 
that refer to the input at a
given point in time but also

155
00:09:02.542 --> 00:09:05.545 
continuously update their
state, they propagate

156
00:09:05.545 --> 00:09:09.549 
state over time. These are
the things that are behind

157
00:09:09.549 --> 00:09:13.553 
the speech recognizers in
your phones and the big trend

158
00:09:13.553 --> 00:09:16.556 
for making them even better is
a lot of unsupervised training.

159
00:09:16.556 --> 00:09:20.560 
Because even with abundant data
having labels for your data

160
00:09:20.560 --> 00:09:23.563 
when you're not facebook, you
don't have two billion people

161
00:09:23.563 --> 00:09:27.567 
for free labeling images for you.
Labeling is actually expensive

162
00:09:27.567 --> 00:09:29.569 
so people are trying to do more
and more with unsupervised.

163
00:09:31.571 --> 00:09:35.000 
If you have limited data, there
are two new approaches that

164
00:09:35.000 --> 00:09:37.577 
are tremendously exciting
and hold a lot of promise.

165
00:09:37.577 --> 00:09:41.581 
Deep reinforcement learning
is about agents in interacting

166
00:09:41.581 --> 00:09:44.584 
with their environment.
You typically have a very

167
00:09:44.584 --> 00:09:47.587 
rich environment where you can
turn a camera on or you have

168
00:09:47.587 --> 00:09:51.591 
a simulated camera in a video game
and you get a lot of information

169
00:09:51.591 --> 00:09:55.595 
but you only get very
very sparse feedback back.

170
00:09:56.596 --> 00:09:59.599 
That feedback is usually
temporally decoupled from what

171
00:09:59.599 --> 00:10:03.603 
you do. You trigger an action, there
is some temporal flow, something

172
00:10:03.603 --> 00:10:06.606 
is happening in the environment
and some time later in

173
00:10:06.606 --> 00:10:08.608 
the future you're
going to get a reward.

174
00:10:09.609 --> 00:10:11.611 
The challenge of
reinforcement learning is

175
00:10:11.611 --> 00:10:14.614 
how to learn from these super sparse
feedback signals of, "hey,

176
00:10:14.614 --> 00:10:17.617 
you've won the game of chess,
you've won the game of goal,"

177
00:10:17.617 --> 00:10:21.621 
when you've made all these
moves and to propagate

178
00:10:21.621 --> 00:10:26.626 
these sparse signals and rewards
back in time to actually learn

179
00:10:26.626 --> 00:10:30.630 
what optimal moves are.
That's how Alpha go

180
00:10:30.630 --> 00:10:33.633 
has beat the champion
in Go! and this is how

181
00:10:33.633 --> 00:10:36.636 
self-driving vehicles, many
flavors of robot control

182
00:10:36.636 --> 00:10:41.641 
are being done. The key for it
is usually having simulation

183
00:10:41.641 --> 00:10:45.645 
environments that can
generate or that visual and

184
00:10:45.645 --> 00:10:49.000 
environment data that you need,
but the feedback about the success

185
00:10:49.000 --> 00:10:52.652 
of your actions always stays
sparse. Last but not least

186
00:10:52.652 --> 00:10:56.656 
memory networks are the
newest thing, this is what

187
00:10:56.656 --> 00:11:00.660 
happens if you take neural
networks and you try to couple them

188
00:11:00.660 --> 00:11:03.663 
with a memory. People try to
do that with turing machines

189
00:11:03.663 --> 00:11:08.668 
for example, the trick here is to
make the read and write operations

190
00:11:08.668 --> 00:11:10.670 
of these turing machines
in all these memories

191
00:11:11.671 --> 00:11:14.674 
somehow compatible with neural
networks and that's the main

192
00:11:14.674 --> 00:11:17.677 
design challenge because
neural networks are all about

193
00:11:17.677 --> 00:11:21.681 
gradients, stochastic behavior
sort of things changing gradually

194
00:11:21.681 --> 00:11:25.685 
over time. Memory is about I
write a 1, I write a 0, it stays

195
00:11:25.685 --> 00:11:28.688 
at 1, it stays at 0 and hopefully
I can retrieve it without

196
00:11:28.688 --> 00:11:31.691 
corruption thanks
to ECC memory.

197
00:11:31.691 --> 00:11:34.694 
That's a very different
world, so bridging

198
00:11:34.694 --> 00:11:37.697 
these two is one of the most
exciting areas of fundamental

199
00:11:37.697 --> 00:11:41.701 
research. The big promise
it holds is that with memory

200
00:11:41.701 --> 00:11:45.705 
we can suddenly learn
from just one example or

201
00:11:45.705 --> 00:11:48.708 
two examples, not like
a human learns because

202
00:11:48.708 --> 00:11:52.712 
if I give you a drawing of
objects to look for and tell you

203
00:11:52.712 --> 00:11:57.717 
to look for people with a tall
hat, actually you can do that

204
00:11:57.717 --> 00:12:00.720 
I don't need to feed you twenty
million images with lists

205
00:12:00.720 --> 00:12:03.723 
of hats and you do
not turn on sort of a

206
00:12:04.724 --> 00:12:07.727 
number crunching matrix
multiplying machine in your head

207
00:12:07.727 --> 00:12:10.730 
in order to quickly crunch these
20 billion provided images.

208
00:12:10.730 --> 00:12:14.734 
The one shot thing is sufficient
because we have a working

209
00:12:14.734 --> 00:12:17.737 
memory and we have an active
memory that we can use for these

210
00:12:17.737 --> 00:12:21.741 
kinds of tasks, so it's
one of the most promising

211
00:12:21.741 --> 00:12:23.743 
things I'm
looking forward.
