WEBVTT

1
00:00:00.300 --> 00:00:04.799 
All right, I think people are still trickling in, but I'm going to go ahead

2
00:00:04.800 --> 00:00:09.419 
and get started for our final session of

3
00:00:09.420 --> 00:00:11.849 
the Learning at Scale conference.

4
00:00:11.850 --> 00:00:16.649 
I am very pleased to welcome you all here to our third of three

5
00:00:16.650 --> 00:00:18.839 
really amazing keynotes.

6
00:00:18.840 --> 00:00:24.179 
I'm so pleased to introduce Dr. Motahhare Eslami, who is

7
00:00:24.180 --> 00:00:29.039 
an incredibly promising junior faculty member from my own university,

8
00:00:29.040 --> 00:00:33.689 
Carnegie Mellon University, who is in the Human Computer Interaction

9
00:00:33.690 --> 00:00:38.189 
Institute and also has an appointment at the Institute for Software Research.

10
00:00:39.750 --> 00:00:44.459 
I think the topic that Motahhare will be talking about today is

11
00:00:44.460 --> 00:00:48.450 
one of extreme importance for our community.

12
00:00:49.470 --> 00:00:54.479 
If you recall back to our opening session, we talked about the need for

13
00:00:54.480 --> 00:00:58.919 
effective learning solutions that have become even more pressing in

14
00:00:58.920 --> 00:01:03.389 
the pandemic this year, but also the issues

15
00:01:03.390 --> 00:01:08.489 
around equity and our understanding of who is benefiting

16
00:01:08.490 --> 00:01:12.569 
and who is not benefiting from the use of these technologies.

17
00:01:12.570 --> 00:01:17.939 
And the Motahhare's work at large touches on these issues in a very

18
00:01:17.940 --> 00:01:23.609 
important way, thinking also about fairness, accountability, transparency

19
00:01:23.610 --> 00:01:28.949 
and ethics, which I cannot imagine a better topic for our closing

20
00:01:28.950 --> 00:01:31.109 
session here today.

21
00:01:31.110 --> 00:01:35.759 
So her work looks at sort of opening up the black

22
00:01:35.760 --> 00:01:40.469 
box of algorithms and understanding how users

23
00:01:40.470 --> 00:01:45.239 
can actually be more informed and have a better experience

24
00:01:45.240 --> 00:01:50.279 
and also be better served by the technologies that they may not have previously

25
00:01:50.280 --> 00:01:54.719 
understood. And as we all know, education is so

26
00:01:54.720 --> 00:01:59.249 
important, and yet we ourselves don't always

27
00:01:59.250 --> 00:02:03.989 
treat learners with the respect that they deserve

28
00:02:03.990 --> 00:02:08.879 
while understanding the types of systems and experiences

29
00:02:08.880 --> 00:02:13.319 
that they are being exposed to. So I look forward to Motahhare telling

30
00:02:13.320 --> 00:02:18.119 
us how exactly we can do that in our educational systems, but also

31
00:02:18.120 --> 00:02:22.739 
her work that touches on many other areas of societal importance.

32
00:02:22.740 --> 00:02:27.179 
And one extra thing that I think is really fantastic about Motahhare's

33
00:02:27.180 --> 00:02:32.189 
work is how widely it has been reported in the news media.

34
00:02:32.190 --> 00:02:36.899 
And I think this is something that's also a great lesson for our community about the need

35
00:02:36.900 --> 00:02:41.729 
to communicate to a broader public outside our scientific

36
00:02:41.730 --> 00:02:46.259 
audience about the importance of our findings and how we can go

37
00:02:46.260 --> 00:02:50.819 
about really causing that beneficial societal

38
00:02:50.820 --> 00:02:54.959 
change that I think we're all here to help catalyze.

39
00:02:54.960 --> 00:02:59.429 
So with no further ado, I'm going to turn it over to Motahhare to

40
00:02:59.430 --> 00:03:01.979 
talk to us about her research today.

41
00:03:03.710 --> 00:03:08.179 
Thank you very much for that great introduction, appreciate that and thanks for having me

42
00:03:08.180 --> 00:03:12.799 
today. Learning at the Scale was telling me is one of my favorite conferences.

43
00:03:12.800 --> 00:03:17.409 
When it started, I was joining the session and I'm really happy to be here again.

44
00:03:18.590 --> 00:03:23.899 
So I'm going to just share my screen. So and I'd be happy to take questions afterward.

45
00:03:23.900 --> 00:03:28.219 
Amy is going to help to manage that when we are done.

46
00:03:28.220 --> 00:03:32.809 
And with that, I'm going to talk about

47
00:03:32.810 --> 00:03:36.949 
revisiting transparency and fairness in algorithmic systems through the lens of public

48
00:03:36.950 --> 00:03:38.270 
education and engagement.

49
00:03:39.380 --> 00:03:44.089 
So Google's autocomplete algorithm says algorithms are taking over the world,

50
00:03:44.090 --> 00:03:46.039 
and I can agree more.

51
00:03:46.040 --> 00:03:50.539 
So they tell us what to read, what to watch, what to buy

52
00:03:50.540 --> 00:03:52.609 
or even whom to date.

53
00:03:52.610 --> 00:03:56.809 
And then in doing so, they exert so much power in our everyday life.

54
00:03:56.810 --> 00:03:59.029 
They are still hidden in black boxes.

55
00:03:59.030 --> 00:04:01.969 
And I want to emphasize that these black boxes are there for a reason.

56
00:04:01.970 --> 00:04:04.459 
It's not just a random design decision.

57
00:04:04.460 --> 00:04:09.079 
They are there to protect intellectual property, they are there to prevent malicious

58
00:04:09.080 --> 00:04:11.329 
users gaming with the system.

59
00:04:11.330 --> 00:04:15.619 
But most importantly, you can't really review all the technical details of an algorithmic

60
00:04:15.620 --> 00:04:16.909 
system to users.

61
00:04:16.910 --> 00:04:21.679 
You are not on Facebook to learn how its developers use more than one hundred key factors

62
00:04:21.680 --> 00:04:23.719 
to determine what to show you or not.

63
00:04:23.720 --> 00:04:26.989 
You are there to just get connected with your friends and family.

64
00:04:26.990 --> 00:04:31.699 
But at the same time, this black box can cause misinformed

65
00:04:31.700 --> 00:04:35.719 
behavior and as a result also bias and harm.

66
00:04:35.720 --> 00:04:40.549 
The problem with these black boxes are that they can create

67
00:04:40.550 --> 00:04:45.559 
behaviors that are harmful to people without even knowing that this is happening.

68
00:04:45.560 --> 00:04:50.629 
And because of that, there has been a line of work during the past few years

69
00:04:50.630 --> 00:04:55.189 
in research in various areas that try to bring accountability to these

70
00:04:55.190 --> 00:04:58.399 
systems as we bring accountability to people.

71
00:04:58.400 --> 00:05:01.699 
And two of these lines of work are transparency and fairness.

72
00:05:01.700 --> 00:05:06.289 
So today I'm going to talk about these lines of work, what has been done and

73
00:05:06.290 --> 00:05:10.447 
what is missing. What is the gap? Many of you, based on the papers I read from the

74
00:05:10.448 --> 00:05:12.629 
letters and the great people are here.

75
00:05:12.630 --> 00:05:15.289 
I know many of you are familiar with this area, so I'm going to give a quick

76
00:05:15.290 --> 00:05:19.909 
introduction. But the thing is that what has been done is great, but it's not

77
00:05:19.910 --> 00:05:23.779 
enough. And I'm just going to say why and how we can hopefully move forward.

78
00:05:23.780 --> 00:05:28.519 
I just at least a few of many, many work that has been done in this area.

79
00:05:28.520 --> 00:05:33.139 
And I'm going to talk about transparency first and just talk, and

80
00:05:33.140 --> 00:05:37.669 
then I move to fairness and discuss about these efforts and what is missing and why

81
00:05:37.670 --> 00:05:40.069 
we need to take a fresh perspective.

82
00:05:40.070 --> 00:05:43.189 
We are the lens of education into these problems.

83
00:05:43.190 --> 00:05:46.829 
So just that you know that I'm a fan of this work.

84
00:05:46.830 --> 00:05:52.009 
It's not that the work that has been done is not necessary is actually essential.

85
00:05:52.010 --> 00:05:56.689 
I've been working on the transparency and fairness, for example, for the past seven

86
00:05:56.690 --> 00:06:01.489 
or eight years, I've been working on creating new design ways to communicate this opaque

87
00:06:01.490 --> 00:06:06.889 
and potentially biased algorithmic process to users to give them a more informed

88
00:06:06.890 --> 00:06:10.099 
and transparent interaction.

89
00:06:10.100 --> 00:06:14.539 
These are two examples of the systems that I've been working on for a few

90
00:06:14.540 --> 00:06:19.009 
years. The one here this is a tool that would

91
00:06:19.010 --> 00:06:23.539 
reveal to users how to feed on Facebook would look

92
00:06:23.540 --> 00:06:27.979 
like if there was no algorithm filtering some of the stories, just

93
00:06:27.980 --> 00:06:29.930 
giving some awareness about what's going on.

94
00:06:30.990 --> 00:06:35.699 
The other tool here is a ReVeal, review revealer, which is about

95
00:06:35.700 --> 00:06:40.379 
Yelp users not knowing that their reviews sometimes get filtered because

96
00:06:40.380 --> 00:06:44.819 
Yelp actually hid them from them, only they

97
00:06:44.820 --> 00:06:49.169 
would see if they log out from the account or log in from another user.

98
00:06:49.170 --> 00:06:53.579 
So it was revealed that effect of the algorithmic process on their reviews.

99
00:06:53.580 --> 00:06:58.259 
While these kinds of research I love and I appreciate being

100
00:06:58.260 --> 00:07:00.539 
there, I think it's very important.

101
00:07:00.540 --> 00:07:02.759 
I want to argue that this is not enough.

102
00:07:02.760 --> 00:07:07.319 
And the reason it is not enough is because there is transparency

103
00:07:07.320 --> 00:07:09.569 
while there is no transparency there.

104
00:07:09.570 --> 00:07:14.039 
So, for example, this great work recently

105
00:07:14.040 --> 00:07:18.629 
has shown that while there has been assignment, school assignment

106
00:07:18.630 --> 00:07:23.129 
algorithms in California to help actually students to

107
00:07:23.130 --> 00:07:27.209 
be assigned to the right school and not be limited only in the neighborhood that could be

108
00:07:27.210 --> 00:07:30.989 
affected by socioeconomic status.

109
00:07:30.990 --> 00:07:34.649 
This is a school assignment algorithm actually fit.

110
00:07:34.650 --> 00:07:39.059 
They fill the transparency, equity, and also community engagement.

111
00:07:39.060 --> 00:07:43.649 
In other words, there has been a great discussion about how transparency can

112
00:07:43.650 --> 00:07:48.629 
become seen without knowing if that transparency is great.

113
00:07:48.630 --> 00:07:53.129 
But bringing the right level of transparency is hard, and sometimes even if you make

114
00:07:53.130 --> 00:07:56.768 
these transparent and you find the right level of transparency,

115
00:07:56.769 --> 00:08:01.349 
a few people don't know what's going on and that's going to stop them from being able to

116
00:08:01.350 --> 00:08:06.329 
impact and act on their everyday interaction

117
00:08:06.330 --> 00:08:08.939 
with algorithmic and A.I. systems.

118
00:08:08.940 --> 00:08:13.049 
So I'm going to today is just an example, talk about a group of these algorithms that

119
00:08:13.050 --> 00:08:17.609 
transparency, of course, has not been successful in doing it for

120
00:08:17.610 --> 00:08:22.049 
the past few years. Public algorithms are tools, algorithms that are used

121
00:08:22.050 --> 00:08:26.939 
in public sector, for example, in education department, in policing

122
00:08:26.940 --> 00:08:32.369 
department, in social services

123
00:08:32.370 --> 00:08:37.109 
departments or for example, the facial recognition algorithms

124
00:08:37.110 --> 00:08:41.729 
that police has been using has been a target of a lot of controversy about

125
00:08:41.730 --> 00:08:46.289 
how these algorithms are affecting everyday people's life without

126
00:08:46.290 --> 00:08:47.510 
them knowing what's going on.

127
00:08:48.600 --> 00:08:53.039 
So I'm going to talk about Pittsburgh because I live here and it's

128
00:08:53.040 --> 00:08:57.929 
a fabulous city. But at the same time, it's a very unique platform

129
00:08:57.930 --> 00:09:02.789 
here, at least in the US, along with a few other cities that hosts

130
00:09:02.790 --> 00:09:07.679 
a lot of these public algorithms. This is just a list of existing algorithmic systems

131
00:09:07.680 --> 00:09:10.949 
right now at Pittsburgh and the Allegheny County.

132
00:09:10.950 --> 00:09:15.090 
And as you see, it goes from family screening tool, to

133
00:09:16.200 --> 00:09:21.179 
small traffic laws, to assigning housing to people via algorithms,

134
00:09:21.180 --> 00:09:23.399 
risk assessment tools.

135
00:09:23.400 --> 00:09:27.719 
And we know there has been an effort of a group of

136
00:09:29.040 --> 00:09:33.699 
people, activist, government, official

137
00:09:33.700 --> 00:09:38.219 
sometimes or like researchers to bring transparency about these algorithms

138
00:09:38.220 --> 00:09:42.779 
used in the city. So you see here, for example, this is a page on the Allegheny

139
00:09:42.780 --> 00:09:46.199 
County Department trying to explain the algorithm to people.

140
00:09:46.200 --> 00:09:50.189 
There has been community meetings by groups like this Pittsburgh Task Force on Public

141
00:09:50.190 --> 00:09:54.839 
Employee Labs trying to discuss the equity and justice in these governmental

142
00:09:54.840 --> 00:09:59.159 
and public algorithms and board for the purpose of bringing transparency to these

143
00:09:59.160 --> 00:10:02.519 
algorithms and also engaging people with them.

144
00:10:02.520 --> 00:10:06.949 
However, a few months ago, we ran a survey of more and more than

145
00:10:06.950 --> 00:10:11.759 
fifteen hundred Pittsburghers and Allegheny County residents

146
00:10:11.760 --> 00:10:15.449 
about their awareness of these public algorithms.

147
00:10:15.450 --> 00:10:19.949 
And we gave them a list and asking them like, do they know any of these algorithms?

148
00:10:19.950 --> 00:10:24.419 
And as you see, less than 10 percent of the use of at least one of

149
00:10:24.420 --> 00:10:28.305 
those algorithms and there is no need to review or they're not sure even it

150
00:10:29.790 --> 00:10:32.140 
is the right algorithm that you're talking about or not.

151
00:10:33.180 --> 00:10:36.909 
So we thought, wow, why this is happening.

152
00:10:36.910 --> 00:10:40.709 
I mean, we know that these algorithms are hard to explain and transparency, but at least

153
00:10:40.710 --> 00:10:42.659 
we thought people are aware of these algorithms.

154
00:10:43.780 --> 00:10:48.429 
Hopefully at least more than 10 percent, so for those who want to be followed

155
00:10:48.430 --> 00:10:51.999 
with this question, how did you came up these algorithms?

156
00:10:52.000 --> 00:10:57.219 
And we were thinking maybe personal experience or friends and figured out some options.

157
00:10:57.220 --> 00:11:01.689 
But interestingly, the one that had the highest level of awareness,

158
00:11:01.690 --> 00:11:03.397 
broader level, highest level of awareness was news.

159
00:11:04.570 --> 00:11:09.189 
So people were just getting these algorithms by just reading

160
00:11:09.190 --> 00:11:12.729 
news and or exploring you Internet and just reading.

161
00:11:12.730 --> 00:11:16.269 
They just stumbled upon some news and they discussed that.

162
00:11:16.270 --> 00:11:21.039 
That's the way they became aware of these algorithms and we're like,

163
00:11:21.040 --> 00:11:24.099 
OK, so it sounds like these is the main resource.

164
00:11:24.100 --> 00:11:28.599 
And we kind of failed as a city to make people aware of this

165
00:11:28.600 --> 00:11:32.049 
will even have this material online here and there.

166
00:11:32.050 --> 00:11:33.050 
But why?

167
00:11:34.240 --> 00:11:38.739 
Why the effort was not really effective because people

168
00:11:38.740 --> 00:11:41.649 
don't read about that. People don't find out about that.

169
00:11:41.650 --> 00:11:46.359 
It's like I asked a person walking on the street, hey, come to our website, read

170
00:11:46.360 --> 00:11:47.647 
about these algorithms.

171
00:11:47.648 --> 00:11:52.449 
they are affecting your neighborhood. People it's like similar to, like

172
00:11:52.450 --> 00:11:55.839 
other platforms, if you are online on Facebook, you are not there to learn about the

173
00:11:55.840 --> 00:12:00.849 
algorithm necessarily. But if you want to learn about it should be in context.

174
00:12:00.850 --> 00:12:06.459 
But people don't have the context like how many of us daily work with these algorithms

175
00:12:06.460 --> 00:12:09.189 
or the systems as public algorithms.

176
00:12:09.190 --> 00:12:12.639 
Not many of us, I think, particularly if you are not professional users, but there is

177
00:12:12.640 --> 00:12:15.309 
still affecting our everyday life.

178
00:12:15.310 --> 00:12:19.869 
So the question is that where is the place where people read,

179
00:12:19.870 --> 00:12:25.419 
how we can communicate that to people, assume that we have the right material, even

180
00:12:25.420 --> 00:12:28.239 
if it doesn't sound that people still are aware of that.

181
00:12:28.240 --> 00:12:33.759 
And then we started thinking about the public and going out in public places.

182
00:12:33.760 --> 00:12:38.289 
So we were thinking, OK, if the news sounds like one of the most successful

183
00:12:38.290 --> 00:12:42.759 
sources for bringing awareness about these algorithms, let's think about other ways

184
00:12:42.760 --> 00:12:46.029 
like news that can get the public's attention.

185
00:12:46.030 --> 00:12:50.019 
You know, this started with this idea of this billboard here, and I remember when I was

186
00:12:50.020 --> 00:12:54.489 
living in San Francisco, I know it's a very big difference in Silicon Valley versus

187
00:12:54.490 --> 00:12:59.069 
other cities. All the billboards and advertisements are about technology, like,

188
00:12:59.070 --> 00:13:03.549 
hey, improve your database. Let's get this new I remember Zoom actually was

189
00:13:03.550 --> 00:13:07.659 
a new thing for like five, four years ago.

190
00:13:07.660 --> 00:13:11.739 
And then there were ads about zoom everywhere that you are using now widely.

191
00:13:11.740 --> 00:13:15.729 
So the idea was like, OK, it sounds like that's one of the ideas, one of the other places

192
00:13:15.730 --> 00:13:20.049 
that people can learn, because that's the context that people are at.

193
00:13:20.050 --> 00:13:23.319 
And they're like museums. There are bus stops.

194
00:13:23.320 --> 00:13:27.939 
So we have started a project trying to develop the right material for education about

195
00:13:27.940 --> 00:13:30.909 
this algorithms and put it in the right places.

196
00:13:30.910 --> 00:13:33.759 
And these could be some options for that.

197
00:13:33.760 --> 00:13:38.319 
So this is a poster, an example of one of my peers, JuJu's

198
00:13:38.320 --> 00:13:42.969 
students, and a group of collaborators made about algorithms

199
00:13:42.970 --> 00:13:47.439 
in your neighborhood in Pittsburgh. So it's about telling people

200
00:13:47.440 --> 00:13:51.939 
which neighborhoods are affected by algorithms, what these algorithms do, it just

201
00:13:51.940 --> 00:13:56.589 
giving them some idea about how they are,

202
00:13:56.590 --> 00:13:58.569 
how they should interact with the systems.

203
00:13:58.570 --> 00:14:03.039 
And I said, OK, what if we put this poster in a box, stop and try to intercept

204
00:14:03.040 --> 00:14:07.449 
when people are reading about it to understand what what how much aware of that or what

205
00:14:07.450 --> 00:14:11.939 
is missing? And we are in

206
00:14:11.940 --> 00:14:16.569 
the middle of this project. But I want to tell you that these kinds

207
00:14:16.570 --> 00:14:21.159 
of awareness efforts are not only about bringing

208
00:14:21.160 --> 00:14:24.279 
awareness, it's about empowering people.

209
00:14:24.280 --> 00:14:28.809 
It's about giving them a tool and educational tool that they can learn

210
00:14:28.810 --> 00:14:33.819 
and they can act on these algorithms when these algorithms fail the society.

211
00:14:34.870 --> 00:14:38.590 
Exactly one year ago, actually, one year and one day ago

212
00:14:39.610 --> 00:14:44.319 
in Pittsburgh, d'hiver protest going on that resulted in a suspending

213
00:14:44.320 --> 00:14:49.029 
the predictive policing algorithm which used the algorithm algorithms

214
00:14:49.030 --> 00:14:53.589 
in predicting crime hotspots by sending patrols to the areas that an algorithm

215
00:14:53.590 --> 00:14:56.559 
would predict. If it's going to be in that area of crime,

216
00:14:58.090 --> 00:15:02.109 
like maybe today or the next day, tomorrow.

217
00:15:02.110 --> 00:15:06.579 
What happened was that people started learning about this algorithm in news and there

218
00:15:06.580 --> 00:15:11.619 
were researchers and activists who are worried about racial bias and disorder

219
00:15:11.620 --> 00:15:14.229 
because it has been discussed in other cities like L.A.

220
00:15:14.230 --> 00:15:18.069 
and Chicago, which also adopted these algorithms.

221
00:15:18.070 --> 00:15:22.659 
And then this is despite the fact that actually the use of the algorithm showed

222
00:15:22.660 --> 00:15:27.159 
improvement like more than 30 percent improvement in the crime

223
00:15:27.160 --> 00:15:31.629 
rate. But still, people were protesting because

224
00:15:31.630 --> 00:15:36.309 
the data that was used to believe this algorithm was

225
00:15:36.310 --> 00:15:40.989 
the data that the police were used to during the past few years

226
00:15:40.990 --> 00:15:44.109 
to the arrest record and the hottest spot of crimes.

227
00:15:44.110 --> 00:15:48.669 
And you can imagine when our police could be

228
00:15:48.670 --> 00:15:53.379 
racist and the data that is created by the decisions on racial have racial

229
00:15:53.380 --> 00:15:58.359 
is racially biased. So the problem is that if these data has been using

230
00:15:58.360 --> 00:16:01.209 
to develop the algorithm, even the algorithm itself is not bias.

231
00:16:01.210 --> 00:16:04.600 
It's going to learn and it's going to predict more hotspots that are

232
00:16:06.400 --> 00:16:08.889 
in the neighborhoods of people of color.

233
00:16:08.890 --> 00:16:13.389 
So what ended up in the suspension and this great movement is education

234
00:16:13.390 --> 00:16:17.949 
and awareness. And we hope to be bringing more of these materials in

235
00:16:17.950 --> 00:16:22.719 
the right places and in the context we can have more engaged citizens.

236
00:16:22.720 --> 00:16:27.249 
But I want to also invite you to take it a step further back to where

237
00:16:27.250 --> 00:16:31.779 
we learn things. So if it's about the context, think about where you first

238
00:16:31.780 --> 00:16:32.799 
learned about math.

239
00:16:33.900 --> 00:16:38.579 
It was not when you became an adult, you learned in elementary school

240
00:16:38.580 --> 00:16:43.109 
and then getting more advanced education and you move

241
00:16:43.110 --> 00:16:45.089 
forward in your K-12.

242
00:16:45.090 --> 00:16:50.159 
So the idea is that why we don't educate people, particularly youth and teenagers,

243
00:16:50.160 --> 00:16:54.809 
to also learn about these algorithms and particularly the inequity and biases

244
00:16:54.810 --> 00:16:58.499 
they can introduce because they're going to be the next generation.

245
00:16:58.500 --> 00:17:02.129 
And, yes, we need to teach the adults who are already in the society and they're not

246
00:17:02.130 --> 00:17:06.719 
going to go back to school to read. And it's not easy as we have curriculum designed

247
00:17:06.720 --> 00:17:10.679 
for college students or university students who are in computer science or relevant

248
00:17:10.680 --> 00:17:15.279 
areas. But it's not about only these people, it's about everybody, because

249
00:17:15.280 --> 00:17:17.189 
there's algorithms are now affecting everybody.

250
00:17:17.190 --> 00:17:21.689 
Teenagers are using Instagram, Snapchat and even just the filtering process

251
00:17:21.690 --> 00:17:26.189 
that they get through. These tools can affect their lives significantly, let alone the

252
00:17:26.190 --> 00:17:29.999 
public algorithm's and affecting more even high stakes situations.

253
00:17:30.000 --> 00:17:32.309 
So how we can do that?

254
00:17:32.310 --> 00:17:36.599 
I remember I was discussing this idea and people were saying how you can teach a medium

255
00:17:36.600 --> 00:17:39.689 
of school or even a high school about these algorithms and coding.

256
00:17:39.690 --> 00:17:42.329 
And it was like these people are powerful.

257
00:17:43.380 --> 00:17:48.899 
They can actually the path of a system you particularly people in learning

258
00:17:48.900 --> 00:17:53.939 
and education, might have heard about last year's algorithmic

259
00:17:53.940 --> 00:17:59.339 
grading failure in UK, which was a result of

260
00:17:59.340 --> 00:18:02.939 
the pandemic hit, and that they couldn't really have enough teachers.

261
00:18:02.940 --> 00:18:07.259 
So what they did is that they use an algorithm to predict who gets to get to college or

262
00:18:07.260 --> 00:18:11.729 
not. And if students learn about that, they didn't learn about the racial bias and

263
00:18:11.730 --> 00:18:13.739 
other biases algorithm could have.

264
00:18:13.740 --> 00:18:18.269 
And they started protesting that the Department of Education had

265
00:18:18.270 --> 00:18:22.049 
to put the algorithm in suspension.

266
00:18:22.050 --> 00:18:26.479 
And I can't agree more with what here it is, a Guardian article says of why it is

267
00:18:26.480 --> 00:18:29.189 
still going on is the future political protest.

268
00:18:29.190 --> 00:18:34.199 
It's going to be a future when people are getting insurance coverage or

269
00:18:34.200 --> 00:18:38.729 
denials while going, which they are already getting like police reports.

270
00:18:38.730 --> 00:18:41.219 
Every aspect of our life is going to be affected with that.

271
00:18:41.220 --> 00:18:45.959 
But we need to prepare the right people to interact with this.

272
00:18:45.960 --> 00:18:48.239 
If they are not aware they can defend their rights.

273
00:18:48.240 --> 00:18:52.799 
They catch really changed a path of the where the society should go.

274
00:18:52.800 --> 00:18:57.359 
So we are in the process of arranging workshops

275
00:18:57.360 --> 00:19:02.159 
with K-12 students as well as their families, bringing them together

276
00:19:02.160 --> 00:19:06.389 
because it's all going to particular public language affecting both to understand what

277
00:19:06.390 --> 00:19:09.000 
they know and how they can actually

278
00:19:11.010 --> 00:19:14.699 
understand is algorithm's in a way that they can have an impact on.

279
00:19:14.700 --> 00:19:18.989 
And with that, we actually have a high schooler also being in our group helping us to

280
00:19:18.990 --> 00:19:22.919 
understand what a high schooler would think about the systems, which I think is

281
00:19:22.920 --> 00:19:27.359 
important, that when we design these educational

282
00:19:27.360 --> 00:19:32.369 
material for them, we have them say what's going on, otherwise we won't be able to

283
00:19:32.370 --> 00:19:34.589 
understand and be in their shoes.

284
00:19:36.180 --> 00:19:40.769 
So with that, I'm going to talk about fairness, of course, which is very

285
00:19:40.770 --> 00:19:45.419 
relevant as another line of work, trying

286
00:19:45.420 --> 00:19:48.959 
to bring accountability to this algorithmic systems.

287
00:19:48.960 --> 00:19:53.969 
So do bring fairness to the system. We have two steps we need to find biases

288
00:19:53.970 --> 00:19:57.390 
or potential biases, and then we need to mitigate or fix them.

289
00:19:58.960 --> 00:20:03.669 
Would we talk about bias mitigation, some of you might heard about this term

290
00:20:03.670 --> 00:20:07.569 
coined by this article a few years ago called All These Ingall Goydos.

291
00:20:07.570 --> 00:20:12.519 
So we've had audits in many housings or like job

292
00:20:12.520 --> 00:20:17.019 
applications that offline or online agencies try to understand

293
00:20:17.020 --> 00:20:20.829 
if there's discrimination between people. So assume that now we have algorithms making

294
00:20:20.830 --> 00:20:25.369 
those decisions and we need to audit them because they could be as boyos.

295
00:20:25.370 --> 00:20:29.959 
As human and actually but their bias could be much more harmful than

296
00:20:29.960 --> 00:20:34.429 
human because they are at a scale they are making decisions in large companies or

297
00:20:34.430 --> 00:20:38.929 
like trying to decide like when ten thousands of students would go

298
00:20:38.930 --> 00:20:40.099 
in a school district.

299
00:20:41.460 --> 00:20:46.079 
So for all of these efforts, I can talk about

300
00:20:46.080 --> 00:20:50.459 
the many, many work has been done during the past few years trying to find buyers.

301
00:20:50.460 --> 00:20:54.779 
So these are just some examples, for example, understanding political biases and search

302
00:20:54.780 --> 00:20:59.489 
engines or gender discrimination in housing platforms or reading

303
00:20:59.490 --> 00:21:04.109 
biases in algorithms that try to filter small businesses using

304
00:21:04.110 --> 00:21:08.789 
some platforms. And all of these efforts are necessary and essential for

305
00:21:08.790 --> 00:21:12.299 
and I've been involved in many of those because I believe in auditing and I think all the

306
00:21:12.300 --> 00:21:17.009 
things necessary for a society that can be engaged and mitigate those biases

307
00:21:17.010 --> 00:21:21.659 
by algorithms. However, there is some shortcomings

308
00:21:21.660 --> 00:21:26.219 
here for coming here for these systems because they need third party intervention,

309
00:21:26.220 --> 00:21:30.869 
meaning that all these efforts I've discussed, they are done by

310
00:21:30.870 --> 00:21:34.259 
experts, by people outside the system.

311
00:21:34.260 --> 00:21:38.969 
I might be a Facebook users audit Facebook, for example, but instead I'm doing it

312
00:21:38.970 --> 00:21:43.919 
in the shoes of an auditor, not an actual user.

313
00:21:43.920 --> 00:21:48.779 
But the problem is that not all biases are detectable by experts.

314
00:21:48.780 --> 00:21:53.279 
This is what we call emergent bias is a type of harmful behavior

315
00:21:53.280 --> 00:21:57.719 
that emerge in the context of views and in the presence of social dynamics.

316
00:21:57.720 --> 00:22:02.609 
It could just happen in the change of norms and over time.

317
00:22:02.610 --> 00:22:07.259 
So these biases can't be found when a system is just that is in the development

318
00:22:07.260 --> 00:22:11.849 
process or just developed or even when it's developed, you can you can't think that, hey,

319
00:22:11.850 --> 00:22:14.279 
we can have a routine audit and we're going to find that.

320
00:22:14.280 --> 00:22:15.280 
This is an example.

321
00:22:16.440 --> 00:22:21.569 
A few months ago, a regular user of Twitter found about the cropping

322
00:22:21.570 --> 00:22:26.039 
issue, which if you have a picture, there's a cropping algorithm that suggests you'll

323
00:22:26.040 --> 00:22:29.309 
wish for to remove the but to keep if it's oversize.

324
00:22:29.310 --> 00:22:34.199 
And it turned out that it's if there are two people, one person of color and one white,

325
00:22:34.200 --> 00:22:37.919 
it would suggest to remove it would crop automatically the person of color.

326
00:22:37.920 --> 00:22:42.669 
And then a trend, a trend happened on Twitter

327
00:22:42.670 --> 00:22:45.239 
with people start to test this hypothesis.

328
00:22:45.240 --> 00:22:49.709 
People started thinking about different ideas like, hey, sounds that it might be

329
00:22:49.710 --> 00:22:54.329 
the color of the shirt or people start to have black versus white

330
00:22:54.330 --> 00:22:59.099 
cars like what's going on? And it turned out, yes, that's about color and it's removing

331
00:22:59.100 --> 00:23:02.699 
people or pits of color out of the picture.

332
00:23:02.700 --> 00:23:07.479 
And then Twitter in a day apologize and actually announce that we did have all

333
00:23:07.480 --> 00:23:12.119 
had all the talk about this algorithm, but we did not found

334
00:23:12.120 --> 00:23:14.489 
I did not find about this problem.

335
00:23:14.490 --> 00:23:19.289 
But what happened was that everyday users, they stumbled upon this problem.

336
00:23:19.290 --> 00:23:23.339 
The public tried to publicize about it and bring awareness.

337
00:23:23.340 --> 00:23:27.779 
And then Twitter had to fix it in a day so that the power of

338
00:23:27.780 --> 00:23:28.780 
everyday users.

339
00:23:29.890 --> 00:23:33.899 
And users have been shown really good attitudes in the past, this is like two examples of

340
00:23:33.900 --> 00:23:38.409 
trading platforms. I worked for a few years on this one here

341
00:23:38.410 --> 00:23:42.999 
as a booking dotcom that for a while, rather than having ratings from two points from

342
00:23:43.000 --> 00:23:45.729 
one to 10, that I would go for a hotel.

343
00:23:45.730 --> 00:23:50.469 
It would make it to a point five to 10, which is screwing up the ratings for basketballs.

344
00:23:50.470 --> 00:23:55.449 
And here on this side, you see Yelp and people discussing about

345
00:23:55.450 --> 00:23:59.979 
its mysterious algorithm and how people are

346
00:23:59.980 --> 00:24:04.290 
suspicious that Twitter tweaks,

347
00:24:05.810 --> 00:24:10.239 
tweaks this algorithm to have businesses that do not advertise with them, to

348
00:24:10.240 --> 00:24:14.559 
have more good videos filtered or more bad reviews on filter.

349
00:24:14.560 --> 00:24:18.759 
And these are all the discussions happen by everyday users without any guidance.

350
00:24:18.760 --> 00:24:23.769 
And they ended up being very impactful in the course of these platforms design.

351
00:24:23.770 --> 00:24:26.679 
So this is harder to think about a different perspective.

352
00:24:26.680 --> 00:24:31.119 
If we can educate people to know about this algorithms, we can actually maybe have

353
00:24:31.120 --> 00:24:34.359 
them as actual auditors of the systems.

354
00:24:34.360 --> 00:24:38.589 
What if everyday users can detect and understand that interrogatives problematic machine

355
00:24:38.590 --> 00:24:42.009 
behaviors Veldon day to day interaction with the systems?

356
00:24:42.010 --> 00:24:45.939 
And it depends on many factors like how much they have algorithmic experience or

357
00:24:45.940 --> 00:24:50.229 
knowledge. It's a collective effort or not, because, for example, the Twitter one was a

358
00:24:50.230 --> 00:24:52.569 
collective effort and it ended up in meals in a day.

359
00:24:52.570 --> 00:24:57.189 
But the example I told you was very individual, the way people connect

360
00:24:57.190 --> 00:25:02.559 
to each other. I just stayed for maybe few years until it was fixed.

361
00:25:02.560 --> 00:25:04.959 
Also, it's about organic as of these US.

362
00:25:04.960 --> 00:25:09.819 
Sometimes they just start by a person and then continued by others.

363
00:25:09.820 --> 00:25:14.589 
And it's important to keep these features there, but also try to understand

364
00:25:14.590 --> 00:25:19.269 
what's the lifetime of this process which help these can be can help people educate

365
00:25:19.270 --> 00:25:23.859 
them to be their own, like the old police

366
00:25:23.860 --> 00:25:28.339 
of the systems, like, you know, like we talk about the citizens being a

367
00:25:28.340 --> 00:25:32.469 
so to the part. And also they need to be aware and also act on the systems.

368
00:25:32.470 --> 00:25:37.479 
This is a of to be found from an everyday auditing

369
00:25:37.480 --> 00:25:38.480 
processes.

370
00:25:39.790 --> 00:25:44.659 
People started usually initiating displays, stumbling upon some problematic

371
00:25:44.660 --> 00:25:48.909 
behavior. Then they try awareness to raise awareness.

372
00:25:48.910 --> 00:25:53.679 
That's at least the the thing that they can do without these platforms.

373
00:25:53.680 --> 00:25:58.099 
And then they start to test hypothesis hypotheses about how the system works, like

374
00:25:58.100 --> 00:26:01.539 
the Twitter example, people were trying to think about these difficult people.

375
00:26:01.540 --> 00:26:03.609 
Is it did they shoot the people?

376
00:26:03.610 --> 00:26:06.249 
Where is it about animals to or not?

377
00:26:06.250 --> 00:26:08.289 
And then they try to remedy it if they can.

378
00:26:08.290 --> 00:26:12.579 
They try to publicize about that. They might actually try to tweak the algorithm to at

379
00:26:12.580 --> 00:26:17.259 
least change its behavior, even in smaller cases.

380
00:26:17.260 --> 00:26:21.849 
So. What is important here is that how we can empower the everyday

381
00:26:21.850 --> 00:26:26.269 
users through education. We've discussed we've been discussing about different

382
00:26:26.270 --> 00:26:28.839 
guidance. People can get community guidance.

383
00:26:28.840 --> 00:26:33.769 
You know, we can have more experts try to build educational materials about bias and

384
00:26:33.770 --> 00:26:38.199 
in the systems and how people can be exposed in the

385
00:26:38.200 --> 00:26:42.759 
systems themselves. And like we can have algorithms to guide people like

386
00:26:42.760 --> 00:26:47.079 
these systems could be potentially biased or show them where to go and find biases.

387
00:26:47.080 --> 00:26:51.639 
It's also important how we can help people with organizing efforts, how we educate

388
00:26:51.640 --> 00:26:55.479 
them to find the right roles for auditing a system.

389
00:26:55.480 --> 00:26:59.379 
At the same time, it's important that we find the right and timely interventions.

390
00:26:59.380 --> 00:27:04.299 
Otherwise, if they intervene the law in these efforts, we might ruin the organicness

391
00:27:04.300 --> 00:27:09.309 
or the collectiveness of that. So it's important to find the right level of education

392
00:27:09.310 --> 00:27:13.959 
without stopping the inherent

393
00:27:13.960 --> 00:27:15.583 
features of these outlets.

394
00:27:17.120 --> 00:27:18.499 
So as the last part.

395
00:27:19.900 --> 00:27:24.459 
Even if you find biases, be it everydayusers, be it public, with the help

396
00:27:24.460 --> 00:27:26.819 
of experts, we still need to mitigate them.

397
00:27:27.880 --> 00:27:32.439 
And I'm going to argue that while there has been a lot of research,

398
00:27:32.440 --> 00:27:37.119 
which is essential, again in in our field about mitigating

399
00:27:37.120 --> 00:27:40.765 
these biases by building fairer databases or building fairer algorithms.

400
00:27:43.120 --> 00:27:45.073 
Not all biases are going to be mitigated.

401
00:27:46.300 --> 00:27:50.919 
Unfortunately, we have or maybe fortunately, but have different values of fairness

402
00:27:50.920 --> 00:27:52.989 
and they can be in conflict with each other.

403
00:27:52.990 --> 00:27:57.429 
I'm going to give you an example. A few years ago, research has shown that

404
00:27:57.430 --> 00:28:01.999 
if you search for seeing on Google, it's going to just show you about

405
00:28:02.000 --> 00:28:05.139 
11 per cent of women pictures and the rest are male.

406
00:28:05.140 --> 00:28:09.879 
And that time even it was not discussed about non binary or trans people,

407
00:28:09.880 --> 00:28:14.919 
which is another issue. But the actual percentage of sales for women was twenty-seven

408
00:28:14.920 --> 00:28:19.719 
per cent. So there was an argument here that, OK, what should we do?

409
00:28:19.720 --> 00:28:25.299 
Should Google tweak its algorithm to make it very similar to what the society

410
00:28:25.300 --> 00:28:27.969 
we have in terms of percentage of sales?

411
00:28:27.970 --> 00:28:30.019 
But then there were other research found.

412
00:28:30.020 --> 00:28:34.179 
Oh, look, there are also areas that men are underrepresented in.

413
00:28:34.180 --> 00:28:36.159 
What should we do about all of these areas?

414
00:28:36.160 --> 00:28:41.469 
And there is a similar risk for other jobs that women are underrepresented.

415
00:28:41.470 --> 00:28:44.879 
So the argument should make it about less like 50 50.

416
00:28:44.880 --> 00:28:48.699 
Also, what about the number of non-minoritytrans people?

417
00:28:48.700 --> 00:28:51.729 
And then it was like, is there a right solution here?

418
00:28:51.730 --> 00:28:53.350 
Because each of these definitions bring

419
00:29:20.160 --> 00:29:20.172 
ssome type of fairness, but it ruins another type of fairness.

420
00:29:20.173 --> 00:29:20.207 
I was actually one of the people when we were discussing in our groups, I was like, I was not the advocate of making fifty fifty or whatever percentage you want to have for women

421
00:29:20.208 --> 00:29:20.245 
and men and then also including minority because. Because if I if I have a kid and he's a teenager, for example, and he goes to this Web page and say, oh, this is a very nice war,

422
00:29:20.246 --> 00:29:22.229 
we have very balanced people in this every job, that's not true. That's not what we have in society. So. Right. I don't

423
00:29:22.230 --> 00:29:27.149 
want to like my friend, my my kids or like my mom

424
00:29:27.150 --> 00:29:31.709 
would like go to the society and say, oh, the life is great, but it's

425
00:29:31.710 --> 00:29:33.989 
not that much crude in the offline board.

426
00:29:33.990 --> 00:29:36.149 
It's a society. We have these biases.

427
00:29:36.150 --> 00:29:39.419 
So we need to be aware of these biases at the same time.

428
00:29:39.420 --> 00:29:43.649 
So how we can mitigate this bias, there's no right answer to that.

429
00:29:43.650 --> 00:29:48.119 
There's no one answer to that. And then that's the idea that I think

430
00:29:48.120 --> 00:29:50.999 
I want to end this talk by bias, transparency.

431
00:29:51.000 --> 00:29:54.149 
So we talked about transparency. We talked about bias and fairness.

432
00:29:54.150 --> 00:29:56.759 
I think what we need is to bring this together.

433
00:29:56.760 --> 00:30:01.289 
We need to censor bias and fairness in the design of these systems.

434
00:30:01.290 --> 00:30:05.339 
As long as society is biased, we are going to have bias systems.

435
00:30:05.340 --> 00:30:09.749 
And I'm not saying this to scare people off these systems.

436
00:30:09.750 --> 00:30:14.399 
I'm a fan of algorithmic systems. I think they are essential and important in our

437
00:30:14.400 --> 00:30:18.839 
everyday life. We got we went a long way with them

438
00:30:18.840 --> 00:30:22.949 
because they enabled many new innovations and help for society.

439
00:30:22.950 --> 00:30:25.649 
But at the same time, we need to be aware of that.

440
00:30:25.650 --> 00:30:26.939 
And this is an example.

441
00:30:26.940 --> 00:30:30.889 
We are working in one of the public algorithmic systems here in Pittsburgh to gain the

442
00:30:30.890 --> 00:30:35.489 
preacher terrorist preacher risk assessment tool, which is the idea

443
00:30:35.490 --> 00:30:37.679 
is that the judges are using these tools.

444
00:30:37.680 --> 00:30:42.089 
Should we tell them that this risk assessment tools have been shown to be racially-biased

445
00:30:42.090 --> 00:30:45.209 
, for example, in the past or other biases?

446
00:30:45.210 --> 00:30:50.249 
And it's similar to if you tell a judge that, hey, the jury had made a decision to

447
00:30:50.250 --> 00:30:54.449 
not allow a bail for this person has been shown to be racially biased in the past.

448
00:30:54.450 --> 00:30:59.039 
We can only do that in offline or if we know we not at that percent as a jury.

449
00:30:59.040 --> 00:31:03.989 
But if you have a system that will even be fixed up by us to think about, that might

450
00:31:03.990 --> 00:31:07.979 
get a bias again in other ways. So what if the judge knows about that?

451
00:31:07.980 --> 00:31:10.709 
Keep that in mind when they are making a final decision.

452
00:31:10.710 --> 00:31:14.609 
But it's a very challenging question. You know, like what if the judge thinks I don't

453
00:31:14.610 --> 00:31:18.809 
want to use the system anymore? And what if the judge knowledge is not enough for making

454
00:31:18.810 --> 00:31:23.459 
a decision? So I don't some of these systems might have existential

455
00:31:23.460 --> 00:31:26.879 
problems, like, for example, predictive policing is one of them should even have a

456
00:31:26.880 --> 00:31:31.409 
predictive policing in the border. Not some of the systems are better to be there,

457
00:31:31.410 --> 00:31:36.659 
but they need to be designed in a way that make sure that users who are using the systems

458
00:31:36.660 --> 00:31:39.749 
are informed about the challenges the system.

459
00:31:39.750 --> 00:31:44.639 
With that, I'm going to invite you all as a community that is very active

460
00:31:44.640 --> 00:31:49.109 
in learning and education to think about and revisiting these topics of transparency and

461
00:31:49.110 --> 00:31:53.579 
fairness via the lines of education, because, without education, we can't have

462
00:31:53.580 --> 00:31:58.139 
it for society and for society to take action on these algorithmic

463
00:31:58.140 --> 00:32:00.730 
systems when they create harmful behavio

464
00:33:29.990 --> 00:33:29.991 
ur.

465
00:33:29.992 --> 00:33:30.025 
With that, I'd be happy to open up for questions. Thank you very much. Thank you so much for that really excellent and provocative talk. I invite folks to raise their hand and I

466
00:33:30.026 --> 00:33:30.060 
will search through the participant list and then you can speak up or you can also use the chat as well. And I think in the meantime of note, we've already got a hand. Marcus,

467
00:33:30.061 --> 00:33:30.093 
please, can you hear me? Yes, yeah, a great talk. I was I was wondering a little bit about issues around literacy and information literacy and how to educate people in that in

468
00:33:30.094 --> 00:33:30.125 
that sense. So first, a question of understanding. If I got it right. I mean, for me, what you're talking about is also something like algorithmic literacy. So we need to better

469
00:33:30.126 --> 00:33:30.159 
learn and understand how these algorithms work and what they do and if they're biased. So can you explain a little bit better or more about I mean, how would you educate or how

470
00:33:30.160 --> 00:33:32.179 
are you educating people to understand these black boxes? Because these are complex things. So how would that work and how do you

471
00:33:32.180 --> 00:33:35.539 
educate people on that matter?

472
00:33:35.540 --> 00:33:39.979 
That's an excellent question, and unfortunately, it's a it's a very

473
00:33:39.980 --> 00:33:44.329 
challenging question as well, because it's not even people who don't understand its

474
00:33:44.330 --> 00:33:46.879 
algorithms. The developers of these algorithms don't understand.

475
00:33:46.880 --> 00:33:51.189 
I've been in industry and I know how it's hard because many of these are going to put you

476
00:33:51.190 --> 00:33:55.549 
in machine learning algorithms. You just give them input and they learn and then give you

477
00:33:55.550 --> 00:33:58.459 
an output. You can't explain them in human language.

478
00:33:58.460 --> 00:34:02.659 
The features they create are meaningless to us all is meaningless, meaningful to

479
00:34:02.660 --> 00:34:07.159 
algorithms. So it's an open-endedquestion about how we

480
00:34:07.160 --> 00:34:11.569 
can educate people about this, about the development of these algorithms.

481
00:34:11.570 --> 00:34:16.009 
Might not know what's exactly going on, but the good news is that you don't need

482
00:34:16.010 --> 00:34:20.839 
to give people a full, transparent version of an item that they can actually

483
00:34:20.840 --> 00:34:22.730 
act on. That I've been doing.

484
00:34:23.810 --> 00:34:27.439 
I've been doing studies on the level of transparency people want about the specific

485
00:34:27.440 --> 00:34:30.059 
algorithms, for example, in advertising algorithms.

486
00:34:30.060 --> 00:34:33.738 
What I found is that even if you give the filter out to people, this is the reason this

487
00:34:33.739 --> 00:34:38.149 
algorithm targeted you to get this ad and then put them in the shoes

488
00:34:38.150 --> 00:34:42.888 
of a person who is explaining that algorithm or like even the automatic

489
00:34:42.889 --> 00:34:47.419 
generated algorithmic explanations people don't want all, because

490
00:34:47.420 --> 00:34:51.499 
it's getting overwhelming. It's not getting the right message.

491
00:34:51.500 --> 00:34:56.059 
So the idea is that even if you have the data, the human

492
00:34:56.060 --> 00:35:00.049 
exploitable data to show to users, it's not about showing everything, it's about finding

493
00:35:00.050 --> 00:35:02.329 
the right level of transparency.

494
00:35:02.330 --> 00:35:06.139 
But that goes back to the question of what is the right level of transparency.

495
00:35:06.140 --> 00:35:07.140 
It's very

496
00:35:45.500 --> 00:35:45.531 
context-oriented. It depends on the decision to be made. So there is no one answer fits all, you know, a judge needs a different level of transparency, probably from the person

497
00:35:45.532 --> 00:35:45.569 
who is asking for a bail. So it depends on who is the user, what they need. But I think the most important part is that even if you can't describe the details of a system, which I

498
00:35:45.570 --> 00:35:45.600 
don't think even you should, because that's not the goal of that system, you should make users aware of the challenges. Like, for example, hey, this system could be biased. This

499
00:35:45.601 --> 00:35:47.899 
is could cause these wrong decisions. And that that can't happen without participatory practice.

500
00:35:47.900 --> 00:35:52.429 
You need to have users, developers and researchers, designers

501
00:35:52.430 --> 00:35:55.369 
all in the same room discussing your learning.

502
00:35:55.370 --> 00:35:59.359 
And that's how we are trying now to develop that material for people here, for example,

503
00:35:59.360 --> 00:36:03.859 
in Pittsburgh and some of the systems is that we have with the high school example, we

504
00:36:03.860 --> 00:36:07.759 
are bringing high schoolers in participatory design workshops.

505
00:36:07.760 --> 00:36:09.769 
So these are very context-oriented.

506
00:36:09.770 --> 00:36:14.209 
But you need to engage all the users. You can't just have your own algorithm and design

507
00:36:14.210 --> 00:36:16.939 
the educational material to send it out. It's not going to work.

508
00:36:16.940 --> 00:36:19.250 
You need to work on that on iterations.

509
00:36:22.180 --> 00:36:26.959 
Thanks. My guess is you want to ask your follow up and

510
00:36:26.960 --> 00:36:31.469 
yeah, I was just wondering, I mean, participatory.

511
00:36:31.470 --> 00:36:36.209 
I wonder if you have something like an interest in courses on information literacy or you

512
00:36:36.210 --> 00:36:40.229 
have certain dimensions or met the criteria you educate people on.

513
00:36:40.230 --> 00:36:42.509 
How valid is the source, for example?

514
00:36:42.510 --> 00:36:46.979 
Yeah. So do you have an analogy used in algorithmic

515
00:36:46.980 --> 00:36:51.340 
transparency or literacy like you mean it's

516
00:36:52.440 --> 00:36:54.750 
you can't say this is the right material for people

517
00:37:01.300 --> 00:37:03.549 
?Nadia, you educate kids, for example, on the question, like, where does this information

518
00:37:03.550 --> 00:37:08.449 
come from? So that's one way to evaluate the validity

519
00:37:08.450 --> 00:37:13.239 
and get estimates and develop a sense for how true

520
00:37:13.240 --> 00:37:17.949 
or. Yeah, so and I wonder if you have something like that for

521
00:37:17.950 --> 00:37:19.510 
algorithms, but.

522
00:37:21.230 --> 00:37:26.299 
Yeah, I mean, I don't think we have I mean, like a terminology

523
00:37:26.300 --> 00:37:30.829 
or a specific method for that, unfortunately, but what we are doing right now

524
00:37:30.830 --> 00:37:35.309 
is pre and post evaluations, you know, understanding what

525
00:37:35.310 --> 00:37:38.959 
other students or the public know before and then what they after.

526
00:37:38.960 --> 00:37:43.369 
But that's just a very simple way to measure dad, but

527
00:37:44.780 --> 00:37:47.179 
also in terms of validating the resources.

528
00:37:47.180 --> 00:37:51.589 
I think what we are doing is that validating different types of users.

529
00:37:51.590 --> 00:37:56.059 
Again, like we we build materials with experts

530
00:37:56.060 --> 00:38:00.739 
in that area, but then again, bring like users or everyday, for example,

531
00:38:00.740 --> 00:38:03.529 
public people to see if they understand that.

532
00:38:03.530 --> 00:38:06.259 
So comprehension of those also is something.

533
00:38:06.260 --> 00:38:10.759 
But unfortunately, this is a very new area or maybe, fortunately,this

534
00:38:10.760 --> 00:38:12.350 
starting there is no.

535
00:38:13.430 --> 00:38:17.059 
Like standard about like it's not like math that

536
00:38:18.160 --> 00:38:22.609 
is developmental. This is like where everyone wants to go, it's very new.

537
00:38:22.610 --> 00:38:27.139 
And I know Amy is working on this. So, Amy, jump in

538
00:38:27.140 --> 00:38:31.079 
if you have any other suggestions about how to value the material.

539
00:38:31.080 --> 00:38:36.050 
But hopefully,this is going to be more on this literacy.

540
00:38:37.690 --> 00:38:43.229 
Actually, I think I'm going to jump on to make sure our other questions get asked.

541
00:38:43.230 --> 00:38:47.699 
So, Asheesh, would you like to ask your question out loud, OK?

542
00:38:47.700 --> 00:38:49.889 
Yes. So are you able to hear me?

543
00:38:49.890 --> 00:38:54.329 
Yes. OK, so you talked about the example of

544
00:38:54.330 --> 00:38:59.099 
the Google CEO images, you know, where we really don't know

545
00:38:59.100 --> 00:39:01.709 
what should be the ideal criteria? Should it be 50 per cent?

546
00:39:01.710 --> 00:39:02.710 
Should it be 27

547
00:39:10.920 --> 00:39:12.929 
per centin such cases when systems do show bias? You talked about bias, transparency. So can we see that if we

548
00:39:12.930 --> 00:39:17.719 
are maybe if we have any statement or any trigger

549
00:39:17.720 --> 00:39:22.739 
which says, you know, this this this has a bias, transparency,

550
00:39:22.740 --> 00:39:27.419 
upfront message, then maybe that takes into account the system?

551
00:39:27.420 --> 00:39:31.590 
And does that address some systems which have such biased?

552
00:39:32.670 --> 00:39:36.959 
Know, that's the whole that's the hope that the for example, what if the idea was like

553
00:39:36.960 --> 00:39:39.809 
what if Google also says, hey, the searches, the search results?

554
00:39:39.810 --> 00:39:44.389 
I guess, for example, I could have, like, you know, gender bias or, for example,

555
00:39:44.390 --> 00:39:45.390 
racial bias.

556
00:39:46.560 --> 00:39:50.339 
Again, we don't want that. I mean, if you find about this bias to try to fix it, but

557
00:39:50.340 --> 00:39:54.989 
sometimes it's not fixable. But the challenge here is that it might mitigate one

558
00:39:54.990 --> 00:39:56.789 
risk, but it can add other risks.

559
00:39:56.790 --> 00:39:59.429 
What if people don't trust the systems at all?

560
00:39:59.430 --> 00:40:03.929 
Then we have this phenomenal tool from two sides of the spectrum

561
00:40:03.930 --> 00:40:07.799 
are called algorithm appreciation versus algorithm aversion.

562
00:40:07.800 --> 00:40:12.209 
So I got a reputation is that we just trust algorithms blindly because we think they are

563
00:40:12.210 --> 00:40:16.559 
all-knowingand all-powerful. And this has been shown as a phenomenon in research.

564
00:40:16.560 --> 00:40:21.059 
At the same time, if people see an algorithm that makes them one mistake, they start to

565
00:40:21.060 --> 00:40:23.279 
lose their trust completely.

566
00:40:23.280 --> 00:40:27.421 
But they don't do that with people. So they really like to see it.

567
00:40:27.422 --> 00:40:30.029 
The reason is that if my friend makes a mistake, it's a human.

568
00:40:30.030 --> 00:40:34.799 
So you find any mistakes, I can still trust him, but if it's all the same.

569
00:40:34.800 --> 00:40:39.269 
But if I am going to make a mistake, even if it's small

570
00:40:39.270 --> 00:40:42.209 
or large, you are like this is math.

571
00:40:42.210 --> 00:40:44.429 
This is rigid, which is not true that they learn.

572
00:40:44.430 --> 00:40:47.789 
So if you give them feedback, they learn and they improve themselves.

573
00:40:47.790 --> 00:40:52.409 
But people don't tend to believe in that system anymoreat all.

574
00:40:52.410 --> 00:40:57.029 
So finding the right level, the sweet spot in the spectrum is hard.

575
00:40:57.030 --> 00:41:01.559 
But if you can find that, yes, I think bias, transparency definitely

576
00:41:01.560 --> 00:41:06.259 
can mitigate the risk of bias. But every day, every time you

577
00:41:06.260 --> 00:41:11.009 
say, hey, this system is biased, hey, this is like why I'm using this, then.

578
00:41:11.010 --> 00:41:15.419 
So you need to find the right level of information to give it to users.

579
00:41:15.420 --> 00:41:19.109 
And also what type of users maybe it George needs to know that, but not maybe not a

580
00:41:19.110 --> 00:41:20.110 
criminal.

581
00:41:21.460 --> 00:41:24.079 
Thank you. Does that answer your question?

582
00:41:24.080 --> 00:41:25.179 
Yes, yes, thank you.

583
00:41:27.600 --> 00:41:32.099 
We will be moving on to our closing session in just a second, but I want to take

584
00:41:32.100 --> 00:41:35.129 
this chance to ask a final question.

585
00:41:35.130 --> 00:41:40.349 
So many of our systems contain many algorithms.

586
00:41:40.350 --> 00:41:45.179 
And you look at something like the protests you showed

587
00:41:45.180 --> 00:41:49.679 
in the U.K. against the computer system, determining grades and

588
00:41:49.680 --> 00:41:54.269 
so on and so forth. I think your last answer started to get into

589
00:41:54.270 --> 00:41:57.510 
this a little bit about aversion versus

590
00:41:58.710 --> 00:42:03.269 
appreciation. But how do we get to

591
00:42:03.270 --> 00:42:08.309 
a place where people can say are unwilling

592
00:42:08.310 --> 00:42:12.809 
to use the system even though it has flaws or should

593
00:42:12.810 --> 00:42:17.849 
we not be in that place? What's the takeaway sort of for a community that uses

594
00:42:17.850 --> 00:42:22.289 
quite a lot of algorithms to make a lot of, you know, some sometimes more

595
00:42:22.290 --> 00:42:26.549 
high stakes, many times lower stakes evaluations?

596
00:42:26.550 --> 00:42:30.869 
Question. And honestly, I don't have one answer to that, but finding that sweet spot is

597
00:42:30.870 --> 00:42:35.519 
going to be hard. But that's why I brought education, because if you teach someone

598
00:42:35.520 --> 00:42:39.389 
like 20 years from now, if there is critical many schools about this algorithm, the

599
00:42:39.390 --> 00:42:42.089 
mistakes they did today, they become a part of our life.

600
00:42:42.090 --> 00:42:46.619 
So whenever we use them, it's not like, hey, I don't use it, just teach people.

601
00:42:46.620 --> 00:42:49.709 
And that takes time. It's like teaching math, teaching literacy.

602
00:42:49.710 --> 00:42:55.019 
So I think with that education, if you can bring it in from earlier,

603
00:42:55.020 --> 00:42:59.619 
that be a start because these next generation are going to be living with algorithms

604
00:42:59.620 --> 00:43:02.219 
that's different from our generation. So we are learning.

605
00:43:02.220 --> 00:43:06.029 
That's why we need both public places teaching people, but at the same time, school

606
00:43:06.030 --> 00:43:11.219 
schools. But I think if we do that, people are going to get used to algorithms as humans

607
00:43:11.220 --> 00:43:15.419 
like they are going to make mistakes but think they are better to be there than not to be

608
00:43:15.420 --> 00:43:18.280 
there. But it's a long term effort.

609
00:43:20.200 --> 00:43:24.639 
All right, so we'll just hold our breath for another decade

610
00:43:24.640 --> 00:43:29.139 
or so until we got to the new generation that is born

611
00:43:29.140 --> 00:43:30.609 
into algorithms.

612
00:43:30.610 --> 00:43:33.159 
All right. Thank you again so much, Matahhare.

613
00:43:33.160 --> 00:43:36.129 
There's a lot of praise for this talk in the chat.

614
00:43:36.130 --> 00:43:40.860 
And I really think that it gave us a lot to think about to close out this conference.
