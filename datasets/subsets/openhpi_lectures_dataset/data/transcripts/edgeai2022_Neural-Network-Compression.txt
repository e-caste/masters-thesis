WEBVTT

1
00:00:00.440 --> 00:00:01.550 
Hello and welcome!

2
00:00:02.140 --> 00:00:03.700 
Starting from this video,

3
00:00:04.139 --> 00:00:11.939 
we will learn several methods of deep learning
model compression in the next two weeks.

4
00:00:11.939 --> 00:00:21.199 
We have already learned that transformer is a deep learning
model developed by Google researchers in 2017. It uses a self

5
00:00:21.199 --> 00:00:26.059 
attention mechanism that works well for many NLP problems.

6
00:00:27.140 --> 00:00:34.850 
The self attention can differentially weigh in
the significance of each token of the input data.

7
00:00:35.939 --> 00:00:44.869 
An example is machine translation and the self attention
will learn the correlations between each input token and

8
00:00:44.869 --> 00:00:45.950 
all the other tokens.

9
00:00:46.539 --> 00:00:53.759 
Thus, transformer has a solid capability to learn the
global context of this input sentences.

10
00:00:55.140 --> 00:01:02.460 
The basic building block that consists of this self
attention module and the feed forward neural network layers.

11
00:01:04.140 --> 00:01:12.859 
BERT model applies the encoder part of transformer and it
has become the most popular language model in recent years.

12
00:01:14.040 --> 00:01:21.290 
GPT is a is a generative pre-trained transformer,
another name of the transformer model.

13
00:01:21.299 --> 00:01:30.230 
So basically there are all transformer architectures, they
are trained with large language data sets, corpus, such

14
00:01:30.230 --> 00:01:37.060 
as the Wikipedia corpus and they achieve great success on
many downstream NLP tasks.

15
00:01:45.140 --> 00:01:51.459 
Now, I'm going to show you some example that people are
using transformer and BERT, such models

16
00:01:51.459 --> 00:02:03.379 
every day. BERT was applied to Google Search changing in
72 languages in the December 2019 to help better understand

17
00:02:03.390 --> 00:02:14.439 
the semantics of the search queries from the user.
Transformer has been applied to google translate also in
the recent years. There are 5.6

18
00:02:14.439 --> 00:02:21.750 
billion search queries per day and more than 100 billion
words are translated by Google translator a day.

19
00:02:22.939 --> 00:02:32.939 
About 60% of Google's TPU resources are applied to these
derived transformer models

20
00:02:32.939 --> 00:02:41.050 
On the one hand transformer model give us a lot
of breakthroughs on NLP tasks.

21
00:02:41.439 --> 00:02:45.719 
On the other hand, AI computing brings a lot of carbon emissions.

22
00:02:45.729 --> 00:02:54.009 
For example, training a transformer model with the help
of neural architecture search technique created carbon emissions

23
00:02:54.020 --> 00:03:01.150 
equivalent to 300 round trip flights between San Francisco
to New York city for one person.

24
00:03:02.439 --> 00:03:14.439 
Another example is the energy consumption cost of training
of a GPT3 model equals that of 43 cars or 24

25
00:03:14.439 --> 00:03:16.460 
US families per year.

26
00:03:17.439 --> 00:03:25.560 
As you can see, we definitely should not ignore the
massive impact of the large scale language model on
the environment.

27
00:03:28.039 --> 00:03:37.219 
So on the one hand, we hope that the electric energy used for
AI model training comes from clean energy sources,

28
00:03:37.229 --> 00:03:44.960 
such as solar or wind energy and google already has
some promising progress in this regard.

29
00:03:45.439 --> 00:03:54.199 
On the other hand, before we can fully utilize clean energy,
we hope to improve the efficiency of AI algorithm as much as

30
00:03:54.199 --> 00:04:01.439 
possible to reduce the negative impact on the environment.

31
00:04:01.439 --> 00:04:02.759 
As I mentioned it before,

32
00:04:03.340 --> 00:04:13.069 
deep learning models heavily rely on Gpu power and
massive computation resources. For example, the resnet-152 model

33
00:04:13.069 --> 00:04:23.540 
first time surpasses the human performance in
image classification but if we look at the number of
operations, it requires 11.3

34
00:04:23.550 --> 00:04:32.970 
billion Flops and it's apparently not sufficient for low
power devices in the NLP domain, as I already mentioned it.

35
00:04:32.980 --> 00:04:41.550 
Open AI GPT three model has 175 billion parameters pushing
the term large model to another level.

36
00:04:43.240 --> 00:04:50.459 
As you can see such neural network, deep neural networks need
to run on powerful servers in the cloud.

37
00:04:55.040 --> 00:05:04.389 
An increasing number of applications including
autonomous driving voice assistant applications
demand machine learning models

38
00:05:04.389 --> 00:05:14.610 
that can be deployed on low powered and low resource devices
and typically has substantial latency requirements.

39
00:05:14.610 --> 00:05:15.670 
In such applications

40
00:05:15.680 --> 00:05:19.459 
the notion of model compression has gathered the popularity.

41
00:05:21.839 --> 00:05:33.709 
So, the goal model compression is to take an extensive
reverence neural network and output a smaller, less
expensive compressed

42
00:05:33.709 --> 00:05:38.050 
one, that is functionally equivalent to the reference.

43
00:05:39.040 --> 00:05:42.550 
The typical techniques including compact network design,

44
00:05:43.040 --> 00:05:52.139 
So, to design a more efficient, more compact, smaller
network architecture. Knowledge Distillation involves a
teacher model

45
00:05:52.149 --> 00:05:59.970 
and a student model. Training of the student model is based
on the transferred knowledge extracted from the teacher.

46
00:05:59.980 --> 00:06:09.439 
The teacher model is usually a pre trained convolution
model. Pruning technique removes those neurons relatively

47
00:06:09.439 --> 00:06:18.769 
weakly weighted for the final predictions and we can also
apply quantization or binarization techniques to the parameters

48
00:06:18.769 --> 00:06:23.459 
of the neural networks to reduce the memory and
computation overhead.

49
00:06:26.240 --> 00:06:34.680 
We will further discuss the following four methods
in this course. In the next video, we will start the
compact network

50
00:06:34.689 --> 00:06:35.850 
architecture design.

51
00:06:38.540 --> 00:06:39.560 
Thank you for watching.
