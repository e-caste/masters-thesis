WEBVTT

1
00:00:00.640 --> 00:00:01.750 
Hello and welcome.

2
00:00:02.330 --> 00:00:06.559 
This video is the last one of our ConvNets architecture theory.

3
00:00:07.240 --> 00:00:12.060 
We will be briefly talking about the automatic neural architecture search approaches.

4
00:00:14.300 --> 00:00:24.839 
In fact, when we analyze the design of InceptionNet V3, we felt that many design choices were empirical. At the same

5
00:00:24.839 --> 00:00:33.969 
time, it can be felt that based on the manual parameter adjustment, the possible choices of various hyper parameters cannot

6
00:00:33.979 --> 00:00:35.450 
be fully explored.

7
00:00:36.840 --> 00:00:42.659 
Therefore, automatic search is a very straightforward way that can be applied here.

8
00:00:43.240 --> 00:00:47.659 
So the Google researchers soon proposed NASNet.

9
00:00:47.659 --> 00:00:57.649 
It introduced two sort of layers - the normal layer and the reduction layer created by automatic search.

10
00:00:58.539 --> 00:01:04.870 
They perform the hyper parameter (Cell) prediction using a Recurrent Neural Network.

11
00:01:04.879 --> 00:01:06.069 
So the RNN,

12
00:01:06.079 --> 00:01:17.079 
which is trained using a loss-feedback created by NASNet. It is no longer necessary to proper experts to use human knowledge

13
00:01:17.090 --> 00:01:19.650 
to build a convolutional network architecture.

14
00:01:20.150 --> 00:01:22.780 
NASNet achieved the state of the art result.

15
00:01:22.870 --> 00:01:29.150 
ImageNet classification and also the COCOA challenge for object detection.

16
00:01:32.239 --> 00:01:36.159 
This is a general overview of neural architecture search method.

17
00:01:37.140 --> 00:01:43.469 
NAS has been proven to exceed the network structure manually designed by experts.

18
00:01:43.480 --> 00:01:52.049 
The network architecture is often based on several stages, which is in turn based on the multiple repeating blocks.

19
00:01:52.640 --> 00:01:56.060 
So there are very huge configuration possibility here.

20
00:01:56.540 --> 00:02:03.329 
The possible configuration factors like the kernel size of convolutional layer used in a certain stage.

21
00:02:03.340 --> 00:02:11.969 
The number of filter channels, number of blocks in a stage, number of total stages which kind of non-linear activation

22
00:02:11.969 --> 00:02:15.949 
function should be used in certain stage and so on and so forth.

23
00:02:17.139 --> 00:02:19.819 
So the aforementioned configurations

24
00:02:19.830 --> 00:02:28.620 
built a search space and we can pick an architecture according to a certain search principle and further train the model

25
00:02:28.629 --> 00:02:31.860 
and evaluate it according to certain metrics.

26
00:02:32.340 --> 00:02:42.280 
The commonly used metrics are accuracy for example influence speed, computation complexity and the evaluation result as

27
00:02:42.280 --> 00:02:52.330 
feedback is returned to the search strategy which will sample the next architecture based on the current result. The search

28
00:02:52.330 --> 00:02:55.750 
process will be terminated.

29
00:02:55.759 --> 00:02:59.449 
Either we found the target architecture or we failed.

30
00:03:02.039 --> 00:03:06.280 
The slides show the basic principle has been used by NASNet.

31
00:03:06.939 --> 00:03:15.360 
A controller RNN predicts the probability of sampling architecture A from predefined search space.

32
00:03:17.340 --> 00:03:25.370 
So a child network with architecture A is trained to convergence achieving accuracy R,

33
00:03:25.370 --> 00:03:30.449 
scale the gradient of P by R to update the RNN controller.

34
00:03:31.139 --> 00:03:37.349 
The principle is easy to understand but the searching process is really time and cost consuming.

35
00:03:37.939 --> 00:03:48.250 
We usually are not able to fully explore the search space due to the hardware limitation in the practice.

36
00:03:48.250 --> 00:03:57.479 
In the NASNet paper, even those researchers from Google is not able to directly apply automatic search algorithm to ImageNet

37
00:03:57.479 --> 00:04:06.750 
dataset, which may require several months of training using previous methods. They proposed to use the smaller proxy dataset

38
00:04:06.759 --> 00:04:16.139 
like CIFAR dataset and then transfer the best found architecture to ImageNet classification or the COCOA object detection

39
00:04:16.139 --> 00:04:16.660 
task.

40
00:04:20.540 --> 00:04:28.350 
Now we briefly introduce how does the RNN model predict the network cell. Given two input hidden states.

41
00:04:28.740 --> 00:04:32.160 
The RNN will predict the following block component -

42
00:04:33.740 --> 00:04:35.420 
First, select the hidden state,

43
00:04:35.430 --> 00:04:39.850 
H1 from the given set. And select the second hidden state,

44
00:04:39.850 --> 00:04:42.230 
H2 from the same options,

45
00:04:42.240 --> 00:04:49.529 
in the step one. Select an operation to apply to H1

46
00:04:49.529 --> 00:04:56.550 
indicated by the yellow color and select an option to apply to H2 as well.

47
00:04:58.339 --> 00:05:05.379 
After that, select a method to combine the outputs of steps three and four to create a new hidden state,

48
00:05:05.389 --> 00:05:07.100 
H3.

49
00:05:07.110 --> 00:05:16.360 
In the green color. Append H3 to the existing set of hidden states at the potential input in the subsequent blocks.

50
00:05:16.939 --> 00:05:23.399 
Finally we repeat B times, in this paper, B=5.

51
00:05:23.410 --> 00:05:33.769 
It is the best choice. We will repeat B times to find the best configuration and this slides show the possible operations

52
00:05:33.769 --> 00:05:35.860 
for the step three and step four.

53
00:05:36.439 --> 00:05:41.860 
We can see that among other standard operations, there are two operators

54
00:05:42.240 --> 00:05:50.810 
we have not seen before and for the depthwise separable convolution, we will introduce later

55
00:05:50.819 --> 00:05:53.939 
and another one is 3x3

56
00:05:53.939 --> 00:05:55.879 
dilated convolution.

57
00:05:55.889 --> 00:06:02.439 
We may want to take a closer look.

58
00:06:02.439 --> 00:06:12.959 
Dilation or dilated convolution supports exponential expansion of the receptive field without loss of resolution or coverage.

59
00:06:14.139 --> 00:06:17.709 
In the dilated convolution,

60
00:06:17.720 --> 00:06:20.069 
a small size kernel with k×k

61
00:06:20.069 --> 00:06:26.360 
filter, k is enlarged to k+(k−1)(r−1)

62
00:06:26.370 --> 00:06:30.959 
So here the parameter r is the dilation stride.

63
00:06:31.740 --> 00:06:40.759 
Thus it allows the flexible aggregation of the multi skills context for information while keeping the same computation budgets

64
00:06:41.139 --> 00:06:49.560 
According to the example in the figure where standard convolution gets 3 x 3 receptive field and two dilated

65
00:06:50.139 --> 00:06:57.290 
convolutions deliver 5×5 or 7×7 receptive ﬁelds respectively.

66
00:06:57.300 --> 00:07:03.850 
However, using dilated convolution will increase the memory consumption during the model training.

67
00:07:06.240 --> 00:07:13.870 
Let's go back to the NASNet Cell. The architecture of the best convolutional cell, the NASNet-A

68
00:07:13.879 --> 00:07:25.089 
in the paper, it has B = 5 blocks identified by the CIFAR-10 dataset and the wider input is the hidden state from the

69
00:07:25.089 --> 00:07:31.509 
previous activations or from the input image and the output in pink.

70
00:07:31.519 --> 00:07:43.350 
In another color, the result of the concatenation operations across all the results branches and each convolution cell

71
00:07:43.360 --> 00:07:46.490 
is the result of B blocks.

72
00:07:46.500 --> 00:07:55.360 
A single block is corresponding to two primitive operations and combination operation in the green color.

73
00:07:58.839 --> 00:08:07.149 
The two figures show the comparison results regarding accuracy computation demand on the left side and regarding the accuracy

74
00:08:07.149 --> 00:08:10.959 
number of parameters right on the right hand side.

75
00:08:10.959 --> 00:08:15.759 
Across a top performing ConvNets architectures on ImageNet.

76
00:08:16.439 --> 00:08:23.850 
The computational demand is measured in the number of floating point operations to process single image.

77
00:08:25.040 --> 00:08:27.199 
So the black circles indicates

78
00:08:27.199 --> 00:08:33.950 
the previous published work results and the red squares highlight the proposed method.

79
00:08:35.340 --> 00:08:47.210 
So ImageNet classification task, the prediction accuracy of NASNet on the validation set reached 82.7% and this result

80
00:08:47.210 --> 00:08:53.259 
was 1.2% higher than all the previously published results.

81
00:08:53.740 --> 00:09:02.049 
NASNet can also adjust the skill to generate a theory of models that can achieve higher accuracy while keeping the

82
00:09:02.049 --> 00:09:05.450 
computational budget at the low level.

83
00:09:06.139 --> 00:09:16.970 
And for example, a small version of NASNet can achieve 74% accuracy, which is 3% higher than the most advanced

84
00:09:16.970 --> 00:09:19.490 
model of the same scale,

85
00:09:19.649 --> 00:09:23.690 
for the mobile platforms. Large-scale

86
00:09:23.690 --> 00:09:30.659 
NASNet can achieve the highest accuracy rate while having the computational overhead of SENet.

87
00:09:31.340 --> 00:09:37.120 
So NAS method is effective but not energy efficient in the training stage.

88
00:09:37.129 --> 00:09:42.559 
On the other hand, it further increased the difficulty of interpretability.

89
00:09:44.840 --> 00:09:45.950 
Thank you for watching.
