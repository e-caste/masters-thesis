WEBVTT

1
00:00:00.470 --> 00:00:04.350 
This is knowledge graphs lecture
five knowledge graph applications.

2
00:00:05.010 --> 00:00:09.220 
In this lecture we are going to talk
about RDF and OWL knowledge graphs.

3
00:00:11.030 --> 00:00:15.530 
So let's talk about how do we
create these knowledge graphs. So

4
00:00:15.800 --> 00:00:20.520 
there are several ways to create this
knowledge graph such as curated approaches.

5
00:00:20.640 --> 00:00:25.250 
In the curated approaches the
small closed group of experts

6
00:00:25.480 --> 00:00:30.300 
sit together and manually create
triples. In the collaborative

7
00:00:30.300 --> 00:00:34.540 
approaches open group of volunteers
manually create triples.

8
00:00:34.720 --> 00:00:39.080 
Now we move towards automated
approaches. So in the automated

9
00:00:39.080 --> 00:00:42.640 
approaches what happens
is the triples are

10
00:00:43.320 --> 00:00:47.190 
extracted automatically from
semi structured text with the

11
00:00:47.190 --> 00:00:51.380 
help of rules, regular expressions
and hand-crafted rules.

12
00:00:51.980 --> 00:00:56.490 
And then we have the automated
structured umm automated unstructured

13
00:00:56.500 --> 00:01:00.490 
approaches where the triples
are extracted automatically

14
00:01:00.650 --> 00:01:05.120 
from the unstructured data such as
text with the help of machine

15
00:01:05.120 --> 00:01:08.760 
learning or natural language
processing techniques and then

16
00:01:08.760 --> 00:01:12.490 
we have linking existing datasets,
different datasets are

17
00:01:12.490 --> 00:01:15.090 
connected using
linked data.

18
00:01:17.140 --> 00:01:21.470 
So now we go towards our first approach
which is the curated approach.

19
00:01:21.770 --> 00:01:25.990 
So as I said before that in the
curated approach a closed

20
00:01:25.990 --> 00:01:33.020 
group of experts actually curate manually
create these triples. So since

21
00:01:33.190 --> 00:01:37.490 
it is manually created by the
experts these kinds of triples

22
00:01:37.490 --> 00:01:40.220 
are highly accurate and
they don't scale.

23
00:01:40.650 --> 00:01:44.860 
As soon as we go towards more
automated approaches with the

24
00:01:44.870 --> 00:01:49.520 
scaling it we create more noise
and there is the scaling issue.

25
00:01:50.690 --> 00:01:55.690 
So the examples of curated approaches
in the curated datasets are

26
00:01:56.230 --> 00:02:01.920 
Cyc/OpenCyc, WordNet, UMLS,
you know which is the United

27
00:02:01.920 --> 00:02:04.780 
Medical Language System
and SNOMED CT.

28
00:02:06.470 --> 00:02:11.070 
The next approach for creating knowledge
graphs is collaborative approach.

29
00:02:11.390 --> 00:02:16.920 
In this open group of volunteers
create the triples manually.

30
00:02:17.320 --> 00:02:21.280 
So a famous example of this
is Wikidata and Freebase.

31
00:02:21.700 --> 00:02:27.720 
So the problem with these is
incompleteness, so the mandatory

32
00:02:27.760 --> 00:02:31.600 
place of birth attribute is
missing for seventy one percent

33
00:02:31.610 --> 00:02:36.220 
of all the people included in freebase
and the growth of wikipedia

34
00:02:36.220 --> 00:02:38.520 
and wikidata is really
slowing down.

35
00:02:40.620 --> 00:02:44.610 
Then we have automated semi-structured
approaches. So in these

36
00:02:44.610 --> 00:02:49.450 
approaches as it is as I said before
that the triples are extracted

37
00:02:49.450 --> 00:02:55.360 
automatically from semi structured text with
the help of rules and regular expressions.

38
00:02:56.070 --> 00:03:01.080 
So such kind of approaches one of the
examples is wikipedia where the

39
00:03:01.250 --> 00:03:05.790 
information the triples are extracted
from the wikipedia info boxes.

40
00:03:06.460 --> 00:03:11.350 
They are scalable and the accuracy
and they have good accuracy.

41
00:03:11.640 --> 00:03:17.460 
So the example of such knowledge graphs
are YAGO, DBpedia and freebase.

42
00:03:17.670 --> 00:03:22.960 
So here the accuracy of YOGO that
as it was measured manually

43
00:03:23.290 --> 00:03:28.410 
it was over ninety five percent
and the accuracy of freebase

44
00:03:28.410 --> 00:03:30.180 
is about ninety
nine percent.

45
00:03:31.240 --> 00:03:35.450 
However the problem is it
covers only a small fraction

46
00:03:35.450 --> 00:03:37.710 
of information
stored on the web.

47
00:03:39.000 --> 00:03:43.690 
Then we move towards unstructured
approaches where we automatically

48
00:03:44.030 --> 00:03:47.680 
extract information from the
unstructured text with the help

49
00:03:47.680 --> 00:03:51.130 
of machine learning or natural
language processing algorithms.

50
00:03:51.980 --> 00:03:56.340 
So the of course this kind the
facts which are represented

51
00:03:56.340 --> 00:03:59.740 
on the in the knowledge graph are
then extracted with the help

52
00:03:59.740 --> 00:04:04.900 
of these nlp techniques from the
text of the web pages. So there

53
00:04:04.900 --> 00:04:09.460 
here are some of the examples
which are created automatically

54
00:04:09.460 --> 00:04:13.690 
from the unstructured data from
text. So there are there is

55
00:04:13.690 --> 00:04:17.970 
NULL, knowledge vault and and
you can see more of them and I

56
00:04:17.970 --> 00:04:21.140 
would leave this task up to you
to look up for these knowledge

57
00:04:21.140 --> 00:04:22.800 
graphs and know
more about them.

58
00:04:23.690 --> 00:04:27.710 
Now the point is as I said before
that as soon as you move

59
00:04:27.710 --> 00:04:32.180 
towards more automated approaches
the noise increases. However

60
00:04:32.640 --> 00:04:36.430 
noise can be reduced by using the
knowledge from existing high

61
00:04:36.430 --> 00:04:38.100 
quality repositories.

62
00:04:40.750 --> 00:04:44.570 
So now you have seen this slide
like several times during the

63
00:04:44.570 --> 00:04:50.550 
course of this whole lecture. So
by now I sort of assume that

64
00:04:50.550 --> 00:04:55.170 
you remember the statistics by heart
but I will say it again. So uh

65
00:04:55.320 --> 00:04:59.760 
the statistics of in the web
of data you have around

66
00:04:59.760 --> 00:05:04.900 
nine thousand data sets, more than
one umm more than one forty

67
00:05:04.900 --> 00:05:08.560 
billion facts and more than
eight hundred million links.

68
00:05:09.170 --> 00:05:14.220 
By and the statistics were
obtained up until April 2017.

69
00:05:15.530 --> 00:05:18.890 
So these knowledge graphs that
are created in the several ways

70
00:05:18.890 --> 00:05:23.730 
that we just showed should be published
or should be published using

71
00:05:23.960 --> 00:05:28.470 
these linked data principles. So
for the names of the things

72
00:05:29.040 --> 00:05:34.520 
URIs should be used, then
http URIs should be used so

73
00:05:34.520 --> 00:05:37.960 
that the people can look up
for their for these names

74
00:05:38.810 --> 00:05:43.130 
and when somebody looks up for
a URI or a useful information

75
00:05:43.130 --> 00:05:47.720 
using the standards like RDF or
SPARQL should be presented.

76
00:05:48.610 --> 00:05:52.980 
And then the links to other
URIs the links to other data

77
00:05:52.980 --> 00:05:57.590 
sets should be included so that it is
more easily easily discoverable.

78
00:05:58.820 --> 00:06:03.080 
So now what is the advantage or
advantages of linked open data

79
00:06:03.890 --> 00:06:08.500 
upon other APIs? So it is
simple and generic APIs

80
00:06:08.500 --> 00:06:13.090 
for various heterogeneous data
sources which helps in reusing

81
00:06:13.090 --> 00:06:17.810 
and sharing of these data
sources among the application.

82
00:06:18.260 --> 00:06:24.540 
And then RDF data model it actually
guarantees very simple extensibility.

83
00:06:25.600 --> 00:06:30.640 
For the transport you use with
the help of http you use the

84
00:06:30.640 --> 00:06:35.410 
standard port eighty which prevents
firewall adaptation and then

85
00:06:35.550 --> 00:06:39.860 
with the help of ontologies you
get meaningful connections

86
00:06:39.860 --> 00:06:43.900 
between the data sources and
based as and based on what

87
00:06:43.910 --> 00:06:48.450 
we saw so far in the lectures
we can perform reasoning over

88
00:06:48.520 --> 00:06:52.350 
linked data which helps us in
generating new knowledge or

89
00:06:52.360 --> 00:06:54.510 
with the help of
inferences.

90
00:06:57.180 --> 00:07:02.930 
So here you see the statistics of
the popular open-knowledge graphs.

91
00:07:03.250 --> 00:07:08.290 
So here you have DBpedia YAGO
wikidata opencyc and nell.

92
00:07:08.600 --> 00:07:12.980 
So here you can see the number of
instances the number of axioms

93
00:07:12.980 --> 00:07:18.530 
classes and relations they have
and also the releases when

94
00:07:18.540 --> 00:07:22.770 
how often do they release their next
versions the newest versions.

95
00:07:23.320 --> 00:07:27.250 
So one of the things that I
didn't see it didn't say right

96
00:07:27.250 --> 00:07:31.510 
now is it also describes the
average in degree and out degree

97
00:07:31.950 --> 00:07:35.530 
which actually means in degree
means the nodes which are coming

98
00:07:35.530 --> 00:07:39.480 
towards a class and the
out degrees are the

99
00:07:39.480 --> 00:07:42.500 
edges which are going
out of the class.

100
00:07:43.320 --> 00:07:48.370 
So these are the statistics which
are coming from the paper

101
00:07:48.370 --> 00:07:52.410 
towards profiling knowledge
graphs, from 2017.

102
00:07:55.060 --> 00:07:58.840 
And now these are the ones that you
just saw were the open-knowledge

103
00:07:58.840 --> 00:08:03.190 
graphs and now we are talking about
popular proprietary knowledge graphs

104
00:08:03.400 --> 00:08:07.520 
where you have the knowledge
graph from microsoft google

105
00:08:07.850 --> 00:08:14.130 
facebook ebay and ibm you can actually
see the statistics of these graphs also

106
00:08:14.390 --> 00:08:17.270 
where we say about the
size of the graphs.

107
00:08:17.780 --> 00:08:22.940 
So all of them are actually very
actively used right now instead

108
00:08:22.940 --> 00:08:27.200 
of ebay which is in the early stages
of development and deployment.

109
00:08:27.600 --> 00:08:32.740 
So these details were actually taken
from the paper of Natasha Noy

110
00:08:33.290 --> 00:08:35.590 
titled

111
00:08:35.590 --> 00:08:38.880 
"The Industry Scale Knowledge
Graphs: lessons and challenges"

112
00:08:38.880 --> 00:08:40.120 
in 2019.

113
00:08:41.720 --> 00:08:46.210 
So now the question is what keeps
the web of linked data together?

114
00:08:47.270 --> 00:08:52.930 
So for doing so we use OWL sameAs
which connects identical

115
00:08:52.930 --> 00:08:57.470 
individuals so as we saw that it
can be used to connect same

116
00:08:57.470 --> 00:09:03.000 
individuals in two different data sources
which allows the connection between two

117
00:09:03.930 --> 00:09:09.300 
data sources that can be of the same
domain or they can also connected

118
00:09:09.480 --> 00:09:13.650 
through the similar predicate
to the cross domain knowledge

119
00:09:13.650 --> 00:09:17.530 
graph such as YAGO or
DBpedia or wikidata.

120
00:09:18.650 --> 00:09:22.990 
And then similarly OWL equivalent
classes can also be used

121
00:09:23.390 --> 00:09:26.540 
to the classes which are equivalent
to some classes in the

122
00:09:26.590 --> 00:09:28.760 
in two different
data sets.

123
00:09:31.310 --> 00:09:35.060 
So these kinds that I just
described previously can actually

124
00:09:35.060 --> 00:09:41.400 
be helpful in enhancing the
interoperability of the datasets.

125
00:09:41.640 --> 00:09:46.150 
So the other vocabularies
that are used are SKOS. So

126
00:09:46.150 --> 00:09:50.240 
you have simple knowledge
organization system which uses SKOS

127
00:09:50.240 --> 00:09:54.100 
concept which defines a concept,
then you have a SKOS narrower

128
00:09:54.440 --> 00:10:00.190 
which goes from the more generic
to the narrow category or

129
00:10:00.450 --> 00:10:05.950 
class the opposite of the
narrower is SKOS broader. Then

130
00:10:05.950 --> 00:10:10.230 
you have related, exact match, narrow
match, broad match, and related

131
00:10:10.230 --> 00:10:15.500 
match. This way you have several
a predicates that is several

132
00:10:15.510 --> 00:10:18.870 
predicates that you can use
for defining your data sets.

133
00:10:20.820 --> 00:10:25.830 
So we have also UMBEL which is
an upper mapping and binding

134
00:10:25.830 --> 00:10:29.920 
exchange layer which is an upper
ontology, so it is a subset

135
00:10:29.930 --> 00:10:34.560 
of open cyc as RDF triples which
is based on SKOS and OWL

136
00:10:34.560 --> 00:10:39.440 
which I just described before. So as
I said it is an upper ontology with

137
00:10:39.640 --> 00:10:44.300 
twenty eight thousand concepts which
are described by the SKOS concept

138
00:10:44.820 --> 00:10:49.410 
and then it has mappings to DBpedia
and geo names with the help of

139
00:10:49.520 --> 00:10:53.370 
equivalent class and RDFS subclass
of these are the constructs

140
00:10:53.370 --> 00:10:56.700 
that we have been studying
so far in our lectures

141
00:10:57.430 --> 00:11:01.820 
and then it links to more than
two million wikipedia pages.

142
00:11:04.600 --> 00:11:08.340 
So in the next lecture you will
see more details on knowledge

143
00:11:08.340 --> 00:11:09.640 
graph and programming.
