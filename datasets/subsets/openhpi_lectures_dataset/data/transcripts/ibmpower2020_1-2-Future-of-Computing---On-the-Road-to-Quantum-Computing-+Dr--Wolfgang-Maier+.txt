WEBVTT

1
00:00:00.430 --> 00:00:04.540 
Hi there and welcome to a short
introduction into the Future

2
00:00:04.540 --> 00:00:08.730 
of Computing - on the road
to Quantum Computing.

3
00:00:09.150 --> 00:00:14.970 
My name is Wolfgang Maier and I am the director
of the hardware development organization

4
00:00:15.260 --> 00:00:18.980 
in the IBM research and
development lab in Böblingen,

5
00:00:19.460 --> 00:00:26.840 
close to Stuttgart in the south of Germany.
The hardware development team

6
00:00:27.380 --> 00:00:31.340 
is acting within a worldwide
cluster of IBM research

7
00:00:31.790 --> 00:00:33.520 
and development
facilities

8
00:00:34.730 --> 00:00:40.710 
and the topics I will discuss today are
based mainly on the collaboration

9
00:00:40.980 --> 00:00:44.200 
with the sites in Yorktown,
a New York state,

10
00:00:44.990 --> 00:00:50.740 
in the Almaden, California as well as
with a research lab in Zürich,

11
00:00:51.130 --> 00:00:59.020 
Switzerland. Now the embracing subject
all our activities are focused on

12
00:00:59.730 --> 00:01:04.870 
is certainly well described by
the term digital transformation

13
00:01:05.210 --> 00:01:11.230 
which stands for the ongoing and pervasive
usage of advance technologies

14
00:01:11.550 --> 00:01:15.980 
based on the digitization
of information.

15
00:01:17.710 --> 00:01:21.970 
So to speak, the basic requirement to
make the digital transformation

16
00:01:21.970 --> 00:01:26.540 
reality were delivered by the
advancement of technologies which

17
00:01:26.540 --> 00:01:31.480 
is followed the empirically formulated
law defined by Gordon Moore,

18
00:01:31.920 --> 00:01:34.960 
which got famous as Moore's law.

19
00:01:36.870 --> 00:01:41.130 
Now even though one should be a
little bit more precise and

20
00:01:41.130 --> 00:01:43.890 
also mentioned Dennard's
and Amdahl's law

21
00:01:44.530 --> 00:01:48.680 
which circle around power
density parallelization.

22
00:01:49.170 --> 00:01:55.490 
This law sent for a tremendous increase
of digitization, respectively

23
00:01:55.630 --> 00:02:00.670 
compute resources while keeping
power consumption as well

24
00:02:00.670 --> 00:02:03.620 
as cost at a
reasonable spot.

25
00:02:05.170 --> 00:02:09.240 
However as these laws are also
bound to the laws of physics,

26
00:02:09.250 --> 00:02:14.650 
it is getting obvious that over time
there will be limitations in the future

27
00:02:14.980 --> 00:02:19.040 
which will restrict an infinite
growth of compute capacity.

28
00:02:20.480 --> 00:02:26.050 
In this class you will get a deep insight
into the architecture of IT systems

29
00:02:26.250 --> 00:02:30.480 
which are based on such principles
and the ancestor class already

30
00:02:30.480 --> 00:02:35.720 
took you through the details of the
characteristics of the IBM power architecture.

31
00:02:37.610 --> 00:02:40.380 
That's how I will also
look ahead to the future

32
00:02:40.860 --> 00:02:47.380 
and share some thoughts which new concepts
had the potential to begame changers.

33
00:02:48.670 --> 00:02:52.850 
As mentioned before, Moore's
law is facing limits.

34
00:02:53.740 --> 00:02:59.890 
With this in mind it seems wisely to think
of alternative processing of information

35
00:03:00.280 --> 00:03:03.390 
like it is known for example
from the human brain

36
00:03:03.950 --> 00:03:07.890 
which makes use of a complex
network spent by a large number

37
00:03:07.890 --> 00:03:13.200 
of neurons and synapses which
communicate in a kind of mix

38
00:03:13.400 --> 00:03:17.540 
of cloth but at the same
time analog approach.

39
00:03:19.270 --> 00:03:24.340 
Based on this insights of information
processing, new architectures

40
00:03:24.390 --> 00:03:27.180 
have been deployed which are
aligned to the knowledge

41
00:03:27.600 --> 00:03:30.380 
of the operating principles
of the human brain.

42
00:03:31.100 --> 00:03:35.470 
These are realized through classical
for Neumann based computing

43
00:03:35.480 --> 00:03:39.880 
or more recently in the context
of completely new technologies

44
00:03:40.240 --> 00:03:44.600 
like neuromorphic approaches
respectively resistive memory based

45
00:03:44.600 --> 00:03:46.220 
on face change
materials.

46
00:03:48.150 --> 00:03:51.520 
These are good examples
how to learn from nature.

47
00:03:52.570 --> 00:03:59.280 
Now, besides all these approaches, there are
also underlying theoretical thoughts

48
00:03:59.560 --> 00:04:02.910 
which allow an even more
abstracting perspective

49
00:04:03.380 --> 00:04:07.440 
of how to deal with information
and its characteristics.

50
00:04:09.120 --> 00:04:15.660 
These basic considerations are specified
by Shannon's information theory.

51
00:04:16.340 --> 00:04:20.320 
In order to quantify the amount of
information, Shannon introduced

52
00:04:20.330 --> 00:04:22.590 
the so called
Shannon entropy

53
00:04:23.510 --> 00:04:28.720 
which can be calculated from the
probability distribution of

54
00:04:28.720 --> 00:04:33.560 
an entity, which is mapped
to a stochastic variable.

55
00:04:35.130 --> 00:04:42.160 
The entropy can be thought of as a measure
for the misorder or the uncertainty,

56
00:04:42.390 --> 00:04:48.120 
the entity analysed and it is at a max
when it is evenly distributed.

57
00:04:50.470 --> 00:04:54.580 
The theory is circled around the
mechanics of processing such a

58
00:04:54.690 --> 00:04:57.340 
notation of information
also for the case,

59
00:04:57.770 --> 00:05:00.470 
if the processing is
disturbed by noise.

60
00:05:01.460 --> 00:05:07.220 
Basically this is the essential theory
for the input output model as depicted

61
00:05:07.420 --> 00:05:10.270 
in the chart. As
an example,

62
00:05:12.370 --> 00:05:18.120 
let's assume we have a neural net
which is doing image recognition

63
00:05:18.130 --> 00:05:25.590 
of k independent images categorized
like, for example, dog, cat or so on.

64
00:05:27.380 --> 00:05:31.260 
Is our machine, which is the neural
network doesn't know which

65
00:05:31.260 --> 00:05:35.480 
category the incoming image belongs
to, the entropy or in other

66
00:05:35.480 --> 00:05:39.970 
words the uncertainty at the start
of the processing is at a max

67
00:05:40.090 --> 00:05:43.950 
as the probability that it belongs
to a certain category is equal.

68
00:05:45.230 --> 00:05:49.020 
Now as the machine starts to work
it tries to narrow down the

69
00:05:49.020 --> 00:05:51.680 
belonging of the image
to a specific category.

70
00:05:52.330 --> 00:05:56.660 
The more it is capable to narrow
the probability distribution,

71
00:05:56.910 --> 00:06:01.840 
the smaller the entropy gets.
Ideally, it can narrow down to

72
00:06:01.840 --> 00:06:05.800 
a delta function which then means
the entropy is zero or in

73
00:06:05.800 --> 00:06:10.210 
other words there is no uncertainty
about which category the image belongs.

74
00:06:11.560 --> 00:06:15.340 
But even if the distribution
doesn't have a delta shape, we

75
00:06:15.340 --> 00:06:19.950 
would have made a step forward
in our information processing.

76
00:06:23.830 --> 00:06:29.570 
So, one line. First, the higher the
number of categories the net

77
00:06:29.570 --> 00:06:33.940 
can handle, the more powerful it
is, or in other words, as higher

78
00:06:33.940 --> 00:06:38.300 
k equally distributed means more
entropy, the more entropy the net

79
00:06:38.300 --> 00:06:44.460 
can handle, the more
powerful it is. Second,

80
00:06:45.200 --> 00:06:49.700 
the better the net can filter
into categories, the better the

81
00:06:49.700 --> 00:06:53.950 
fidelity of the net is, or in
other words, as narrowing of the

82
00:06:53.950 --> 00:06:57.290 
probability distribution
means decrease of entropy,

83
00:06:57.990 --> 00:07:04.250 
the more entropy the machine can use,
the more precise the results are.

84
00:07:07.080 --> 00:07:12.000 
In general this means it is
desirable to have a machine which

85
00:07:12.000 --> 00:07:15.730 
can deal with a big amount of
entropy and which can reduce

86
00:07:15.730 --> 00:07:23.010 
the entropy to a minimum in
an efficient way. This is more

87
00:07:23.010 --> 00:07:25.830 
or less what a quantum
computer is doing.

88
00:07:26.560 --> 00:07:31.070 
And this as it is still affected by
noise at the moment it is called NISQ

89
00:07:31.210 --> 00:07:36.200 
which stands for Noisy Intermediate
State Quantum computing

90
00:07:36.950 --> 00:07:40.830 
which means the reduction of the
entropy to zero is still a

91
00:07:40.830 --> 00:07:48.930 
goal to reach. Now, how is the
quantum computer doing this?

92
00:07:49.040 --> 00:07:53.990 
A big amount of entropy can be realized
by quantum entities called qubits.

93
00:07:54.160 --> 00:07:59.650 
John von Neumann calculated the entropy of

94
00:07:59.650 --> 00:08:03.330 
quantum ensembles from the
so called density matrix

95
00:08:03.890 --> 00:08:08.800 
and due to the very special
characteristics of quantum systems

96
00:08:09.010 --> 00:08:12.520 
which are given by superposition
and entanglement,

97
00:08:13.330 --> 00:08:18.260 
the amount of entropy can be
raised very fast by even small

98
00:08:18.260 --> 00:08:22.500 
number of qubits. What we still
need is a way to reduce and

99
00:08:22.500 --> 00:08:25.910 
control the entropy for
a specific problem

100
00:08:26.440 --> 00:08:29.890 
in order to realize a
kind of processing.

101
00:08:30.740 --> 00:08:36.200 
As quantum processes have the specific
requirement of reversibility, it's not possible

102
00:08:36.500 --> 00:08:41.800 
to apply boolean logic to qubits
since it's violating reversibility.

103
00:08:42.840 --> 00:08:46.340 
For this reason there is a special
set of requirements which

104
00:08:46.340 --> 00:08:51.440 
have to be met which are known as the
so called divigence criterion.

105
00:08:53.120 --> 00:08:57.150 
Introducing new logical gates
specifically for qubits

106
00:08:57.730 --> 00:09:01.600 
allows to meet the criteria and
with this allows to define

107
00:09:01.600 --> 00:09:07.520 
a completely new class of quantum
algorithms which then serve

108
00:09:07.530 --> 00:09:11.610 
to fulfill the reduction of
entropy respectively narrowing

109
00:09:11.610 --> 00:09:15.190 
down on a specific
solution.

110
00:09:16.040 --> 00:09:22.940 
So our machine based on qubits is capable to
represent a tremendous amount of information

111
00:09:23.170 --> 00:09:28.410 
and it is possible to apply
algorithms which are capable to

112
00:09:28.410 --> 00:09:33.260 
control this amount of information
in an efficient way. Now,

113
00:09:33.320 --> 00:09:38.200 
practically these tasks are done
on a new type of chip which

114
00:09:38.200 --> 00:09:42.630 
is hosting a number of qubits
implemented its so called transmons.

115
00:09:42.820 --> 00:09:45.450 
as you can see
on the chart.

116
00:09:46.330 --> 00:09:50.260 
In this chip there are five
transmons placed on the chip

117
00:09:50.270 --> 00:09:52.730 
which are connected
through special wires.

118
00:09:54.490 --> 00:09:59.130 
Microwaves send along these
wires realized the application

119
00:09:59.130 --> 00:10:02.550 
of the special quantum
gates we have seen before and

120
00:10:03.000 --> 00:10:09.760 
they are also used to prop the results from
the applied microwave sequences. In practice

121
00:10:10.030 --> 00:10:12.120 
it is not quite
as simple.

122
00:10:13.190 --> 00:10:20.630 
As we learned before, noise is influencing
the fidelity of the quantum processes and

123
00:10:20.990 --> 00:10:27.010 
not quite unexpected, quantum behavior
is known as extremely sensitive.

124
00:10:28.220 --> 00:10:34.920 
For this reason, implementation of qubits shown
here is based on superconducting effects

125
00:10:35.110 --> 00:10:39.090 
which are required to cool down
to some few micro-kelvin

126
00:10:39.780 --> 00:10:47.430 
which is a challenge
as such and even then,

127
00:10:48.130 --> 00:10:52.150 
the time window within the quantum
ensemble of qubits can be

128
00:10:52.150 --> 00:10:55.460 
regarded as sticking to the
law of quantum mechanics

129
00:10:56.300 --> 00:10:57.540 
is timer limited.

130
00:10:59.050 --> 00:11:03.400 
This so called coherence time is
a typical measurement which

131
00:11:03.400 --> 00:11:07.870 
typifies the fidelity and describes
one of the many challenges

132
00:11:08.380 --> 00:11:16.400 
quantum systems bring with it.
So, on one hand there is a huge

133
00:11:16.400 --> 00:11:20.910 
potential of quantum computing
to find solutions for problems

134
00:11:20.910 --> 00:11:23.160 
deemed to be
unsolvable so far,

135
00:11:24.840 --> 00:11:30.420 
on the other hand there are
tremendous challenges to realize

136
00:11:31.070 --> 00:11:35.500 
quantum ensemble of qubits which
show an adequate fidelity

137
00:11:36.000 --> 00:11:42.970 
as we allow to be scaled in a sufficient
manner. The transmon

138
00:11:42.970 --> 00:11:46.240 
technology is not the only
way to realize qubits.

139
00:11:46.660 --> 00:11:51.380 
there is significant research done
at present to find the best

140
00:11:51.380 --> 00:11:55.780 
solution for the implementation
of such qubits. I hope you found

141
00:11:55.780 --> 00:11:59.910 
some exciting motivation what
this class is trying to target

142
00:12:00.430 --> 00:12:03.920 
and you will have even more fun
with the subsequent modules

143
00:12:03.930 --> 00:12:08.220 
on these topics.
Stay tuned and goodbye.
