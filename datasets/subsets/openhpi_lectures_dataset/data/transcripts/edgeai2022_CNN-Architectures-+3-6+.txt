WEBVTT

1
00:00:00.840 --> 00:00:07.459 
Hello and welcome! In this video we will continue our topic on convolutional neural network architectures.

2
00:00:08.339 --> 00:00:13.460 
We will review Google's Inception networks and look at the batch normalization layer.

3
00:00:16.239 --> 00:00:25.649 
The convolutional neural network tries to learn the future in the 3D space with two spatial dimensions - width and height and

4
00:00:25.649 --> 00:00:26.910 
one channel dimension.

5
00:00:27.739 --> 00:00:35.560 
In this example the input image has three channels. And the convolutional Kernel also has three channels.

6
00:00:35.939 --> 00:00:45.109 
The task of a single convolution kernel is to learn the channel correlation and the spatial correlation at

7
00:00:45.109 --> 00:00:48.740 
the same time.

8
00:00:48.740 --> 00:00:56.750 
Inception modules assumption is that channel correlation and spatial correlation has been fully decoupled.

9
00:00:57.539 --> 00:01:02.359 
Therefore bottleneck layer can be used to reduce the number of channels.

10
00:01:02.939 --> 00:01:11.209 
On the other hand, it is into relatively more challenging to learn the special correlation and the channel correlation of

11
00:01:11.209 --> 00:01:12.920 
the filter simultaneously.

12
00:01:12.930 --> 00:01:19.620 
So if we try to learn two different things it will be more complicated than that.

13
00:01:19.620 --> 00:01:24.060 
We decouple them

14
00:01:24.069 --> 00:01:35.939 
and to try to learn it one by one. Thus the Inception module explicitly decomposes this process into four parallel computing

15
00:01:35.939 --> 00:01:44.969 
branches that independently consider channel correlation and spatial correlation, making the learning process easier and

16
00:01:44.969 --> 00:01:46.159 
more effective.

17
00:01:47.340 --> 00:01:55.980 
This concept also influenced the design of a theory of lightweight deep learning models and that also came from Google

18
00:01:55.980 --> 00:02:00.239 
Researchers later.

19
00:02:00.239 --> 00:02:08.300 
After GoogleNet which is sometimes also called InceptionNet V1, researchers from Google then proposed a new network

20
00:02:08.300 --> 00:02:11.370 
called BN-inceptionNet.

21
00:02:11.370 --> 00:02:20.860 
BM means batch normalization. In this model, a simple but efficient method is used as I said, it's called batch normalization.

22
00:02:21.439 --> 00:02:29.780 
Let's take a look first of all, when we are going to train a machine learning model, we will normally normalize the input

23
00:02:29.789 --> 00:02:30.360 
data.

24
00:02:30.840 --> 00:02:32.590 
Why do we do that?

25
00:02:32.689 --> 00:02:44.139 
Let's take a simple example, predicting a price of a house based on two features - the size and the square meter with a range

26
00:02:44.139 --> 00:02:46.979 
from 0 to 2000.

27
00:02:46.990 --> 00:02:57.650 
And other feature is the number of bedrooms which normally varies from 1 to 5. Without normalization, regarding the contour

28
00:02:57.840 --> 00:02:59.199 
of the cost function

29
00:02:59.199 --> 00:03:10.080 
J, in our case, with respect to the weight parameter Î¸, you will get a long and narrow, very narrow ellipses, which leads

30
00:03:10.090 --> 00:03:19.360 
to a zigzag routine in the direction of the gradient perpendicular to the contour when the gradient drops.

31
00:03:19.370 --> 00:03:23.650 
So this view makes the optimization process very slow.

32
00:03:24.539 --> 00:03:32.340 
On the other hand, if we normalize the data,

33
00:03:32.340 --> 00:03:41.810 
so if we normalize the feature data into the same value range, for example, between 0 and 1, you will get a more balanced

34
00:03:41.819 --> 00:03:52.520 
contours of the cost function and the gradient descent will be much faster, normalizing input data can speed up the convergence

35
00:03:52.520 --> 00:03:53.159 
normally.

36
00:03:53.169 --> 00:04:04.069 
So now input normalization is actually the standard processing step of a machine learning application and

37
00:04:04.080 --> 00:04:11.069 
it can also improve the accuracy, especially when it comes to some distance calculation algorithms.

38
00:04:11.080 --> 00:04:13.050 
The effect is significant.

39
00:04:14.039 --> 00:04:16.899 
Therefore normalizing is necessary.

40
00:04:16.910 --> 00:04:22.459 
It can make each feature contribute the same roughly the same to the result.

41
00:04:26.240 --> 00:04:34.959 
Thus the core idea of batch normalization is to normalize the input for each hidden layer, not just the input of the network

42
00:04:34.970 --> 00:04:37.850 
but for each hidden layer in the deep neural network.

43
00:04:38.439 --> 00:04:45.850 
This can help to mitigate interdependencies between distributions and hidden layers.

44
00:04:46.339 --> 00:04:54.350 
We can see that BN will always always create data distribution with zero mean and unit variance.

45
00:04:55.040 --> 00:05:02.360 
The effect in practice is that we can train the model faster and the convergence process is more stable.

46
00:05:05.529 --> 00:05:15.069 
This slide shows some comparison results. On the left hand side, the author shows several convergence curves on the ImageNet

47
00:05:15.069 --> 00:05:16.560 
classification task.

48
00:05:17.040 --> 00:05:26.819 
We can see that the black dotted line in the vanilla InceptionNet which trained with a very small initial learning rate of

49
00:05:26.829 --> 00:05:30.459 
0.0015, very small number.

50
00:05:31.740 --> 00:05:34.279 
All the batch norm related methods

51
00:05:34.290 --> 00:05:42.759 
add a BN layer before the input of the non-linear activation function of each hidden layer and their inputs

52
00:05:42.769 --> 00:05:45.029 
are better than InceptionNet.

53
00:05:45.040 --> 00:05:47.959 
Their results are better than the InceptionNet.

54
00:05:48.240 --> 00:06:00.050 
And the batch norm baseline uses the same initial learning rate. And BNx5 and BNx30 use 5x and 30x larger

55
00:06:00.050 --> 00:06:02.459 
initial learning rate respectively.

56
00:06:03.139 --> 00:06:05.540 
BNx5 Sigmoid, instead ReLU,

57
00:06:05.730 --> 00:06:15.500 
it uses the sigmoid activation function but still without any convergence problem. On the right hand side, regarding the classification

58
00:06:15.500 --> 00:06:23.860 
accuracy, BN with larger initial learning rate regularly improve the accuracy and they converge much faster.

59
00:06:26.839 --> 00:06:34.959 
So here, I just want to give you an overview of how to implement a batch norm layer in the forward propagation.

60
00:06:35.540 --> 00:06:41.379 
In short, during the training we will per each day to batch to the following operations.

61
00:06:42.040 --> 00:06:50.920 
First we will calculate the mean and variance channel-wise across the batch and then we will subtract the mean and divide by

62
00:06:50.920 --> 00:06:52.259 
the standard deviation.

63
00:06:52.740 --> 00:07:00.360 
Finally, we will use two additional learnable parameters scale and shift the input.

64
00:07:00.939 --> 00:07:02.110 
During the testing,

65
00:07:02.319 --> 00:07:10.759 
We just use the fixed empirical mean and variance parameters during the training, for instance, they can be estimated by running

66
00:07:10.759 --> 00:07:11.459 
average.

67
00:07:15.540 --> 00:07:25.779 
BN relies on the batch, the 1st and 2nd statistical moments which are mean and variance to normalize hidden layer

68
00:07:25.779 --> 00:07:36.750 
activations, so the output values are then strongly tied to the current batch statistics, such transformation adds some

69
00:07:36.750 --> 00:07:45.980 
noise depending on the input examples used in the current batch. This behavior can be considered as a sort of regularization

70
00:07:45.980 --> 00:07:46.660 
effect.

71
00:07:47.040 --> 00:07:56.970 
So in the practice with batch norm we can make the training faster and more accurate. Batch normalization significantly improve

72
00:07:56.970 --> 00:08:03.259 
the training stability and reduces the strong dependence on good initialization.

73
00:08:04.339 --> 00:08:08.920 
So previously we have to pay attention to a initialization function.

74
00:08:08.930 --> 00:08:13.889 
Two different machine learning applications but now with the BN layer.

75
00:08:13.899 --> 00:08:24.459 
So actually we can easily achieve a similar result without choosing a numbers of initialization method and batch norm offers

76
00:08:24.470 --> 00:08:29.759 
regularization effects and to some extent to replace the use of dropout.

77
00:08:34.840 --> 00:08:42.360 
We also try to understand why BN works. Here to discuss with you the current three different hypothesis.

78
00:08:43.639 --> 00:08:53.840 
According to the explanation in the original batch norm paper, BN effectiveness is due to the reduction of the internal

79
00:08:53.850 --> 00:08:55.259 
covariate shift.

80
00:08:56.039 --> 00:09:03.200 
Covariate shift describes the shifting of a model's input distribution. By extension,

81
00:09:03.299 --> 00:09:10.659 
the internal covariate shift described this phenomenon when it happens in the hidden layers of a deep neural network.

82
00:09:11.440 --> 00:09:19.990 
The corresponding correction for those shifted intermediate distributions require more training steps as you can see the

83
00:09:19.990 --> 00:09:24.759 
back propagation adapt the ways to achieve the distribution adaption.

84
00:09:26.039 --> 00:09:35.580 
So if there is a huge variant shift in the input signal, the optimizer will have trouble to generalize.

85
00:09:35.580 --> 00:09:44.690 
While in contrast, if the input signal always follow the standard non distribution, the optimizer might be able to easily

86
00:09:44.690 --> 00:09:54.539 
generalize so, following the author's aspect, that is forcing to have ðœ‡ = 0, Ïƒ = 1, and adding two trainable

87
00:09:54.539 --> 00:09:58.559 
parameter, gamma and beta to suggest the distribution.

88
00:09:59.039 --> 00:10:01.649 
This will help the network generalization.

89
00:10:05.940 --> 00:10:14.460 
The second hypothesis is normalizing the intermediate activations, reduces the interdependency between hidden layers.

90
00:10:15.539 --> 00:10:23.399 
This sound somehow similar to the previous hypothesis but it actually describes the phenomenon from the quite a different

91
00:10:23.399 --> 00:10:32.649 
perspective, the purpose of normalization is to reduce the interdependency between layers, which focus on the distribution

92
00:10:32.649 --> 00:10:33.649 
stability.

93
00:10:34.039 --> 00:10:44.039 
So the optimizer could choose the optimal distribution by adjusting only two parameters, gamma and beta.

94
00:10:44.039 --> 00:10:46.700 
As already mentioned in the previous session.

95
00:10:46.710 --> 00:10:52.649 
If all gradients are large and the gradient of B1 will be very large.

96
00:10:52.659 --> 00:10:58.860 
On the contrary, if all gradients are small, the gradient of B1 will be negligible.

97
00:10:59.740 --> 00:11:09.879 
So we can quickly figure out that hidden units are pretty dependent on each other. On modification of weight,

98
00:11:09.889 --> 00:11:13.950 
W1 will modify the input distribution of the neuron

99
00:11:13.950 --> 00:11:20.409 
node W2 eventually modifying subsequent neural nodes input signal

100
00:11:20.429 --> 00:11:29.730 
sequentially. If we want to adjust the input distribution of a specific hidden unit, we need to consider the whole sequence

101
00:11:29.730 --> 00:11:30.649 
of layers.

102
00:11:31.340 --> 00:11:38.360 
However batch normalization regulates that using just two parameters gamma and beta.

103
00:11:38.940 --> 00:11:48.740 
It is no longer necessary to consider all parameters to have clues about distribution inside the hidden units which significantly

104
00:11:48.750 --> 00:11:53.639 
is the training.

105
00:11:53.639 --> 00:12:02.710 
In this paper from machine learning conference neurIPS 2018 empirically demonstrated that batch norm effectiveness is likely

106
00:12:02.710 --> 00:12:10.559 
not related to the internal covariate shift against the original argument from the original authors.

107
00:12:11.039 --> 00:12:18.250 
So the third hypothesis here is that batch norm make the optimization landscape smoother.

108
00:12:19.039 --> 00:12:26.549 
The benefits of BN are due to this motion effect. In the left most figure,

109
00:12:26.559 --> 00:12:28.009 
The authors compares VGGNet

110
00:12:28.009 --> 00:12:39.269 
trained without and with BatchNorm, and with explicit "covariate shift" being added to BN layer, referring

111
00:12:39.279 --> 00:12:41.960 
to as noisy batch norm in the figure.

112
00:12:43.340 --> 00:12:52.370 
In the latter case, the author introduced distributional instability by adding time varying non zero mean and a non unit

113
00:12:52.370 --> 00:12:57.259 
variance noise independently to each batch normalization activation.

114
00:12:58.539 --> 00:13:05.860 
The figures show that noisy batch norm model nearly matches the performance of the standard BN model.

115
00:13:06.340 --> 00:13:16.370 
Despite substantial internal covariate shifts of the distributions and this finding significantly changes the hypothesis

116
00:13:16.370 --> 00:13:25.659 
from the original batch norm paper and the author showed that BN makes the optimization landscapes smoother while preserving

117
00:13:25.669 --> 00:13:29.960 
all the local minimum of the normal landscape.

118
00:13:30.440 --> 00:13:35.750 
So they realized the optimization landscape and observe such phenomenon.

119
00:13:36.240 --> 00:13:45.360 
In other words, BN re-parametrizes the optimization process which achieved the training faster and easier.

120
00:13:45.940 --> 00:13:53.259 
Furthermore, they also observe similar training performance using L1 and L2 normalization as well.

121
00:13:53.929 --> 00:14:04.710 
Thus, the authors speculate that BatchNorm makes the optimization landscape converge on more flat minima, which should have

122
00:14:04.710 --> 00:14:06.659 
better generalization ability.

123
00:14:06.899 --> 00:14:12.840 
We can find this effect in the right most figure.

124
00:14:12.840 --> 00:14:13.960 
As a summary,

125
00:14:13.970 --> 00:14:22.909 
however, please keep in mind all of those hypothesis presented in this session are mostly speculations

126
00:14:22.909 --> 00:14:32.820 
and the discussion are helpful for building your intuition regarding the effectiveness of BN, but

127
00:14:32.820 --> 00:14:38.879 
we still don't know exactly why batch norm is so efficient in practice.

128
00:14:38.889 --> 00:14:40.090 
So let's wait.

129
00:14:40.100 --> 00:14:47.750 
So let's wait patiently for the new research results to help ours reveal the mystery of batch normalization layer.
