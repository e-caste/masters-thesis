WEBVTT

1
00:00:00.740 --> 00:00:02.060 
Hello and welcome.

2
00:00:02.640 --> 00:00:04.269 
As mentioned before,

3
00:00:04.330 --> 00:00:09.150 
The convolutional neural network opened the door of the current AI revolution.

4
00:00:09.939 --> 00:00:13.669 
Let's review the development and evolution of ConvNets

5
00:00:13.669 --> 00:00:17.940 
together.

6
00:00:17.940 --> 00:00:27.230 
In the year 1980, Fukushima used the convolutional neural networks to realize pattern recognition for the first time and

7
00:00:27.230 --> 00:00:31.160 
he's considered to be the inventor of convolutional neural networks.

8
00:00:32.039 --> 00:00:40.640 
His paper talked about the origin of the components of convolutional neural network such as convolution and pooling layers.

9
00:00:40.649 --> 00:00:51.439 
And these two concepts were introduced or inspired by Hubel and Wiesel's experiment on the visual system of cats and he

10
00:00:51.439 --> 00:01:00.450 
performed simulations on the structure of neuroscience and proposed a step by step filter, commonly used in the ConvNets,

11
00:01:01.140 --> 00:01:10.370 
which uses average pooling to do the down sampling and ensure the translation invariance of the network and realize the

12
00:01:10.370 --> 00:01:11.959 
sparse interaction.

13
00:01:13.140 --> 00:01:22.409 
But the most significant limitation is that it uses an unsupervised learning method based on the Winner Take All criteria

14
00:01:22.420 --> 00:01:23.799 
to train the network.

15
00:01:23.810 --> 00:01:33.680 
So this model has always lacked practical meaning. In the next 10 years, there were no breakthroughs in the convolutional

16
00:01:33.680 --> 00:01:35.159 
neural network domain.

17
00:01:35.340 --> 00:01:47.340 
And 10 years later, around 1989 to 1990, Yann LeCun applied backpropagation to a network like Neocoginitron

18
00:01:47.349 --> 00:01:58.170 
for supervised learning and achieved a great success. Since then, CNN has gradually started and towards various application

19
00:01:58.170 --> 00:02:10.560 
areas. In 1998, Yann LeCun proposed the LeNet-5. This network became the cornerstone of the modern neural network structure.

20
00:02:14.840 --> 00:02:24.050 
So let's take a brief look at the structure of LeNet-5 and it is no different from the CNN architecture we often

21
00:02:24.050 --> 00:02:24.949 
see now.

22
00:02:24.960 --> 00:02:32.169 
So it's already very well structured form. Compared with the previous work,

23
00:02:32.180 --> 00:02:41.960 
the number of network layers here has deepened to 7 layers, which is almost extremely deep in that time.

24
00:02:42.340 --> 00:02:51.150 
And two layers of convolution and two layers of pooling. The input is a 32 x 32 pixel image.

25
00:02:51.539 --> 00:03:00.060 
Then the first convolution layer with kernel size 5 x 5 and the output is a feature map with width and height equals

26
00:03:00.060 --> 00:03:04.349 
28 and the channel number is equal 6.

27
00:03:04.939 --> 00:03:10.659 
The second layer is average pooling with a 2 x 2 window size stride equals 2.

28
00:03:11.740 --> 00:03:19.159 
Then the second convolution layer will further reduce the resolution of a feature map and increase the number of

29
00:03:19.159 --> 00:03:19.960 
channels.

30
00:03:20.439 --> 00:03:24.629 
16 convolution kernels with the size 5 x 5,

31
00:03:24.639 --> 00:03:35.280 
and with the step size of 1. However, only 10 of the 16 convolution kernels in this layer are connected to the subsequent

32
00:03:35.280 --> 00:03:36.090 
layers.

33
00:03:36.099 --> 00:03:46.810 
And because of this characteristics it only scan three of them, out of the six channels

34
00:03:46.810 --> 00:03:55.580 
and the reason for this is to break the symmetry of the image and reduce the

35
00:03:55.580 --> 00:03:57.009 
number of connections.

36
00:03:57.020 --> 00:04:01.250 
It can also save some memory cost and the computation.

37
00:04:02.240 --> 00:04:06.949 
The third Conv layer is followed by two fully connected layers.

38
00:04:07.340 --> 00:04:16.339 
So finally, after softmax function, we will have 10 output numbers which are corresponding to the probability of 10 digit

39
00:04:16.339 --> 00:04:17.149 
classes.

40
00:04:18.139 --> 00:04:28.060 
By picking the one with the highest probability we will get the classification result.

41
00:04:28.060 --> 00:04:29.060 
As aforementioned,

42
00:04:29.060 --> 00:04:34.550 
AlexNet is the winner of ImageNet Challenge in 2012.

43
00:04:35.339 --> 00:04:43.050 
The achieved top-5 classification result surpasses the runner-up's method by more than 10%.

44
00:04:43.740 --> 00:04:48.149 
AlexNet has five convolutional layers and three fully connected layer.

45
00:04:48.839 --> 00:04:59.949 
The model has 62.3 million parameters where convolution layers account for only 6% and three fully connected layers accounted

46
00:04:59.949 --> 00:05:07.550 
for 94% of the parameters. In the contrary regarding the computation complexity,

47
00:05:07.939 --> 00:05:16.189 
the convolutional layers are accounting for 95% of the floating point operations and fully connected layer only account for

48
00:05:16.189 --> 00:05:17.149 
5%.

49
00:05:17.939 --> 00:05:25.350 
So AlexNet uses max pooling for down sampling and the ReLU activation function.

50
00:05:25.939 --> 00:05:35.660 
The original papers, primary result was that the depth of the model was essential for its high performance which was computationally

51
00:05:35.660 --> 00:05:42.560 
expensive but made feasible due to the utilization of GPU acceleration for the training.

52
00:05:43.040 --> 00:05:54.240 
The network takes 6 days to train for 90 epochs on two GTX 580 GPUs.

53
00:05:54.240 --> 00:06:02.550 
AlexNet uses ReLU activation function and demonstrates that it surpasses Sigmoid function in the deeper network.

54
00:06:03.339 --> 00:06:13.750 
Let's first recap the sigmoid function here we list the mathematical formula of the sigmoid non-linear activation function.

55
00:06:14.439 --> 00:06:25.199 
The mass curve is shown in the figure, it inputs real values and squeeze them to the range of between 0 and 1, which

56
00:06:25.199 --> 00:06:31.040 
is suitable for the case where the output is the probability representations.

57
00:06:31.050 --> 00:06:36.810 
And now a few people use sigmoid in the process of building neural networks.

58
00:06:36.819 --> 00:06:41.920 
The main reason are following:

59
00:06:41.920 --> 00:06:46.250 
Sigmoid function saturation causes the gradient vanishing problem.

60
00:06:46.939 --> 00:06:54.350 
When activation of the neuron is close to 0 or 1, the gradient will be almost 0 in this area.

61
00:06:55.839 --> 00:07:04.790 
This will cause the gradient to disappear and almost no signal will be transmitted back to the previous layer in the back-

62
00:07:04.790 --> 00:07:06.250 
propagation process.

63
00:07:07.350 --> 00:07:16.850 
Second, the output of the sigmoid function is not zero-centered because the data of the input neuron is always positive.

64
00:07:16.860 --> 00:07:24.949 
The gradient of weights will be either all positive or all negative during the backpropagation, which will cause the zig

65
00:07:24.949 --> 00:07:32.560 
zag shape to appear in the gradient descent, which leading to instability in the training process.

66
00:07:34.839 --> 00:07:42.589 
The ReLU non-linear function is shown in the figure compared with the sigmoid and tanh functions.

67
00:07:42.620 --> 00:07:51.589 
ReLU has a significant acceleration effect on the convergence of stochastic gradient descent, sigmoid and tanh

68
00:07:51.600 --> 00:08:01.449 
contains exponential calculations when computing the derivative, while the computation overhead of ReLU derivation

69
00:08:01.459 --> 00:08:03.459 
is almost negligible.

70
00:08:04.040 --> 00:08:10.550 
The most important benefit is that ReLU can effectively avoid gradient vanishing.

71
00:08:10.939 --> 00:08:14.860 
We can see that its derivative always equals 1.

72
00:08:15.439 --> 00:08:24.160 
The problem of ReLU is also very obvious, if you look at the figure and the ReLU unit is relatively fragile and may die

73
00:08:24.170 --> 00:08:29.660 
because of the negative axis and it leads to the loss of data diversity.

74
00:08:30.040 --> 00:08:31.110 
In practice,

75
00:08:31.170 --> 00:08:41.039 
the probability of "dead" neurons can be reduced by setting the learning rate reasonably.

76
00:08:41.039 --> 00:08:43.250 
To overcome the death ReLU problem,

77
00:08:43.740 --> 00:08:48.480 
its negative axis is always equal to zero.

78
00:08:48.490 --> 00:08:57.440 
No data gest routed and there are different derived rectified linear unit function been proposed in recent years

79
00:08:57.450 --> 00:09:05.840 
such as the Leaky ReLU, which is a small portion of information through the negative axis. And the derivative

80
00:09:05.840 --> 00:09:07.730 
calculation is very simple.

81
00:09:07.740 --> 00:09:12.259 
It's constant number 0.01 for the negative case.

82
00:09:12.840 --> 00:09:16.429 
A similar approach is called the Parametric ReLU.

83
00:09:16.440 --> 00:09:25.470 
Instead of choosing a constant number by hand, it process to use the learnable parameter to control the slope of the negative

84
00:09:25.470 --> 00:09:26.059 
part.

85
00:09:27.440 --> 00:09:34.840 
Exponential linear unit uses exponential function to handle the information flow on the negative access.

86
00:09:34.850 --> 00:09:43.159 
It is very useful to improve accuracy but brings more computation overhead because of the exponential calculation.

87
00:09:45.039 --> 00:09:54.039 
There's another important technology proposed in AlexNet, which can effectively solve the over-fitting problem of a over

88
00:09:54.039 --> 00:10:00.360 
parametrized models, it is drop out function. In a machine learning model,

89
00:10:00.840 --> 00:10:07.549 
if the model has too many parameters like deep learning models and too few training examples,

90
00:10:07.620 --> 00:10:15.779 
the trained model is prone to over-fitting. When training neural networks, we often encounter over-fitting problems.

91
00:10:16.039 --> 00:10:25.789 
As we mentioned it before, AlexNet has 62.3 million parameters, which is obviously an over parametrized machine learning

92
00:10:25.789 --> 00:10:35.049 
model. Thus effective regularization techniques are essential to the generalization ability of the model.

93
00:10:35.639 --> 00:10:44.460 
Although the ImageNet dataset is quite large, using regularizer like dropouts can further improve the testing accuracy.

94
00:10:45.039 --> 00:10:49.259 
Dropout can be used as a trick for training deep neural networks.

95
00:10:49.549 --> 00:11:00.710 
Each training batch, by ignoring a certain proportion of feature nodes, for example, making half of the hidden neuron nodes

96
00:11:00.710 --> 00:11:06.490 
value to 0 and the over-fitting phenomenon can be significantly reduced.

97
00:11:07.039 --> 00:11:11.470 
This method can reduce the interaction between hidden layer nodes.

98
00:11:11.480 --> 00:11:21.309 
The interaction here means that the representation features of certain neurons is strongly dependent on the other neurons. Simply

99
00:11:21.309 --> 00:11:23.700 
speaking, when we do forward pass,

100
00:11:24.340 --> 00:11:32.909 
we just let the activation value of certain neurons stop working with a certain probability, which can make the

101
00:11:32.909 --> 00:11:42.250 
model more generalized because it will not rely too much on certain parts of the network model.

102
00:11:44.440 --> 00:11:48.940 
Okay,

103
00:11:48.940 --> 00:11:57.840 
as shown in this figure, we can just randomly drop some neural nodes in the network.

104
00:11:57.840 --> 00:12:03.960 
And why dropout works? Here, we can give you a brief explanation.

105
00:12:04.840 --> 00:12:10.259 
One interpretation is that dropout prevents co-adaption of features.

106
00:12:11.740 --> 00:12:12.659 
What it means?

107
00:12:13.340 --> 00:12:15.570 
What does the co-adaption of features mean?

108
00:12:15.580 --> 00:12:16.909 
In this example,

109
00:12:16.909 --> 00:12:27.379 
We can see that there are several different features used to represent a dog. We will then randomly drop out some of

110
00:12:27.389 --> 00:12:30.190 
those features in different forward paths.

111
00:12:30.740 --> 00:12:33.559 
Trying to decouple these representations.

112
00:12:34.039 --> 00:12:43.710 
In other words, if our neural network is making a certain prediction, it should now be too sensitive to some specific clues.

113
00:12:44.139 --> 00:12:52.950 
Even if the specific clue is lost, the network should be able to learn some common feature from many other clues. From this

114
00:12:52.950 --> 00:12:54.129 
perspective,

115
00:12:54.139 --> 00:12:58.029 
Dropout is a bit like L1 or

116
00:12:58.029 --> 00:12:58.389 
L2

117
00:12:58.389 --> 00:13:01.299 
regularization. Reducing the weight

118
00:13:01.740 --> 00:13:07.159 
makes the network more robust to the loss of specific neuron connections.

119
00:13:08.039 --> 00:13:17.360 
Another interpretation is that dropout is like training a large example of sub models with shared weights.

120
00:13:17.940 --> 00:13:27.059 
This is similar to how we use same training data to train multiple neural networks and generally get different results.

121
00:13:28.139 --> 00:13:37.350 
At this time we can use the "average" or "majority voting" to determine the final prediction result of multiple models.

122
00:13:38.240 --> 00:13:48.159 
And this comprehensive average strategy can usually effectively prevent over-fitting because different networks may produce

123
00:13:48.159 --> 00:13:58.159 
different over-fitting and averaging may make them some "opposite" fits offset to each other and drop out different hidden

124
00:13:58.159 --> 00:14:06.750 
neurons, similar to training various networks and randomly deleting half of the hidden neurons, results in a different

125
00:14:06.750 --> 00:14:07.850 
network structure.

126
00:14:08.740 --> 00:14:18.950 
The entire dropout process is equivalent to taking an average of many other neural networks and various networks produce

127
00:14:18.950 --> 00:14:25.860 
different over-fitting and some reverse fitting cancel out to reduce the over-fitting in the end.

128
00:14:29.840 --> 00:14:32.059 
Okay, thank you very much for watching the video.
