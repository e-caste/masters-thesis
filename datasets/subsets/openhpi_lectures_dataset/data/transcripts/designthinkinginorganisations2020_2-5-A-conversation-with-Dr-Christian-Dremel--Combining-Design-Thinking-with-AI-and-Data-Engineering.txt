WEBVTT

1
00:00:16.810 --> 00:00:21.970 
Welcome to our master class
on mastering design thinking

2
00:00:21.970 --> 00:00:25.590 
in large organizations. Today I
have the pleasure to have an

3
00:00:25.590 --> 00:00:31.130 
interview with a good friend, former
colleague and also

4
00:00:31.130 --> 00:00:36.170 
scientific colleague of mine
Christian Dremel. He is

5
00:00:36.380 --> 00:00:40.650 
an expert in artificial intelligence
and information systems

6
00:00:40.880 --> 00:00:45.910 
and today we have planned to talk
about the intersection of design

7
00:00:45.910 --> 00:00:49.940 
thinking and artificial
intelligence together with you.

8
00:00:50.810 --> 00:00:55.540 
Christian, can I head over quickly
to you for a brief introduction

9
00:00:55.540 --> 00:00:56.300 
of yourself?

10
00:01:03.680 --> 00:01:07.750 
I'm Christian Dremer, I'm currently a guest
lecturer at the University

11
00:01:07.750 --> 00:01:11.150 
in Bamberg and also associate
senior research fellow at the

12
00:01:11.150 --> 00:01:14.790 
University of St. Gallan. I am currently
heading the initiative

13
00:01:14.860 --> 00:01:18.840 
for data driven
enterprise at a large German

14
00:01:19.290 --> 00:01:23.950 
automotive or original equipment
supplier where I actually try

15
00:01:23.950 --> 00:01:27.910 
to build up technical and also
organizational preconditions

16
00:01:27.910 --> 00:01:34.280 
for leveraging AI and also big data. So it's
pretty much the direction of this module

17
00:01:34.520 --> 00:01:39.340 
and also in line of this we build up
competence for advances in

18
00:01:39.340 --> 00:01:43.500 
artificial intelligence where we
took our first steps in understanding

19
00:01:43.510 --> 00:01:47.990 
what does it really mean to leverage
AI on a daily basis so to

20
00:01:48.000 --> 00:01:51.350 
say. So what are the preconditional,
what is also necessary for

21
00:01:51.350 --> 00:01:55.630 
organizational way of working, or maybe
also for design thinking in this context.

22
00:01:56.640 --> 00:02:00.940 
Maybe lets start a discussion
with briefly talking above what

23
00:02:00.940 --> 00:02:05.120 
AI is from your
perspective because nowadays if

24
00:02:05.120 --> 00:02:11.920 
you look at the media and
articles, AI is everywhere and

25
00:02:11.930 --> 00:02:16.250 
it seems to be the next big
password but on the other hand

26
00:02:16.400 --> 00:02:19.990 
working at HPI, I know
it's a serious technology.

27
00:02:20.410 --> 00:02:26.580 
So, what is your perspective
on AI and your own definition?

28
00:02:26.590 --> 00:02:30.080 
I think that in the end artificial
intelligence is more or

29
00:02:30.080 --> 00:02:34.620 
less about allowing computer systems
to take over tasks which

30
00:02:34.620 --> 00:02:39.370 
normally require human intelligence.
In this regard, it's not entirely new,

31
00:02:39.380 --> 00:02:42.360 
it's a traditional perspective
of computer science and also

32
00:02:42.360 --> 00:02:47.060 
our original research field
information system science

33
00:02:47.180 --> 00:02:52.230 
but there are advancements for a
leveraging big data and also new

34
00:02:52.240 --> 00:02:55.320 
approaches like machine learning
and deep learning which allow

35
00:02:55.320 --> 00:03:01.210 
for tackling new problems which
before were not really feasible or doable

36
00:03:01.590 --> 00:03:05.760 
and I think the easiest
way to do it is to divide it in the

37
00:03:05.770 --> 00:03:10.420 
classical distinction general AI
and the narrow AI meaning that

38
00:03:10.420 --> 00:03:16.110 
general AI aiming for really general
human intelligence, so acting like

39
00:03:16.110 --> 00:03:20.750 
human, understanding problems, building
on the lessons learned and so on

40
00:03:21.150 --> 00:03:28.490 
and narrow AI really tackling one narrow task,
a task which is reproducible,

41
00:03:28.620 --> 00:03:34.720 
repetitive and maybe also easier
to be represented in data

42
00:03:34.720 --> 00:03:38.100 
so to say because only if
the data presents the problem

43
00:03:38.220 --> 00:03:41.640 
you can actually use AI,
because AI is a means to an end

44
00:03:41.960 --> 00:03:46.240 
and not the end itself, so to say. So
you need to understand your problem

45
00:03:46.380 --> 00:03:50.350 
such that you are able to get to
your goal and maybe on that way

46
00:03:50.740 --> 00:03:55.150 
if the data is there, you can
use AI. I mean by looking

47
00:03:55.150 --> 00:03:59.410 
at the industry where are we right
now, I mean google, facebook,

48
00:03:59.570 --> 00:04:04.500 
all the tech giants for sure,
they use the technology

49
00:04:04.510 --> 00:04:08.730 
not just since yesterday and I think
they're really big in this topic

50
00:04:08.910 --> 00:04:12.770 
but by looking to the
traditional industry and you just

51
00:04:12.770 --> 00:04:16.450 
mention that you are
working

52
00:04:17.440 --> 00:04:22.170 
for a supplier in the automotive
industry, so is the topic

53
00:04:22.180 --> 00:04:27.990 
already there? It is absolutely,
but for several reasons, right?

54
00:04:28.000 --> 00:04:32.390 
As you mentioned, one is the hyped
topic and also the password itself, so

55
00:04:32.390 --> 00:04:36.190 
a lot of consultancies, also technology
departments say it makes sense to invest

56
00:04:36.190 --> 00:04:40.570 
in that and I have the honest opinion
that it makes sense but the right way

57
00:04:41.020 --> 00:04:46.750 
and slowly the companies understand
it might be helpful. But

58
00:04:46.880 --> 00:04:51.360 
besides that of course
traditional companies were not

59
00:04:51.380 --> 00:04:56.400 
digital born and not really have a technology
background which is able

60
00:04:56.400 --> 00:05:01.270 
to collect data in a sofisticated way. They
struggle with the homework in normal cases

61
00:05:01.540 --> 00:05:05.720 
and that's rather interesting.
Also not only the homework from a

62
00:05:05.720 --> 00:05:09.150 
technology perspective but also to
understand the problem themselves.

63
00:05:09.230 --> 00:05:13.820 
It seems obvious and trivial so
to say, but something I feel

64
00:05:13.820 --> 00:05:17.050 
and also that's my experience from
a practitioner role

65
00:05:17.190 --> 00:05:22.640 
where we still need to take some
steps. I mean by looking

66
00:05:22.640 --> 00:05:28.630 
from now on into the future what do you
think will be the impact in ten years?

67
00:05:28.790 --> 00:05:33.550 
Is the hype over or will
it stay? I mean it's like

68
00:05:33.550 --> 00:05:37.930 
programming today, right? In the end
AI might be comparable to

69
00:05:37.930 --> 00:05:43.120 
programming but programming on the base
of data itself, so you give your

70
00:05:43.120 --> 00:05:46.010 
model some data and
the model itself finds the

71
00:05:46.010 --> 00:05:50.080 
rules, so to say, and you can tune
it and tweak it a little bit and you

72
00:05:50.250 --> 00:05:54.180 
do not have to ride your if clause,
while clause and so on.

73
00:05:54.670 --> 00:05:58.500 
So I think it will be
more or less a normal method

74
00:05:58.510 --> 00:06:02.940 
of approach which just in a
which is applicable for anyone

75
00:06:02.950 --> 00:06:06.710 
but the companies themselves
need to decide do I need to build

76
00:06:06.710 --> 00:06:11.440 
up expertise on this, so like we do
today as well, what can

77
00:06:11.550 --> 00:06:15.760 
I buy from external and
exactly which problems

78
00:06:16.190 --> 00:06:21.530 
might be the fitting ones.
And it's I think a natural development

79
00:06:21.540 --> 00:06:26.270 
in our society today.
There is a new event you are able to,

80
00:06:26.270 --> 00:06:30.980 
a new technology and
you slowly get to adopt it throughout

81
00:06:30.980 --> 00:06:36.100 
the enterprises, from small to medium
to large companies and of course

82
00:06:36.600 --> 00:06:40.680 
as always it's a matter of culture,
human and also the expertise

83
00:06:40.680 --> 00:06:42.370 
in the organization
itself.

84
00:06:43.030 --> 00:06:47.900 
Can you give us a brief example
of the application, may be of

85
00:06:47.900 --> 00:06:52.480 
narrow AI nowadays in
industry, is there something?

86
00:06:53.070 --> 00:06:57.020 
Absoluetely. So I think the most interesting
one and also the most easiest

87
00:06:57.030 --> 00:07:01.260 
two problems where it's
understandable how AI might help

88
00:07:01.610 --> 00:07:06.990 
is the topic of computer vision, meaning
that you aim for understanding

89
00:07:06.990 --> 00:07:12.350 
the image of self and let the AI model
decide what do we see in this image.

90
00:07:12.500 --> 00:07:15.550 
For instance this could be cancer
detection, also not really

91
00:07:15.550 --> 00:07:20.510 
in you, right? It's something healthcare
does a lot of time but I still

92
00:07:21.030 --> 00:07:27.010 
see the potential and also in the industry
I currently as some manufacturing focus

93
00:07:27.270 --> 00:07:31.070 
it's just quality checks or
understanding, slowly we're getting

94
00:07:31.070 --> 00:07:35.900 
in a way where our products are
not okay and we need to change that.

95
00:07:36.510 --> 00:07:44.490 
So in fact removing the I
and the human itself, so to say are

96
00:07:44.500 --> 00:07:47.960 
the most promising ones currently,
but despite that of course

97
00:07:47.970 --> 00:07:53.390 
fraud detection and also understanding unusual
behavior and organizations is also key.

98
00:07:53.660 --> 00:07:59.820 
Also regard to IT security for instance,
sometimes organizations

99
00:07:59.820 --> 00:08:04.260 
tend to neglect
this but the key issue today

100
00:08:04.260 --> 00:08:08.080 
and something where AI is already
helpful and was helpful beforehand

101
00:08:08.080 --> 00:08:12.160 
also most likely five
years ago already. Maybe let's

102
00:08:12.160 --> 00:08:15.900 
start talking about this
intersection between AI and

103
00:08:15.910 --> 00:08:19.050 
design-thinking and our
listeners might now wonder

104
00:08:19.800 --> 00:08:23.290 
is there any and why are we
talking about this, right?

105
00:08:23.930 --> 00:08:30.130 
And maybe just as info to the
listeners of this programme,

106
00:08:32.150 --> 00:08:36.220 
Christian was basically a colleague
at the University of St.

107
00:08:36.220 --> 00:08:41.490 
Gallan when I was teaching this class
for the last ten twelve years and

108
00:08:41.860 --> 00:08:46.340 
he was always seen around basically
with his technological view.

109
00:08:46.940 --> 00:08:49.880 
Therefore the
question to you Christian,

110
00:08:51.020 --> 00:08:55.490 
do you see any any overlap and
connection point between

111
00:08:55.820 --> 00:09:00.750 
the two? Between design thinking and
artificial intelligence or other technology?

112
00:09:00.890 --> 00:09:04.420 
Absolutely and I think
that's also a key issue

113
00:09:04.430 --> 00:09:07.120 
we need to tackle if we
look at AI adoption and

114
00:09:07.120 --> 00:09:10.260 
maybe being data driven as an
enterprise for instance.

115
00:09:10.780 --> 00:09:17.140 
I think the way to understand that
you have most likely something already thought,

116
00:09:17.470 --> 00:09:20.830 
for our problems we need
to understand what is obviously

117
00:09:20.830 --> 00:09:24.270 
the problem we are talking
about here. How can we define this

118
00:09:24.270 --> 00:09:27.670 
problem, what do we want to
address the value in it

119
00:09:28.170 --> 00:09:30.240 
and on the other hand you
have your solution space

120
00:09:30.770 --> 00:09:34.150 
and you want to understand what
is the right real solution which

121
00:09:34.150 --> 00:09:36.660 
helps a company or human
being, so to say.

122
00:09:37.310 --> 00:09:41.920 
And only if the problem space
itself is data related and you

123
00:09:41.920 --> 00:09:46.850 
also have the problem represented
in data, you can actually

124
00:09:46.860 --> 00:09:51.450 
use AI in the next step or use it
as an option for your solution

125
00:09:51.450 --> 00:09:55.300 
space, so to say, or for one
solution. And on top of that

126
00:09:55.730 --> 00:09:59.800 
AI itself is data
analysis simplified, right? So

127
00:09:59.800 --> 00:10:05.220 
you might meet excellent user interface, you
might need to define your product around it

128
00:10:05.550 --> 00:10:10.020 
and maybe also might need to hide AI
because AI is not always

129
00:10:10.280 --> 00:10:15.540 
the way of bringing your
product to your customer. So there

130
00:10:15.540 --> 00:10:21.220 
are a lot of aspects which still
hold true and which even reinforce

131
00:10:21.230 --> 00:10:23.850 
the importance of deciding
today from my point of view.

132
00:10:24.610 --> 00:10:28.210 
But now that sounds very
logical and simple, right?

133
00:10:28.700 --> 00:10:33.440 
Analyze the problem space and
please consider the data and

134
00:10:33.480 --> 00:10:39.600 
I mean we both know sometimes if it sounds
simple it's then hard to realize, right?

135
00:10:40.120 --> 00:10:46.830 
So is that the case and if
yes, why? So, why is it so

136
00:10:46.870 --> 00:10:51.910 
hard to understand this simple
relationship between problem and data?

137
00:10:52.070 --> 00:10:56.920 
I think one issue on this regard
is, first all, you need really

138
00:10:56.920 --> 00:11:00.520 
to understand the business problems
and most likely everyone and

139
00:11:00.520 --> 00:11:04.030 
also before I joined the practice,
to be honest I thought

140
00:11:04.030 --> 00:11:06.780 
everyone knows the business
problems and that something

141
00:11:06.960 --> 00:11:10.630 
not always hold true. It's
better to ask what is all the

142
00:11:10.630 --> 00:11:14.550 
real problem and start there.
And on the next step, for having

143
00:11:14.550 --> 00:11:19.350 
a problem be reconsidered in
data you have, you need to have

144
00:11:19.360 --> 00:11:22.240 
the information systems and
your architecture in place which

145
00:11:22.240 --> 00:11:26.820 
is able to do so. So you have to do your
homework beforehand or you have

146
00:11:26.970 --> 00:11:31.560 
to do it on the fly. And with that
there needs to be a shift from

147
00:11:31.560 --> 00:11:35.010 
my point of view and also
from my academic perspective.

148
00:11:35.310 --> 00:11:39.880 
We need to think for organizations,
how do we really need to

149
00:11:39.880 --> 00:11:44.450 
make a technology future-proof
such that I'm able to use

150
00:11:44.450 --> 00:11:49.490 
AI systems and applications on a large
scale and also enterprise, so to say.

151
00:11:49.880 --> 00:11:53.710 
So I think these are the
two main issues and the first one,

152
00:11:54.110 --> 00:11:57.180 
understanding the problem is still
key and will always be still key

153
00:11:57.180 --> 00:11:58.590 
from my point of view.

154
00:12:00.240 --> 00:12:04.540 
And I mean by looking at the
human in specifically

155
00:12:04.830 --> 00:12:07.310 
as part of the
problem space,

156
00:12:08.450 --> 00:12:11.560 
what would you consider
the role of that

157
00:12:12.290 --> 00:12:17.430 
human being by searching
for problems and also then

158
00:12:17.700 --> 00:12:19.790 
matching to
the data layer?

159
00:12:21.050 --> 00:12:24.580 
I think there are several aspects
to consider, right? If

160
00:12:24.580 --> 00:12:28.190 
you think from a prspective, from
an IT department for instance,

161
00:12:28.190 --> 00:12:30.670 
or even data science
and data engineering,

162
00:12:31.320 --> 00:12:35.340 
you need to get your business
department to even understand

163
00:12:35.790 --> 00:12:40.180 
what is actually possible
today. So there needs to be this

164
00:12:40.180 --> 00:12:45.080 
interaction from the
human being knowing the business problem

165
00:12:45.720 --> 00:12:49.870 
or at least understanding together those
with these guys the business problem.

166
00:12:50.090 --> 00:12:53.630 
On the other hand also the
solution space when you actually

167
00:12:53.630 --> 00:12:55.190 
arrive there,

168
00:12:55.930 --> 00:12:59.400 
show them what's possible and
what we actually need to do

169
00:12:59.410 --> 00:13:04.170 
to be able to leverage this. And
I think there needs to be some,

170
00:13:04.380 --> 00:13:06.460 
how can I say,
some kind of

171
00:13:07.320 --> 00:13:11.580 
Confidence and also trust that there
might be a solution because companies

172
00:13:11.580 --> 00:13:17.270 
today at least the ones I am
aware of always try to exactly

173
00:13:17.270 --> 00:13:19.490 
pinpoint the solution
from day one

174
00:13:20.300 --> 00:13:24.100 
and you should be able to at
least give it sometime, righ? Give

175
00:13:24.100 --> 00:13:27.240 
it some time to understand the
problem and also get the people

176
00:13:27.240 --> 00:13:30.790 
together who're experts in
their field and collaborate, not

177
00:13:30.790 --> 00:13:36.330 
really like, that's my requirement, please solve it
with AI, because this will not work.

178
00:13:36.580 --> 00:13:41.300 
It's very much like, Christian, that this
goes deeply into the software

179
00:13:41.300 --> 00:13:45.190 
development processes as well,
right? I mean nowadays everyone

180
00:13:45.190 --> 00:13:49.610 
is talking about AGILE, SCRUM
implementation. Is this not solving

181
00:13:49.610 --> 00:13:56.820 
this issue or what's the challenge here? It
might solve this issue, definitely, but

182
00:13:57.210 --> 00:14:03.040 
as design thinking, agile or agility
might be a method but besides

183
00:14:03.040 --> 00:14:06.280 
that there is always a mindset
and the human behind the drug.

184
00:14:06.340 --> 00:14:11.990 
You can not force an employee or
any human being to act and use raw

185
00:14:11.990 --> 00:14:15.490 
methods or use design thinking.
So they have

186
00:14:15.490 --> 00:14:19.350 
to be in line with this and I
think that's a really changed

187
00:14:19.360 --> 00:14:23.630 
aspect to be honest and change today
and also organizational change

188
00:14:23.840 --> 00:14:26.950 
increases from my point of view
also in importance if you

189
00:14:26.950 --> 00:14:30.670 
look at data themselves design
thinking, activity

190
00:14:31.090 --> 00:14:34.910 
and I think you pretty much pinpoint
to the right direction but

191
00:14:34.910 --> 00:14:38.320 
the interesting thing from
my point of view was my experience,

192
00:14:38.600 --> 00:14:42.630 
all these things need to converge
for new technology such that

193
00:14:42.630 --> 00:14:46.020 
you can actually leverage this
and of course it's like juggling a

194
00:14:46.020 --> 00:14:50.000 
lot of balls, right? It's difficult
to do and you have to

195
00:14:50.010 --> 00:14:52.640 
have people willing to
learn in this regard.

196
00:14:54.270 --> 00:15:00.720 
That's interesting. So,
I mean, by looking at your

197
00:15:00.800 --> 00:15:04.760 
picture that you just showed
with the problem

198
00:15:04.760 --> 00:15:07.850 
space and the solution
space, I am now wondering

199
00:15:08.320 --> 00:15:13.700 
if there is also a possibility
to support with AI another

200
00:15:13.700 --> 00:15:16.110 
paradigm that's
called prototyping.

201
00:15:16.880 --> 00:15:22.620 
So what's about this, do you think
that we can even prototype AI

202
00:15:22.730 --> 00:15:29.300 
quickly to gain confidence, if
certain data are enough

203
00:15:30.010 --> 00:15:35.310 
to work on a real AI model? Actually
this is the approach we're currently

204
00:15:35.310 --> 00:15:39.360 
doing, to get a one
hundred percent model is

205
00:15:39.360 --> 00:15:43.670 
even not feasible, but at
least you go to high percentages. You need

206
00:15:43.670 --> 00:15:47.120 
to understand what's the range we're currently
in and what can be tweaked.

207
00:15:47.610 --> 00:15:52.400 
So you definitely need to exploit the
problem not only from the space of

208
00:15:52.630 --> 00:15:57.830 
problem space, so to say, but also is it
really feasible in the first place? So,

209
00:15:58.080 --> 00:16:02.480 
how you would approach a data
related problem if you look at

210
00:16:02.480 --> 00:16:07.920 
it conceptually. It will be pretty much the
same as you do with design thinking. So

211
00:16:08.130 --> 00:16:13.250 
that's it. So that means,
sorry for interrupting you,

212
00:16:13.260 --> 00:16:19.190 
but by looking at the math and
statistics you would then go for

213
00:16:19.410 --> 00:16:24.640 
a simplified version of a certain model,
basically with

214
00:16:24.790 --> 00:16:29.720 
a smaller set of data and then
you can kind of test prototype

215
00:16:29.730 --> 00:16:34.090 
what you imagine to do in
the future driven AI algorithm and

216
00:16:34.240 --> 00:16:37.600 
for sure it will not have the
quality level, but .. Maybe

217
00:16:37.600 --> 00:16:41.780 
to dig a little deeper into the
details if you look for instance

218
00:16:41.830 --> 00:16:45.870 
quality chesks, while doing manufacturing
right? In the first place

219
00:16:45.870 --> 00:16:49.840 
to understand if this really makes
sense from an organizational perspective,

220
00:16:50.090 --> 00:16:53.210 
you need to understand one, the
business case behind this

221
00:16:53.300 --> 00:16:57.240 
and what is actually a problem
we are solving here. So we already discussed

222
00:16:57.410 --> 00:17:04.020 
this. Second, do we have any data
and with this data we have can

223
00:17:04.020 --> 00:17:07.920 
we at least show the direction
is worthwhile doing because data

224
00:17:08.410 --> 00:17:12.540 
scientists are expensive and you
need to put in a lot of effort

225
00:17:12.710 --> 00:17:16.760 
and afterwards you will need to go the
next steps of the development

226
00:17:16.760 --> 00:17:20.760 
maybe also prototype with an AI model
and a short user interface, as you

227
00:17:20.760 --> 00:17:25.490 
do always with design thinking as well and
go the next steps and tune your model,

228
00:17:25.500 --> 00:17:29.200 
make hyperparameters right, maybe
change a little architecture

229
00:17:29.200 --> 00:17:33.250 
or something like that. But that's
only worthwhile if you know

230
00:17:33.250 --> 00:17:36.360 
that the direction is the right
one, right? So you need to get your

231
00:17:36.360 --> 00:17:39.800 
feedback from the human who's
actually profiting from this.

232
00:17:40.030 --> 00:17:44.330 
This might be a department or
department head, but in manufacturing

233
00:17:44.370 --> 00:17:47.900 
also an employee at the line for
instance and afterwards you

234
00:17:47.900 --> 00:17:52.150 
can take the next steps because only
if this combination of the

235
00:17:52.150 --> 00:17:56.670 
assistance of a human and with
IT system as we know this already,

236
00:17:56.930 --> 00:18:01.420 
works then the adoption will work
as well and we would profit from it.

237
00:18:03.240 --> 00:18:07.890 
Do you think we also need to work on
new kinds of design requirements

238
00:18:07.890 --> 00:18:09.430 
or that we

239
00:18:10.170 --> 00:18:15.750 
need to look at new design considerations
in the future when we

240
00:18:16.010 --> 00:18:20.090 
want to adopt more AI? I mean you
said in the beginning it's becoming

241
00:18:20.380 --> 00:18:25.500 
a new programming language or it will
be like a programming language

242
00:18:25.510 --> 00:18:27.500 
in our daily life.

243
00:18:28.230 --> 00:18:30.960 
What do you mean exactly with
design considerations?

244
00:18:31.670 --> 00:18:35.830 
So that we are on the same page
on this. So, good question.

245
00:18:36.320 --> 00:18:41.780 
So I think we also need to incorporate
in certain case ethical aspects

246
00:18:42.210 --> 00:18:45.880 
design AI. For sure not with
quality checking system

247
00:18:45.880 --> 00:18:52.370 
I think, or at least I don't see any
ethical problems arising but

248
00:18:52.620 --> 00:18:58.820 
for end consumer systems for
example we might need to think

249
00:18:58.820 --> 00:19:03.520 
about biases as early as
possible because with data

250
00:19:04.370 --> 00:19:06.810 
and so that we not

251
00:19:07.790 --> 00:19:11.910 
run in trouble ethically
with certain results. So

252
00:19:12.360 --> 00:19:16.370 
do you think that we as a
design-thinking community need to

253
00:19:16.800 --> 00:19:19.960 
look for new guidelines as
well that we haven't thought

254
00:19:19.960 --> 00:19:25.200 
about so far? I think, yes but
because there is one

255
00:19:25.200 --> 00:19:29.760 
direction where also artificial intelligence
and machine learning in particular go

256
00:19:29.760 --> 00:19:34.410 
and this is when your solution or your
product itself learns on the fly

257
00:19:34.410 --> 00:19:39.910 
so to say. So when a human is interacting
with the application or the system

258
00:19:40.520 --> 00:19:44.520 
it might be the case that
the system wants to learn

259
00:19:44.520 --> 00:19:49.730 
to prove, so to say. Only if of
course this process of learning

260
00:19:49.730 --> 00:19:53.990 
from the interaction
with the human is designed for,

261
00:19:54.270 --> 00:19:59.620 
actually your product is getting better.
And similarly as discussed beforehand

262
00:19:59.760 --> 00:20:03.810 
only if you give the or build
your AI system in such a way that the

263
00:20:03.810 --> 00:20:08.400 
data used presentable for your
problem and this means not only

264
00:20:08.400 --> 00:20:13.770 
just a problem in a narrow sense but
also from a generalized perspective. So

265
00:20:13.920 --> 00:20:19.160 
it needs to be balanced in the
statistical way because otherwise

266
00:20:19.160 --> 00:20:23.880 
your model is going nuts to be
honest. So you will have an end

267
00:20:24.100 --> 00:20:28.920 
result which is totally out of the
scope, so to say, if it's understandable.

268
00:20:29.130 --> 00:20:33.610 
So I think there are multiple
aspects, one from the using AI

269
00:20:33.630 --> 00:20:36.300 
systems, so do you
want to actually

270
00:20:37.600 --> 00:20:42.860 
get people to understand that there
is AI behind it. Interestingly

271
00:20:42.870 --> 00:20:46.580 
there are lots of applications today in
which not all of the people would

272
00:20:46.580 --> 00:20:48.620 
be aware that AI
is in it, so to say,

273
00:20:49.230 --> 00:20:53.560 
simplified chunk plates and
anything like that.

274
00:20:53.570 --> 00:20:57.040 
Also spotify playlists, netflix playlists

275
00:20:57.040 --> 00:21:01.720 
or recommendations, it's natural
but we don't know what's under it, right?

276
00:21:02.340 --> 00:21:06.140 
It needs work and I think that's
also an interesting question

277
00:21:06.140 --> 00:21:08.600 
when designing AI system,
do we need to show it

278
00:21:10.040 --> 00:21:16.060 
and is this worth doing or do we
purposely decide to hide it

279
00:21:16.370 --> 00:21:20.350 
because it makes sense,
it makes it easier

280
00:21:20.350 --> 00:21:23.110 
to get its head around or
something like that.

281
00:21:24.780 --> 00:21:30.270 
If your question is answered with
this? Yeah absolutely absolutely.

282
00:21:30.500 --> 00:21:32.850 
I'm now thinking if
we can touch

283
00:21:33.600 --> 00:21:37.930 
the opposite topic. I mean, so far
we have only talked or what

284
00:21:37.930 --> 00:21:41.520 
means only, I mean that's already
a lot, so we have talked about the

285
00:21:42.390 --> 00:21:47.050 
from design or we have been
looking from design thinking into AI

286
00:21:47.190 --> 00:21:52.790 
and what design thinking can provide
to the AI community to

287
00:21:53.440 --> 00:21:57.910 
build better systems by at the
end of the day by incorporating

288
00:21:57.910 --> 00:22:04.530 
early enough the problem definition and
also seeking for the right data to

289
00:22:05.110 --> 00:22:12.410 
approach that problem but I'm now
thinking about what could AI

290
00:22:12.610 --> 00:22:16.620 
and it may be very undesign
thinking but still now I have you

291
00:22:16.620 --> 00:22:20.930 
as an expert on the phone, what
could AI may be provide to

292
00:22:20.930 --> 00:22:22.910 
the design thinking
community?

293
00:22:23.820 --> 00:22:26.740 
I think one interesting aspect
and it's something where the

294
00:22:26.740 --> 00:22:32.440 
AI is already exercised or
tried for is using artificial

295
00:22:32.440 --> 00:22:36.830 
intelligence to build on designs
and recommend designs, so

296
00:22:36.860 --> 00:22:42.320 
to say. For instance a quite
easy an obvious applicability for

297
00:22:42.320 --&gt; 00:22:45.530
myself being a lot of my time
in the automotive industry

298
00:22:46.120 --> 00:22:49.340 
taking the cars and understanding
which type of cars has one

299
00:22:49.340 --> 00:22:52.560 
print actually manufacturing
and just deciding just make a

300
00:22:52.570 --> 00:22:56.380 
decision or design
proposal and that's something which

301
00:22:56.380 --> 00:22:59.660 
is today already feasible and
afterwards build on that, right?

302
00:22:59.810 --> 00:23:03.550 
Maybe team it there, maybe
you do not need to reinvent

303
00:23:03.550 --> 00:23:07.970 
the whole image because in
all cases if you look at design

304
00:23:07.970 --> 00:23:11.880 
and also design from a prospective
where a brand is behind this design

305
00:23:12.210 --> 00:23:15.050 
you're always and
oftentimes try to

306
00:23:15.940 --> 00:23:20.590 
make the customer understand that
must be an iPhone for instance,

307
00:23:20.600 --> 00:23:25.020 
or something like that, right? Or this
needs to be a car from this manufacturer

308
00:23:25.400 --> 00:23:27.740 
and I think there are quite
interesting, at least,

309
00:23:28.980 --> 00:23:33.110 
proposals you can decide on and
also get your hand and head

310
00:23:33.110 --> 00:23:35.940 
around to expand your
your own imagination.

311
00:23:36.360 --> 00:23:40.040 
And I think that's something where
you really could leverage

312
00:23:40.050 --> 00:23:43.980 
AI, more in a creative
way, so to say. The

313
00:23:43.980 --> 00:23:49.920 
support system that helps me in shaping certain
types of prototype by proposing me.

314
00:23:50.330 --> 00:23:54.390 
Exactly, so easing
the way of being creative and

315
00:23:54.390 --> 00:23:58.510 
as always, right, if you want
to be creative or also want

316
00:23:58.510 --> 00:24:02.360 
to change on some kind
of impulse so to say and AI

317
00:24:02.360 --> 00:24:04.470 
might be able to provide
something like that

318
00:24:04.890 --> 00:24:09.140 
and I'm pretty sure that some
companies already do this because

319
00:24:09.140 --> 00:24:10.970 
that's quite
obvious now.

320
00:24:12.300 --> 00:24:14.770 
So maybe last question.

321
00:24:15.380 --> 00:24:19.900 
Is there anything that was
really surprising you when

322
00:24:19.900 --> 00:24:27.440 
you started in industry with AI? Either
a good surprise or a bad surprise,

323
00:24:27.560 --> 00:24:33.710 
whatever? What surprised you
really? To be honest, I think

324
00:24:33.720 --> 00:24:37.500 
we touched upon this
pretty much and it's that

325
00:24:37.990 --> 00:24:42.490 
business departments and also
if you look at a problem, the

326
00:24:42.500 --> 00:24:46.120 
most difficult thing is to
understand the problem itself and

327
00:24:46.120 --> 00:24:50.290 
in the next thing if you look at
AI is this problem really

328
00:24:50.290 --> 00:24:54.110 
solvable with a new method such
as AI because it's just

329
00:24:54.110 --> 00:25:00.160 
means to an end and do we have the data? These
are quite simple questions but

330
00:25:00.420 --> 00:25:04.830 
you have to state them and
look for answers to this

331
00:25:05.100 --> 00:25:09.830 
and they are obvious. In the normal
cases you would think that

332
00:25:09.840 --> 00:25:15.190 
the companies are aware of this and I
think with this course mastering design thinking,

333
00:25:15.950 --> 00:25:20.240 
this will actually be at the right
direction for just understanding

334
00:25:20.240 --> 00:25:24.280 
the problem and then look for the solution.
So thank you very much

335
00:25:24.280 --> 00:25:27.590 
for taking the time today,
talking with me and sharing

336
00:25:27.590 --> 00:25:32.180 
your insights and thoughts with all the
participants of this class and

337
00:25:32.460 --> 00:25:35.920 
to all the listeners, I wish you
a good day and I hope to see

338
00:25:35.920 --> 00:25:39.760 
you soon as part of the
one of the next series.

339
00:25:40.260 --> 00:25:42.000 
Thanks. Bye.
