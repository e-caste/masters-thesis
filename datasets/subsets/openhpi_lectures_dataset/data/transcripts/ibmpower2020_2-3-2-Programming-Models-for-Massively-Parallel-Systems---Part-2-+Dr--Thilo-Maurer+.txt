WEBVTT

1
00:00:00.870 --> 00:00:04.950 
ok. So in the second part of
this presentation, I will talk

2
00:00:04.950 --> 00:00:09.160 
about more practical aspects of
parallelizing applications.

3
00:00:10.130 --> 00:00:15.120 
so when we start to do parallelize
applications, there are

4
00:00:16.020 --> 00:00:21.230 
three different ways of doing
it. The first way and

5
00:00:21.230 --> 00:00:26.140 
usually, the easiest way is to use
something that's already available,

6
00:00:26.270 --> 00:00:29.240 
which means to use
libraries. That, at least

7
00:00:30.580 --> 00:00:34.920 
in a large extent covers
what the application does.

8
00:00:35.330 --> 00:00:39.080 
Usually, there are other
programmers that have around

9
00:00:39.080 --> 00:00:42.730 
the same problem is
itself. It is now

10
00:00:43.490 --> 00:00:48.850 
about to tackle. and so this
problem may actually already

11
00:00:48.850 --> 00:00:52.640 
be solved, and then you have the
way to go is to use libraries.

12
00:00:52.750 --> 00:00:55.760 
For example, there's a very good
library for deep learning, it is

13
00:00:55.910 --> 00:01:01.210 
cuDNN provided by the video and implements
pretty much everything one needs to

14
00:01:01.570 --> 00:01:09.520 
run. It's that your deep neural
network calculations on GPU.

15
00:01:11.610 --> 00:01:14.080 
and well the
second way,

16
00:01:15.270 --> 00:01:19.190 
when there's nothing readily
available like a library,

17
00:01:19.470 --> 00:01:23.280 
to actually modifies on applications
in a more intense way.

18
00:01:23.690 --> 00:01:29.410 
but we still produce
portable code, which means

19
00:01:29.420 --> 00:01:33.750 
that it can run on multiple
architectures without modification.

20
00:01:33.980 --> 00:01:37.690 
Let it be on a regular CPU
or massively parallel

21
00:01:38.180 --> 00:01:43.040 
GPU, and this method is worked by
using of compiler directives.

22
00:01:43.040 --> 00:01:46.670 
They're relatively easy
to be learned and

23
00:01:48.330 --> 00:01:53.180 
they help the
compiler to

24
00:01:53.930 --> 00:01:59.890 
understand well understand
maybe not the right word, to

25
00:02:00.030 --> 00:02:05.430 
implement a way of parallelizing
sections of code or loops.

26
00:02:05.650 --> 00:02:08.890 
in a way, as you requested by
these compiler directives.

27
00:02:10.000 --> 00:02:15.070 
Another thing is that when you start
to mix program languages like OpenMP

28
00:02:15.070 --> 00:02:19.890 
and OpenACC. the application
code may become hard to read

29
00:02:20.100 --> 00:02:25.160 
since in the end, you might have
fifty percent of lines by

30
00:02:25.160 --> 00:02:30.100 
OpenMP and fifty percent lines
by OpenACC. Then it is still

31
00:02:30.570 --> 00:02:35.380 
like fifty percent
of the lines from

32
00:02:35.380 --> 00:02:37.260 
your own portable code.

33
00:02:38.140 --> 00:02:43.570 
so what I want to say is that your code
could become to be very convoluted

34
00:02:43.990 --> 00:02:46.560 
with the compiler
directives,

35
00:02:48.260 --> 00:02:51.260 
the actual application
code might become not

36
00:02:52.160 --> 00:02:56.160 
clear to the third person that
starts to read your code.

37
00:02:57.660 --> 00:02:59.470 
And the method
which is the most

38
00:03:00.670 --> 00:03:06.730 
difficult and can also provide
the best performance to

39
00:03:06.960 --> 00:03:12.690 
your application could be to use (
extensions to) programming languages.

40
00:03:12.840 --> 00:03:16.010 
That is well suited for massively
parallel applications.

41
00:03:16.500 --> 00:03:19.440 
It's actually a
huge effort to

42
00:03:20.130 --> 00:03:25.180 
realize such implementations
because you usually have to

43
00:03:25.360 --> 00:03:29.670 
completely rewrite his own
application. And examples for

44
00:03:29.670 --> 00:03:33.810 
MPI, we used it here for
example MPI and CUDA.

45
00:03:38.040 --> 00:03:42.560 
Now let's talk a little bit about
these programming models.

46
00:03:42.760 --> 00:03:48.070 
So that's Pthreads. that's the most
basic thing in shared memory

47
00:03:48.500 --> 00:03:52.360 
processing. It's
also very old.

48
00:03:53.590 --> 00:03:58.880 
When people started building parallel
processes, it has the meaning of that

49
00:03:59.310 --> 00:04:04.880 
CPU cause could hold
multiple threats.

50
00:04:06.190 --> 00:04:10.480 
It wasn't easy to do this because
every processor was different. So

51
00:04:10.860 --> 00:04:13.600 
people looked for a
standard in 1995

52
00:04:14.170 --> 00:04:18.380 
they created so-called
attributes POSIX standard

53
00:04:18.750 --> 00:04:23.580 
which is short for a portable
operating system interface. And it

54
00:04:25.120 --> 00:04:29.210 
allows a software
developer to define

55
00:04:29.710 --> 00:04:38.230 
threats within a Linux process that runs
concurrently to the main program.

56
00:04:38.820 --> 00:04:41.360 
there are a lot of

57
00:04:42.610 --> 00:04:45.620 
management

58
00:04:46.540 --> 00:04:51.400 
called set you can do, to create terminate,
join and detach these threats.

59
00:04:51.650 --> 00:04:55.050 
And also there's a lot of
infrastructures to create sections

60
00:04:55.050 --> 00:04:58.680 
of mutual exclusion,
creating, destroying

61
00:04:59.090 --> 00:05:01.780 
, unlocking,
and locking.

62
00:05:03.740 --> 00:05:09.060 
And as said earlier, as
being an SNP system

63
00:05:09.970 --> 00:05:14.650 
the communication
between

64
00:05:14.700 --> 00:05:17.520 
the threats of the
workers, in this case,

65
00:05:18.030 --> 00:05:22.750 
is by the use of common
variables and these common

66
00:05:22.750 --> 00:05:26.680 
variables are often called condition
variables in this context, which means

67
00:05:26.880 --> 00:05:31.530 
that you write something to these variables
and then the second thread will

68
00:05:31.880 --> 00:05:35.390 
notice the change and this
event will drive. then the

69
00:05:35.390 --> 00:05:37.190 
synchronization
between the tasks.

70
00:05:37.850 --> 00:05:41.960 
the largest system currently
available is the

71
00:05:41.960 --> 00:05:45.940 
IBM E880 that can
host this massively

72
00:05:46.580 --> 00:05:48.020 
parallel

73
00:05:49.380 --> 00:05:53.080 
thread count. and you can have
up to fifteen hundred parallel

74
00:05:53.080 --> 00:05:55.590 
threads at the same time in
the shared memory system.

75
00:06:00.660 --> 00:06:04.530 
The next thing I want
to talk about is

76
00:06:04.530 --> 00:06:09.580 
the message passing interface. It's also
extremely old and before was most to be

77
00:06:09.980 --> 00:06:14.320 
used in high-performance
computing,

78
00:06:14.790 --> 00:06:17.920 
pretty much every high-performance
computing application

79
00:06:17.920 --> 00:06:20.880 
is using this MPI to
communicate between

80
00:06:21.580 --> 00:06:26.340 
its workers which are here called the
ranks of the application. so each

81
00:06:27.010 --> 00:06:31.610 
process is what they call the rank
and these ranks then communicate

82
00:06:32.070 --> 00:06:37.630 
with each other by sending
messages to each other.

83
00:06:38.080 --> 00:06:43.530 
a rank and then
triggering

84
00:06:44.230 --> 00:06:48.310 
processing by the reception
of such messages.

85
00:06:50.080 --> 00:06:55.830 
So MPI is not a product.
It's standard,

86
00:06:56.280 --> 00:07:00.650 
which means that there is a vast number
of implementations of this standard.

87
00:07:01.120 --> 00:07:04.710 
each a hardware when it is pretty
much has its own implementation.

88
00:07:04.830 --> 00:07:08.680 
For example as the IBM PE-MPI
, there is Intel MPI ,

89
00:07:08.680 --> 00:07:10.700 
Microsoft there's
MPI.NET.

90
00:07:11.920 --> 00:07:14.330 
But there are also open
implementations of OpenMPI.

91
00:07:15.270 --> 00:07:17.780 
and the most commonly
used one today is

92
00:07:18.350 --> 00:07:19.740 
most likely OpenMPI.

93
00:07:21.870 --> 00:07:25.760 
It also supports massive
parallelism and it also has not

94
00:07:25.760 --> 00:07:32.280 
been designed from the beginning to be
supportive of massive parallelism,

95
00:07:32.890 --> 00:07:36.980 
since it has been developing
for an age where massively

96
00:07:36.990 --> 00:07:39.580 
parallel computing systems
have not been available.

97
00:07:40.300 --> 00:07:44.940 
But it's still used today in large
collocation sort of service

98
00:07:44.940 --> 00:07:47.320 
so-called clusters for
example like the BG/Q

99
00:07:47.730 --> 00:07:50.860 
where would run for five hundred
thousand ranks in parallel

100
00:07:50.870 --> 00:07:51.620 
successfully.

101
00:07:56.660 --> 00:07:59.240 
Now how does it
work? As I said,

102
00:08:00.170 --> 00:08:05.850 
each processor is called
rank, and each rank

103
00:08:05.860 --> 00:08:08.460 
has a number which is
called the rank number.

104
00:08:08.960 --> 00:08:13.970 
And the rank number decides
which role of this sub-program

105
00:08:13.970 --> 00:08:16.740 
takes into the full
task decomposition.

106
00:08:17.550 --> 00:08:20.880 
but the main thing which
needs to understand

107
00:08:20.880 --> 00:08:25.220 
is that actually each of these rank
absolutely executes in the same

108
00:08:25.590 --> 00:08:31.030 
program code. it is not meaning
that it executes the identical

109
00:08:31.030 --> 00:08:34.770 
instructions at all times, but
executes the same application

110
00:08:34.770 --> 00:08:40.570 
code. Only this rank number discriminates
which rank fulfills which tasks,

111
00:08:41.110 --> 00:08:46.700 
In this task composition
that I have shown earlier.

112
00:08:47.120 --> 00:08:52.630 
so earlier you were seeing these
the acyclic directed graph.

113
00:08:52.820 --> 00:08:56.130 
where you

114
00:08:56.800 --> 00:09:00.710 
the lines connecting the work
items that had to be done.

115
00:09:01.100 --> 00:09:04.610 
And you were seeing
five a vertical

116
00:09:05.350 --> 00:09:08.630 
a grouping of
these dots.

117
00:09:09.870 --> 00:09:15.070 
And these groupings resample
what is called a rank here. So

118
00:09:16.310 --> 00:09:21.380 
the rank decides which task executes
in the sequence is defined by

119
00:09:21.850 --> 00:09:23.660 
the directed
acyclic graph.

120
00:09:25.720 --> 00:09:29.970 
something else needs to understand
here is with a message passing

121
00:09:29.970 --> 00:09:35.580 
a paradigm or the with MPI is
that not every rank sees

122
00:09:35.780 --> 00:09:40.250 
or has full access to all of the
data that is to be processed,

123
00:09:40.760 --> 00:09:44.930 
instead the full set of
data is divided into

124
00:09:45.580 --> 00:09:50.690 
a number of chunks. And each
chunk is attributed to a single

125
00:09:50.690 --> 00:09:55.160 
rank. So each rank just accesses a
chunk or a part of the whole data.

126
00:09:56.100 --> 00:09:58.450 
And then it also only

127
00:09:59.870 --> 00:10:01.700 
computes using
this data.

128
00:10:03.360 --> 00:10:06.730 
Subtask does use this data,
and then communication

129
00:10:06.730 --> 00:10:10.660 
takes place after that the subtask is
done, then to communicate the result

130
00:10:10.660 --> 00:10:14.560 
back to all the other
ranks, and then

131
00:10:15.040 --> 00:10:21.700 
to use this to
reduce a partially

132
00:10:22.860 --> 00:10:27.630 
fulfilled sub-tasks
to trigger

133
00:10:28.270 --> 00:10:29.710 
the following subtasks.

134
00:10:32.180 --> 00:10:35.680 
and the communication itself as I
said in message-passing which

135
00:10:35.680 --> 00:10:39.220 
was usually done explicitly by the programmer,
which means that the programmer will

136
00:10:39.360 --> 00:10:43.570 
have to take care of this orchestration
of what I just said namely

137
00:10:43.730 --> 00:10:47.690 
sending data around. a generator
running the subtasks deciding

138
00:10:47.690 --> 00:10:50.710 
which subtask, what it
is doing and so on.

139
00:10:54.250 --> 00:10:56.680 
And then there
is an OpenMP,

140
00:10:57.570 --> 00:11:02.980 
which is essentially an extension or a
simplification of the Pthreads model.

141
00:11:03.750 --> 00:11:08.100 
In that sense, the programmer
doesn't have to explicitly work

142
00:11:08.100 --> 00:11:14.220 
with all these threats, but it can
provide annotations to the compiler,

143
00:11:15.190 --> 00:11:19.390 
so that the compiler can
paralyze sections of code

144
00:11:21.360 --> 00:11:25.280 
onto multiple tasks. in the
way the programmer said

145
00:11:26.100 --> 00:11:29.350 
within these short annotations.
For example, if you have a code

146
00:11:29.350 --> 00:11:32.310 
block which does a set
of instructions,

147
00:11:32.720 --> 00:11:37.450 
then you can prefix that code
block by pragma OMP parallel.

148
00:11:38.040 --> 00:11:41.500 
And then the compiler will
generate a code, such that

149
00:11:41.950 --> 00:11:44.290 
on each thread,

150
00:11:45.030 --> 00:11:50.240 
this single block
is executed

151
00:11:51.770 --> 00:11:55.570 
by each of the threats. and i mean
actually by each of the threads,

152
00:11:55.570 --> 00:11:56.930 
they all do the
same thing.

153
00:11:58.010 --> 00:12:04.220 
so they can be used for a lot
of things, but it's not maybe

154
00:12:04.450 --> 00:12:09.680 
the most frequent pragma
that people use., The most

155
00:12:09.680 --> 00:12:14.690 
frequently used pragma would
be OpenMP parallel form,

156
00:12:15.580 --> 00:12:19.260 
which means that the compiler
knows that it is a pragma

157
00:12:19.420 --> 00:12:25.600 
that the following for loop blocks
or loop blocks will be divided

158
00:12:25.850 --> 00:12:31.050 
between the executing
threads. Let's say,

159
00:12:31.780 --> 00:12:36.150 
five threats get the first five
federations sent home. the next

160
00:12:36.500 --> 00:12:39.520 
five threads get the next tending
to raise incent and so on.

161
00:12:39.830 --> 00:12:46.080 
there are different ways of distributing
which loops are executed on which thread.

162
00:12:46.560 --> 00:12:51.180 
but what it doesn't do is that it
doesn't do the task decomposition

163
00:12:51.180 --> 00:12:54.330 
for you in any way. the compiler
just can't do it. it doesn't

164
00:12:54.330 --> 00:12:59.310 
know from the code how to generate
this acyclic directed graph.

165
00:12:59.430 --> 00:13:04.620 
and you could use but you can use this
program to realize your task decomposition.

166
00:13:05.370 --> 00:13:11.270 
open p is also one of the most successful
languages for doing multiprocessing.

167
00:13:11.950 --> 00:13:14.490 
it is an SMP system.

168
00:13:15.710 --> 00:13:21.820 
but in the past people have started
to integrate a massively parallel

169
00:13:22.350 --> 00:13:26.490 
compute accelerators
into openMP,

170
00:13:27.720 --> 00:13:33.410 
which in the past

171
00:13:33.410 --> 00:13:36.700 
was necessary to talk about
while a message passing.

172
00:13:37.010 --> 00:13:41.520 
so for example with GPUs,
they are attached

173
00:13:41.520 --> 00:13:46.260 
to the host processor via pc express. pc
expresses is a message passing interface.

174
00:13:46.770 --> 00:13:50.090 
so the whole process needs
to send the request

175
00:13:50.100 --> 00:13:54.110 
to the GPU, and then the
GPU can do that task

176
00:13:54.460 --> 00:13:56.570 
intensity since results
back to the host.

177
00:13:57.540 --> 00:14:01.190 
but this is kind of a conflict
because message passing an

178
00:14:01.310 --> 00:14:06.110 
openMP which shared memory system
doesn't really fit together.

179
00:14:06.410 --> 00:14:08.070 
but using this

180
00:14:08.760 --> 00:14:13.850 
this pragmatic annotations
or compiler directives.

181
00:14:14.030 --> 00:14:18.150 
it is very easy for the compiler
tool to also hide that difficulty

182
00:14:18.460 --> 00:14:21.920 
away from him. and the compiler
will take care of the

183
00:14:21.930 --> 00:14:24.440 
complex communication
explicit implicitly.

184
00:14:25.300 --> 00:14:33.290 
still the user will have to take
care of the transfer of the data

185
00:14:33.490 --> 00:14:36.450 
by adding extra

186
00:14:37.460 --> 00:14:43.340 
a compiler directives. and these usually
start with pragma OpenMP device x,y,z.

187
00:14:43.640 --> 00:14:49.500 
in order to discriminate the
regular snp progress from the

188
00:14:50.240 --> 00:14:51.730 
device of lord promise.

189
00:14:53.560 --> 00:14:58.160 
and then there's
openACC, which is also

190
00:14:58.580 --> 00:15:00.210 
a language

191
00:15:01.710 --> 00:15:06.620 
using this term so-called a
compiler directives approach.

192
00:15:06.930 --> 00:15:11.840 
but this time it's
accelerator-driven.

193
00:15:12.500 --> 00:15:16.810 
and it doesn't have to do
anything with the host process

194
00:15:16.820 --> 00:15:22.090 
that is just you know
hosting the GPU device,

195
00:15:23.820 --> 00:15:27.230 
which means that you can't control
anything that the host stars.

196
00:15:27.290 --> 00:15:30.330 
but what you can do is you
can control from the host

197
00:15:30.330 --> 00:15:33.260 
what is being of-loaded within
your program to the device,

198
00:15:33.540 --> 00:15:37.510 
how things are calculated there,
then how data is transferred

199
00:15:37.510 --> 00:15:38.400 
back to the host.

200
00:15:40.680 --> 00:15:47.800 
the compiler directives are very
similar to openMP. as you

201
00:15:47.800 --> 00:15:49.120 
can see on the
right they are

202
00:15:49.810 --> 00:15:52.930 
pretty much the
same. just the

203
00:15:53.810 --> 00:15:57.270 
the naming is a little bit
different. something that is a

204
00:15:58.020 --> 00:16:02.580 
new or actually conceptually
different from OpenMP is that

205
00:16:02.580 --> 00:16:07.560 
there's a case called
pragma ACC kernels.

206
00:16:07.990 --> 00:16:10.280 
and

207
00:16:11.400 --> 00:16:14.760 
that's a directive that
allows the compiler

208
00:16:15.390 --> 00:16:20.220 
to automatically infer what a
potential directed acyclic

209
00:16:20.220 --> 00:16:23.450 
graph for the section under
inspection could potentially be,

210
00:16:23.450 --> 00:16:24.830 
and come up with
suggestions.

211
00:16:25.490 --> 00:16:30.600 
and implement them into
the usable receive

212
00:16:30.600 --> 00:16:34.940 
report on what has parallelized in
which way and the user can then use

213
00:16:35.180 --> 00:16:42.280 
what it is and
improvements using

214
00:16:42.560 --> 00:16:49.970 
more involved parallelization,
which is using multiple

215
00:16:51.460 --> 00:16:55.660 
separate pragmatic instructions.
one thing that's

216
00:16:57.820 --> 00:17:02.070 
nice is that OpenACC can be used for
multiple programming languages.

217
00:17:02.200 --> 00:17:06.840 
and that is compatible with
with other libraries that

218
00:17:06.850 --> 00:17:12.220 
the same vendor provides. so they are you
can mix it with libraries like glass

219
00:17:12.500 --> 00:17:17.640 
cool for your transform calls
bar's matrix operations in

220
00:17:18.080 --> 00:17:21.080 
many more. since they come
from the same window.

221
00:17:28.220 --> 00:17:33.890 
but that is what I explained
as so far very parts. but

222
00:17:34.340 --> 00:17:38.110 
the thing is today you have to
use all of them in the same

223
00:17:38.120 --> 00:17:43.770 
program at the same time. today HPC,
high-performance computing is hybrid

224
00:17:43.940 --> 00:17:50.440 
and is what would program have to
use as many core host that

225
00:17:50.930 --> 00:17:55.460 
eventually coherently attaches a
massively parallel accelerator.

226
00:17:55.960 --> 00:17:59.320 
the massive accelerator
needs to either be

227
00:17:59.320 --> 00:18:02.180 
programmed in OpenACC or
cuDNN at the most many cost

228
00:18:02.180 --> 00:18:06.140 
need to be programmed using openACC
or Pthreads. and that means

229
00:18:06.140 --> 00:18:09.850 
that the typical app is actually
a very convoluted use of all

230
00:18:09.850 --> 00:18:15.240 
the models that are available.
and of course in HPC you

231
00:18:15.240 --> 00:18:18.140 
have multiple hosts who will
have to use OpenMP as well.

232
00:18:18.650 --> 00:18:20.830 
so you put it pretty much
need to use all of what it

233
00:18:21.260 --> 00:18:22.820 
have shown earlier.

234
00:18:24.420 --> 00:18:28.120 
there's a nice tutorial
around on the web,

235
00:18:28.600 --> 00:18:34.710 
which guides you through six
or seven steps that show

236
00:18:34.710 --> 00:18:41.660 
you how to do simple things in OpenMP
and OpenACC during the steps

237
00:18:41.910 --> 00:18:44.930 
well, they quickly become non-trivial.
and you'll actually have

238
00:18:44.930 --> 00:18:48.600 
to do a lot of work to parallelize the
application and you will actually realise

239
00:18:48.900 --> 00:18:53.970 
what are the typical problems and
how does it feel to realize

240
00:18:54.410 --> 00:18:59.490 
the task of compositions and
implementing such using OpenACC

241
00:18:59.500 --> 00:19:01.030 
and OpenMP.

242
00:19:02.160 --> 00:19:07.100 
the next to the last thing is
CUDA. CUDA is great because it

243
00:19:07.100 --> 00:19:11.250 
allows the programmer
to have

244
00:19:12.360 --> 00:19:14.250 
the greatest
freedom in what

245
00:19:14.890 --> 00:19:18.780 
he can do with his massive
parallel architecture.

246
00:19:19.440 --> 00:19:23.560 
this is mentioned earlier
and extension to the C

247
00:19:24.100 --> 00:19:28.130 
and C++ programming languages.
it can also be used by

248
00:19:28.140 --> 00:19:29.560 
FORTRAN  in python

249
00:19:30.270 --> 00:19:34.980 
and it exposes the complete GPU
to the programmers, such that

250
00:19:35.090 --> 00:19:39.810 
he can make the best use of all
the resources that are that

251
00:19:39.810 --> 00:19:41.450 
are in this device.

252
00:19:42.340 --> 00:19:47.670 
CUDA has actually been short for computer
unified device architecture.

253
00:19:47.840 --> 00:19:50.560 
but they quickly dropped this name
because it was just you know

254
00:19:50.670 --> 00:19:51.820 
nobody could
remember it.

255
00:19:57.480 --> 00:20:02.870 
it's actually a very small set
of extensions to c and c++

256
00:20:03.170 --> 00:20:09.010 
that  allows

257
00:20:09.020 --> 00:20:12.160 
to parallelize C and C++
applications on the GPUs.

258
00:20:12.860 --> 00:20:18.480 
so which means that most of
your regular code can stay

259
00:20:18.760 --> 00:20:22.730 
standard c. and just from time to
time we will have to use APIs

260
00:20:22.730 --> 00:20:26.580 
to manage the devices to the
memory of the data transfers.

261
00:20:26.690 --> 00:20:30.800 
and obviously also occur
randomly on the GPU.

262
00:20:33.320 --> 00:20:37.690 
CUDA has started out a long
time ago when the first

263
00:20:37.690 --> 00:20:42.380 
GPU came on the market. and at that
point in the time, there were no

264
00:20:42.740 --> 00:20:45.080 
shared memory
accelerators available.

265
00:20:45.760 --> 00:20:51.270 
but today this has changed and with a feeling.
it is now possible to coherently couple

266
00:20:51.780 --> 00:20:53.830 
accelerators to the
host processes,

267
00:20:54.540 --> 00:20:56.650 
which means that
in principle,

268
00:20:57.260 --> 00:21:02.030 
you can now see the set of CPU puls GPU
as a single of shared memory system.

269
00:21:02.530 --> 00:21:08.660 
and CUDA is also now embracing this
approach the latest versions like CUDA6

270
00:21:08.930 --> 00:21:10.400 
,7,8,9.

271
00:21:11.110 --> 00:21:13.150 
such dead the user

272
00:21:14.070 --> 00:21:20.530 
can remove lots of his
added GPU codes.

273
00:21:21.080 --> 00:21:25.920 
such that the application
codes are much more

274
00:21:26.390 --> 00:21:27.460 
well

275
00:21:30.120 --> 00:21:32.930 
much more simple,

276
00:21:33.920 --> 00:21:36.770 
like it was in
the beginning,

277
00:21:36.770 --> 00:21:41.140 
before he added his extra CUDA
code to support the GPU.

278
00:21:44.060 --> 00:21:49.000 
and the last thing I want to talk about
today is about an apache spark,

279
00:21:49.540 --> 00:21:53.210 
which is an open source framework for
cluster computing that has been

280
00:21:53.600 --> 00:22:00.130 
designed to cope with
problems in big data. and

281
00:22:01.370 --> 00:22:05.920 
the big data set here is being
managed by an abstraction

282
00:22:05.920 --> 00:22:08.780 
that is called a resilient
distributed data set

283
00:22:09.450 --> 00:22:12.690 
that is cared about by
the spark runtime.

284
00:22:13.180 --> 00:22:19.220 
such that the programmer just has
to specify rather high level

285
00:22:19.460 --> 00:22:25.680 
requests. and the parallelization
will then be done implicitly

286
00:22:25.930 --> 00:22:29.400 
using the spark
framework, using

287
00:22:29.440 --> 00:22:31.030 
message passing
under the hood.

288
00:22:32.340 --> 00:22:37.290 
there are a lot of libraries that
can extend this basic spark core

289
00:22:37.450 --> 00:22:41.200 
to the programmers' needs. let it
be for databases like spark

290
00:22:41.200 --> 00:22:47.640 
SQL or for machine learning using in
the lab or for graph processing using

291
00:22:48.100 --> 00:22:55.480 
GraphML. the user can use multiple
programming languages to attach to

292
00:22:55.580 --> 00:22:59.580 
a spark service, which is
themselves implemented using

293
00:22:59.580 --> 00:23:03.080 
scala, which is a functional
language. but and only using this

294
00:23:03.080 --> 00:23:05.260 
functional language, it
will be possible to

295
00:23:06.570 --> 00:23:08.390 
realize

296
00:23:11.510 --> 00:23:16.120 
involved and still
parallelisable

297
00:23:17.230 --> 00:23:21.510 
algorithms, since only with the
functional approach of scholars

298
00:23:21.800 --> 00:23:25.080 
the compiler or the executing
runtime will be able to

299
00:23:25.510 --> 00:23:30.930 
at least partly decomposed
your code into

300
00:23:31.770 --> 00:23:35.960 
a highly efficient task
decomposition.

301
00:23:36.880 --> 00:23:41.660 
it's currently very efficient on
many-core and cluster hardware, since

302
00:23:42.480 --> 00:23:49.530 
a scholar isn't intrinsically data-parallel
and not compute parallel,

303
00:23:50.070 --> 00:23:54.040 
which means that typical
GPUs which suffer from

304
00:23:54.570 --> 00:23:59.530 
divergence  will

305
00:24:00.160 --> 00:24:03.540 
not be the best thing to
execute such code.

306
00:24:04.090 --> 00:24:08.280 
that brings the question of
whether this can be a framework

307
00:24:08.280 --> 00:24:11.850 
that can be used for massive
parallelism? well,

308
00:24:13.210 --> 00:24:17.780 
there we will see that in the
future, as it seems right now

309
00:24:17.780 --> 00:24:21.210 
a massive parallel
systems are mainly

310
00:24:22.200 --> 00:24:25.830 
driven by GPU
developments.

311
00:24:26.490 --> 00:24:30.380 
and these  GPUs  are

312
00:24:31.300 --> 00:24:36.780 
in sense, as it is explained in the first
part of this talk, mainly, governed by

313
00:24:36.970 --> 00:24:41.460 
weaker execution units, which
doesn't really match together

314
00:24:41.460 --> 00:24:47.120 
with the way that spark is
implemented where the granularity

315
00:24:47.120 --> 00:24:48.380 
isn't that fine.

316
00:24:52.060 --> 00:24:57.210 
so I talked about a lot of things
in these two presentations.

317
00:24:57.210 --> 00:25:00.270 
and you have your curious, you
can't go into these details and

318
00:25:00.270 --> 00:25:04.620 
do some further reading to understand
how it actually does feel with you by

319
00:25:04.870 --> 00:25:08.650 
using these techniques
that I described.
