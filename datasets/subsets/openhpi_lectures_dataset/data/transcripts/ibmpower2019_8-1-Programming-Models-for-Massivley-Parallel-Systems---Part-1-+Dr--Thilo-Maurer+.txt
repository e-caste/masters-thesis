WEBVTT

1
00:00:00.420 --> 00:00:02.579 
Hi, this talk is

2
00:00:02.580 --> 00:00:05.069 
about programming models for

3
00:00:05.070 --> 00:00:06.719 
Massively Parallel Systems.

4
00:00:08.130 --> 00:00:09.130 
My name is

5
00:00:10.500 --> 00:00:12.029 
Thilo Maurer and I work at IBM

6
00:00:13.440 --> 00:00:15.449 
in BÃ¶blingen which is one of

7
00:00:15.450 --> 00:00:17.609 
our research labs, one of our worldwide

8
00:00:17.610 --> 00:00:20.069 
research labs in a department

9
00:00:20.070 --> 00:00:22.229 
which deals with the

10
00:00:22.230 --> 00:00:24.329 
acceleration of POWER systems

11
00:00:24.330 --> 00:00:26.639 
and the integration of massively

12
00:00:26.640 --> 00:00:28.469 
parallel accelerators into this

13
00:00:28.470 --> 00:00:29.470 
architecture.

14
00:00:30.510 --> 00:00:32.189 
And so,

15
00:00:33.510 --> 00:00:35.519 
Massive Parallel Systems, what is

16
00:00:35.520 --> 00:00:36.419 
it?

17
00:00:36.420 --> 00:00:38.669 
I'll give a short disambiguation what

18
00:00:38.670 --> 00:00:40.679 
determines what memory

19
00:00:40.680 --> 00:00:42.839 
models are employed to realize

20
00:00:42.840 --> 00:00:45.029 
implementations of algorithms

21
00:00:45.030 --> 00:00:47.399 
using massively parallel systems.

22
00:00:47.400 --> 00:00:49.739 
I'll talk about the task

23
00:00:49.740 --> 00:00:51.809 
decomposition and the challenges

24
00:00:51.810 --> 00:00:53.789 
that you

25
00:00:53.790 --> 00:00:54.790 
will encounter.

26
00:00:55.710 --> 00:00:57.329 
And then later, I'll talk about easy

27
00:00:57.330 --> 00:00:59.759 
ways to parallelize applications and

28
00:00:59.760 --> 00:01:01.709 
go in detail about multiple programing

29
00:01:01.710 --> 00:01:03.200 
models that one employs today.

30
00:01:05.880 --> 00:01:08.609 
So massively

31
00:01:08.610 --> 00:01:10.589 
parallel processing is the term that

32
00:01:10.590 --> 00:01:11.717 
is of interest and

33
00:01:13.080 --> 00:01:15.209 
in principle, it says

34
00:01:15.210 --> 00:01:17.369 
what it says, it's

35
00:01:17.370 --> 00:01:19.559 
the distribution of

36
00:01:19.560 --> 00:01:22.229 
all the processing of a task

37
00:01:22.230 --> 00:01:24.329 
using an extremely large number

38
00:01:24.330 --> 00:01:26.309 
of independent workers, also known

39
00:01:26.310 --> 00:01:28.205 
as processes and

40
00:01:30.320 --> 00:01:32.929 
well, the disambiguation between

41
00:01:32.930 --> 00:01:35.809 
standard processing and

42
00:01:35.810 --> 00:01:38.149 
massively parallel processing shifts

43
00:01:38.150 --> 00:01:40.219 
very much with the availability of new

44
00:01:40.220 --> 00:01:42.559 
hardware, and

45
00:01:42.560 --> 00:01:44.149 
that's bad for a definition.

46
00:01:44.150 --> 00:01:46.639 
I mean, the feelings

47
00:01:46.640 --> 00:01:48.559 
that one has, the feeling that

48
00:01:48.560 --> 00:01:50.719 
programmers have in 2019 is like

49
00:01:51.770 --> 00:01:54.059 
sequential would be parallelism

50
00:01:54.060 --> 00:01:55.069 
of one.

51
00:01:55.070 --> 00:01:57.259 
Multicore would be parallelism of

52
00:01:57.260 --> 00:02:00.139 
less than 16 because the largest

53
00:02:00.140 --> 00:02:02.539 
GPUs available today use more

54
00:02:02.540 --> 00:02:04.219 
or less 16 cores.

55
00:02:04.220 --> 00:02:06.249 
And then there's maybe a little bit

56
00:02:06.250 --> 00:02:09.319 
future systems that are many core

57
00:02:09.320 --> 00:02:12.169 
that have more than 16 cores.

58
00:02:12.170 --> 00:02:14.729 
And well that's a category

59
00:02:14.730 --> 00:02:16.759 
that's kind of in decline

60
00:02:16.760 --> 00:02:18.800 
right now since Intel

61
00:02:20.030 --> 00:02:22.418 
just announced the Xeon

62
00:02:22.419 --> 00:02:23.389 
5 architecture.

63
00:02:23.390 --> 00:02:25.999 
And then there's currently this

64
00:02:26.000 --> 00:02:28.189 
realm of more than

65
00:02:28.190 --> 00:02:29.289 
a parallelism with more than 64

66
00:02:31.700 --> 00:02:33.739 
which is massively parallel.

67
00:02:33.740 --> 00:02:35.779 
But the main point is that it can be

68
00:02:35.780 --> 00:02:37.969 
much larger than 64 like thousands

69
00:02:37.970 --> 00:02:38.899 
and millions.

70
00:02:38.900 --> 00:02:41.149 
But this feeling

71
00:02:41.150 --> 00:02:42.379 
of these numbers shifts very much.

72
00:02:42.380 --> 00:02:43.969 
And it's not a clear disambiguation of

73
00:02:43.970 --> 00:02:45.739 
what is massively parallel and what

74
00:02:45.740 --> 00:02:48.109 
not. So what is actually

75
00:02:48.110 --> 00:02:49.049 
massively parallel?

76
00:02:49.050 --> 00:02:50.179 
What do we mean? What is the

77
00:02:50.180 --> 00:02:51.180 
background?

78
00:02:52.370 --> 00:02:54.529 
The background is that also

79
00:02:54.530 --> 00:02:57.199 
a transistor is extremely cheap

80
00:02:57.200 --> 00:02:59.330 
and I mean really extremely cheap.

81
00:03:01.910 --> 00:03:04.099 
People use a lot of them, but there's

82
00:03:04.100 --> 00:03:05.930 
a limit of how much you can

83
00:03:07.250 --> 00:03:09.379 
economically fabricate into

84
00:03:09.380 --> 00:03:11.569 
a single computing device,

85
00:03:11.570 --> 00:03:13.969 
so the largest devices today are like

86
00:03:13.970 --> 00:03:16.549 
20, 30 billion transistors.

87
00:03:16.550 --> 00:03:18.169 
But then if you want to have more than

88
00:03:18.170 --> 00:03:19.170 
these.

89
00:03:20.240 --> 00:03:22.999 
The price of these integration's

90
00:03:23.000 --> 00:03:24.619 
the cost of these integrations just

91
00:03:24.620 --> 00:03:26.509 
explodes so that each point in time

92
00:03:26.510 --> 00:03:28.759 
there's a limit

93
00:03:28.760 --> 00:03:31.149 
of what is economically feasible,

94
00:03:31.150 --> 00:03:33.289 
but and one will have to live

95
00:03:33.290 --> 00:03:34.550 
with this amount.

96
00:03:35.570 --> 00:03:38.329 
But the applications

97
00:03:38.330 --> 00:03:40.549 
that you want to run on, these

98
00:03:40.550 --> 00:03:42.739 
are what they want to calculate using

99
00:03:42.740 --> 00:03:44.129 
these transistors.

100
00:03:44.130 --> 00:03:46.519 
They are different in all kinds

101
00:03:46.520 --> 00:03:49.169 
of, there's a lot of applications.

102
00:03:49.170 --> 00:03:50.809 
They're totally different.

103
00:03:50.810 --> 00:03:53.119 
And some of them might require

104
00:03:53.120 --> 00:03:55.149 
different set

105
00:03:55.150 --> 00:03:56.769 
up of these transistors, I mean, a

106
00:03:56.770 --> 00:03:59.259 
different interconnect

107
00:03:59.260 --> 00:04:00.459 
between these transistors.

108
00:04:02.330 --> 00:04:03.889 
Then other applications, so, for

109
00:04:03.890 --> 00:04:05.929 
example, there's deep

110
00:04:05.930 --> 00:04:07.129 
neural networks which

111
00:04:08.150 --> 00:04:10.939 
do very symmetric processing

112
00:04:10.940 --> 00:04:13.399 
of information,

113
00:04:13.400 --> 00:04:15.499 
and then there's on the other hand,

114
00:04:15.500 --> 00:04:17.660 
there's applications like databases

115
00:04:19.279 --> 00:04:21.768 
which need to deal with

116
00:04:21.769 --> 00:04:24.259 
a huge amount of different requests.

117
00:04:24.260 --> 00:04:26.569 
At the same time, that all may

118
00:04:26.570 --> 00:04:28.639 
be different in nature, but still

119
00:04:28.640 --> 00:04:31.549 
need to be processed in parallel.

120
00:04:31.550 --> 00:04:32.929 
So that means applications have

121
00:04:32.930 --> 00:04:35.029 
different demands and therefore it may

122
00:04:35.030 --> 00:04:36.350 
make sense. And it actually,

123
00:04:37.580 --> 00:04:39.149 
that's what people do right now, is to

124
00:04:39.150 --> 00:04:41.239 
generate multiple

125
00:04:41.240 --> 00:04:43.579 
architectures, compute architectures

126
00:04:43.580 --> 00:04:45.409 
using the same transistors.

127
00:04:45.410 --> 00:04:47.659 
So in the past,

128
00:04:47.660 --> 00:04:50.299 
there was and there is

129
00:04:50.300 --> 00:04:52.519 
regular CPU's as one

130
00:04:52.520 --> 00:04:55.039 
is using in laptops and

131
00:04:55.040 --> 00:04:57.589 
servers and mobile phones and watches

132
00:04:57.590 --> 00:04:58.759 
and pretty much everything.

133
00:04:59.780 --> 00:05:01.309 
And then on the other hand, there is

134
00:05:01.310 --> 00:05:03.649 
this new architecture type, which

135
00:05:03.650 --> 00:05:05.959 
is commonly called Massively Parallel

136
00:05:05.960 --> 00:05:07.969 
Systems, which uses

137
00:05:07.970 --> 00:05:10.489 
a different computer architecture

138
00:05:10.490 --> 00:05:12.660 
that uses a reduced complexity

139
00:05:13.760 --> 00:05:15.919 
for each processor, so that means

140
00:05:15.920 --> 00:05:17.989 
there are fewer transistors for

141
00:05:17.990 --> 00:05:19.549 
each of these processors, but then

142
00:05:21.140 --> 00:05:22.699 
given the still

143
00:05:23.750 --> 00:05:25.759 
valid maximum transistors

144
00:05:25.760 --> 00:05:27.529 
count that is economically feasible,

145
00:05:27.530 --> 00:05:29.629 
you will have a higher processor

146
00:05:29.630 --> 00:05:32.029 
count per system in comparison

147
00:05:32.030 --> 00:05:33.679 
to the more traditional architecture

148
00:05:33.680 --> 00:05:35.539 
where you try to make very strong

149
00:05:35.540 --> 00:05:37.609 
processors with a

150
00:05:37.610 --> 00:05:39.690 
high transistor count each processor.

151
00:05:41.570 --> 00:05:44.479 
And now this is just

152
00:05:44.480 --> 00:05:46.519 
the disambiguation.

153
00:05:46.520 --> 00:05:48.439 
When you do a special-purpose

154
00:05:48.440 --> 00:05:50.839 
implementation towards

155
00:05:50.840 --> 00:05:52.729 
high core counts or high processor

156
00:05:52.730 --> 00:05:54.769 
counts, then this is

157
00:05:54.770 --> 00:05:56.359 
what is called a massively parallel

158
00:05:56.360 --> 00:05:57.360 
system.

159
00:06:00.520 --> 00:06:01.520 
Now, what

160
00:06:03.360 --> 00:06:04.859 
massively parallel systems are

161
00:06:04.860 --> 00:06:05.860 
available today?

162
00:06:06.720 --> 00:06:08.849 
The latest silicon-based,

163
00:06:08.850 --> 00:06:10.829 
massively parallel system is

164
00:06:10.830 --> 00:06:13.709 
in nVidia's Tesla GPU.

165
00:06:13.710 --> 00:06:16.069 
As of today, the latest generation

166
00:06:16.070 --> 00:06:18.509 
is Volter codename

167
00:06:18.510 --> 00:06:20.939 
V100. And this massive parallelism

168
00:06:20.940 --> 00:06:22.086 
features 5000

169
00:06:23.310 --> 00:06:25.769 
independent workers that can

170
00:06:25.770 --> 00:06:27.749 
collectively work on solving

171
00:06:27.750 --> 00:06:28.750 
a problem.

172
00:06:31.150 --> 00:06:33.039 
They are the leaders in the field at

173
00:06:33.040 --> 00:06:34.779 
the moment regarding massive

174
00:06:34.780 --> 00:06:36.939 
parallelism on a single chip or

175
00:06:36.940 --> 00:06:39.123 
single CPU and

176
00:06:40.430 --> 00:06:42.139 
it is possible to then combine these

177
00:06:42.140 --> 00:06:43.189 
two larger systems,

178
00:06:44.300 --> 00:06:46.669 
but historically they have

179
00:06:46.670 --> 00:06:48.949 
these are rather new

180
00:06:48.950 --> 00:06:52.069 
these types of so-called GPU's

181
00:06:52.070 --> 00:06:54.289 
and in the past, one had

182
00:06:54.290 --> 00:06:56.299 
more, but

183
00:06:56.300 --> 00:06:58.129 
one didn't have this massively parallel

184
00:06:58.130 --> 00:07:00.289 
chips available, but nevertheless,

185
00:07:00.290 --> 00:07:01.801 
one required to generate

186
00:07:03.340 --> 00:07:05.609 
large parallelism for computing

187
00:07:05.610 --> 00:07:07.559 
large problems, and one of

188
00:07:08.910 --> 00:07:10.949 
these machines that could do

189
00:07:10.950 --> 00:07:12.569 
so was the IBM BlueGene/Q, which

190
00:07:12.570 --> 00:07:14.560 
featured up to 1.6

191
00:07:14.561 --> 00:07:16.769 
million independent workers.

192
00:07:16.770 --> 00:07:19.049 
But this time that this was a big

193
00:07:19.050 --> 00:07:21.719 
machine, room-filling machine

194
00:07:21.720 --> 00:07:22.720 
with a high

195
00:07:23.820 --> 00:07:26.139 
power consumption of megawatts.

196
00:07:27.210 --> 00:07:29.789 
But in 2010 or 2011,

197
00:07:29.790 --> 00:07:31.319 
that was the best one could do.

198
00:07:32.580 --> 00:07:33.580 
Today,

199
00:07:35.580 --> 00:07:36.580 
usually,

200
00:07:38.280 --> 00:07:40.029 
this has changed a bit in the sense

201
00:07:40.030 --> 00:07:42.399 
that system has become more hybrid,

202
00:07:42.400 --> 00:07:44.529 
so current installations

203
00:07:44.530 --> 00:07:46.809 
of big cluster machines

204
00:07:46.810 --> 00:07:48.429 
have become hybrid and use a

205
00:07:48.430 --> 00:07:49.430 
combination of

206
00:07:50.860 --> 00:07:51.860 
classical

207
00:07:52.840 --> 00:07:54.939 
CPUs using laptops

208
00:07:54.940 --> 00:07:57.249 
and accelerating

209
00:07:57.250 --> 00:07:59.709 
massively parallel devices

210
00:07:59.710 --> 00:08:01.106 
like Nvidia Tesla GPUs.

211
00:08:03.640 --> 00:08:06.189 
And, well, there are other massively

212
00:08:06.190 --> 00:08:07.989 
parallel systems today.

213
00:08:07.990 --> 00:08:08.990 
Actually,

214
00:08:10.330 --> 00:08:12.549 
I'm using one of it, which is my brain.

215
00:08:12.550 --> 00:08:14.559 
It consists of 68

216
00:08:14.560 --> 00:08:17.169 
billion transistors, which is no,

217
00:08:17.170 --> 00:08:18.220 
not transistors, it's

218
00:08:20.050 --> 00:08:21.850 
neurons and well

219
00:08:23.300 --> 00:08:25.429 
you're on the computer data, like

220
00:08:25.430 --> 00:08:27.919 
maybe one doesn't really understand

221
00:08:27.920 --> 00:08:29.899 
how in detail

222
00:08:29.900 --> 00:08:32.029 
one creates the human brain,

223
00:08:32.030 --> 00:08:33.649 
creates the understanding of things.

224
00:08:33.650 --> 00:08:35.629 
But what one understands is

225
00:08:35.630 --> 00:08:37.548 
how they process information.

226
00:08:37.549 --> 00:08:39.229 
And one understands that these neurons

227
00:08:39.230 --> 00:08:40.230 
are very tiny

228
00:08:41.750 --> 00:08:44.089 
data processing units that use

229
00:08:44.090 --> 00:08:46.309 
electric potentials to communicate

230
00:08:47.600 --> 00:08:50.089 
data further to neighboring

231
00:08:50.090 --> 00:08:52.399 
neurons. And this

232
00:08:52.400 --> 00:08:54.289 
flow of filters that

233
00:08:55.520 --> 00:08:57.019 
is generated by

234
00:08:58.430 --> 00:09:00.799 
this connection of neural networks of

235
00:09:00.800 --> 00:09:02.749 
neurons, then somehow processes

236
00:09:02.750 --> 00:09:04.909 
information and reduces it to

237
00:09:04.910 --> 00:09:06.661 
a more or less meaningful

238
00:09:08.240 --> 00:09:10.249 
output that can be then derived

239
00:09:10.250 --> 00:09:11.250 
further.

240
00:09:12.500 --> 00:09:13.500 
So

241
00:09:15.580 --> 00:09:17.559 
68 billion is a number

242
00:09:17.560 --> 00:09:20.049 
that is vastly larger than

243
00:09:20.050 --> 00:09:22.089 
anything one can currently fabricate

244
00:09:22.090 --> 00:09:25.239 
in transistors,

245
00:09:25.240 --> 00:09:27.879 
but it's still an incredible

246
00:09:27.880 --> 00:09:30.309 
amount less than the largest

247
00:09:30.310 --> 00:09:31.629 
massively parallel systems

248
00:09:33.520 --> 00:09:35.649 
that is still larger

249
00:09:35.650 --> 00:09:37.629 
than the largest massively

250
00:09:37.630 --> 00:09:39.429 
parallel system that one knows today,

251
00:09:39.430 --> 00:09:40.929 
which is the universe itself.

252
00:09:40.930 --> 00:09:43.029 
And it has hundreds of billions

253
00:09:43.030 --> 00:09:44.679 
of billions of observable stars.

254
00:09:44.680 --> 00:09:46.359 
And they all calculate something in

255
00:09:46.360 --> 00:09:47.360 
parallel,

256
00:09:48.340 --> 00:09:50.409 
mainly, I mean, the

257
00:09:50.410 --> 00:09:52.539 
laws of physics, they're just

258
00:09:52.540 --> 00:09:53.769 
they just live it.

259
00:09:53.770 --> 00:09:54.999 
These laws of physics.

260
00:09:55.000 --> 00:09:57.039 
But someone can say that

261
00:09:57.040 --> 00:09:58.040 
they calculate something, but

262
00:09:59.410 --> 00:10:00.609 
one doesn't really understand what it

263
00:10:00.610 --> 00:10:02.669 
means maybe in the future,

264
00:10:02.670 --> 00:10:03.909 
one will also understand what this

265
00:10:03.910 --> 00:10:06.009 
massively parallel system is actually

266
00:10:06.010 --> 00:10:07.010 
computing.

267
00:10:08.680 --> 00:10:10.989 
Now back to small things,

268
00:10:12.790 --> 00:10:14.859 
task decomposition is one of the

269
00:10:14.860 --> 00:10:16.929 
most important things that you have

270
00:10:16.930 --> 00:10:19.179 
to do when you start out

271
00:10:19.180 --> 00:10:20.180 
parallelizing applications.

272
00:10:21.220 --> 00:10:22.220 
So

273
00:10:23.530 --> 00:10:25.509 
an algorithm is usually made up of a

274
00:10:25.510 --> 00:10:27.109 
lot of parts.

275
00:10:27.110 --> 00:10:28.778 
And they

276
00:10:30.050 --> 00:10:31.939 
build upon each other and

277
00:10:34.840 --> 00:10:36.826 
they use the input of the previous one

278
00:10:38.430 --> 00:10:40.619 
and compute some changes

279
00:10:40.620 --> 00:10:42.869 
and generate an output and then forward

280
00:10:42.870 --> 00:10:43.870 
this information to a

281
00:10:45.060 --> 00:10:46.060 
worker

282
00:10:48.390 --> 00:10:50.234 
down the drain, and

283
00:10:52.740 --> 00:10:55.049 
as you can see on the

284
00:10:55.050 --> 00:10:57.809 
chart, on the drawing on the right,

285
00:10:57.810 --> 00:11:00.239 
what you can see there is an example

286
00:11:00.240 --> 00:11:02.219 
of a typical task decomposition, where

287
00:11:02.220 --> 00:11:04.379 
the data comes in from the top and

288
00:11:04.380 --> 00:11:05.380 
then is

289
00:11:06.660 --> 00:11:09.059 
processed by a few workers.

290
00:11:09.060 --> 00:11:12.089 
And then it goes down to the bottom and

291
00:11:12.090 --> 00:11:13.890 
information flows as

292
00:11:15.840 --> 00:11:17.849 
depicted using this so-called

293
00:11:17.850 --> 00:11:19.619 
directed acyclic graph.

294
00:11:22.950 --> 00:11:25.229 
But as you can see, there

295
00:11:25.230 --> 00:11:27.269 
are there's a parallelism of

296
00:11:27.270 --> 00:11:30.669 
like five in this depiction.

297
00:11:30.670 --> 00:11:32.669 
So you have like like 15 different

298
00:11:32.670 --> 00:11:33.670 
tasks, but

299
00:11:35.100 --> 00:11:36.329 
starting from the top to the bottom,

300
00:11:36.330 --> 00:11:37.319 
it's like 15 tasks.

301
00:11:37.320 --> 00:11:39.329 
But they don't all run

302
00:11:39.330 --> 00:11:40.589 
in parallel.

303
00:11:40.590 --> 00:11:42.689 
And it's not necessarily the case

304
00:11:42.690 --> 00:11:44.429 
that you can always decompose your

305
00:11:44.430 --> 00:11:45.430 
problem into

306
00:11:47.950 --> 00:11:48.950 
perfectly

307
00:11:50.260 --> 00:11:52.570 
symmetric tasks that

308
00:11:53.910 --> 00:11:54.910 
can run in parallel,

309
00:11:56.050 --> 00:11:58.089 
so usually, it's the case that

310
00:11:58.090 --> 00:12:00.159 
there are more like

311
00:12:00.160 --> 00:12:02.139 
difficult things that are not

312
00:12:02.140 --> 00:12:03.140 
parallelizable,

313
00:12:04.510 --> 00:12:05.739 
difficult subtask, that are not

314
00:12:05.740 --> 00:12:07.869 
parallelizable and

315
00:12:07.870 --> 00:12:09.866 
therefore these

316
00:12:13.360 --> 00:12:15.819 
are the best possible decomposition

317
00:12:15.820 --> 00:12:16.820 
of a

318
00:12:18.160 --> 00:12:19.200 
specific problem,

319
00:12:20.970 --> 00:12:23.469 
it creates

320
00:12:23.470 --> 00:12:25.599 
a maximal possible execution

321
00:12:25.600 --> 00:12:26.769 
efficiency.

322
00:12:26.770 --> 00:12:29.229 
So in

323
00:12:29.230 --> 00:12:31.119 
what I'm showing here in this

324
00:12:31.120 --> 00:12:32.850 
depiction, you can see there's a

325
00:12:34.730 --> 00:12:36.559 
parallelism of like five, when that

326
00:12:36.560 --> 00:12:39.619 
means that the maximum achievable

327
00:12:39.620 --> 00:12:41.179 
acceleration by this

328
00:12:42.230 --> 00:12:43.819 
parallelization would be like a factor

329
00:12:43.820 --> 00:12:45.829 
of five, but since

330
00:12:45.830 --> 00:12:47.569 
not all of the workers work at the same

331
00:12:47.570 --> 00:12:49.910 
time, they realized

332
00:12:52.450 --> 00:12:54.129 
speed-up will be less than five,

333
00:12:54.130 --> 00:12:55.130 
obviously, I mean

334
00:12:57.250 --> 00:12:59.229 
not all of the task, there are empty

335
00:12:59.230 --> 00:13:00.230 
spots that the

336
00:13:01.490 --> 00:13:03.639 
workers are just waiting for input and

337
00:13:03.640 --> 00:13:05.739 
don't have any, so they're just

338
00:13:05.740 --> 00:13:07.689 
idling and the speed up will be less

339
00:13:07.690 --> 00:13:08.859 
than optimal.

340
00:13:08.860 --> 00:13:10.899 
And that's, in a sense,

341
00:13:10.900 --> 00:13:11.900 
what is

342
00:13:13.330 --> 00:13:15.429 
formulated in Amdahl's law,

343
00:13:15.430 --> 00:13:17.859 
which just says if there's only a

344
00:13:17.860 --> 00:13:19.749 
parallelism of like two application,

345
00:13:19.750 --> 00:13:21.699 
you will also only be able to speed up

346
00:13:21.700 --> 00:13:23.209 
your application by a factor of two.

347
00:13:23.210 --> 00:13:25.389 
And if

348
00:13:25.390 --> 00:13:27.489 
there's just 95 percent

349
00:13:27.490 --> 00:13:29.559 
of the application parallelizable,

350
00:13:29.560 --> 00:13:31.659 
then only then the remaining

351
00:13:31.660 --> 00:13:34.059 
five percent will need to be

352
00:13:34.060 --> 00:13:36.519 
remain unparallelised and only

353
00:13:36.520 --> 00:13:37.959 
95 percent can be parallelized.

354
00:13:37.960 --> 00:13:39.070 
But on the other hand, that means

355
00:13:40.420 --> 00:13:42.489 
only the only be able to gain a factor

356
00:13:42.490 --> 00:13:44.739 
of 20 in execution speed because

357
00:13:44.740 --> 00:13:46.809 
five percent times 20 is 100

358
00:13:46.810 --> 00:13:49.029 
and you could only

359
00:13:49.030 --> 00:13:50.349 
tell these five percent, they will

360
00:13:50.350 --> 00:13:52.479 
remain. And the best case would be that

361
00:13:52.480 --> 00:13:54.519 
you reduce the 95 percent to

362
00:13:54.520 --> 00:13:56.529 
zero and the five percent they will

363
00:13:56.530 --> 00:13:58.209 
remain. And that's a speed, a factor of

364
00:13:58.210 --> 00:13:59.210 
20.

365
00:13:59.710 --> 00:14:01.269 
On the theoretical side, this has been

366
00:14:01.270 --> 00:14:03.159 
worked extensively on in the past with

367
00:14:04.300 --> 00:14:06.909 
so-called extensions

368
00:14:06.910 --> 00:14:08.829 
to the so-called RAM model, which is

369
00:14:08.830 --> 00:14:10.989 
called the parallel random access

370
00:14:10.990 --> 00:14:12.669 
memory model.

371
00:14:12.670 --> 00:14:13.949 
And it's a,

372
00:14:16.780 --> 00:14:18.849 
well, it's everything's

373
00:14:18.850 --> 00:14:21.519 
pretty clear in that sense, but

374
00:14:21.520 --> 00:14:23.800 
let's continue a bit about

375
00:14:24.850 --> 00:14:27.069 
what hardware features

376
00:14:27.070 --> 00:14:29.079 
that are required in

377
00:14:29.080 --> 00:14:31.299 
order to actually execute

378
00:14:31.300 --> 00:14:33.939 
tasks in parallel.

379
00:14:33.940 --> 00:14:36.909 
So in the past, when we started with

380
00:14:36.910 --> 00:14:39.339 
computing, one had just a single

381
00:14:39.340 --> 00:14:40.839 
worker in a single chip.

382
00:14:40.840 --> 00:14:43.179 
And that means that

383
00:14:45.250 --> 00:14:46.899 
one didn't have the infrastructure to

384
00:14:46.900 --> 00:14:49.149 
actually communicate between

385
00:14:49.150 --> 00:14:50.709 
multiple workers in a single word

386
00:14:50.710 --> 00:14:52.119 
because there was nothing like that.

387
00:14:52.120 --> 00:14:53.536 
So one had to create

388
00:14:54.710 --> 00:14:57.139 
a different architecture

389
00:14:57.140 --> 00:14:59.539 
that allows multiple workers

390
00:14:59.540 --> 00:15:01.819 
to work on a common dataset,

391
00:15:01.820 --> 00:15:03.859 
but still execute them in their

392
00:15:03.860 --> 00:15:06.019 
own well instructions or

393
00:15:06.020 --> 00:15:07.699 
realize their own tasks.

394
00:15:07.700 --> 00:15:09.889 
And there have been

395
00:15:09.890 --> 00:15:12.739 
and are still today three

396
00:15:12.740 --> 00:15:14.829 
process interaction and memory models

397
00:15:14.830 --> 00:15:16.189 
that are employed.

398
00:15:16.190 --> 00:15:17.989 
The first one is shared memory

399
00:15:17.990 --> 00:15:18.990 
processing,

400
00:15:20.930 --> 00:15:23.209 
which means that all

401
00:15:23.210 --> 00:15:25.159 
of the tasks work on a common

402
00:15:25.160 --> 00:15:27.469 
knowledge, a common memory

403
00:15:27.470 --> 00:15:30.369 
and they

404
00:15:30.370 --> 00:15:32.559 
use variables to access these

405
00:15:32.560 --> 00:15:34.689 
variables and then to communicate

406
00:15:34.690 --> 00:15:36.819 
to the other workers by

407
00:15:36.820 --> 00:15:39.669 
modifying these shared variables.

408
00:15:39.670 --> 00:15:41.649 
And then there's the other thing, which

409
00:15:41.650 --> 00:15:44.169 
is a message passing

410
00:15:44.170 --> 00:15:45.669 
this means that one of the workers

411
00:15:45.670 --> 00:15:47.169 
sends a message.

412
00:15:47.170 --> 00:15:49.299 
Let it be a block of data

413
00:15:49.300 --> 00:15:51.769 
like 100 bytes and

414
00:15:51.770 --> 00:15:53.739 
contains "please do

415
00:15:53.740 --> 00:15:55.779 
this for me and then feedback

416
00:15:55.780 --> 00:15:57.849 
the output to there".

417
00:15:57.850 --> 00:16:00.159 
And this message is communicated to

418
00:16:00.160 --> 00:16:01.569 
a different worker and then this worker

419
00:16:01.570 --> 00:16:04.179 
will execute it and

420
00:16:04.180 --> 00:16:05.180 
do what is requested.

421
00:16:06.910 --> 00:16:07.910 
This is

422
00:16:09.520 --> 00:16:11.499 
logically different from

423
00:16:11.500 --> 00:16:13.489 
shared memory processing.

424
00:16:13.490 --> 00:16:15.759 
But today the common

425
00:16:16.930 --> 00:16:19.089 
way of realizing

426
00:16:19.090 --> 00:16:21.189 
large computers is by

427
00:16:21.190 --> 00:16:23.199 
using hybrid hardware, which

428
00:16:23.200 --> 00:16:25.909 
means that is a mixture of both

429
00:16:25.910 --> 00:16:27.609 
shared memory processing and message

430
00:16:27.610 --> 00:16:28.610 
passing.

431
00:16:29.430 --> 00:16:32.339 
And I will talk about these three

432
00:16:32.340 --> 00:16:34.829 
things in detail

433
00:16:34.830 --> 00:16:35.830 
now,

434
00:16:37.080 --> 00:16:39.269 
so shared memory processing

435
00:16:39.270 --> 00:16:41.369 
is usually abbreviated just by

436
00:16:41.370 --> 00:16:42.847 
SMP and that's

437
00:16:44.520 --> 00:16:45.750 
not many people just

438
00:16:46.800 --> 00:16:48.779 
say shared memory processing, just

439
00:16:48.780 --> 00:16:51.089 
say SMP, so that's

440
00:16:51.090 --> 00:16:52.090 
the short term. As

441
00:16:53.550 --> 00:16:55.919 
said the tasks work with

442
00:16:55.920 --> 00:16:57.899 
shared data, so each

443
00:16:57.900 --> 00:16:59.879 
of the tasks sees all the

444
00:16:59.880 --> 00:17:01.919 
information at the same time and can

445
00:17:01.920 --> 00:17:03.899 
access these information at the

446
00:17:03.900 --> 00:17:06.479 
same time by process of instructions.

447
00:17:06.480 --> 00:17:08.279 
And the communication between these

448
00:17:08.280 --> 00:17:10.559 
tasks is done using common variables

449
00:17:10.560 --> 00:17:12.629 
that control and

450
00:17:12.630 --> 00:17:14.608 
steer the behavior of these set of

451
00:17:14.609 --> 00:17:15.568 
tasks.

452
00:17:15.569 --> 00:17:16.799 
And the typical

453
00:17:18.329 --> 00:17:20.818 
programing task is then to realize

454
00:17:20.819 --> 00:17:22.649 
this acyclic graph that I was shown

455
00:17:22.650 --> 00:17:24.569 
earlier using

456
00:17:26.220 --> 00:17:28.829 
communication and

457
00:17:28.830 --> 00:17:30.450 
with common variables to steer the

458
00:17:31.770 --> 00:17:33.779 
orchestration of the

459
00:17:33.780 --> 00:17:34.950 
execution of this task.

460
00:17:36.360 --> 00:17:38.519 
This shared memory processing

461
00:17:38.520 --> 00:17:40.919 
is well, the

462
00:17:40.920 --> 00:17:43.829 
most widely used way of doing

463
00:17:43.830 --> 00:17:45.839 
parallel processing,

464
00:17:45.840 --> 00:17:48.149 
and it is used in processes, multi

465
00:17:48.150 --> 00:17:50.519 
circle servers and mainframes

466
00:17:50.520 --> 00:17:52.739 
and phones and whatnot,

467
00:17:52.740 --> 00:17:55.289 
everything that does

468
00:17:55.290 --> 00:17:57.059 
data processing these days is using

469
00:17:57.060 --> 00:17:59.069 
this shared memory

470
00:17:59.070 --> 00:18:00.070 
processing.

471
00:18:01.170 --> 00:18:03.249 
Typical API programmers have to use

472
00:18:03.250 --> 00:18:05.219 
is Pthreads, which is a

473
00:18:05.220 --> 00:18:07.259 
very old topic, but it is

474
00:18:07.260 --> 00:18:09.660 
still used today extensively

475
00:18:11.160 --> 00:18:12.809 
and very successful.

476
00:18:12.810 --> 00:18:15.089 
And building upon these

477
00:18:15.090 --> 00:18:16.139 
Pthreads is a different model called

478
00:18:16.140 --> 00:18:18.179 
OpenMP, which is a little bit easier

479
00:18:18.180 --> 00:18:20.219 
for programmers to use because it

480
00:18:20.220 --> 00:18:22.319 
doesn't require deep knowledge into how

481
00:18:22.320 --> 00:18:23.630 
operating systems work.

482
00:18:25.290 --> 00:18:27.239 
And in the field of the message

483
00:18:27.240 --> 00:18:29.579 
passing, as I said,

484
00:18:29.580 --> 00:18:32.069 
the tasks workers distribute the data,

485
00:18:32.070 --> 00:18:34.259 
which means that the dataset is

486
00:18:34.260 --> 00:18:36.629 
divided into multiple

487
00:18:36.630 --> 00:18:38.699 
parts and each of the workers can

488
00:18:38.700 --> 00:18:41.099 
only see their individual

489
00:18:41.100 --> 00:18:42.100 
part of the data.

490
00:18:43.740 --> 00:18:45.869 
And the communication between these

491
00:18:45.870 --> 00:18:48.149 
workers is done using explicit

492
00:18:48.150 --> 00:18:50.489 
messages, which means that one worker

493
00:18:50.490 --> 00:18:53.279 
uses a message

494
00:18:53.280 --> 00:18:55.469 
consisting of like

495
00:18:55.470 --> 00:18:56.470 
an instruction

496
00:18:57.990 --> 00:18:59.579 
and sending that to another worker and

497
00:18:59.580 --> 00:19:01.449 
then this other worker will also will

498
00:19:01.450 --> 00:19:03.749 
receive it and

499
00:19:03.750 --> 00:19:05.489 
do the job that was requested.

500
00:19:08.710 --> 00:19:11.019 
So this is a method that

501
00:19:11.020 --> 00:19:12.519 
can be used when

502
00:19:15.070 --> 00:19:16.439 
shared memory processing is not

503
00:19:16.440 --> 00:19:18.479 
available, and it's typically used in

504
00:19:18.480 --> 00:19:20.579 
colocations of multiple servers, like a

505
00:19:20.580 --> 00:19:22.919 
data center where you have racks

506
00:19:22.920 --> 00:19:25.079 
of multiple servers in them and they

507
00:19:25.080 --> 00:19:26.969 
are connected with network

508
00:19:26.970 --> 00:19:28.949 
interconnects like InfiniBand or

509
00:19:28.950 --> 00:19:30.599 
Ethernet or something.

510
00:19:30.600 --> 00:19:32.819 
And so this is called something

511
00:19:32.820 --> 00:19:34.742 
like cluster today, it's a common term

512
00:19:36.060 --> 00:19:37.079 
that people use.

513
00:19:37.080 --> 00:19:38.759 
So in these clusters, message passing

514
00:19:38.760 --> 00:19:39.779 
is the way to go.

515
00:19:39.780 --> 00:19:42.929 
And the application APIs

516
00:19:42.930 --> 00:19:43.930 
that have

517
00:19:45.330 --> 00:19:47.130 
survived until now

518
00:19:48.150 --> 00:19:49.150 
both is

519
00:19:50.430 --> 00:19:51.509 
MPI and Spark.

520
00:19:51.510 --> 00:19:53.549 
And MPI is

521
00:19:53.550 --> 00:19:55.349 
very old as well.

522
00:19:55.350 --> 00:19:57.479 
So it was one of the oldest programing

523
00:19:57.480 --> 00:19:58.770 
models for doing

524
00:20:00.270 --> 00:20:01.270 
parallel processing

525
00:20:02.250 --> 00:20:04.529 
and the programmer has to do explicit

526
00:20:04.530 --> 00:20:05.530 
data distribution.

527
00:20:07.020 --> 00:20:08.729 
And on the other hand, there Spark,

528
00:20:08.730 --> 00:20:10.799 
which is a very new

529
00:20:10.800 --> 00:20:13.199 
programing model that doesn't require

530
00:20:13.200 --> 00:20:15.629 
that much attention by the programmer

531
00:20:15.630 --> 00:20:16.630 
for

532
00:20:18.300 --> 00:20:20.099 
the message passing because the message

533
00:20:20.100 --> 00:20:22.169 
passing is done implicitly by

534
00:20:22.170 --> 00:20:23.170 
the

535
00:20:24.420 --> 00:20:27.249 
runtime of Spark and

536
00:20:27.250 --> 00:20:29.309 
the proper execution of the task

537
00:20:29.310 --> 00:20:31.559 
requested is then realized

538
00:20:31.560 --> 00:20:33.539 
by a so-called resilient distributed

539
00:20:33.540 --> 00:20:35.909 
dataset via the

540
00:20:35.910 --> 00:20:36.910 
runtime of Spark.

541
00:20:38.060 --> 00:20:39.060 
And then

542
00:20:40.040 --> 00:20:42.079 
today, the most common thing is

543
00:20:42.080 --> 00:20:43.549 
actually hybrid hardware.

544
00:20:43.550 --> 00:20:45.919 
So in HPC,

545
00:20:45.920 --> 00:20:47.509 
it is usually the case that you have

546
00:20:47.510 --> 00:20:49.969 
large clusters where each cluster

547
00:20:49.970 --> 00:20:51.680 
has a shared memory node,

548
00:20:52.700 --> 00:20:54.739 
where each node has

549
00:20:54.740 --> 00:20:56.929 
a shared memory subsystem

550
00:20:56.930 --> 00:20:59.959 
and the shared memory subsystem

551
00:20:59.960 --> 00:21:02.420 
can be accessed by the host processors

552
00:21:03.440 --> 00:21:05.479 
while an

553
00:21:05.480 --> 00:21:07.849 
attached massively parallel

554
00:21:07.850 --> 00:21:10.129 
accelerator can

555
00:21:10.130 --> 00:21:12.499 
either be attached also

556
00:21:12.500 --> 00:21:14.419 
by shared memory or by message passing

557
00:21:14.420 --> 00:21:16.429 
within the same node of this

558
00:21:16.430 --> 00:21:17.430 
cluster.

559
00:21:18.320 --> 00:21:20.429 
So that's the typical setup today,

560
00:21:20.430 --> 00:21:22.579 
so that's the case with

561
00:21:22.580 --> 00:21:24.619 
CPU plus GPU or

562
00:21:24.620 --> 00:21:27.190 
that's the case for CPU plus FPGA.

563
00:21:29.450 --> 00:21:31.429 
But the trend today

564
00:21:31.430 --> 00:21:33.199 
is towards integrating these

565
00:21:33.200 --> 00:21:35.449 
accelerators like GPUs or FPGA

566
00:21:35.450 --> 00:21:37.909 
is not by message passing.

567
00:21:37.910 --> 00:21:39.889 
So by means of attaching

568
00:21:39.890 --> 00:21:41.959 
these accelerators to the

569
00:21:41.960 --> 00:21:44.119 
processor using a

570
00:21:44.120 --> 00:21:45.919 
external processor bus, like PCI

571
00:21:45.920 --> 00:21:48.109 
Express, but rather to integrate

572
00:21:48.110 --> 00:21:49.130 
them into the

573
00:21:50.600 --> 00:21:52.279 
shared memory system of the host

574
00:21:52.280 --> 00:21:54.289 
processor using newer

575
00:21:54.290 --> 00:21:55.490 
technologies like

576
00:21:56.900 --> 00:21:59.509 
NVLink, which allows

577
00:21:59.510 --> 00:22:01.219 
these accelerators to use the

578
00:22:01.220 --> 00:22:03.379 
load-store model in

579
00:22:03.380 --> 00:22:05.359 
the way that the host processor

580
00:22:05.360 --> 00:22:08.419 
is accessing its own memory.

581
00:22:08.420 --> 00:22:10.549 
And the APIs that can be used for this

582
00:22:10.550 --> 00:22:13.279 
hybrid hardware, today's

583
00:22:13.280 --> 00:22:15.289 
OpenACC and OpenCL.

584
00:22:21.290 --> 00:22:22.290 
So,

585
00:22:24.560 --> 00:22:26.719 
that all is very theoretical

586
00:22:26.720 --> 00:22:28.939 
until one then tries to actually

587
00:22:28.940 --> 00:22:31.219 
do a task decompositional, one

588
00:22:31.220 --> 00:22:33.409 
realizes that one can run

589
00:22:33.410 --> 00:22:36.079 
into a lot of difficulties

590
00:22:36.080 --> 00:22:38.149 
and create issues in the

591
00:22:38.150 --> 00:22:40.609 
task in particular position that one

592
00:22:40.610 --> 00:22:41.659 
choose.

593
00:22:41.660 --> 00:22:43.879 
And the most common thing is

594
00:22:43.880 --> 00:22:45.529 
a so-called Race Condition.

595
00:22:45.530 --> 00:22:47.719 
So this is a typical bug that arises

596
00:22:47.720 --> 00:22:50.299 
when there are two threads

597
00:22:50.300 --> 00:22:53.059 
in the SMP and

598
00:22:53.060 --> 00:22:54.949 
for example, the second thread loads

599
00:22:54.950 --> 00:22:56.569 
data before the first

600
00:22:58.220 --> 00:23:00.589 
thread stores the data,

601
00:23:00.590 --> 00:23:02.119 
which means that the second thread

602
00:23:02.120 --> 00:23:04.249 
eventually reads the wrong data since

603
00:23:04.250 --> 00:23:05.719 
the first one hasn't written it yet.

604
00:23:05.720 --> 00:23:07.429 
And then the result will be that the

605
00:23:07.430 --> 00:23:09.139 
second thread does wrong calculations

606
00:23:09.140 --> 00:23:11.369 
and things go bad.

607
00:23:11.370 --> 00:23:12.370 
There's two ways

608
00:23:14.910 --> 00:23:16.949 
to treat this

609
00:23:16.950 --> 00:23:19.439 
box, either one does it explicitly,

610
00:23:19.440 --> 00:23:20.699 
which meaning that the programmer has

611
00:23:20.700 --> 00:23:22.410 
to do work and

612
00:23:25.260 --> 00:23:27.929 
changes this application such that

613
00:23:27.930 --> 00:23:30.389 
the threads request temporary exclusive

614
00:23:30.390 --> 00:23:32.579 
access to these data sets, to all these

615
00:23:32.580 --> 00:23:34.559 
parts of the whole dataset, such

616
00:23:34.560 --> 00:23:36.479 
that it is guaranteed that the second

617
00:23:36.480 --> 00:23:38.579 
thread always comes

618
00:23:38.580 --> 00:23:40.349 
after the first thread.

619
00:23:40.350 --> 00:23:42.179 
And then there's more implicit

620
00:23:42.180 --> 00:23:44.159 
treatments to this issue, which

621
00:23:44.160 --> 00:23:45.160 
is

622
00:23:46.440 --> 00:23:48.449 
by the support of programing languages.

623
00:23:48.450 --> 00:23:50.399 
For example, there is this new Pony

624
00:23:50.400 --> 00:23:52.469 
language which provides

625
00:23:52.470 --> 00:23:54.239 
a lot of guarantees to the programmer

626
00:23:54.240 --> 00:23:56.849 
like race free and

627
00:23:56.850 --> 00:23:58.619 
deadlock free.

628
00:23:58.620 --> 00:24:00.609 
Also the

629
00:24:00.610 --> 00:24:03.039 
language itself is still compatible,

630
00:24:04.210 --> 00:24:05.679 
which is, for example, not the case

631
00:24:05.680 --> 00:24:06.680 
with more

632
00:24:07.840 --> 00:24:09.369 
with all the programing languages like

633
00:24:09.370 --> 00:24:10.370 
C and C++.

634
00:24:11.410 --> 00:24:12.549 
The other thing is that deadlocks,

635
00:24:14.860 --> 00:24:15.888 
well deadlocks,

636
00:24:18.250 --> 00:24:19.659 
they're actually pretty easy to

637
00:24:19.660 --> 00:24:21.789 
understand. It's just that when

638
00:24:21.790 --> 00:24:23.406 
there you have multiple processes and

639
00:24:24.850 --> 00:24:27.159 
you have mutually

640
00:24:27.160 --> 00:24:29.379 
blocked resources between them or

641
00:24:29.380 --> 00:24:31.659 
so. Let's say one of the

642
00:24:31.660 --> 00:24:33.969 
processes one wants to use the monitor

643
00:24:33.970 --> 00:24:35.949 
and the other process is currently

644
00:24:35.950 --> 00:24:37.179 
using the printer, but they need to do

645
00:24:37.180 --> 00:24:39.639 
it exclusively for some case.

646
00:24:39.640 --> 00:24:41.079 
And then at some point,

647
00:24:42.400 --> 00:24:44.919 
the first one having the monitor

648
00:24:44.920 --> 00:24:46.719 
will also want to access the printer

649
00:24:46.720 --> 00:24:47.919 
exclusively.

650
00:24:47.920 --> 00:24:49.929 
Then the first one would be

651
00:24:49.930 --> 00:24:51.099 
waiting on the second.

652
00:24:51.100 --> 00:24:53.379 
But the

653
00:24:53.380 --> 00:24:55.719 
situation is not going to be resolved

654
00:24:55.720 --> 00:24:58.389 
by itself and the only way to

655
00:24:58.390 --> 00:24:59.390 
escape that

656
00:25:01.340 --> 00:25:02.809 
problem is to do something that's

657
00:25:02.810 --> 00:25:04.819 
called preemption, which just

658
00:25:04.820 --> 00:25:05.929 
means to have

659
00:25:07.070 --> 00:25:09.139 
a third person

660
00:25:09.140 --> 00:25:11.719 
in the game that

661
00:25:11.720 --> 00:25:13.849 
controls access

662
00:25:13.850 --> 00:25:16.490 
to these devices and then removes.

663
00:25:18.930 --> 00:25:20.999 
Well, just pause this execution

664
00:25:21.000 --> 00:25:23.129 
of one of these processes

665
00:25:23.130 --> 00:25:25.439 
to allow transfer

666
00:25:25.440 --> 00:25:27.269 
of rights to one of these processes

667
00:25:27.270 --> 00:25:29.699 
until this one, so this one then can

668
00:25:29.700 --> 00:25:31.889 
complete its job and then

669
00:25:31.890 --> 00:25:34.049 
once, such as when it's done,

670
00:25:34.050 --> 00:25:35.549 
resumes the second process,

671
00:25:37.560 --> 00:25:38.939 
they won't get into a conflict.

672
00:25:41.240 --> 00:25:43.309 
Then the third thing that

673
00:25:43.310 --> 00:25:45.349 
usually creates problems

674
00:25:45.350 --> 00:25:47.779 
with parallelising applications

675
00:25:47.780 --> 00:25:49.729 
can be resolved by so-called critical

676
00:25:49.730 --> 00:25:50.749 
sections.

677
00:25:50.750 --> 00:25:53.269 
So, for example, when a program

678
00:25:53.270 --> 00:25:56.449 
instructs a sequence of instructions,

679
00:25:56.450 --> 00:25:59.089 
it is usually the case that

680
00:25:59.090 --> 00:26:01.669 
only the combination of these

681
00:26:01.670 --> 00:26:03.739 
instructions makes

682
00:26:03.740 --> 00:26:05.149 
logical sense.

683
00:26:05.150 --> 00:26:07.189 
For example, creating a user

684
00:26:07.190 --> 00:26:09.679 
and then changing its password

685
00:26:09.680 --> 00:26:11.719 
to its initial value

686
00:26:11.720 --> 00:26:13.039 
is a two-step process.

687
00:26:13.040 --> 00:26:15.049 
But it's essentially one thing.

688
00:26:15.050 --> 00:26:17.149 
And if the second

689
00:26:17.150 --> 00:26:18.150 
thread

690
00:26:21.430 --> 00:26:23.769 
watches this process from the outside

691
00:26:25.300 --> 00:26:26.300 
and

692
00:26:28.330 --> 00:26:29.949 
uses the password before

693
00:26:33.210 --> 00:26:34.929 
the second part of the first thread is

694
00:26:34.930 --> 00:26:36.429 
done, meaning the password is actually

695
00:26:36.430 --> 00:26:38.409 
set, then also bad things is

696
00:26:38.410 --> 00:26:39.410 
going to happen. So.

697
00:26:40.540 --> 00:26:42.699 
It needs to be the case that

698
00:26:42.700 --> 00:26:45.009 
there are groupings of instructions

699
00:26:45.010 --> 00:26:47.199 
that must

700
00:26:47.200 --> 00:26:49.539 
be executed as kind of one piece,

701
00:26:49.540 --> 00:26:51.609 
also looking at them in detail, there

702
00:26:51.610 --> 00:26:53.709 
are actually multiple steps of the same

703
00:26:53.710 --> 00:26:54.579 
thing.

704
00:26:54.580 --> 00:26:55.580 
So

705
00:26:56.740 --> 00:26:58.179 
one needs to have a guarantee that

706
00:26:59.320 --> 00:27:01.449 
the second process can't watch

707
00:27:01.450 --> 00:27:03.549 
the first process while executing

708
00:27:03.550 --> 00:27:04.550 
these steps.

709
00:27:05.830 --> 00:27:06.830 
And

710
00:27:07.930 --> 00:27:10.119 
the method used to

711
00:27:11.380 --> 00:27:13.329 
treat this problem is to use so-called

712
00:27:13.330 --> 00:27:15.249 
critical sections, which essentially

713
00:27:15.250 --> 00:27:18.629 
just mark these

714
00:27:18.630 --> 00:27:20.649 
stream of grouped

715
00:27:20.650 --> 00:27:23.109 
instructions as a single block

716
00:27:23.110 --> 00:27:24.110 
and

717
00:27:25.720 --> 00:27:27.999 
one tries to open

718
00:27:28.000 --> 00:27:30.579 
successfully can make these

719
00:27:30.580 --> 00:27:31.750 
transactions look like

720
00:27:34.540 --> 00:27:35.540 
two things.

721
00:27:36.310 --> 00:27:37.310 
Either one uses

722
00:27:40.840 --> 00:27:42.819 
by requesting temporary access to

723
00:27:42.820 --> 00:27:45.189 
the variables used by the first thread,

724
00:27:45.190 --> 00:27:46.809 
such as the second thread, can't read

725
00:27:46.810 --> 00:27:48.489 
or write to them and has to wait until

726
00:27:48.490 --> 00:27:51.129 
they become available, and

727
00:27:51.130 --> 00:27:53.199 
that will solve the problem or

728
00:27:53.200 --> 00:27:55.239 
a more modern way is to use so-called

729
00:27:55.240 --> 00:27:57.459 
transactional memory, which is a

730
00:27:57.460 --> 00:27:59.069 
hardware feature that

731
00:28:01.250 --> 00:28:03.589 
virtually attributes

732
00:28:04.640 --> 00:28:06.829 
to duplications of

733
00:28:06.830 --> 00:28:09.049 
the memory space to these

734
00:28:09.050 --> 00:28:11.089 
threads so they can

735
00:28:11.090 --> 00:28:12.259 
seemingly work on

736
00:28:13.550 --> 00:28:15.409 
different datasets, although it's the

737
00:28:15.410 --> 00:28:16.459 
same dataset.

738
00:28:16.460 --> 00:28:18.709 
And once

739
00:28:18.710 --> 00:28:20.749 
conflicts arise,

740
00:28:20.750 --> 00:28:23.209 
the hardware guarantees that

741
00:28:23.210 --> 00:28:24.210 
the correct

742
00:28:25.670 --> 00:28:27.829 
order of execution is

743
00:28:27.830 --> 00:28:29.899 
actually fulfilled as

744
00:28:29.900 --> 00:28:31.819 
defined by these critical sections.
