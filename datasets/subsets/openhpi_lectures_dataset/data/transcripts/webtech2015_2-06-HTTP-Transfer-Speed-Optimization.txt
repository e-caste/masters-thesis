WEBVTT

1
00:00:04.780 --> 00:00:10.470 
Now let's consider some ideas to speed up HTTP transfer in

2
00:00:10.470 --> 00:00:15.550 
the world wide web. So the plain HTTP is quite inefficient.

3
00:00:16.680 --> 00:00:21.080 
Remember there was every time a request response cycle

4
00:00:21.590 --> 00:00:26.510 
and then  the HTTP communication was over.

5
00:00:27.310 --> 00:00:33.130 
So in order to send such request and get the response a TCP

6
00:00:33.130 --> 00:00:34.940 
connection has to be established.

7
00:00:35.930 --> 00:00:41.570 
So the problem with today's web sites is that they include

8
00:00:41.890 --> 00:00:44.310 
a big number of embedded resources,

9
00:00:45.100 --> 00:00:49.730 
images, stylesheets, javascript files. Here for example

10
00:00:50.140 --> 00:00:56.330 
opening the OpenHPI homepage produces a 43

11
00:00:56.850 --> 00:01:03.960 
http request response cycle. So to see one web page such a big number

12
00:01:04.150 --> 00:01:08.530 
of request response cycles have to be performed.

13
00:01:09.050 --> 00:01:12.660 
And if you remember that before one can perform the action

14
00:01:12.660 --> 00:01:15.810 
response cycle a TCP connection has to be established

15
00:01:16.340 --> 00:01:20.370 
after the cycles the TCP connection has to be

16
00:01:20.830 --> 00:01:24.340 
closed then you see there is a big overhead.

17
00:01:26.620 --> 00:01:32.850 
41 of this in our example to open the openHPI home page

18
00:01:34.370 --> 00:01:38.880 
41 of the 43 http requests response cycles

19
00:01:39.410 --> 00:01:42.700 
point to the same openHPI server.

20
00:01:43.480 --> 00:01:47.990 
So with the same server the established connection censor request,

21
00:01:48.040 --> 00:01:51.190 
gets a response, close a connection,

22
00:01:52.240 --> 00:01:55.270 
open a new connection. So you see it's very

23
00:01:56.960 --> 00:01:59.160 
very loudly organized.

24
00:02:00.470 --> 00:02:04.720 
The problem with the TCP is that TCP to establish a connection

25
00:02:04.910 --> 00:02:07.950 
needs to perform a TCP handshake

26
00:02:08.400 --> 00:02:13.750 
and this causes a significant overhead without

27
00:02:14.450 --> 00:02:15.680 
additional optimization.

28
00:02:17.450 --> 00:02:22.010 
Many requested resources do not change very often.

29
00:02:22.560 --> 00:02:24.780 
This is another thing one can

30
00:02:26.330 --> 00:02:28.620 
think how to make this more efficient.

31
00:02:29.590 --> 00:02:36.930 
HTTP offers means for request reduction and speed enhancement and the

32
00:02:37.640 --> 00:02:42.700 
most important idea is to allow persistent connections

33
00:02:43.190 --> 00:02:46.150 
and to allow HTTP pipelining.

34
00:02:46.950 --> 00:02:49.210 
Another idea to

35
00:02:50.940 --> 00:02:56.830 
make it more efficient see HTTP communication is to compress the

36
00:02:56.960 --> 00:03:01.580 
information that have to be transferred over the internet, and

37
00:03:01.630 --> 00:03:04.040 
the third strategy is a caching.

38
00:03:04.960 --> 00:03:06.870 
About the caching we will discuss

39
00:03:08.420 --> 00:03:14.860 
in another clip. Let's start to consider the idea behind the persistent connections.

40
00:03:15.990 --> 00:03:20.830 
The idea is that HTTP allows multiple

41
00:03:21.590 --> 00:03:26.690 
request response cycles within one HTTP session.

42
00:03:28.830 --> 00:03:31.830 
So this is called persistent connections

43
00:03:32.470 --> 00:03:36.660 
and there is a maximum of two persistent connections that should

44
00:03:36.660 --> 00:03:41.630 
be established between client and server to avoid TCP congestion

45
00:03:42.270 --> 00:03:48.230 
and the persistent connections are established by setting the connection header field.

46
00:03:48.760 --> 00:03:55.160 
So the idea is to have and to hold and TCP connection open over several

47
00:03:56.190 --> 00:04:00.990 
HTTP request response cycles. So

48
00:04:02.150 --> 00:04:08.830 
to spare the afford that are needed to close a TCP connection

49
00:04:08.850 --> 00:04:13.440 
after finishing a first cycle and then to open TCP connection for next cycle.

50
00:04:14.860 --> 00:04:18.530 
The persistent connection are established by setting the connection header field,

51
00:04:18.680 --> 00:04:25.700 
keep alive, and if the server supports persistent connections it also adds

52
00:04:25.870 --> 00:04:29.050 
connections keep alive to the response.

53
00:04:30.210 --> 00:04:36.740 
Then we have a session so a session HTTP session is a number of

54
00:04:36.850 --> 00:04:39.350 
connected request response cycles.

55
00:04:40.260 --> 00:04:44.320 
So the client terminates the session

56
00:04:45.100 --> 00:04:49.470 
when he finishing for example to work with openHPI

57
00:04:50.080 --> 00:04:53.160 
the client terminates the session by setting the option close

58
00:04:53.500 --> 00:04:55.040 
in the connection header field.

59
00:04:56.210 --> 00:05:00.310 
And this indicates that this is the last http request.

60
00:05:01.430 --> 00:05:07.520 
If the client misbehaves then the connection will be kept open at the server

61
00:05:07.650 --> 00:05:11.050 
until the time-out and leading to possibly

62
00:05:12.030 --> 00:05:17.430 
to overload. So persistent connections the idea is

63
00:05:17.690 --> 00:05:21.360 
to use one or two to

64
00:05:22.210 --> 00:05:26.670 
combine several request response cycles to one HTTP

65
00:05:27.230 --> 00:05:31.850 
session over one TCP connection.

66
00:05:33.590 --> 00:05:40.370 
So persistent connections are default behavior since HTTP/1.1.

67
00:05:40.770 --> 00:05:46.830 
The advantages are that more efficient use of the operation system resources

68
00:05:47.150 --> 00:05:52.950 
as well as CPU as memory due to fewer simultaneous connections.

69
00:05:53.740 --> 00:05:57.280 
And then a better utilisation of the bandwidth

70
00:05:57.770 --> 00:06:03.330 
because fewer TCP packets need to be transferred

71
00:06:03.840 --> 00:06:09.610 
reduced latency for requests on embedded resources no more

72
00:06:09.610 --> 00:06:12.330 
TCP handshakes required in between,

73
00:06:12.960 --> 00:06:18.220 
and persistent connections allow HTTP pipelining.

74
00:06:19.610 --> 00:06:26.120 
With HTTP/1.1 one can even more speed up the

75
00:06:26.660 --> 00:06:28.030 
persistent connections.

76
00:06:28.810 --> 00:06:35.240 
The idea is that HTTP/1.1 allows clients to send multiple requests

77
00:06:35.510 --> 00:06:41.450 
through an open TCP connection without waiting without waiting for the response

78
00:06:41.710 --> 00:06:46.570 
of the previous request. This technique is called HTTP pipelining.

79
00:06:48.040 --> 00:06:52.540 
Pipelining allows in this way a great speed up particularly

80
00:06:52.540 --> 00:06:54.420 
for low latency connections.

81
00:06:55.590 --> 00:07:00.660 
But of course it is clear that only sequences of requests can be pipelined

82
00:07:00.850 --> 00:07:04.990 
where later ones do not rely on earlier requests.

83
00:07:06.030 --> 00:07:10.260 
For example sequences of get or have head request they can always

84
00:07:10.260 --> 00:07:14.250 
be pipelined because here later requests do not

85
00:07:14.860 --> 00:07:17.600 
depend from the response on earlier one.

86
00:07:19.950 --> 00:07:26.830 
The start of each next HTTP request must be featured explicitly

87
00:07:27.540 --> 00:07:30.900 
in using the HTTP pipelining and

88
00:07:31.590 --> 00:07:35.540 
this is done by archiving the length specification

89
00:07:36.340 --> 00:07:39.100 
in the content length header field.

90
00:07:40.770 --> 00:07:46.390 
Most browsers support HTTP pipelining pipelining however most

91
00:07:46.390 --> 00:07:51.490 
browsers disables the feature in default settings so you have to do this

92
00:07:52.180 --> 00:07:56.240 
manually when configuring your browser to allow HTTP pipelining.

93
00:07:57.810 --> 00:08:00.410 
This illustration shows the

94
00:08:01.380 --> 00:08:06.190 
power of pipelining here is an example of a persistent connections

95
00:08:06.190 --> 00:08:10.850 
or an open connection and here closed TCP connection but without pipelining.

96
00:08:11.640 --> 00:08:16.320 
A client after the TCP connection is established can send a request

97
00:08:16.750 --> 00:08:20.420 
that deserves an answer the request is inspects the response

98
00:08:20.420 --> 00:08:23.340 
and then the next a client request is ended.

99
00:08:24.100 --> 00:08:26.480 
If we compare this with

100
00:08:27.290 --> 00:08:31.050 
the pipelined version then you immediately see that the

101
00:08:33.230 --> 00:08:40.450 
speed is increasing very much here the idea is after the TCP connection is established

102
00:08:40.850 --> 00:08:47.630 
the client can send many requests without waiting for the responses of the server

103
00:08:47.780 --> 00:08:51.020 
and then the server responds to the requests and if all the

104
00:08:51.020 --> 00:08:54.550 
data are transferred to the client the connection can be closed.

105
00:08:54.890 --> 00:09:00.140 
So here we can by using this HTTP pipelining idea,

106
00:09:01.010 --> 00:09:06.610 
get a further speed up compared to the persistent connections

107
00:09:06.610 --> 00:09:10.240 
that persist the idea and the technique of persistent connection allows

108
00:09:10.640 --> 00:09:13.220 
to introduce such an pipelining technique.

109
00:09:15.030 --> 00:09:17.990 
There was another idea to make the

110
00:09:19.020 --> 00:09:23.030 
connections the HTTP communication more efficient,

111
00:09:23.470 --> 00:09:31.160 
and this was the idea to compress the data that are transferred over the internet.

112
00:09:32.240 --> 00:09:38.020 
HTTP/1.1 allows the server to deliver data

113
00:09:38.940 --> 00:09:43.160 
where the data in the message body are compressed.

114
00:09:44.710 --> 00:09:45.570 
This of course

115
00:09:47.220 --> 00:09:52.490 
results in a higher transfer speed and in a better bandwith utilisation.

116
00:09:53.480 --> 00:09:57.900 
The client announce as they are compression capabilities

117
00:09:58.400 --> 00:10:02.950 
in the request header except encoding and here are methods

118
00:10:03.480 --> 00:10:06.240 
which methods

119
00:10:06.960 --> 00:10:10.950 
compression methods mentioned the client can work with.

120
00:10:11.770 --> 00:10:16.860 
The server gets this request these the client is able to

121
00:10:17.610 --> 00:10:23.190 
understand Gzip information and though the server chooses

122
00:10:23.220 --> 00:10:26.460 
a matching compression method in this case

123
00:10:27.040 --> 00:10:34.570 
he chooses gzip and then he applies gzip compression method

124
00:10:34.870 --> 00:10:38.520 
to the message body in the entity header

125
00:10:39.140 --> 00:10:44.630 
with the content encoding he gives the client information which

126
00:10:44.680 --> 00:10:49.500 
compression method was used. Here awesome common encoding tokens,

127
00:10:49.500 --> 00:10:53.980 
for example the deflate algorithm can be used which is described in

128
00:10:54.460 --> 00:10:58.600 
an RFC 1951 one for compression as the gzip

129
00:10:59.220 --> 00:11:05.230 
as a new gzip algorithms described in RFC 1951 too

130
00:11:05.760 --> 00:11:11.560 
can be used by the way this is the most broadly supported compression

131
00:11:11.560 --> 00:11:15.900 
method for HTTP/1.1 then there is exi

132
00:11:17.650 --> 00:11:19.400 
world wide web consortium

133
00:11:21.160 --> 00:11:27.220 
designed by the world wide web consortium for efficient xml document interchange

134
00:11:27.430 --> 00:11:34.460 
and as an identity shows that no compression is applied at all.
