WEBVTT

1
00:00:01.130 --> 00:00:03.760 
hello and welcome to
this new course.

2
00:00:04.400 --> 00:00:08.000 
the goal of this presentation is to
give you a better understanding

3
00:00:08.430 --> 00:00:13.170 
of what is RIT well today,
what is changing and why

4
00:00:13.170 --> 00:00:14.610 
we need a new
memory fabric?

5
00:00:15.270 --> 00:00:19.950 
we'll go through your concepts such as
OMI memory inception and see also

6
00:00:20.120 --> 00:00:25.240 
that we can do unbelievable things in other
domains such as huge data acquisition.

7
00:00:26.060 --> 00:00:30.600 
but that's all about the fusion of
IT, but not early, since all of

8
00:00:30.600 --> 00:00:31.850 
this can be used today.

9
00:00:32.540 --> 00:00:35.900 
I'm Brenno Mesnet and working
on openCAPI implement

10
00:00:35.910 --> 00:00:41.230 
in IBM company and I am delighted to
share with you this awesome technology.

11
00:00:43.570 --> 00:00:47.470 
let's start with the basics.
here we recognize the CPU

12
00:00:47.470 --> 00:00:51.240 
of fabric architecture. we
have the memory bus which is

13
00:00:51.240 --> 00:00:55.560 
latency and bandwidth critical.
the processor interconnect buses

14
00:00:55.560 --> 00:01:00.240 
which are all proprietary, which are
latency and bandwidth critical.

15
00:01:00.780 --> 00:01:05.140 
then we have storage at that which
is latency and bandwidth tolerant.

16
00:01:05.620 --> 00:01:11.480 
and finally, we have the network-attached which
is latency relaxed and bandwidth critical.

17
00:01:12.020 --> 00:01:15.840 
so that's the way it works
today, why would we check it?

18
00:01:19.550 --> 00:01:23.720 
we see this they are centered shift
where we see memory everywhere.

19
00:01:23.920 --> 00:01:28.450 
on the processor interface this
interconnect is essentially

20
00:01:28.460 --> 00:01:32.580 
at current memory interface. the
storage is becoming persistent.

21
00:01:32.780 --> 00:01:36.460 
and we are remotely connecting
to memory through the network.

22
00:01:38.370 --> 00:01:43.180 
so a better way to represent it
would be the difference buses

23
00:01:43.180 --> 00:01:47.130 
connecting to memory memory
memory. but to evolve this way

24
00:01:47.670 --> 00:01:50.370 
we have several compromises
to take care about.

25
00:01:51.010 --> 00:01:55.190 
there is a complex logical translation
between all these buses.

26
00:01:55.610 --> 00:01:59.590 
and on the processor side, there
is a wast of IO Resources

27
00:01:59.600 --> 00:02:04.200 
to handle these buses. I told that
the cost of these data-centric

28
00:02:04.200 --> 00:02:09.170 
services and we can clearly see that
we are spending most of our money

29
00:02:09.440 --> 00:02:13.080 
and the memory infrastructure rather
than on the processes side.

30
00:02:16.290 --> 00:02:20.150 
so ideally we would like to
convert these memories and put

31
00:02:20.150 --> 00:02:22.410 
them all together
into a tube memory.

32
00:02:23.090 --> 00:02:27.370 
access all through a new memory bus
which has the lowest possible

33
00:02:27.370 --> 00:02:31.130 
latency, the highest
bandwidth and scalable.

34
00:02:32.280 --> 00:02:36.660 
remember that memories are not of
a single type and they can have

35
00:02:36.670 --> 00:02:40.770 
huge differences in latencies.
we'll come back later on this

36
00:02:40.770 --> 00:02:41.620 
specific point.

37
00:02:44.700 --> 00:02:49.680 
but what about that PCIe's bus which
can even have the internet bus.

38
00:02:50.300 --> 00:02:56.260 
we are seeing more and more these
interconnect buses being integrated to CPU

39
00:02:56.570 --> 00:03:00.100 
which now changed from a pure
processor into what we call a

40
00:03:00.100 --> 00:03:05.400 
system on a chip. and the PCI is
now becoming a guest at us.

41
00:03:06.810 --> 00:03:10.860 
so putting all that together we
have this data-centric core

42
00:03:10.860 --> 00:03:13.590 
building block connected
to this two memory.

43
00:03:14.090 --> 00:03:18.510 
and openCAPI is the new memory
bus which offers this very low

44
00:03:18.510 --> 00:03:20.800 
latency high bandwidth
and scalable.

45
00:03:23.660 --> 00:03:28.270 
what about the other processes
accelerators. we have processes

46
00:03:28.370 --> 00:03:34.230 
and SoC variance which comes
with CPU GPU FPGA or AI chips.

47
00:03:34.740 --> 00:03:38.840 
and we can really separate them
into two categories. one on

48
00:03:38.840 --> 00:03:43.130 
the left which latency critical
and need volume and coherence.

49
00:03:43.620 --> 00:03:46.960 
the other on the right
which is latency relaxed

50
00:03:47.360 --> 00:03:51.180 
and used often for the data
reduction by filtering the data

51
00:03:51.190 --> 00:03:53.270 
before sending them to
the system for example.

52
00:03:54.080 --> 00:03:59.350 
this last category often doesn't need
coherence, except if you use your FPGA

53
00:03:59.550 --> 00:04:03.540 
as an accelerator of your CPU
for very specific processing.

54
00:04:05.710 --> 00:04:11.100 
putting all that together we end up with
the distributed memory data plane

55
00:04:11.410 --> 00:04:16.010 
with all the different processes
attached which gives us an

56
00:04:16.270 --> 00:04:20.680 
heterogeneous distributed
computing. we can see that

57
00:04:21.130 --> 00:04:26.850 
from the rack scale up to the date center
scale and we will represent this.

58
00:04:27.010 --> 00:04:32.570 
computing shows that even in our
minds we are in a death data

59
00:04:32.570 --> 00:04:33.480 
centric world.
In currently

60
00:04:39.200 --> 00:04:45.510 
we can see that here we have the cloud,
the edge and the intelligence IloT.

61
00:04:45.840 --> 00:04:50.190 
This internet of
things will bring us

62
00:04:50.360 --> 00:04:55.020 
so much data that we may need to
fill them with dedicated ships

63
00:04:55.020 --> 00:04:58.400 
before entering the data into
this architecture. think about

64
00:04:58.400 --> 00:05:02.850 
extracting just peoples faces from
the multiple video streaming

65
00:05:02.850 --> 00:05:07.370 
that we can get from different characters
to reduce the amount of data

66
00:05:07.580 --> 00:05:09.820 
and keep only the
interesting information.

67
00:05:12.030 --> 00:05:14.520 
so great concept
or reality?

68
00:05:15.430 --> 00:05:18.920 
well openCAPI is in production
today and we are already at

69
00:05:18.920 --> 00:05:22.950 
the third generation of it. it
was first the board in twenty

70
00:05:23.080 --> 00:05:27.250 
fourteen with the focus on data
centric and open community.

71
00:05:27.960 --> 00:05:31.660 
a lot of work is still ongoing to improve
it and the next implementations.

72
00:05:32.500 --> 00:05:37.470 
openCAPI is an eight-lane bus
for us introduced in POWER9.

73
00:05:37.650 --> 00:05:42.860 
and provides a 25GB/s bandwidth
and a very low latency.

74
00:05:43.730 --> 00:05:48.250 
and the last openCAPI is also
introducing since 2019,

75
00:05:48.600 --> 00:05:52.030 
the open memory interface
also called OMI,

76
00:05:52.430 --> 00:05:55.630 
with the differential
DDIMMs. and this

77
00:05:55.630 --> 00:05:57.330 
is really a
game changer.

78
00:05:59.690 --> 00:06:04.960 
so today we have a processor and a
family of FPGA which are supporting

79
00:06:04.970 --> 00:06:09.050 
this openCAPI high speed interconnect
and this memory sharing.

80
00:06:10.910 --> 00:06:14.660 
OCL framework has been
developed to easily connect

81
00:06:14.670 --> 00:06:20.640 
all this together. but also to
make it much easier to program

82
00:06:20.650 --> 00:06:24.030 
your FPGA for your
application in C or HDL.

83
00:06:24.480 --> 00:06:29.370 
the software stack is provided also and
integrates even with python support.

84
00:06:30.070 --> 00:06:34.490 
we have also the microchip did
your proper chip which adds

85
00:06:34.500 --> 00:06:40.090 
less than ten minutes a contingency and
i asked to connect these themes dips

86
00:06:40.270 --> 00:06:45.560 
into the FPGA fabrics to amplify the
memory bandwidth of the FPGA fabric.

87
00:06:47.090 --> 00:06:53.040 
the take-away are that archon and software are
not just proven, but also fully open source.

88
00:06:57.560 --> 00:07:01.270 
so, what I'm trying to do is
really to be able together

89
00:07:02.330 --> 00:07:07.380 
together all these interconnects.
the train is depth versus

90
00:07:07.380 --> 00:07:11.390 
latency purse of bandwidth. there suspicious
and thirst of power versus cost.

91
00:07:11.820 --> 00:07:16.510 
all I am is trying to know what
all this and we actually

92
00:07:16.520 --> 00:07:18.480 
use a buffer to
achieve that.

93
00:07:19.380 --> 00:07:22.810 
we have today that he didn't
connect up as around.

94
00:07:23.590 --> 00:07:27.950 
the bittware 250-hms connected
at the low latency NAND

95
00:07:28.240 --> 00:07:31.790 
and the bittware 250-SoC
that connects the NAND

96
00:07:31.980 --> 00:07:33.600 
to this young
knight bus.

97
00:07:34.530 --> 00:07:40.130 
and we may have emerging memories
directly connecting to this OMI.

98
00:07:42.060 --> 00:07:46.850 
so now let's have a quick look to what will
be in the next generation of processes

99
00:07:47.150 --> 00:07:49.020 
that you will get in
a couple of years.

100
00:07:49.780 --> 00:07:52.590 
let's think of the idea of
a PowerAXON for example

101
00:07:53.050 --> 00:07:57.230 
and have a close look at what's
it. on the right all am i

102
00:07:57.230 --> 00:08:01.890 
connections directly connected
on the chip and this provides

103
00:08:01.900 --> 00:08:06.240 
six times more bandwidth per square
millimeters than to the DDR4.

104
00:08:06.840 --> 00:08:08.760 
on the left in
each corner,

105
00:08:09.390 --> 00:08:11.980 
PowerAXON Buses were
the same treatment.

106
00:08:13.790 --> 00:08:19.190 
so on the all I am say, the different
options of memorial we just spoke about.

107
00:08:19.960 --> 00:08:23.510 
you can connect them with
the maximum bandwidth and

108
00:08:23.520 --> 00:08:24.720 
and the minimum
latency.

109
00:08:26.590 --> 00:08:28.520 
on the left side, on
the excellent side,

110
00:08:29.520 --> 00:08:34.500 
you will see the smp interconnect which
connects all the processes together.

111
00:08:35.480 --> 00:08:42.010 
but also the openCAPI bus which
connects to the outside world

112
00:08:42.260 --> 00:08:43.510 
as it is equal
with FPGA.

113
00:08:47.800 --> 00:08:51.530 
but now that we have all these
memories interconnection.

114
00:08:51.690 --> 00:08:53.530 
what is the
idea behind?

115
00:08:56.360 --> 00:08:59.490 
as you can see this high bandwidth
and low latency of the buses

116
00:08:59.520 --> 00:09:05.060 
provides huge new capabilities and we connect
course, but also memories together.

117
00:09:05.280 --> 00:09:11.030 
we can now use that as a huge pool of
memory that can be used by every core.

118
00:09:11.510 --> 00:09:16.910 
every system is now able to steal the
memory of another system and use it

119
00:09:17.240 --> 00:09:18.010 
as its owner.

120
00:09:20.580 --> 00:09:24.020 
this brings to a totally new
concept where expensive memory

121
00:09:24.020 --> 00:09:28.140 
is now optimized as much as we can.
applications that need a huge amount

122
00:09:28.140 --> 00:09:31.460 
of memory can not work
without being limited.

123
00:09:31.890 --> 00:09:33.470 
and this is important.

124
00:09:36.650 --> 00:09:41.680 
I hear you, do we need to wait two years to
experiment with this new memory inception.

125
00:09:42.120 --> 00:09:45.040 
no worry, having time
is this project.

126
00:09:45.540 --> 00:09:49.470 
this is an open-source project
that implements this new concept

127
00:09:49.780 --> 00:09:53.250 
with the power 9 and with
alpha-beta officials.

128
00:09:53.960 --> 00:09:58.440 
the times as flow approach consists
of a compute node on the

129
00:09:58.440 --> 00:10:02.220 
left side of the figure that
this thyming the memory from a

130
00:10:02.220 --> 00:10:04.870 
memory note on the right
side of the figure.

131
00:10:05.740 --> 00:10:11.800 
computer and point is based on the
openCAPI m one note also called LPC.

132
00:10:12.480 --> 00:10:15.900 
why the memory M point uses
the open copy c one mode?

133
00:10:17.050 --> 00:10:21.310 
on the computing point that
disagrees gated memory is mad

134
00:10:22.100 --> 00:10:25.400 
at a specific branch of address
in the physical address space

135
00:10:25.630 --> 00:10:29.840 
and can be directly humped to
a running language system.

136
00:10:30.520 --> 00:10:34.860 
no software modification is needed
practice. this is a great memory.

137
00:10:35.300 --> 00:10:39.250 
neither in the linux kernel nor
from the user application.

138
00:10:41.500 --> 00:10:48.690 
ok so openCAPI is good for memory management,
but provides also unbelievable capabilities

139
00:10:48.890 --> 00:10:53.890 
to other applications such as also
fast data acquisition. let's

140
00:10:53.890 --> 00:10:57.240 
have a quick look at an experience
which is actually developed

141
00:10:57.420 --> 00:11:00.520 
for macromolecular crystallography
at PSI institute.

142
00:11:03.090 --> 00:11:06.140 
the purpose of this
crystallography experience

143
00:11:06.150 --> 00:11:09.890 
is to generate electrons, accelerate
them as close as possible

144
00:11:09.890 --> 00:11:13.590 
to the speed of light and passed
them through strong magnets

145
00:11:13.590 --> 00:11:17.850 
to generate bright x rays which
are directed to a crystal on

146
00:11:17.850 --> 00:11:21.230 
the top of a needle or flowing in
a medium in front of the beam.

147
00:11:21.820 --> 00:11:25.300 
these conditions create an image
of the destructive diffraction

148
00:11:25.340 --> 00:11:28.940 
and pixel detectors and the raw
data issued from these detectors

149
00:11:28.950 --> 00:11:34.590 
are processed and filtered to generate a clean
image and then stored after compression.

150
00:11:35.920 --> 00:11:40.090 
a post-processing and then performed
on stored data to extract

151
00:11:40.100 --> 00:11:42.340 
the three d representation
of the molecule

152
00:11:42.950 --> 00:11:47.440 
to understand how macromoleculars
carry their function. it's

153
00:11:47.440 --> 00:11:51.860 
also heavily used by pharmaceutical
companies in the rational drug design.

154
00:11:54.580 --> 00:11:59.170 
today the former pixel detector
actually used generates

155
00:11:59.170 --> 00:12:02.100 
a flow of ninety GB
of data per second.

156
00:12:03.140 --> 00:12:06.020 
the aim is to first double the
rate of the transition to be

157
00:12:06.020 --> 00:12:09.820 
able to conduct much more
experiments at the same time,

158
00:12:10.550 --> 00:12:13.810 
then by increasing the detector
resolution from 4 to 10

159
00:12:13.810 --> 00:12:16.690 
up itself. more accurate
results will be obtained.

160
00:12:17.430 --> 00:12:23.070 
the huge challenge here is to have the ability
to acquire process compressive store

161
00:12:23.330 --> 00:12:27.620 
up to forty six gigabyte of
data per second, meaning five

162
00:12:27.620 --> 00:12:29.420 
times more than today.

163
00:12:32.570 --> 00:12:36.360 
so let on some business acquisition
team. today frames of

164
00:12:36.370 --> 00:12:39.780 
data are fed by other sensors
are received by network calls

165
00:12:39.780 --> 00:12:44.680 
and handled by operating systems
receiving ninety point per second of

166
00:12:44.830 --> 00:12:49.810 
data in a single server. thanks
for a significant CPU resource

167
00:12:49.820 --> 00:12:53.550 
to make sure no friends are lost and
processing on the fly is not possible.

168
00:12:54.020 --> 00:12:56.330 
data need to be safe
and around discs.

169
00:12:57.220 --> 00:13:01.750 
once all data are acquired, offline
conversion of raw data to real nets

170
00:13:01.870 --> 00:13:05.820 
and then image processing and
compression are performance very

171
00:13:05.830 --> 00:13:09.330 
efficiently in CPU
before data starts.

172
00:13:10.240 --> 00:13:14.320 
limitation to ninety GB/s
isn't optimization of

173
00:13:14.320 --> 00:13:17.950 
the kernels networks that a
network card interrupts him.

174
00:13:21.210 --> 00:13:24.810 
the more pixels for detectors
with a bigger acquisition rate,

175
00:13:24.970 --> 00:13:26.740 
the more links
to the server.

176
00:13:27.350 --> 00:13:32.170 
if each table has the ability to
night now acquired that data

177
00:13:32.180 --> 00:13:36.290 
by a couple of hundred GB
infinite links for a card.

178
00:13:36.800 --> 00:13:42.000 
even more image conversion is processed on
the fight in this reprogrammable chip

179
00:13:42.430 --> 00:13:47.120 
that sort it and thanks to the openCAPI
links sent directly to the server memory

180
00:13:47.390 --> 00:13:50.700 
at maximum of twenty two
gigabytes per second.

181
00:13:51.870 --> 00:13:55.630 
the data can then be transferred
to GPU card for example

182
00:13:55.630 --> 00:13:57.910 
which will detect and
move them to images.

183
00:13:58.570 --> 00:14:03.460 
then compression before storage
can be done either by the gzip

184
00:14:03.470 --> 00:14:06.260 
embedded hardware accelerator
of the Power9 chip

185
00:14:06.830 --> 00:14:09.430 
or by the 20 CPU
chips in parallel.

186
00:14:10.140 --> 00:14:15.290 
duplicate this data acquisition
chain and within the same server,

187
00:14:15.580 --> 00:14:19.590 
you can acquire five times more
data than what you are able

188
00:14:19.590 --> 00:14:20.430 
to achieve today.

189
00:14:23.300 --> 00:14:25.680 
now let's summarize all
this in one slide.

190
00:14:29.090 --> 00:14:34.620 
you have on the third story, openCAPI
is not a concept. it is just reality.

191
00:14:34.860 --> 00:14:36.410 
but also part
of the future

192
00:14:37.230 --> 00:14:41.750 
you have everything in hand stability
with you computation engine here

193
00:14:42.110 --> 00:14:43.690 
add your memory here.

194
00:14:44.440 --> 00:14:46.770 
one last word,
everything is proved

195
00:14:47.410 --> 00:14:48.690 
and open source.

196
00:14:50.770 --> 00:14:55.930 
Do you want more? please contact
alexandre based on me. we will be happy

197
00:14:55.930 --> 00:14:59.370 
to provide you an answer and access
or maybe just more arguments

198
00:14:59.370 --> 00:15:02.930 
to show you that this CAPI
technology is really awesome.

199
00:15:03.680 --> 00:15:06.970 
test it once and you'll never
be able to go back to the old

200
00:15:06.970 --> 00:15:08.090 
way of doing things.

201
00:15:10.110 --> 00:15:13.920 
thank you. thanks for listening. good
luck with your following presentation.

202
00:15:14.320 --> 00:15:18.170 
here is a list of references I
would highly recommend you to

203
00:15:18.170 --> 00:15:21.870 
have a look, if you need more
data in details and what i

204
00:15:21.870 --> 00:15:23.980 
have just talked
about. thank you.
