WEBVTT

1
00:00:00.000 --> 00:00:04.220 
Welcome. Today I want to talk
about embeddings; what they are,

2
00:00:04.230 --> 00:00:06.550 
what they are used for and
how we can compute them.

3
00:00:07.140 --> 00:00:10.560 
And I think in order to answer
the first two questions it's

4
00:00:10.560 --> 00:00:11.990 
best to start with
an example.

5
00:00:13.310 --> 00:00:16.850 
Here we can see a somewhat random
collection of words like

6
00:00:16.860 --> 00:00:23.590 
pizza or plane or shark. Now in itself a word
is a rather abstract construct, it is just

7
00:00:23.800 --> 00:00:25.300 
a sequence of letters.

8
00:00:26.290 --> 00:00:31.110 
However as humans we associate
words with meanings, for example,

9
00:00:31.110 --> 00:00:36.600 
we know that the letters f, i,
s, h in that order denote an

10
00:00:36.600 --> 00:00:38.330 
animal that lives
in the water.

11
00:00:39.650 --> 00:00:43.680 
So not only do we associate words
with meanings, but we derive

12
00:00:43.680 --> 00:00:45.320 
these meanings using
other words.

13
00:00:46.240 --> 00:00:50.670 
In that sense words are connected.
So if i asked you to lay

14
00:00:50.670 --> 00:00:54.910 
out these words in a meaningful way you
might come up with something like this.

15
00:00:56.300 --> 00:01:00.790 
Now arguably the structure of these
words is pretty hard to see. However

16
00:01:01.010 --> 00:01:05.020 
if we also show the connections
between these words then the

17
00:01:05.030 --> 00:01:06.630 
picture becomes
more apparent.

18
00:01:07.520 --> 00:01:12.460 
Now it's much easier to see that a fish
is an animal that lives in the water

19
00:01:12.610 --> 00:01:14.780 
and for example that
a shark is a fish.

20
00:01:16.170 --> 00:01:20.680 
Now what we did here is we used
our slide as a two dimensional

21
00:01:20.680 --> 00:01:25.860 
coordinate system and we assigned
each word a coordinate. For example

22
00:01:26.010 --> 00:01:29.530 
fish got x coordinate five
and y coordinate six.

23
00:01:30.830 --> 00:01:34.410 
And of course we didn't assign these
coordinates randomly, but rather

24
00:01:34.630 --> 00:01:38.320 
we position them such did their
positions represent their meaning.

25
00:01:39.220 --> 00:01:44.030 
Over here we have words that are related
to animals like shark and whale

26
00:01:44.340 --> 00:01:47.910 
and over here are words that are
related to water like ocean or lake

27
00:01:48.220 --> 00:01:53.450 
and down here we have words that
are things that belong to humans.

28
00:01:54.980 --> 00:01:59.590 
Now this means that we used the
space to represent the words

29
00:01:59.680 --> 00:02:01.890 
and with this we can do
some interesting things.

30
00:02:02.700 --> 00:02:06.470 
For example, if I gave you a term
like this then you have probably

31
00:02:06.470 --> 00:02:12.080 
no idea what it means. However, if I also
told you that the corresponding coordinate

32
00:02:12.190 --> 00:02:15.560 
is over here, then you would
probably guess that this is an

33
00:02:15.560 --> 00:02:16.970 
animal that lives
in the water.

34
00:02:17.700 --> 00:02:21.060 
And in fact this term refers to
a species of sharks that live

35
00:02:21.060 --> 00:02:21.900 
in the atlantic.

36
00:02:23.990 --> 00:02:27.210 
Another thing that we can do
since the meanings of the words

37
00:02:27.210 --> 00:02:29.610 
are represented in the
space is the following:

38
00:02:30.450 --> 00:02:33.870 
observe the words that belong
together like mother and child

39
00:02:33.880 --> 00:02:37.040 
or bird and eagle are positioned
close to each other.

40
00:02:37.650 --> 00:02:42.340 
Conversely if we find words that are
close but not yet connected we could

41
00:02:42.560 --> 00:02:45.680 
figure out whether these words
should be connected instead.

42
00:02:46.800 --> 00:02:50.860 
So this means we are able to understand
words that we haven't heard before

43
00:02:51.070 --> 00:02:53.990 
and we are also able to infer
new connections between words

44
00:02:54.200 --> 00:02:56.840 
because their meanings are
represented in space.

45
00:02:57.760 --> 00:03:00.700 
And this is something that we
refer to as an embedding.

46
00:03:02.040 --> 00:03:05.670 
Generally speaking an embedding
is just a structure preserving

47
00:03:05.670 --> 00:03:08.110 
map from an object x
to another object y.

48
00:03:09.610 --> 00:03:15.140 
In this case the term structure preserving
is left deliberately wake because

49
00:03:15.770 --> 00:03:19.030 
its meaning depends on the
application meaning what we want

50
00:03:19.030 --> 00:03:20.050 
to do with the
embedding.

51
00:03:20.840 --> 00:03:26.880 
We say that x is embedded into y and we
call E of x the coordinate of an element.

52
00:03:28.250 --> 00:03:32.000 
In the example that we've just
seen x was a set of words like

53
00:03:32.000 --> 00:03:35.980 
animal fish in water and we
embedded these words into the two

54
00:03:35.980 --> 00:03:39.510 
dimensional plane meaning we assign
them coordinates in this plane.

55
00:03:41.020 --> 00:03:45.390 
Another example is dimensionality
reduction. There x could be

56
00:03:45.390 --> 00:03:48.680 
a set of feature vectors in some
high dimensional space and

57
00:03:48.680 --> 00:03:51.640 
we want to map them into some
lower dimensional space in order

58
00:03:51.640 --> 00:03:53.010 
to get smaller
coordinates.

59
00:03:54.510 --> 00:03:58.070 
Another example that you have probably
seen before is visualization.

60
00:03:58.420 --> 00:04:02.670 
Here x could be a set of train stations
in a network of train tracks

61
00:04:02.880 --> 00:04:06.260 
and we want to embed them into
the two dimensional plane such

62
00:04:06.270 --> 00:04:08.340 
that we can see the tracks
in the networks well.

63
00:04:10.480 --> 00:04:14.550 
Now all of these applications are
relevant for machine learning purposes.

64
00:04:15.020 --> 00:04:19.260 
First many machine learning frameworks
take the input in the form of vectors.

65
00:04:19.660 --> 00:04:22.470 
Meaning we cannot use
them on words directly.

66
00:04:23.390 --> 00:04:26.650 
Therefore we can just embed the
words and use the resulting

67
00:04:26.650 --> 00:04:30.320 
coordinates as vectors in order to apply
the same machine learning tools.

68
00:04:31.220 --> 00:04:34.710 
Furthermore many machine learning
frameworks are computationally

69
00:04:34.710 --> 00:04:39.030 
expensive which means we can
process our data more efficiently

70
00:04:39.550 --> 00:04:42.270 
if we reduce the dimensionality
of the input data.

71
00:04:43.430 --> 00:04:47.140 
And while visualizations are more
important for humans we can

72
00:04:47.140 --> 00:04:51.590 
use them to understand the structure
of the data and hand and infer which

73
00:04:51.770 --> 00:04:54.500 
technique is best used to
further process our data.

74
00:04:56.070 --> 00:04:59.930 
Now among these examples the first
and the last one is a bit of

75
00:05:00.170 --> 00:05:04.070 
it's a bit special because here
we used graphs to model our

76
00:05:04.070 --> 00:05:05.590 
data prior to
the embedding.

77
00:05:06.840 --> 00:05:11.340 
A graph or also called a network
is just a set of nodes

78
00:05:11.910 --> 00:05:14.840 
and a set of edges that denote
which nodes are connected

79
00:05:15.620 --> 00:05:18.060 
and since this is just a
very general framework

80
00:05:18.570 --> 00:05:21.800 
graphs are often used as an intermediate
step when embedding data.

81
00:05:21.940 --> 00:05:26.160 
Which means instead of embedding
the data directly we transform

82
00:05:26.160 --> 00:05:28.350 
it into a graph and
then embed the graph.

83
00:05:28.860 --> 00:05:31.940 
And in fact we have seen this in
our very first example where

84
00:05:31.940 --> 00:05:36.160 
our notes were just words and the
connections between the nodes the edges

85
00:05:36.360 --> 00:05:38.800 
represented which words
belong together.

86
00:05:40.180 --> 00:05:43.590 
Another example that we've seen
where visualisations there are

87
00:05:43.590 --> 00:05:46.690 
nodes in the graph, train stations
and we have edges between

88
00:05:46.690 --> 00:05:51.560 
any two that are consecutive on a train
track. And here we want to embed

89
00:05:51.820 --> 00:05:55.950 
this network such that we can easily
follow the tracks and such that the

90
00:05:56.070 --> 00:05:59.500 
original positions of the train
stations are not distorted too much.

91
00:06:01.100 --> 00:06:05.180 
Another example is chip design.
There our network consists of

92
00:06:05.180 --> 00:06:08.830 
transistors that are connected
by wires and we want to embed

93
00:06:08.830 --> 00:06:12.920 
them on to a chip such that as few
wires as possible intersect.

94
00:06:14.120 --> 00:06:17.320 
Or maybe our network is something
like the internet and we

95
00:06:17.320 --> 00:06:22.360 
want to embed it in such a way such that
we can use the coordinates in order to

96
00:06:22.760 --> 00:06:24.660 
send information
through the network.

97
00:06:26.040 --> 00:06:30.330 
Now since embeddings and applications
are very versatile it's

98
00:06:30.330 --> 00:06:35.190 
important to consider some key questions
when considering an embedding approach.

99
00:06:35.930 --> 00:06:39.190 
First it is important to be aware
of what the question is that

100
00:06:39.190 --> 00:06:40.760 
we hope to answer
with the embedding.

101
00:06:41.520 --> 00:06:45.540 
Second we should ask whether we actually
need embedding to answer this question.

102
00:06:46.270 --> 00:06:50.420 
for example it doesn't make much
sense to embed a set of numbers

103
00:06:50.420 --> 00:06:53.960 
in order to sort it because there
are better ways to sort numbers.

104
00:06:55.460 --> 00:06:59.400 
Moreover recall that an embedding
is just a map from our

105
00:06:59.400 --> 00:07:03.350 
input data x into a target space.
So we have to define what

106
00:07:03.350 --> 00:07:04.750 
our target space is.

107
00:07:06.390 --> 00:07:09.780 
in the examples that we've seen
so far this was mostly the

108
00:07:09.780 --> 00:07:13.750 
two dimensional euclidean plane.
However sometimes the data doesn't

109
00:07:13.750 --> 00:07:17.690 
fit very well into two dimensions and
we need higher dimensions instead.

110
00:07:18.710 --> 00:07:22.410 
Additionally the data might not
even fit well into euclidean

111
00:07:22.410 --> 00:07:26.420 
space at all and we might have to consider
other spaces like the hyperbolic space.

112
00:07:27.720 --> 00:07:31.340 
Unfortunately there is not really
a recipe for how to find

113
00:07:31.620 --> 00:07:37.070 
a suitable target space. However looking
at the properties of the input data

114
00:07:37.300 --> 00:07:40.930 
might indicate some reasonable
starting points. Typically it's

115
00:07:40.930 --> 00:07:42.150 
best to start simple.

116
00:07:43.590 --> 00:07:46.110 
And when we have answered all
of these questions we should

117
00:07:46.110 --> 00:07:50.390 
be aware of how to embed the data
and this is particularly important

118
00:07:50.530 --> 00:07:55.600 
because we have to adapt the
embedding technique towards the

119
00:07:55.600 --> 00:07:57.190 
application
that uses it.

120
00:07:58.620 --> 00:08:00.760 
For example it doesn't
make much sense

121
00:08:01.730 --> 00:08:05.470 
to embed a network of train tracks
onto a chip. Then we have to do

122
00:08:05.670 --> 00:08:07.900 
different things for the
different embedding techniques.

123
00:08:09.100 --> 00:08:12.410 
Now in the following i want to
take a look at a particular

124
00:08:12.420 --> 00:08:16.100 
embedding technique that is often
used to represent the knowledge

125
00:08:16.100 --> 00:08:21.440 
that is stored in a graph similar to how we
did in the initial example with words.

126
00:08:22.230 --> 00:08:26.100 
So our input is a graph that
consists of vertices and edges

127
00:08:26.260 --> 00:08:30.290 
and our output is a mapping from
the vertices into some space.

128
00:08:30.800 --> 00:08:32.160 
Let's look at
an example.

129
00:08:33.010 --> 00:08:37.310 
Here our graph consists of eight nodes
that are numbered from one to eight

130
00:08:37.620 --> 00:08:40.980 
and we have edges between the nodes
for example one is connected

131
00:08:40.980 --> 00:08:44.580 
to two three and four and two is
connected to four and so on.

132
00:08:45.760 --> 00:08:51.370 
And in our example we want to map these nodes
into a two dimensional euclidean plane.

133
00:08:53.020 --> 00:08:56.230 
Now in order to best represent the
knowledge that is represented

134
00:08:56.230 --> 00:09:01.880 
in the network our objective is that the distances
in the embedding resembled graphed instances.

135
00:09:02.700 --> 00:09:06.950 
In order to see what I mean by this let's
take a look at an example embedding.

136
00:09:07.900 --> 00:09:11.680 
This is one embedding of the
graph we defined above and we

137
00:09:11.680 --> 00:09:15.420 
can now check how well it fits
our objective by looking how

138
00:09:15.420 --> 00:09:18.790 
the distances in the embedding compare
to the distances in the graph.

139
00:09:19.350 --> 00:09:22.930 
For example three and seven are
pretty close in the embedding.

140
00:09:23.190 --> 00:09:25.250 
But let's look how close
they are in the graph.

141
00:09:25.940 --> 00:09:30.490 
Well seven is only connected to six
which is connected to five and eight

142
00:09:30.740 --> 00:09:35.520 
however eight doesn't have any more connections
in five is only connected to four

143
00:09:35.830 --> 00:09:39.660 
and only then do we reach three.
So the distance between seven

144
00:09:39.660 --> 00:09:43.200 
and three in the graph is actually
pretty large while it's very

145
00:09:43.370 --> 00:09:44.490 
small in the embedding.

146
00:09:45.270 --> 00:09:48.560 
So in this case the embedding
doesn't do a good job of

147
00:09:48.600 --> 00:09:50.310 
representing the
distances in the graph.

148
00:09:51.110 --> 00:09:54.870 
Another example are two and three which
are very far apart in the embedding.

149
00:09:55.130 --> 00:09:58.590 
However they are both connected
to one and four meaning there

150
00:09:58.590 --> 00:10:02.090 
are two very short paths between
two and three. So again the

151
00:10:02.090 --> 00:10:03.750 
embedding doesn't do
a good job here.

152
00:10:05.140 --> 00:10:08.330 
It much better fitted for our
objective is this embedding.

153
00:10:08.900 --> 00:10:13.410 
Arguably we can already see that this does
a better job of representing the graph.

154
00:10:13.650 --> 00:10:17.960 
However we can also check. Again
three and seven are now very

155
00:10:17.960 --> 00:10:21.290 
far away from each other which mentions
their distance in the graph.

156
00:10:22.250 --> 00:10:25.320 
Additionally two and three are
now placed close to each other

157
00:10:25.320 --> 00:10:28.660 
which makes sense because they are
connected by two very short paths.

158
00:10:29.850 --> 00:10:35.820 
Now in order to get to an embedding like
this we have to optimize two criteria.

159
00:10:36.210 --> 00:10:40.030 
First we want large distances between
nodes that are not connected

160
00:10:40.030 --> 00:10:44.010 
for example three and seven.
Second we want small distances

161
00:10:44.010 --> 00:10:48.250 
between nodes that are connected, for
example one and three or six and seven.

162
00:10:49.620 --> 00:10:53.470 
Now one embedding technique that
implements both of these criteria

163
00:10:53.560 --> 00:10:55.830 
are so called
spring embedders.

164
00:10:56.940 --> 00:11:00.290 
There the basic idea is to
simulate physical forces between

165
00:11:00.290 --> 00:11:01.340 
the nodes in the graph.

166
00:11:01.970 --> 00:11:04.960 
This means that we imagine that
the graph forms a mechanical

167
00:11:04.960 --> 00:11:06.760 
system of springs
and magnets.

168
00:11:08.890 --> 00:11:12.660 
Springs simulate the attractive
forces between adjacent nodes,

169
00:11:12.810 --> 00:11:16.290 
which means that between two nodes that
are connected we imagine a spring

170
00:11:16.590 --> 00:11:19.010 
that pulls the nodes
closer to each other.

171
00:11:20.170 --> 00:11:25.580 
Additionally we imagine that all vertices
are equipped with repelling magnets

172
00:11:25.790 --> 00:11:28.010 
that push the vertices
farther away.

173
00:11:28.890 --> 00:11:32.150 
And from this we can already
derive a very simple algorithm.

174
00:11:33.530 --> 00:11:36.870 
We start by assigning each node
a random position and then

175
00:11:36.880 --> 00:11:40.610 
iteratively update the positions
of the nodes by applying forces

176
00:11:40.670 --> 00:11:42.600 
until the system is
in a stable state.

177
00:11:43.520 --> 00:11:47.250 
This means that in each iteration
we look at each node and

178
00:11:47.250 --> 00:11:50.700 
compute the force that is exerted
on it by the other nodes.

179
00:11:50.910 --> 00:11:52.960 
And then we update its
position accordingly.

180
00:11:53.960 --> 00:11:58.430 
There our objective function is
represented by the energy in the system.

181
00:11:58.630 --> 00:12:01.680 
In order to see what this means
let's look at another example.

182
00:12:03.030 --> 00:12:06.310 
This small graph consists of
four vertices and three edges

183
00:12:06.370 --> 00:12:11.330 
and we want to take a look on the forces
that are exerted on vertex one.

184
00:12:12.360 --> 00:12:16.610 
Now recall that all vertices are
equipped with repelling magnets.

185
00:12:16.710 --> 00:12:20.680 
So in this case two and three want
to push one away while four

186
00:12:20.680 --> 00:12:24.410 
is already far apart from one. So there
is not much pushing to be done.

187
00:12:25.290 --> 00:12:30.180 
Additionally one is connected to
four, so we assume that there is an

188
00:12:30.630 --> 00:12:33.750 
spring between them that
pulls one closer to four.

189
00:12:34.310 --> 00:12:38.510 
And so the magnets that want to push
and the spring that wants to pull

190
00:12:38.710 --> 00:12:40.880 
contribute energy
to our system.

191
00:12:41.910 --> 00:12:45.520 
And now we want to reduce this
energy which we can do by letting

192
00:12:45.520 --> 00:12:49.200 
the forces work a bit, meaning
we apply the forces that work

193
00:12:49.200 --> 00:12:51.930 
on vertex one and update
its position accordingly.

194
00:12:52.940 --> 00:12:56.490 
This is done by representing
the forces as vectors.

195
00:12:56.930 --> 00:13:02.040 
So two and three want to push one away,
so we have two vectors that point from

196
00:13:02.150 --> 00:13:03.370 
two and three to one.

197
00:13:04.010 --> 00:13:08.980 
Additionally four wants to pull one
closer so we have another vector

198
00:13:09.090 --> 00:13:11.220 
that pulls one
closer to four.

199
00:13:12.180 --> 00:13:15.720 
And now we obtained a new position
of vertex one by summing

200
00:13:15.730 --> 00:13:20.300 
up these vectors, starting at one
. So two pushes one over here,

201
00:13:20.310 --> 00:13:24.310 
three additionally pushes over there
and then four pulls it over there.

202
00:13:24.900 --> 00:13:28.530 
And now we take the linear
combinations of this vector

203
00:13:28.650 --> 00:13:31.350 
which give us the new
position for vertex one.

204
00:13:32.730 --> 00:13:35.790 
And now we can see that the
embedding is improved because one

205
00:13:35.790 --> 00:13:38.900 
is no longer so close to two
and three to which it was

206
00:13:38.900 --> 00:13:42.250 
not connected and is closer to
four to which it is connected.

207
00:13:42.840 --> 00:13:46.710 
And we can verify that now the
energy in our system is reduced

208
00:13:46.800 --> 00:13:48.890 
by again looking at the
remaining forces.

209
00:13:49.610 --> 00:13:53.920 
So we start with the repelling
forces. The vertices are all

210
00:13:53.920 --> 00:13:56.500 
far away from each other so
there is no pushing to be done

211
00:13:56.500 --> 00:13:58.390 
and the magnets don't
contribute any energy.

212
00:13:59.220 --> 00:14:03.120 
Additionally the spring between
one and four is now compressed

213
00:14:03.120 --> 00:14:06.960 
and there's also less energy
over here and so the energy in

214
00:14:06.960 --> 00:14:08.730 
our system is
reduced overall.

215
00:14:10.070 --> 00:14:13.450 
Now when actually implementing an
algorithm like this ,we have

216
00:14:13.450 --> 00:14:17.580 
to consider some details. First we
have to define how the forces

217
00:14:17.580 --> 00:14:19.350 
actually work and how
strong they are.

218
00:14:20.080 --> 00:14:25.230 
Typically we want that attractive forces
get stronger the farther the objects are

219
00:14:25.690 --> 00:14:28.050 
such that they are put
together more strongly.

220
00:14:28.500 --> 00:14:32.530 
Additionally we want that the
repulsive forces get weaker the

221
00:14:32.530 --> 00:14:35.380 
farther the objects are because
we don't have to push as much

222
00:14:35.380 --> 00:14:37.180 
if the objects are
already far apart.

223
00:14:38.810 --> 00:14:42.970 
Additionally we have to define when we
consider the system to be stable.

224
00:14:43.680 --> 00:14:47.410 
This is usually done using an
energy threshold meaning when

225
00:14:47.470 --> 00:14:52.000 
ever the threshold reaches below
sorry whenever the energy

226
00:14:52.000 --> 00:14:56.480 
reaches below this threshold, we consider
the system to be stable and terminate.

227
00:14:57.680 --> 00:15:03.780 
Additionally we also add a fallback
option which is an iteration threshold

228
00:15:04.530 --> 00:15:08.200 
which simply limits the number of iterations
of force applications that we do

229
00:15:08.480 --> 00:15:12.100 
until we terminate the algorithm
in order to eventually stop

230
00:15:12.110 --> 00:15:14.290 
even if the system is
not in a stable state.

231
00:15:16.040 --> 00:15:20.100 
And of course this is only a very basic
explanation of how spring embedders work

232
00:15:20.310 --> 00:15:23.820 
and this technique has been refined
over and over and I recommend

233
00:15:23.820 --> 00:15:27.340 
the handbook of graph drawing and
visualization for more information on them.

234
00:15:28.940 --> 00:15:31.960 
In conclusion we have seen that
embeddings have very different

235
00:15:31.960 --> 00:15:36.850 
applications like knowledge representation,
dimensionality reduction and visualization

236
00:15:37.560 --> 00:15:43.300 
and that the basic idea is mapping data
into some space such that the structure

237
00:15:43.600 --> 00:15:44.830 
in the data is
preserved.

238
00:15:45.750 --> 00:15:48.650 
We have seen that graphs are a
useful framework for modelling

239
00:15:48.650 --> 00:15:52.730 
data prior to the embedding and
since embeddings are so versatile

240
00:15:52.890 --> 00:15:55.960 
it is important to consider some
key questions when dealing

241
00:15:55.960 --> 00:15:59.080 
with an embedding approach. Do
I actually need an embedding?

242
00:15:59.200 --> 00:16:03.570 
What do I need to embedding for? What
is the target space that I embed into

243
00:16:03.750 --> 00:16:07.600 
and how do I embed the data? The
last question is particularly

244
00:16:07.600 --> 00:16:11.100 
important because we have to
adapt the embedding technique

245
00:16:11.370 --> 00:16:14.100 
towards the application that
we want to use it for.

246
00:16:15.360 --> 00:16:18.740 
And finally we have seen spring
embedders as one simple approach

247
00:16:18.740 --> 00:16:21.730 
to obtaining an embedding where
the coordinates represent the

248
00:16:21.730 --> 00:16:22.780 
knowledge in the graph.

249
00:16:23.620 --> 00:16:27.510 
Now throughout the talk I added
references to the slides and

250
00:16:27.510 --> 00:16:30.250 
I encourage you to take a closer
look at the topics that you

251
00:16:30.250 --> 00:16:31.480 
found most interesting.
