WEBVTT

1
00:00:00.580 --> 00:00:01.960 
Hello and welcome.

2
00:00:02.540 --> 00:00:10.960 
The following video will explore the current academic approaches intended to solve the data related challenges in edge

3
00:00:10.960 --> 00:00:14.300 
AI scenarios.

4
00:00:14.300 --> 00:00:16.809 
As mentioned in the previous videos,

5
00:00:16.979 --> 00:00:26.399 
edgeAI faces several significant challenges. In addition to the uneven distribution of edge side data,

6
00:00:26.410 --> 00:00:33.570 
we may also face the problem of too few samples and that cannot effectively carry out the model training.

7
00:00:34.390 --> 00:00:42.310 
In addition, we may meet unknown data classes leading to the complete failure of the supervised learning method.

8
00:00:43.479 --> 00:00:53.170 
On the other hand, we know that so far the main achievements of deep learning in supervised learning

9
00:00:53.170 --> 00:00:58.859 
requiring a lot of annotated data but this is quite different from human learning behavior.

10
00:00:59.649 --> 00:01:07.239 
Human only need a small learning cost to quickly achieve new results, especially for smart people.

11
00:01:07.250 --> 00:01:16.409 
This is also one of the main difference between the current AI and human intelligence. For example AI needs over 10,000

12
00:01:16.409 --> 00:01:18.819 
books to learn the new language.

13
00:01:18.829 --> 00:01:20.379 
It is hard to imagine.

14
00:01:20.379 --> 00:01:28.489 
Human beings needs to read so many books to learn the new language and this challenge needs to be solved by the machine learning

15
00:01:28.489 --> 00:01:29.319 
community.

16
00:01:31.469 --> 00:01:32.599 
At present,

17
00:01:32.609 --> 00:01:40.120 
the machine learning community has many exciting research directions dedicated to relaxing the reliance on large amounts

18
00:01:40.120 --> 00:01:49.120 
of labeled data. Among them supervised learning combined with data sets synthesis is the most straightforward to apply in the

19
00:01:49.120 --> 00:01:52.250 
specific fields and gets good results.

20
00:01:52.819 --> 00:01:59.329 
For instance the application in text recognition and autonomous driving works pretty well.

21
00:02:00.000 --> 00:02:04.680 
Unfortunately this method can be directly applied to

22
00:02:04.689 --> 00:02:04.900 
edgeAI

23
00:02:04.900 --> 00:02:07.360 
cannot be directly applied to edgeAI

24
00:02:07.360 --> 00:02:10.449 
scenarios because of privacy issue,

25
00:02:10.460 --> 00:02:16.479 
we cannot obtain the detailed distribution information of the edge side data.

26
00:02:16.490 --> 00:02:25.280 
So accurate simulation cannot be performed. Weekly supervised and unsupervised learning methods have been researched for a

27
00:02:25.280 --> 00:02:27.009 
long time. Recently,

28
00:02:27.020 --> 00:02:33.520 
transfer learning, few-shot learning and contrastive learning have become emerging research directions.

29
00:02:33.530 --> 00:02:43.310 
However, for those who are new to the field or not yet familiar with these terms, it can sometimes be a little difficult

30
00:02:43.310 --> 00:02:46.460 
and confusing to tell the difference exactly.

31
00:02:47.020 --> 00:02:53.330 
We will make a brief introduction in the next slides.

32
00:02:53.330 --> 00:02:59.689 
We utilize three dimensional coordinate system to classify current machine learning methods.

33
00:03:00.349 --> 00:03:13.199 
The three access represent the size of datasets, labeled or unlabeled and whether a category of the dataset are independent

34
00:03:13.789 --> 00:03:16.159 
and identically distributed.

35
00:03:17.759 --> 00:03:22.509 
We can locate fully supervised methods at the center point.

36
00:03:22.520 --> 00:03:29.870 
It really requires large label dataset and assume that the sample confirmed the ID

37
00:03:29.870 --> 00:03:30.770 
Distribution.

38
00:03:31.830 --> 00:03:35.569 
Somewhere between the label and unlabeled sets,

39
00:03:35.580 --> 00:03:43.620 
we can find a semi supervised learning which often use partially labeled dataset but still confirm ID

40
00:03:43.629 --> 00:03:46.530 
Data distribution.

41
00:03:46.530 --> 00:03:50.599 
Similarly suppose labels are not used at all.

42
00:03:50.650 --> 00:03:59.050 
In that case there will be unsupervised method including self supervised and contrasted learning methods that have made more

43
00:03:59.050 --> 00:04:03.389 
recent progress as well as traditional clustering methods.

44
00:04:05.180 --> 00:04:12.889 
If dataset is relatively small we need to use the method of few shot learning or even zero shot learning.

45
00:04:16.620 --> 00:04:25.259 
If the data sample out of distribution, we need to introduce transfer learning, domain adoption and domain generalization

46
00:04:25.259 --> 00:04:25.980 
methods.

47
00:04:29.290 --> 00:04:38.149 
The recent emerging research field includes novel class discovery, open set recognition. They are somehow similar to the semi

48
00:04:38.149 --> 00:04:44.459 
supervised setting and also use partially annotated datasets.

49
00:04:44.470 --> 00:04:48.110 
But the data distribution doesn't confirm the IID

50
00:04:48.120 --> 00:04:51.680 
assumption.

51
00:04:51.680 --> 00:05:00.500 
Here is the simple definition of transfer learning given target learning task Ta based on domain knowledge Da,

52
00:05:00.509 --> 00:05:08.100 
and we can get the help from the relative related resource source domain Ds

53
00:05:08.110 --> 00:05:10.110 
for its learning task Ts.

54
00:05:10.120 --> 00:05:20.569 
Transfer learning for its aims to improve the performance of predictive function for the target learning domain.

55
00:05:20.600 --> 00:05:22.560 
Target learning task Ta

56
00:05:22.569 --> 00:05:32.930 
by discovering and transferring latent knowledge from the source domain and the source original task. Here we assume that Da

57
00:05:32.939 --> 00:05:33.769 
And Ds

58
00:05:33.779 --> 00:05:35.970 
have shared knowledge.

59
00:05:35.980 --> 00:05:44.819 
In addition in most cases the size of source domain is much larger than the size of targeted domain. Here we can see that

60
00:05:44.819 --> 00:05:53.540 
transfer learning can effectively help to solve the edgeAI problems such as lack of enough examples on the edge side and the

61
00:05:53.540 --> 00:05:57.660 
problem of model cold start.

62
00:05:57.660 --> 00:06:03.160 
We have a left hand driving rule in Germany and many other countries around the world.

63
00:06:03.170 --> 00:06:08.300 
How can I quickly adapt to the right hand driving rule

64
00:06:08.310 --> 00:06:09.500 
if I go to UK

65
00:06:09.500 --> 00:06:12.410 
Japan or Australia to rent a car?

66
00:06:13.189 --> 00:06:24.810 
We as humans we can find some commonalities in the two rules and these features can really help us learn and

67
00:06:24.819 --> 00:06:28.250 
adapt to a new system or environment quickly.

68
00:06:28.259 --> 00:06:38.160 
For example, we will notice that the driver's position is closer to the central lane line in the both systems.

69
00:06:38.930 --> 00:06:46.629 
In short, whether it is left hand or right hand driving, I only need to drive by the central lane line.

70
00:06:46.639 --> 00:06:53.860 
Thus the core inside of transfer learning is to discover and learn common knowledge from different domains.

71
00:06:56.639 --> 00:07:00.889 
Of course many focus on deep learning models.

72
00:07:00.899 --> 00:07:05.279 
So I will give an example of deep transfer learning here.

73
00:07:06.120 --> 00:07:13.779 
Deep transfer learning refers to a re using the part of the network, pre train it in the source domain including its network

74
00:07:13.779 --> 00:07:22.500 
structure and training parameters and transferring it to the deep neural network using the target as

75
00:07:22.500 --> 00:07:24.490 
part of the neural network structure.

76
00:07:24.500 --> 00:07:32.949 
And this type of scheme is based on the assumption that neural networks are similar to the processing mechanism of the human

77
00:07:32.949 --> 00:07:33.420 
brain.

78
00:07:33.430 --> 00:07:37.959 
It is an iterative and continuous abstract process.

79
00:07:38.819 --> 00:07:48.850 
The front layer of the network is regarded as the feature extractor and extracted features should be very general. And the

80
00:07:48.850 --> 00:07:59.829 
closer the part to the network output, the higher the abstraction, their future representations and the lower the

81
00:07:59.839 --> 00:08:03.680 
various activities.

82
00:08:03.680 --> 00:08:12.790 
Here is a simple example, we know that ImageNet has 1000 categories and it deals with the classification problem for nature

83
00:08:12.790 --> 00:08:13.610 
images.

84
00:08:13.620 --> 00:08:24.540 
The data set is about 1.2 million images. Here, we can regard it as a large scale knowledge base about the Nature images. Then

85
00:08:24.550 --> 00:08:33.539 
a fully trained deep neural network on this data set has very good representations of Nature images when we need to deal

86
00:08:33.539 --> 00:08:44.460 
with a relatively small data sets such as Flickr 100 K, which has about 100 K images and 20 categories or a much smaller

87
00:08:44.460 --> 00:08:46.330 
data sets such as CIFAR 10.

88
00:08:46.340 --> 00:08:54.029 
And it only has 50000 Training Examples and 10 Classes for Classification.

89
00:08:54.549 --> 00:08:58.389 
A feasible transfer learning method is to use a model

90
00:08:58.389 --> 00:09:07.870 
trained with the relatively larger dataset, fix the parameters of most of its trained layers and only train a small number

91
00:09:07.870 --> 00:09:10.899 
of layers closer to the network output.

92
00:09:10.909 --> 00:09:12.159 
In doing so,

93
00:09:12.169 --> 00:09:22.230 
on the one hand, you can make good use of the common features from the frontal layers, only update more task specific layers.

94
00:09:22.240 --> 00:09:32.039 
On the other hand, the number of parameters that need to be optimized or fine tune is significantly reduced. So it can also

95
00:09:32.049 --> 00:09:39.389 
alleviate the overfitting problem of using the smaller data set.

96
00:09:39.389 --> 00:09:47.789 
In the computer vision domain, transfer learning based on the pre trained ImageNet model is a common technique in classification

97
00:09:47.799 --> 00:09:58.999 
object detection, semantic segmentation and a number of other related tasks. Because that annotated dataset as large as ImageNet

98
00:09:58.999 --> 00:10:00.779 
is difficult to obtain,

99
00:10:00.789 --> 00:10:08.289 
we can still benefit from pre trained deep learning model on specific tasks through the transfer learning.

100
00:10:08.970 --> 00:10:18.460 
The same thing happened in the field of NLP recently. From well beginning pre trained word vector is very widely used, has

101
00:10:18.460 --> 00:10:29.090 
been widely used to represent vocabularies to the recent popularity of pretrained large language models such as BERT GPT3.

102
00:10:29.100 --> 00:10:35.669 
They achieved state of the art results through transfer learning on many downstream tasks.

103
00:10:36.740 --> 00:10:44.600 
The knowledge distillation technology we discussed in the previous video has also been widely used in transfer learning

104
00:10:44.610 --> 00:10:46.940 
and achieve very good results.

105
00:10:49.720 --> 00:10:55.110 
As a short summary, in this video, we recap the challenge of edgeAI,

106
00:10:55.110 --> 00:10:56.940 
few shot problem.

107
00:10:56.950 --> 00:11:00.679 
We also highlight one of the deep learning implementation.

108
00:11:00.690 --> 00:11:04.480 
It heavily relies on the large annotated dataset.

109
00:11:05.059 --> 00:11:12.129 
We briefly present the intuition and the motivation of existing methods for solving this problem.

110
00:11:12.139 --> 00:11:17.700 
First of all, we introduce the transfer learning method for deep learning models,

111
00:11:17.710 --> 00:11:25.820 
explain its fundamental idea and introduce its application in computer vision and natural language processing field.

112
00:11:27.539 --> 00:11:29.120 
Thank you for watching the video.
