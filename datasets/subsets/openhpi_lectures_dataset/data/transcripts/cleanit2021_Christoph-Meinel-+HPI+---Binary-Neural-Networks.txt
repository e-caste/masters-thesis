WEBVTT

1
00:00:00.770 --> 00:00:05.240 
Here I want to introduce our research
work about Binary Neural networks,

2
00:00:05.810 --> 00:00:11.340 
and the goal is to make AI training
much more energy efficient.

3
00:00:12.420 --> 00:00:19.290 
And I start with this slide from
MIT technology review, which compares

4
00:00:19.670 --> 00:00:22.790 
the carbon footprint of
a round-trip flight,

5
00:00:23.310 --> 00:00:28.750 
New York to San Francisco.
Human life over one year,

6
00:00:29.390 --> 00:00:35.750 
American life over one year, and US
car including fuels,

7
00:00:35.960 --> 00:00:37.900 
the average
lifetime, and

8
00:00:38.550 --> 00:00:41.860 
the energy that's needed
to train the former

9
00:00:42.930 --> 00:00:45.900 
neural architecture
research.

10
00:00:47.430 --> 00:00:54.020 
Such an AI system needs
a huge amount of energy and

11
00:00:54.020 --> 00:01:01.650 
produces a very big carbon footprint,
compared to a flight that also

12
00:01:02.580 --> 00:01:05.580 
has a considerable
carbon footprint.

13
00:01:06.340 --> 00:01:11.360 
So when we imagine that
such AI systems

14
00:01:11.880 --> 00:01:14.570 
will be applied and
used around the world,

15
00:01:15.210 --> 00:01:21.090 
then it will not work for our
climate, so we need definitely

16
00:01:21.110 --> 00:01:24.810 
much more energy
efficient AI systems.

17
00:01:25.670 --> 00:01:30.490 
And here is an idea we tried to
follow over the last years,

18
00:01:30.920 --> 00:01:35.250 
that we a see Deep
Learning architectures

19
00:01:35.660 --> 00:01:40.580 
and instead of working with
32-bit architectures that

20
00:01:40.600 --> 00:01:46.970 
we try to use binary neural
networks to use networks on the

21
00:01:48.170 --> 00:01:52.820 
1-bit where the computation
are done on the bit level.

22
00:01:53.540 --> 00:01:58.970 
So the state of the art of such AI
networks is 32-bit

23
00:01:59.340 --> 00:02:04.520 
models in the
convolution, as there are

24
00:02:04.740 --> 00:02:08.950 
2-bit numbers
operated with each other and we

25
00:02:09.640 --> 00:02:15.960 
try to design and train deep neural
networks on a binary level

26
00:02:16.430 --> 00:02:21.390 
and show that it's possible. And of course
it produces a lot of energy saving.

27
00:02:22.150 --> 00:02:26.870 
So what's the idea behind
such low-bit neural networks?

28
00:02:27.160 --> 00:02:33.970 
Here I show it on the one bit
level. The extreme case is binary

29
00:02:33.970 --> 00:02:41.020 
neural networks only use 2 weights,
for the networks, +1 and - 1

30
00:02:41.260 --> 00:02:45.540 
for weights as well
as for inputs,

31
00:02:45.980 --> 00:02:50.680 
instead of the 32-bit
floating point that is

32
00:02:51.490 --> 00:02:55.120 
done in the state
of the art models.

33
00:02:55.990 --> 00:03:01.390 
Upto 32 model compression
and 58 times speed up

34
00:03:01.640 --> 00:03:08.270 
is possible during the inference and more
than a 1000 times energy saving

35
00:03:08.540 --> 00:03:12.620 
on dedicated hardware is
possible. This is shown by

36
00:03:13.570 --> 00:03:14.950 
works of colleagues

37
00:03:16.220 --> 00:03:23.220 
in the world. So the challenge of these
low bit networks is they are wonderful

38
00:03:23.400 --> 00:03:27.850 
in using only a small amount of
energy compared to the state-

39
00:03:27.850 --> 00:03:33.640 
of-the-art networks, but they lose accuracy
compared to the 32-bit networks.

40
00:03:34.100 --> 00:03:38.630 
So for example, directly binary
sized binarization of a

41
00:03:39.170 --> 00:03:45.170 
network trained on the image
net(this is a huge

42
00:03:45.610 --> 00:03:52.790 
database of image data) leads to a loss
of accuracy by about 10 percent.

43
00:03:54.090 --> 00:03:59.500 
We believe and with our research
work we try to contribute to

44
00:03:59.900 --> 00:04:04.150 
improve this
accuracy and to

45
00:04:04.900 --> 00:04:10.070 
make the loss in accuracy smaller ,best
to reach the same accuracy level

46
00:04:10.330 --> 00:04:12.180 
like with the 32-bit networks.

47
00:04:12.790 --> 00:04:16.850 
So the goal of our ongoing research
is to achieve the same accuracy

48
00:04:17.140 --> 00:04:22.510 
with binary networks as with
traditional convolutional networks.

49
00:04:25.170 --> 00:04:29.460 
What would be the result,
if this research would

50
00:04:29.970 --> 00:04:33.520 
succeed? If he would be able

51
00:04:33.700 --> 00:04:38.690 
to close the gap between the 32-bit
convolution networks and the binary

52
00:04:40.190 --> 00:04:46.310 
neural networks, then we can
deploy the dedicated hardware

53
00:04:46.930 --> 00:04:52.710 
on servers, and achieve huge
energy saving. But even more,

54
00:04:53.040 --> 00:04:58.780 
the networks can run on mobile
and embedded devices without

55
00:04:58.780 --> 00:05:04.210 
a loss of accuracy, and such mobile and
embedded devices need a lot less

56
00:05:04.420 --> 00:05:08.130 
energy compared with
this that's needed by

57
00:05:09.780 --> 00:05:11.670 
server infrastructures.

58
00:05:12.990 --> 00:05:17.190 
So here are some insights
on our research,

59
00:05:17.890 --> 00:05:23.140 
so what we experiment is with the
clipping threshold that should be

60
00:05:23.290 --> 00:05:25.700 
considered as an
hyperparameter, and

61
00:05:26.670 --> 00:05:31.310 
values between 1.2 and 1.3
lead to better results.

62
00:05:31.620 --> 00:05:36.250 
Since the value one that was
used in most previous work.

63
00:05:36.740 --> 00:05:41.360 
Here are the as sources where
you can follow and see the

64
00:05:41.360 --> 00:05:43.020 
details of this
research work.

65
00:05:44.130 --> 00:05:48.790 
scaling of channels after binary
convolution according to

66
00:05:49.300 --> 00:05:55.850 
Rastegari et al. can be solved by
BatchNorm layers and a tighter

67
00:05:55.850 --> 00:06:01.960 
approximation of the sign function does
not necessarily achieve better results,

68
00:06:02.280 --> 00:06:08.610 
this could be shown. And these are the
models, we are using and experimenting with.

69
00:06:09.630 --> 00:06:13.950 
There is a binary dense net, a dense
net adapter for binary networks

70
00:06:14.140 --> 00:06:19.090 
replaces bottlenecks and
shortcomings. Here are the dense net and

71
00:06:19.670 --> 00:06:25.930 
a bottleneck. Here is no bottleneck
and here are the binary densenet.

72
00:06:26.150 --> 00:06:33.010 
Here are our suggestion which we
are working and which we investigated.

73
00:06:34.480 --> 00:06:36.980 
If we look to and
compare the

74
00:06:38.020 --> 00:06:45.390 
accuracy of the different models and we
here consider our binary dense net

75
00:06:45.780 --> 00:06:50.270 
model, then you can see
here that we

76
00:06:51.150 --> 00:06:58.810 
receive the best results
on the different model size levels,

77
00:06:59.170 --> 00:07:01.210 
both in the BinaryDenseNet

78
00:07:02.230 --> 00:07:08.850 
with different parameters, so it makes
sense to work around and to modify

79
00:07:09.260 --> 00:07:13.770 
the networks that are used. Another
binary

80
00:07:13.830 --> 00:07:17.390 
neural network model we
are experimenting with are

81
00:07:18.010 --> 00:07:20.830 
our Melius networks.

82
00:07:21.940 --> 00:07:28.720 
They are using 1-bit for weights and inputs
and lead to lower quality and capacity.

83
00:07:29.400 --> 00:07:35.060 
But the number of bit and
operation are reduced drastically.

84
00:07:35.580 --> 00:07:41.610 
So the number of possible values
for the weights are reduced from

85
00:07:41.990 --> 00:07:44.190 
2^32 to 2.

86
00:07:44.900 --> 00:07:49.640 
If you have 32-bit representations,
the values and the weights

87
00:07:49.910 --> 00:07:56.810 
could vary in this huge amount
of different weights. When you

88
00:07:56.810 --> 00:08:02.080 
work with the binary, with such
MeliusNet then we have only 2

89
00:08:02.670 --> 00:08:08.720 
weights possible. Ofcourse this leads
as well to a quantization error

90
00:08:08.950 --> 00:08:11.340 
as a lower
feature quality.

91
00:08:12.260 --> 00:08:17.360 
The question is, what can we do? How we
can deal with this to improve

92
00:08:18.330 --> 00:08:21.670 
the quality and to

93
00:08:22.480 --> 00:08:24.110 
lower the
quantization error.

94
00:08:25.020 --> 00:08:28.760 
So the value range of inputs is

95
00:08:29.250 --> 00:08:33.710 
similarly reduced, so fine granular
difference can no longer

96
00:08:33.710 --> 00:08:37.220 
exist in only 2-bits for
example -1 and +1.

97
00:08:37.780 --> 00:08:40.950 
And also here, we have a
lower feature capacity.

98
00:08:41.830 --> 00:08:48.130 
And the idea we experiment is
to solve both challenges

99
00:08:48.580 --> 00:08:52.450 
through a specific
architecture design.

100
00:08:53.300 --> 00:08:58.260 
And this is exactly the
proposal of our Melius network

101
00:08:58.750 --> 00:09:06.420 
which is shown here in a overview
sketch. We have the dense block,

102
00:09:06.580 --> 00:09:11.860 
and we have the improvement block,
and they are connected here and

103
00:09:12.300 --> 00:09:19.790 
designed in this way. So this part in this
block the feature capacity is increased

104
00:09:19.920 --> 00:09:25.620 
and here the feature quality
is increased in the network.

105
00:09:26.370 --> 00:09:31.660 
So these a models if we make it more
fine-granular, you can here see

106
00:09:32.990 --> 00:09:36.870 
how it's designed,
how we work with

107
00:09:37.700 --> 00:09:40.580 
different blocks and
different transitions fields

108
00:09:41.010 --> 00:09:47.930 
to make such networks at space
only as it's working

109
00:09:47.930 --> 00:09:51.270 
with the 2 values.
And to make it more

110
00:09:52.500 --> 00:09:57.240 
accurate and to
provide more quality.

111
00:09:57.740 --> 00:10:03.810 
So if we compare our
network, our architecture with others

112
00:10:04.280 --> 00:10:11.430 
then we see MeliusNet
we are considering

113
00:10:11.430 --> 00:10:14.490 
here in the operations,
and in the

114
00:10:15.560 --> 00:10:19.170 
operation models size
on a very good way, and it shows

115
00:10:19.640 --> 00:10:26.300 
good results,then it makes sense
to vary, to modify,

116
00:10:26.650 --> 00:10:31.410 
to think and to design
new network models,

117
00:10:31.830 --> 00:10:35.790 
new binary networks model for
this AI application.

118
00:10:36.240 --> 00:10:39.270 
It's not necessary
that the

119
00:10:40.260 --> 00:10:46.810 
32-bit networks are so much
better in quality than such kind

120
00:10:47.010 --> 00:10:50.700 
of networks. If you are interested
in the details, here I could

121
00:10:50.700 --> 00:10:54.920 
only give an overview.
Then here are another

122
00:10:55.460 --> 00:11:03.490 
network model, the BMXNet 2, we
designed an open source framework

123
00:11:03.660 --> 00:11:10.180 
for binary neural networks. So a BMXNet 2
is based on MXNet

124
00:11:10.620 --> 00:11:16.910 
which was proposed before. It contains
reduceable models and demos

125
00:11:17.150 --> 00:11:20.910 
to provide a strong basis for
research and industry. And it

126
00:11:20.910 --> 00:11:25.990 
can be used to find new network
architectures, test new ideas.

127
00:11:26.120 --> 00:11:32.210 
Here is a link if you want you
can also find there

128
00:11:32.380 --> 00:11:37.210 
are showcases and demo applications
about this BMXNet tool.

129
00:11:37.610 --> 00:11:40.140 
And there is an entry
to demo app

130
00:11:40.880 --> 00:11:47.520 
also working on the imagenet database
and imagenet classification

131
00:11:48.150 --> 00:11:53.940 
which is based on the RestNet.
Human Pose detection demo

132
00:11:54.170 --> 00:12:01.420 
for a Raspberry Pi, this is a small
device which only

133
00:12:01.750 --> 00:12:07.120 
needs a small amount
of energy and also here this

134
00:12:07.120 --> 00:12:12.830 
is a link to find this. We are interested
to interact with you

135
00:12:13.010 --> 00:12:19.160 
about such ideas. We are interested
if you have further improvements,

136
00:12:19.480 --> 00:12:24.010 
please you are invited
to bring this in to our

137
00:12:24.430 --> 00:12:31.410 
cleanIT forum to share this knowledge with
other research groups, with the goal

138
00:12:31.810 --> 00:12:38.440 
to decrease the carbon
footprint of such AI models

139
00:12:38.440 --> 00:12:39.300 
and applications.
