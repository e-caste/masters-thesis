WEBVTT

1
00:00:00.300 --> 00:00:01.129 
Hello.

2
00:00:01.139 --> 00:00:04.940 
This video will introduce several computing models and Edge

3
00:00:04.940 --> 00:00:09.740 
Cloud synergy methods commonly used in edge AI use cases.

4
00:00:12.240 --> 00:00:18.140 
Generally speaking, Edge AI computing model defines how to execute

5
00:00:18.140 --> 00:00:21.120 
AI algorithms in the edge computing scenarios.

6
00:00:21.719 --> 00:00:24.890 
The term Edge AI contains two words.

7
00:00:24.899 --> 00:00:32.429 
Therefore, there is an emerging field that edge computing and artificial intelligence are closely integrated.

8
00:00:33.159 --> 00:00:42.960 
The AI computing model might need to consider the following three directions to improve the overall efficiency in its scenarios.

9
00:00:43.649 --> 00:00:49.890 
As the current AI model design and research are still in the rapid development stage,

10
00:00:49.899 --> 00:00:59.689 
therefore improving the AI models efficiency is part that is most likely to improve the overall efficiency of the edge AI applications

11
00:00:59.689 --> 00:01:00.840 
significantly.

12
00:01:01.880 --> 00:01:09.769 
We have already learned commonly used compression methods like knowledge distillation, pruning and quantization.

13
00:01:10.349 --> 00:01:18.500 
In addition, there are many effective engineering methods to further optimize the efficiency of AI computing models.

14
00:01:18.510 --> 00:01:24.219 
For example, optimization is performed at the operator level

15
00:01:24.230 --> 00:01:35.760 
to optimize the parallelism strategy, operators simplification and operator fusion. To improve the cache hit rate and optimize

16
00:01:35.760 --> 00:01:43.099 
the memory access, SIMD optimization, SSE/AVX optimization and so on.

17
00:01:44.250 --> 00:01:45.780 
In production projects,

18
00:01:46.500 --> 00:01:55.409 
we only need a close collaboration of algorithm and engineering side optimization to achieve the maximum optimization result.

19
00:01:58.989 --> 00:02:05.620 
To achieve a more efficient computing model commonly used method includes the following two directions -

20
00:02:06.459 --> 00:02:11.169 
The first is the offloading of computations.

21
00:02:11.180 --> 00:02:17.430 
The offloading here refers to offloading a part of the computation from the cloud to the edge.

22
00:02:17.990 --> 00:02:21.849 
Since we are talking about the computational offloading,

23
00:02:21.860 --> 00:02:29.270 
the samples that need to be processed by the AI model also need to be transmitted. Here,

24
00:02:29.280 --> 00:02:34.240 
it involves the security of transmission and data privacy protection.

25
00:02:35.219 --> 00:02:45.259 
A common way to implement computing offloading is to utilize a large and small model collaboratively. The other method which

26
00:02:45.259 --> 00:02:56.270 
is not very popular in the industry but there have been many research outcomes in academia is model partitioning. Using

27
00:02:56.270 --> 00:02:57.310 
this method,

28
00:02:57.400 --> 00:03:07.069 
you really dive a large model divided into at least two parts and place them on the different nodes to complete the inference

29
00:03:07.069 --> 00:03:15.860 
task jointly. The AI reasoning method, combined with this segmentation paradigm is called early exit.

30
00:03:16.469 --> 00:03:24.780 
We discussed the early exit techniques in the dynamic neural network part of the previous video and we'll give an example

31
00:03:24.780 --> 00:03:27.439 
of this application in this video.

32
00:03:28.090 --> 00:03:37.969 
Of course when we discuss model partitioning the choice of cutting point is an important issue, usually to reduce the amount

33
00:03:37.969 --> 00:03:40.870 
of data transmission and the latency.

34
00:03:41.020 --> 00:03:49.419 
We will choose a location where the output dimension of the specific neural network layer is relatively small at the cutting

35
00:03:49.419 --> 00:03:58.849 
point, but at the same time we also need to consider placing the parts that consume relatively high computing resources on

36
00:03:58.849 --> 00:04:08.259 
the cloud side. Only a small amount of computation is kept at the edge side, which is still a trade off problem in our use

37
00:04:08.259 --> 00:04:08.750 
case.

38
00:04:11.229 --> 00:04:20.949 
Cross models collaboration. A simple example of the collaborative reasoning between a large model on the cloud and a small

39
00:04:20.949 --> 00:04:22.259 
model on the edge.

40
00:04:22.269 --> 00:04:28.660 
First we have to train these two models separately on the same dataset.

41
00:04:29.050 --> 00:04:33.689 
Easily the larger model will have better accuracy and capacity.

42
00:04:33.699 --> 00:04:39.910 
Then we deploy them to the cloud server nodes and the client devices, respectively.

43
00:04:40.519 --> 00:04:42.959 
Maybe use the local data for inference.

44
00:04:42.970 --> 00:04:52.360 
We first use a small model on the device to make the inference and then use the specific metric to analyze the confidence

45
00:04:52.370 --> 00:05:01.470 
after that after getting the result. If the result is confident enough, then we directly return the local exit.

46
00:05:02.110 --> 00:05:11.420 
Otherwise the input image is passed to the cloud to use a larger model for inference and obtain a relatively higher precision

47
00:05:11.420 --> 00:05:21.060 
result. The advantage of this collaborative method is that it can ensure the overall accuracy of the AI inference will not

48
00:05:21.060 --> 00:05:23.449 
decrease. At the same time,

49
00:05:23.459 --> 00:05:31.730 
for relatively simple examples, you can obtain good enough result directly on the device which improves the inference speed

50
00:05:31.740 --> 00:05:33.540 
and save the data transmission.

51
00:05:34.980 --> 00:05:44.990 
This method is simple and effective and has become a standard paradigm for mobile applications. But it still has a problem.

52
00:05:45.670 --> 00:05:54.100 
The user data, which is the picture in our case, will still be sent to the remote server for processing and the user privacy

53
00:05:54.110 --> 00:05:56.069 
cannot be protected here.

54
00:05:58.470 --> 00:06:05.420 
A straightforward solution is to transmit feature vectors instead of images to the cloud server.

55
00:06:05.459 --> 00:06:13.540 
This can protect the privacy of users to a large extent, although there are still ways to reverse engineer the image data,

56
00:06:13.550 --> 00:06:16.649 
it can be further protected through encryption.

57
00:06:17.250 --> 00:06:21.430 
We can implement this idea by using the model partitioning method,

58
00:06:21.470 --> 00:06:32.800 
according to the previous example. The client side model has 9 conv blocks and the cloud side large model has 54 conv

59
00:06:32.800 --> 00:06:33.529 
blocks.

60
00:06:34.110 --> 00:06:42.879 
We can segment the first 9 conv blocks from the larger model and then add a classification head and deploy

61
00:06:42.879 --> 00:06:44.100 
it on the client.

62
00:06:45.310 --> 00:06:52.500 
The remaining 45 blocks are then deployed in the cloud. Through joint training,

63
00:06:52.509 --> 00:07:00.990 
we can manage two model partitions to do the classification task and the input of the larger partition is the

64
00:07:00.990 --> 00:07:06.600 
future output of the smaller partition on the client side.

65
00:07:06.620 --> 00:07:11.649 
In this way we no longer need to transmit the user's image data to the cloud.

66
00:07:11.660 --> 00:07:16.040 
And secondly the cloud side computation is reduced as well.

67
00:07:16.839 --> 00:07:21.430 
The overall inference efficiency will be effectively improved.

68
00:07:21.439 --> 00:07:26.920 
The feature victor transmitted here is often much smaller than the image.

69
00:07:26.930 --> 00:07:29.850 
So it can also reduce the bandwidth.

70
00:07:31.600 --> 00:07:41.660 
So let's also look at the simple example for model partitioning, I use different symbols to indicate the

71
00:07:41.670 --> 00:07:45.110 
size comparison of different model stages.

72
00:07:45.120 --> 00:07:49.730 
This example model has three stages of different sizes.

73
00:07:49.740 --> 00:07:57.779 
The first is the smallest and the second is the medium size and last is the most complicated one.

74
00:07:58.370 --> 00:08:05.990 
Therefore we can naturally separate the three stages of this model into different computing carriers.

75
00:08:06.000 --> 00:08:14.750 
For example, the smallest stage is suitable for a client device such as smart glass or smartphone and the medium stage is

76
00:08:14.750 --> 00:08:24.689 
placed on the edge computing device such as the NVIDIA Jetson GPU device or Hawaii Atlas 200 which has the mobile side

77
00:08:24.689 --> 00:08:33.960 
GPU and finally the heaviest part of the network is placed on the cloud to use a powerful computing resources there.

78
00:08:37.129 --> 00:08:47.740 
In the inference stage, we first get the result from the network um part on the device, we check its confidence if it is

79
00:08:47.740 --> 00:08:54.909 
good enough then directly return the local exit and the inferences over here.

80
00:08:59.539 --> 00:09:07.340 
If the result is not confident enough, we will continue to pass the intermediate feature to the model part on the edge note

81
00:09:08.399 --> 00:09:15.149 
where we will continue to make the inference and get the subsequent output.

82
00:09:15.159 --> 00:09:24.879 
Check confidence situation similarly and return the edge exit if it is confident enough.

83
00:09:24.879 --> 00:09:34.169 
If the result is still not good enough if you continue to pass the features to the cloud node where we will do

84
00:09:34.169 --> 00:09:44.529 
the inference on the cloud side using the largest model and to continue using the continued computing

85
00:09:44.529 --> 00:09:46.720 
resources there in the future.

86
00:09:46.730 --> 00:09:57.610 
In the inference stage we will separate the inference into three parts and this adds

87
00:09:57.610 --> 00:10:08.350 
quite flexibility to the inference stage and we will get the final exit on the cloud if necessary um or we will just exit

88
00:10:08.350 --> 00:10:09.860 
on the client devices.

89
00:10:09.870 --> 00:10:15.710 
So basically, this is the inference process described in this example.

90
00:10:16.840 --> 00:10:25.980 
Okay we need to carry out additional inter-node communication in both cross model synergy and model partitioning methods.

91
00:10:25.990 --> 00:10:33.899 
In particular the latter is equivalent to deploying the partitions of a model to multiple nodes.

92
00:10:33.909 --> 00:10:43.190 
Therefore how to reduce the communication overhead if the excellent way to improve the inference efficiency effectively. The

93
00:10:43.190 --> 00:10:50.039 
first maybe the optimization of the network to improve the communication efficiency between its various nodes.

94
00:10:50.049 --> 00:10:55.460 
However, the content of this part is beyond the scope of this course.

95
00:10:55.470 --> 00:11:04.529 
Next we briefly introduce two methods which can effectively improve the communication efficiency between different nodes

96
00:11:04.539 --> 00:11:11.889 
when performing AI inference. The cashing method of the information compression method.

97
00:11:13.779 --> 00:11:17.549 
So I use the same example at the model partitioning parts.

98
00:11:18.250 --> 00:11:24.059 
We separate the AI model into three stages and place them on different computing carriers.

99
00:11:24.580 --> 00:11:30.899 
So the smallest stage on the client device and medium stage on the edge computing device.

100
00:11:30.909 --> 00:11:39.840 
And the heaviest part is placed on the cloud to leverage the powerful computing resource.

101
00:11:39.840 --> 00:11:40.899 
In the inference stage,

102
00:11:40.899 --> 00:11:50.019 
similar to the previous example we will get a result on the device and we check its confidence. If it's good enough then directly

103
00:11:50.019 --> 00:11:51.659 
return to the local exit.

104
00:11:52.240 --> 00:11:58.789 
Otherwise we will try to find the result in the local cache in terms of the input feature vector.

105
00:11:58.830 --> 00:12:05.779 
So here our prediction result and the input feature vector are stored in the cache in pairs.

106
00:12:05.789 --> 00:12:15.830 
If we could find a result from the cache then we can also early exit successfully. We have applied an assumption here. We believe

107
00:12:15.830 --> 00:12:24.019 
that there is a certain probability that the local data with similar characteristics, attributes will repeatedly appear

108
00:12:24.019 --> 00:12:33.330 
in many edge AI scenarios. In this case a neural caching design can significantly improve the inference efficiency and reduce

109
00:12:33.330 --> 00:12:35.149 
the data transmission overhead.

110
00:12:35.159 --> 00:12:40.279 
This design borrows the concept of the traditional caching algorithms.

111
00:12:41.470 --> 00:12:46.000 
If no suitable results can be found in the local cache,

112
00:12:46.009 --> 00:12:51.490 
we then continue to pass intermediate features to the model part on the edge.

113
00:12:53.100 --> 00:13:01.909 
When we continue to make the inference and get the subsequent output, check the confidence situation and return the edge exit

114
00:13:01.919 --> 00:13:08.639 
if it is confident enough.

115
00:13:08.639 --> 00:13:15.059 
After that, we will check whether this result has already been stored in local neural cache.

116
00:13:15.070 --> 00:13:24.340 
If not, we will add it. If the prediction is not confident we will search in the edge side neural cache, if we can find

117
00:13:24.340 --> 00:13:26.820 
the result we then do the edge exit.

118
00:13:26.830 --> 00:13:36.139 
Otherwise we will continue to pass the features to the cloud note where we will do the last reasoning part and get

119
00:13:36.139 --> 00:13:46.720 
the final cloud exits there. We check whether a result already exists in the edge side neural cache, if not we will

120
00:13:46.720 --> 00:13:50.940 
add it after that the inference process ended here.

121
00:13:52.269 --> 00:14:01.169 
So in the distributed system, there are many ways to compress and encrypt communication between multiple nodes.

122
00:14:01.179 --> 00:14:07.799 
I will not give a further introduction in this video and my focus is more on the part of AI.

123
00:14:07.809 --> 00:14:16.909 
Therefore I would like to introduce a method to reduce communication overhead during joint inference across nodes using a

124
00:14:16.909 --> 00:14:18.840 
binary neural network layer.

125
00:14:19.799 --> 00:14:26.169 
As shown in this figure we have selected a cutting point for this neural network model

126
00:14:26.179 --> 00:14:27.629 
after the first layer.

127
00:14:31.610 --> 00:14:41.549 
Then multiple edge nodes and the central cloud node are deployed in this scenario. On the edge side we use

128
00:14:41.549 --> 00:14:50.179 
the neural network layer to extract local data feature and then we transfer them to the cloud for subsequent processing on

129
00:14:50.179 --> 00:14:54.919 
the remaining model parts which may be training our inference.

130
00:14:55.299 --> 00:15:04.000 
In this example we assume that the computing power of the edge device is fragile and can only do simple feature extraction.

131
00:15:05.639 --> 00:15:14.269 
We will now transfer the original user data to the outside and next, what will happen if we place the second

132
00:15:14.269 --> 00:15:17.509 
convolutional layer with a binarised convolutional layer.

133
00:15:18.299 --> 00:15:26.629 
So the sign function will binarise the activations and enter the binary convolution layer. In this time the information

134
00:15:26.629 --> 00:15:30.950 
flow between sign function and the binary conv layer is binary.

135
00:15:30.960 --> 00:15:39.360 
Therefore, if we use it as a cutting point here, the following information transmission can be compressed 32 times.

136
00:15:39.370 --> 00:15:49.809 
Of course this method may also bring additional accuracy loss but as we saw in the previous slides, previous videos, the deep

137
00:15:49.809 --> 00:15:56.320 
neural network model can better compensate for the loss caused by the quantization.

138
00:15:56.549 --> 00:16:05.179 
We can avoid excessive accuracy degradation when we are not binarising, entire model, but only certain layers.

139
00:16:05.190 --> 00:16:14.429 
Therefore, I suggest designing the model according to the specific application and choosing a good cutting position to reduce

140
00:16:14.429 --> 00:16:15.690 
accuracy loss.

141
00:16:17.850 --> 00:16:19.500 
Thank you for watching the video.
