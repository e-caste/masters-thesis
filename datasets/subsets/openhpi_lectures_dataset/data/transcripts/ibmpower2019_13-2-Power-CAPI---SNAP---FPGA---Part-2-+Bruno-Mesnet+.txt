WEBVTT

1
00:00:00.740 --> 00:00:04.080 
Okay, let's go a bit further
now because having

2
00:00:05.990 --> 00:00:08.170 
good bandwidth or good performance
is good but if it's

3
00:00:08.940 --> 00:00:11.440 
difficult to implement
it, everybody will

4
00:00:12.030 --> 00:00:17.140 
run away. So we have today two
two different ways of doing things.

5
00:00:17.700 --> 00:00:22.750 
CAPI is in the middle, meaning
that you can go through the

6
00:00:22.750 --> 00:00:26.590 
CAPI link which is PSL interface or
openCAPI that doesn't change a lot

7
00:00:26.940 --> 00:00:31.660 
and we are just bringing
two different things. On the left

8
00:00:32.120 --> 00:00:37.320 
meaning on the CPU side we have
built some libcxl library for

9
00:00:37.930 --> 00:00:43.630 
CAPI acceleration and this is embedded
in the Ubuntu

10
00:00:46.320 --> 00:00:50.250 
distributions and on the
right we have

11
00:00:51.340 --> 00:00:55.410 
a little box which is what we
called the PSL or the BSP

12
00:00:55.870 --> 00:00:59.710 
which is something which is provided
by IBM to interfere to

13
00:00:59.710 --> 00:01:00.910 
discuss with the power

14
00:01:02.210 --> 00:01:06.210 
directly. This helps to be
sure that we are not

15
00:01:06.670 --> 00:01:10.360 
doing bad things and
crashing the system easily.

16
00:01:11.090 --> 00:01:16.370 
So, this is what is provided when
you buy a card. You can

17
00:01:16.370 --> 00:01:22.070 
put the PSL in any FPGA
cards and the way normally people

18
00:01:22.070 --> 00:01:24.840 
work on that is, they
build a software program

19
00:01:25.330 --> 00:01:28.700 
on the left hand interacts
with hardware logic. Meaning,

20
00:01:29.330 --> 00:01:34.590 
so very local VHDL or writing
in VHDL or very local language.

21
00:01:34.990 --> 00:01:40.490 
So this is absolutely feasible if you
want to have the best performance of

22
00:01:40.840 --> 00:01:43.120 
what you can
take from the code

23
00:01:43.780 --> 00:01:49.030 
but that's the old-fashioned way to
do things and this will

24
00:01:49.030 --> 00:01:55.190 
take a lot of time to use, to debug
and to have a complete application.

25
00:01:56.110 --> 00:01:58.220 
You understand that there
is a second choice.

26
00:01:58.720 --> 00:02:03.320 
We have built what we call the
SNAP and SNAP stands for

27
00:02:03.320 --> 00:02:06.920 
Storage Networking Analytics Programming
Framework. This is

28
00:02:06.920 --> 00:02:10.180 
a framework which is first of
all open source. Everybody can

29
00:02:10.180 --> 00:02:13.200 
have access to it. You can
just have all the tools in it,

30
00:02:13.200 --> 00:02:14.450 
you know what's in
it, you can

31
00:02:16.510 --> 00:02:20.210 
even improve that as
any open source.

32
00:02:21.250 --> 00:02:25.690 
The good thing for this
environment, this framework is that

33
00:02:25.720 --> 00:02:29.520 
we are using
IBM power security.

34
00:02:30.540 --> 00:02:35.930 
Meaning that we are not able from FPGA
to have access to all the host memory

35
00:02:36.140 --> 00:02:38.820 
everything is secured
by the power

36
00:02:40.130 --> 00:02:44.910 
the power technology. And the second
point which is very important is that

37
00:02:45.320 --> 00:02:49.970 
SNAP gives you the portability,

38
00:02:50.690 --> 00:02:54.100 
meaning that you can go from CAPI 1.0, 2.0

39
00:02:54.100 --> 00:02:56.470 
to openCAPI 3.0 without

40
00:02:57.220 --> 00:03:00.520 
without changing a line in
your code. All the

41
00:03:01.660 --> 00:03:05.670 
internals are directly supported
in the SNAP.

42
00:03:06.640 --> 00:03:07.960 
Okay, so this means that

43
00:03:09.390 --> 00:03:14.190 
if you, if I do
a diagram to explain things. You have

44
00:03:14.190 --> 00:03:18.050 
today a CPU which accesses the
host memory in the top and to the

45
00:03:18.050 --> 00:03:19.880 
storage and to the
networking in the bottom

46
00:03:20.550 --> 00:03:25.960 
and you are going to offload
one or several function or actions

47
00:03:26.470 --> 00:03:28.250 
and we have built

48
00:03:29.730 --> 00:03:36.130 
and the FPGA is going to help
you access to the network or storage directly.

49
00:03:36.740 --> 00:03:40.790 
The CAPI as you understood it,
lets you access directly to the

50
00:03:40.920 --> 00:03:45.390 
to the host memory and in the
middle of all that we have put

51
00:03:45.440 --> 00:03:47.810 
the SNAP and the
SNAP is just

52
00:03:48.430 --> 00:03:53.650 
helping you to access all of
these resources. It can be storage

53
00:03:54.020 --> 00:03:57.830 
networking host memory and all
these will be real access

54
00:03:57.830 --> 00:04:00.220 
the same way. So this
is something very friendly

55
00:04:00.710 --> 00:04:04.620 
if you want to use and we
are using a standard AXI interface

56
00:04:05.720 --> 00:04:08.760 
we'll come up on
this standard later.

57
00:04:11.010 --> 00:04:16.590 
We have added also one interface
which is Vivado HLS. HLS

58
00:04:16.900 --> 00:04:18.900 
stands for high
level synthesis.

59
00:04:19.500 --> 00:04:25.050 
This is an interface which
allows you to write your code

60
00:04:25.570 --> 00:04:29.050 
for the FPGA in C or C++ and

61
00:04:30.360 --> 00:04:35.720 
compile it to have something usable for
the FPGA family that you are using.

62
00:04:36.260 --> 00:04:40.910 
So this is not just converting
your language, it is analyzing and

63
00:04:40.910 --> 00:04:43.860 
parallelizing if you can. So it's
much more than just a

64
00:04:45.940 --> 00:04:52.480 
converter. For breaks if you miss one
you will go back to the classic

65
00:04:52.570 --> 00:04:56.250 
way. So this is important to
understand that this is the combination

66
00:04:56.250 --> 00:05:00.660 
of all these four tools that
brings if it's an efficiency and

67
00:05:01.620 --> 00:05:04.300 
quick way to
to program things.

68
00:05:05.360 --> 00:05:07.440 
So what's in
the framework?

69
00:05:08.670 --> 00:05:12.400 
It's important to understand that
because it will show you

70
00:05:12.400 --> 00:05:15.240 
all the different capabilities
of the framework.

71
00:05:15.870 --> 00:05:21.240 
So we have built new snap libraries
built on the libcxl

72
00:05:21.800 --> 00:05:27.320 
libraries and on the FPGA side
we have built a few boxes

73
00:05:28.040 --> 00:05:34.150 
DMA which allows the program to
access directly to the host memory.

74
00:05:35.030 --> 00:05:38.340 
All the different drivers
to the right

75
00:05:39.030 --> 00:05:41.820 
to the external
resources such as DRAM

76
00:05:43.430 --> 00:05:45.810 
flash NVMe
and all the networking.

77
00:05:46.680 --> 00:05:50.140 
And you will see that you have
also what we call the job queue

78
00:05:50.140 --> 00:05:53.820 
on the left or job manager on the
right and this is something very important because

79
00:05:54.280 --> 00:05:59.700 
this allows several threads to
connect to several actions. So,

80
00:05:59.700 --> 00:06:03.250 
several threads or several users
to connect to several actions.

81
00:06:03.890 --> 00:06:07.580 
So, when you write a software program
on the left and it can

82
00:06:07.710 --> 00:06:10.480 
be from any thread,
you can access

83
00:06:11.170 --> 00:06:12.470 
your hardware actions

84
00:06:13.260 --> 00:06:17.080 
on the right and these two
yellow boxes are the two things

85
00:06:17.080 --> 00:06:20.500 
that the user has to write. You
don't need to take care about

86
00:06:20.500 --> 00:06:25.280 
all the other stuff. Everything is made
to have something easy to use

87
00:06:25.580 --> 00:06:27.870 
and easy to compile
and all the flow

88
00:06:28.670 --> 00:06:33.100 
will be integrated to have
something easy for you.

89
00:06:33.740 --> 00:06:38.140 
So, just have a look I'm coming
back on this thing. As we have

90
00:06:38.140 --> 00:06:42.630 
no software driver like a classic
solutions, we are able to

91
00:06:42.630 --> 00:06:46.430 
have multiple users of multiple
threads accessing to multiple

92
00:06:47.650 --> 00:06:49.970 
functions in different

93
00:06:50.870 --> 00:06:51.950 
multiple cards or FPGA cards.

94
00:06:53.710 --> 00:06:57.590 
So that's important because we can
have multiple resource for everybody.

95
00:06:58.990 --> 00:06:59.940 
Two different modes -

96
00:07:01.170 --> 00:07:08.220 
I would just a have a word on
that. You can use the FPGA as just

97
00:07:08.220 --> 00:07:11.860 
doing a function. So, meaning
you want to do compression.

98
00:07:12.560 --> 00:07:18.110 
Well, you just send a computer key
you just call the hardware program

99
00:07:18.270 --> 00:07:21.880 
and he will give you the result
in the host memory and your

100
00:07:21.880 --> 00:07:23.580 
software program can
can continue.

101
00:07:24.180 --> 00:07:30.020 
The second way is parallel mode, meaning that
well as the FPGA card is master,

102
00:07:30.210 --> 00:07:33.350 
you don't need to wait for
it. You can just process

103
00:07:33.970 --> 00:07:39.750 
your software program on one side,
have also the hardware action processed

104
00:07:39.880 --> 00:07:45.660 
on his side and there's just in the
middle the shared memory that you are

105
00:07:46.660 --> 00:07:52.020 
with which the
software program is working also.

106
00:07:52.550 --> 00:07:58.000 
Okay, so you are exchanging information
through this host server memory

107
00:07:58.420 --> 00:08:02.800 
and not waiting for things like

108
00:08:03.430 --> 00:08:04.910 
classical mode
of things.

109
00:08:06.650 --> 00:08:08.140 
Okay, so now that

110
00:08:09.500 --> 00:08:14.260 
you understand what is CAPI,
you also understand that from

111
00:08:14.260 --> 00:08:16.510 
what we call a
CPU centric architecture,

112
00:08:17.190 --> 00:08:23.350 
we will add just a new core. So
the power has twelve, twenty four cores

113
00:08:24.640 --> 00:08:28.830 
you are just adding a new core and
it will have all the same privilege

114
00:08:29.190 --> 00:08:34.250 
and now we are not CPU, it's not
connected to the CPU but connected to the

115
00:08:35.580 --> 00:08:40.120 
host memory. So we now
change the server architecture,

116
00:08:40.540 --> 00:08:43.700 
meaning we are adding a new core
but it's a very specific core

117
00:08:43.850 --> 00:08:45.420 
dedicated for
a function.

118
00:08:47.300 --> 00:08:50.660 
Okay so let's take a very
simple example. And this example can

119
00:08:50.660 --> 00:08:52.980 
be find in the GitHub.

120
00:08:53.610 --> 00:08:56.590 
Let's take a very simple
example. Just think about

121
00:08:57.420 --> 00:09:01.180 
you have a text in somewhere
on a disk you want to

122
00:09:02.650 --> 00:09:07.950 
change all the case of the letters and put
that back somewhere in the storage.

123
00:09:09.490 --> 00:09:14.340 
So, first thing you call the
function it reads the text

124
00:09:14.340 --> 00:09:19.240 
from the file, it put that in memory,
then it is executed. So i's

125
00:09:19.280 --> 00:09:23.610 
lower case, for example processing, the
result is put back in the

126
00:09:24.350 --> 00:09:26.430 
host memory and then

127
00:09:27.380 --> 00:09:31.530 
the result is then written back
to the disk. So

128
00:09:32.850 --> 00:09:37.850 
that's a classic way of doing things and
we have designed the

129
00:09:38.210 --> 00:09:42.110 
the SNAP framework to do exactly
the same thing so that nobody

130
00:09:42.110 --> 00:09:46.630 
is lost with the new architecture. So
just think that we need

131
00:09:46.720 --> 00:09:50.850 
we are going to use
SNAP or the FPGA like the CPU.

132
00:09:51.620 --> 00:09:57.410 
It is, as I told you, like a CPU. So
we are just going to add a switch,

133
00:09:58.070 --> 00:10:03.490 
so that the program is
not executed in the chip

134
00:10:03.710 --> 00:10:07.830 
in the CPU chip but in the FPGA.
So that's the first change you

135
00:10:07.830 --> 00:10:08.840 
need to put in the code.

136
00:10:09.460 --> 00:10:14.940 
And the second thing is well the FPGA
has plenty of new resource. It has embedded

137
00:10:16.130 --> 00:10:20.370 
DRAM, it has access to the
network, access to the storage

138
00:10:20.610 --> 00:10:26.230 
and this needs to be specifically
defined, so that when

139
00:10:26.280 --> 00:10:30.320 
you do a read or write from the
FPGA, you know where to read.

140
00:10:31.810 --> 00:10:36.130 
This problem goes to
the host memory. So,

141
00:10:37.730 --> 00:10:39.700 
once you have
modified your code

142
00:10:40.560 --> 00:10:44.240 
you go through the
the process to

143
00:10:45.020 --> 00:10:49.510 
to check that your code is
good and then to create

144
00:10:49.510 --> 00:10:52.890 
the binary file so
that the FPGA can work.

145
00:10:53.690 --> 00:10:57.540 
So first thing, on the three
steps the first one is

146
00:10:58.160 --> 00:11:01.850 
just keep your program as it
is. Just separate or isolate

147
00:11:02.080 --> 00:11:04.850 
your function that you want to
to put in the FPGA

148
00:11:05.370 --> 00:11:09.070 
and test it like that. So it's
also made common to compile things

149
00:11:09.090 --> 00:11:10.020 
and to test things.

150
00:11:12.250 --> 00:11:14.660 
In this program you
will just implement

151
00:11:15.440 --> 00:11:19.420 
the switch I called I referred

152
00:11:20.100 --> 00:11:25.410 
just a minute ago and the different
access to the the host memory.

153
00:11:26.130 --> 00:11:31.480 
The second thing is well we
will just move this software action

154
00:11:31.480 --> 00:11:33.940 
or software function to
a hardware function

155
00:11:34.660 --> 00:11:37.610 
which means that we
will not execute it

156
00:11:38.240 --> 00:11:44.710 
as if it was on a CPU, but as if
it was executed on FPGA and we are providing

157
00:11:45.220 --> 00:11:49.790 
a model for that. Meaning that
you don't need to build some

158
00:11:50.010 --> 00:11:54.480 
specific test to test your FPGA
function you can just continue

159
00:11:54.480 --> 00:11:56.880 
to take your application
and it will

160
00:11:58.030 --> 00:12:02.910 
emulate all the power an FPGA access as
if it was on the real hardware. So

161
00:12:03.620 --> 00:12:08.360 
if you are able to debug things as
if you were on the real environment,

162
00:12:09.230 --> 00:12:13.190 
or classical environment. And the common
way to do that is just to make

163
00:12:13.720 --> 00:12:18.660 
for simulation. Okay, so once everything
is done and this is I would

164
00:12:18.660 --> 00:12:20.570 
think the longest part
because it's a

165
00:12:21.310 --> 00:12:25.550 
way to debug things very easily.
And you have everything to do

166
00:12:25.680 --> 00:12:27.670 
to locate when you
have an error somewhere

167
00:12:28.510 --> 00:12:29.770 
with all the
normal

168
00:12:30.440 --> 00:12:34.560 
debugging things. Once everything is okay,
you can go through the

169
00:12:35.220 --> 00:12:41.050 
third step which is the execution step.
So you do a make image

170
00:12:41.050 --> 00:12:42.990 
to create the binary
which will be

171
00:12:43.890 --> 00:12:47.770 
put in the FPGA. This program

172
00:12:48.490 --> 00:12:51.780 
takes, sometimes to be built it
can take an hour, but this

173
00:12:51.780 --> 00:12:54.970 
is done only once when
your programming is already done.

174
00:12:55.640 --> 00:13:01.670 
And then you just execute that on
a power system exactly like the two

175
00:13:02.250 --> 00:13:07.830 
previous phases. Okay so three
phases, three steps and

176
00:13:08.870 --> 00:13:12.170 
everything to be able to help
you debugging things and locating

177
00:13:12.170 --> 00:13:13.270 
an error if
you have some.

178
00:13:15.190 --> 00:13:18.670 
Okay, now let's go through two
different examples to show you

179
00:13:18.670 --> 00:13:24.760 
that how things are. I like to put
hands in the engine to show you things.

180
00:13:25.330 --> 00:13:27.400 
So this was an
example that a

181
00:13:28.370 --> 00:13:34.190 
customer asked us. He said well okay, we have
a little benchmark program. It's a short

182
00:13:35.440 --> 00:13:41.330 
calculation key. Just take it and put that
in your FPGA with your magic SNAP

183
00:13:41.510 --> 00:13:42.930 
and tell me how much

184
00:13:43.880 --> 00:13:47.290 
you get out of that.
Okay, so we took this

185
00:13:47.890 --> 00:13:52.380 
program, we just removed
all the system calls

186
00:13:53.760 --> 00:13:58.190 
and out of that and I will not
go into the program because it has

187
00:13:58.590 --> 00:14:01.570 
no real interest, you can find
it in the SNAP GitHub,

188
00:14:02.960 --> 00:14:05.420 
what is interesting is to
understand the program structure.

189
00:14:05.920 --> 00:14:09.280 
Three different things here - on the
bottom you have the mass

190
00:14:09.430 --> 00:14:12.140 
function which is a
combination of multiple loops

191
00:14:13.310 --> 00:14:18.970 
and this is something that the VivadoHLS
tool will be good at, meaning

192
00:14:19.070 --> 00:14:21.150 
that he is able
to combine things

193
00:14:22.250 --> 00:14:25.010 
if you are just directing
him a little bit.

194
00:14:25.590 --> 00:14:29.710 
And we'll see that next
slide. The second thing

195
00:14:29.710 --> 00:14:33.920 
is what we call the recursive
loops, meaning that you need to

196
00:14:33.920 --> 00:14:37.120 
wait for the result of
the function before starting it

197
00:14:37.860 --> 00:14:42.990 
starting this function again and on this
side you will understand that there's not

198
00:14:43.270 --> 00:14:44.510 
not a lot of
things to do.

199
00:14:46.440 --> 00:14:50.540 
And the third thing is if we
can find parallel loops. For example

200
00:14:50.540 --> 00:14:56.420 
we're taking the the result of the

201
00:14:56.420 --> 00:14:59.670 
function and doing an extra for
the result. So this is something

202
00:14:59.670 --> 00:15:02.990 
that can be perfectly
parallelized. Okay, so

203
00:15:03.790 --> 00:15:06.240 
if you look to the
math function for example,

204
00:15:06.790 --> 00:15:12.030 
this is exactly the code we used, if
you can find it. We just added one

205
00:15:12.290 --> 00:15:17.850 
instruction which is the HLS pipeline, to
see if the

206
00:15:17.860 --> 00:15:19.520 
HLS compiler is
able to

207
00:15:20.540 --> 00:15:22.300 
to provide good
results with that.

208
00:15:23.280 --> 00:15:29.230 
And he just asked us when we when
we inserted that to separate the input

209
00:15:29.370 --> 00:15:35.700 
to separate the inputs and
to copy the input to the

210
00:15:36.160 --> 00:15:39.440 
variable and the variable to the output.
So that's the only change he

211
00:15:39.440 --> 00:15:42.840 
asked us to do and this
is to better implement things.

212
00:15:43.380 --> 00:15:49.300 
Okay, so that's the only thing we did
in the modification of the code.

213
00:15:49.450 --> 00:15:54.490 
Okay so we don't know even
what's in the algorithm, we

214
00:15:54.490 --> 00:15:57.510 
don't need to go into
that. Just for comparison

215
00:15:59.910 --> 00:16:04.530 
one of my team-mate had to do the same
thing for the GPU, and he had to

216
00:16:04.980 --> 00:16:09.260 
re code for example just this
function. He had to recode it

217
00:16:10.040 --> 00:16:13.950 
from A to Z because
to have the best performance

218
00:16:15.880 --> 00:16:20.650 
in cuda he had to rewrite and to
refine a lot of things. Okay, so

219
00:16:20.790 --> 00:16:22.190 
just to show
you the comparison.

220
00:16:23.660 --> 00:16:27.110 
Out of that I just spoke about
the parallel loops. Well if you

221
00:16:27.110 --> 00:16:30.490 
have a look to the FPGA
implementation of the program.

222
00:16:31.090 --> 00:16:33.430 
You will see these
different spots.

223
00:16:34.190 --> 00:16:38.750 
These are duplication of the loops
I showed you. So every

224
00:16:41.110 --> 00:16:43.920 
math function is
duplicated as

225
00:16:45.370 --> 00:16:48.890 
long you have space in it. So on the
left I put sixteen, I have been

226
00:16:48.890 --> 00:16:50.680 
able to enter thirty
two on the right.

227
00:16:51.450 --> 00:16:53.950 
But it's the same function,
it's just give you more

228
00:16:55.120 --> 00:16:56.380 
performance at the end.

229
00:16:57.310 --> 00:16:58.850 
So, just to show
you the performance,

230
00:17:00.780 --> 00:17:05.610 
if you look to that, we can see
here that the parallel loops are efficient.

231
00:17:06.590 --> 00:17:10.940 
As soon as you are putting in more
and more and that the different sponge

232
00:17:11.090 --> 00:17:15.460 
or spots are not filled, you will take
the same time to do the thing.

233
00:17:15.910 --> 00:17:19.750 
And out of that
when the parallelized

234
00:17:21.260 --> 00:17:24.700 
lanes are filled you are
going to have

235
00:17:25.280 --> 00:17:30.350 
quite a maintaining performance
of the throughput.

236
00:17:30.890 --> 00:17:33.380 
And we are having quite a
good result because we are having

237
00:17:33.390 --> 00:17:37.030 
more than thirty five. I
took the 35 because I

238
00:17:37.030 --> 00:17:37.900 
wanted to be fair

239
00:17:39.390 --> 00:17:42.910 
but we are having 35X
better than the CPU. And this

240
00:17:42.910 --> 00:17:46.090 
is on a very very small FPGA.
We can do much better today.

241
00:17:49.680 --> 00:17:53.100 
And it just is a little
story. It just took ten eleven

242
00:17:53.100 --> 00:17:57.510 
days to do that from taking
the program, analyzing it, putting

243
00:17:58.410 --> 00:18:00.500 
everything on and
getting the result

244
00:18:01.110 --> 00:18:05.640 
by thirty five. So it was a good
thing to spend these eleven days.

245
00:18:08.180 --> 00:18:12.410 
Second example and I wanted to
go quite quick on that one

246
00:18:12.420 --> 00:18:14.140 
to show you another
way to do things.

247
00:18:14.980 --> 00:18:17.210 
This was implemented
by HPI.

248
00:18:18.700 --> 00:18:22.810 
This is interesting because they used
our interface SNAP and they

249
00:18:23.010 --> 00:18:27.540 
try to find a good application to do
that. So, imagine that you want to use

250
00:18:28.810 --> 00:18:34.220 
an FPGA card but without

251
00:18:34.710 --> 00:18:37.890 
knowing that you have storage on
it, that you can do computing

252
00:18:37.890 --> 00:18:42.280 
on that and this needs to
be absolutely transparent for the operator.

253
00:18:42.780 --> 00:18:46.930 
So if you have things you
want to come online, to be able

254
00:18:46.930 --> 00:18:51.490 
to do some encryption, to do
some processing like upper case and

255
00:18:51.680 --> 00:18:57.440 
to store everything in the FPGA and
just have a common line interface.

256
00:18:57.830 --> 00:19:02.000 
So how did they did that?
It's quite simple.

257
00:19:03.980 --> 00:19:06.100 
They just first
imagined that

258
00:19:06.870 --> 00:19:09.100 
they want to store
things on the

259
00:19:10.140 --> 00:19:14.790 
flash memory which is on the FPGA
card. So they just built a file

260
00:19:14.790 --> 00:19:16.230 
system that just,
imagined it

261
00:19:17.280 --> 00:19:20.090 
so they can have a
good parallelization on it.

262
00:19:20.890 --> 00:19:27.920 
They then used this file system manager
like LMDB and they also

263
00:19:28.260 --> 00:19:31.990 
add the libfuse to
have a file system driver.

264
00:19:32.410 --> 00:19:35.880 
So this is what is embedded in
the system. You don't need to

265
00:19:35.880 --> 00:19:37.810 
to add anything, everything
is done like that.

266
00:19:38.720 --> 00:19:44.210 
On the other side they have
imagine some multiple engines and

267
00:19:46.970 --> 00:19:50.150 
you are able then
to add your user

268
00:19:50.900 --> 00:19:55.300 
engine, user execute table.
And this will

269
00:19:55.930 --> 00:20:02.140 
go directly into the FPGA and will add
a box as a different box you have.

270
00:20:02.550 --> 00:20:06.770 
So all the memory, so your
flash memory is already

271
00:20:07.390 --> 00:20:12.070 
managed in the box. You just
add your user box with the

272
00:20:12.780 --> 00:20:13.590 
algorithm you need.

273
00:20:15.110 --> 00:20:17.880 
Okay, so just to
show the results.

274
00:20:18.390 --> 00:20:22.880 
When you type a command like tree you
can see all the operators you see

275
00:20:23.300 --> 00:20:27.460 
you have in the FPGA but
this is seen on the system

276
00:20:29.440 --> 00:20:34.270 
and if you want to encrypt
something, I just change the colors

277
00:20:34.270 --> 00:20:38.720 
so that you can see where things are
executed or not. Okay, so the green is

278
00:20:38.840 --> 00:20:41.930 
stored in the FPGA
card, the blue is

279
00:20:42.870 --> 00:20:45.340 
executed on the FPGA

280
00:20:46.160 --> 00:20:50.900 
host. If you want to combine
things, it's quite easy to combine

281
00:20:50.900 --> 00:20:55.210 
operators and everything will be done on
FPGA and if you want to

282
00:20:55.740 --> 00:20:58.170 
play with that, you can
do anything you want. So,

283
00:20:58.610 --> 00:21:01.750 
okay two things to get
out of that is well

284
00:21:02.190 --> 00:21:06.940 
you are able to use FPGA or
without knowing a lot about FPGA.

285
00:21:07.180 --> 00:21:10.680 
So a software guy is able to
do that and the result

286
00:21:10.680 --> 00:21:13.560 
is very transparent for the FPGA.
You don't need to have complex

287
00:21:14.090 --> 00:21:19.190 
chords to do that. Okay, so
just to take away some different

288
00:21:19.190 --> 00:21:23.230 
things and to summarize one thing -
CAPI removes all the driver

289
00:21:23.230 --> 00:21:25.410 
latency and all
the resource taken.

290
00:21:26.520 --> 00:21:32.710 
HLS can show that you can
have as good performance as if you

291
00:21:32.710 --> 00:21:34.540 
were writing that
at the old-fashioned HDL log.

292
00:21:35.640 --> 00:21:41.390 
SNAP will give you two good
things - it will follow the CAPI evolution,

293
00:21:41.740 --> 00:21:45.080 
so you don't need to take care
of that but it takes care also

294
00:21:45.080 --> 00:21:50.050 
about FPGA evolution, meaning that if
your FPGA is too old and

295
00:21:50.050 --> 00:21:53.490 
you want to change to a new
card well just enable this new

296
00:21:53.490 --> 00:21:56.540 
card and you will have no change
to do in your code because

297
00:21:56.750 --> 00:21:59.420 
it will always take
the same interfaces.

298
00:22:00.590 --> 00:22:04.410 
It's opensource, meaning that you
can add or improve this code

299
00:22:05.100 --> 00:22:07.440 
if you want to do it and
this is really good because we

300
00:22:07.970 --> 00:22:11.640 
are building a growing
community to help us.

301
00:22:12.630 --> 00:22:16.680 
You can have very complex code. We
have an example with three thousand lines

302
00:22:16.850 --> 00:22:21.310 
in C to do a neural network
just to program the FPGA

303
00:22:22.060 --> 00:22:29.960 
program. And don't forget we have the
CAPI or open CAPI simulation engine.

304
00:22:30.320 --> 00:22:36.930 
That helps a lot because you are
able to test your FPGA program exactly

305
00:22:37.150 --> 00:22:38.490 
as if it was in FPGA, but

306
00:22:39.760 --> 00:22:42.660 
much simpler to debug
that on a software site.

307
00:22:43.590 --> 00:22:48.650 
Well, summarizing all that, well you have
all the good reasons to try

308
00:22:49.590 --> 00:22:51.790 
and to adopt
this technology.

309
00:22:53.900 --> 00:22:57.190 
Just remember, three steps to
go from A to Z.

310
00:22:57.730 --> 00:23:02.780 
And well why not try today? You
can just log to NIMBIX,

311
00:23:03.980 --> 00:23:06.210 
it will give you an
account and for a few

312
00:23:06.780 --> 00:23:16.660 
cents you can just try that and find a good function.
Here Alexandre Castellane and me, Bruno Mesnet,

313
00:23:16.660 --> 00:23:20.370 
we are both here to help you. You
have our address, mail if you want and

314
00:23:20.730 --> 00:23:25.330 
you can know more about accelerator
or access to a system. Just don't

315
00:23:25.980 --> 00:23:28.250 
hesitate to contact
us. Thank you.
