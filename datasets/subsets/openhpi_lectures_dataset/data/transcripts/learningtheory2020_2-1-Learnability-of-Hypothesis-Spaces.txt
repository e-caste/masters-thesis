WEBVTT

1
00:00:00.000 --> 00:00:04.040 
Welcome to the video on the
learnability of hypothesis spaces.

2
00:00:05.190 --> 00:00:09.190 
Last week we looked at the binary
classification with half spaces

3
00:00:09.400 --> 00:00:11.710 
of images that contain
a butterfly or not.

4
00:00:12.320 --> 00:00:15.890 
And we introduced a lot of
definitions which we will use now

5
00:00:15.980 --> 00:00:21.450 
to answer this question. So let's do it.
Okay, so our first theorem is that

6
00:00:22.880 --> 00:00:24.420 
every hypothesis space

7
00:00:25.130 --> 00:00:29.620 
is learnable by a learner. This is
like a nice result and it uses

8
00:00:29.800 --> 00:00:32.900 
a technique that is called
learning by enumeration and the

9
00:00:32.900 --> 00:00:37.670 
idea behind it is choose the minimal
i such that the prediction

10
00:00:37.670 --> 00:00:40.890 
model with a number i is
consistent with the data.

11
00:00:41.740 --> 00:00:45.550 
Ok so what happens? Well I mean
the learner sees some images

12
00:00:45.550 --> 00:00:49.240 
maybe it waits until it has seen
three images and then it says

13
00:00:49.250 --> 00:00:53.360 
ok what is the of first half
space I can come up with

14
00:00:53.360 --> 00:00:56.950 
that is consistent with this data?
And maybe it's this one. So

15
00:00:56.950 --> 00:00:58.430 
this is the first
prediction model.

16
00:00:59.030 --> 00:01:03.860 
And then it sees another image. So the blue
image with a lot of butterflies on it

17
00:01:04.040 --> 00:01:07.950 
and it sees well but my last
prediction model says that

18
00:01:07.950 --> 00:01:11.210 
there is no butterfly on it
but I can see I mean I know

19
00:01:11.210 --> 00:01:14.740 
the data is labelled. So there
is a butterfly on it so I have

20
00:01:14.740 --> 00:01:18.740 
to adjust and I will look for
again the first half space

21
00:01:18.740 --> 00:01:22.140 
I can think of that is consistent with
the data, maybe it is this one.

22
00:01:23.030 --> 00:01:27.530 
And then it sees another image.
So maybe the one with a

23
00:01:27.830 --> 00:01:32.160 
bucket of flowers on it which is
labelled to not contain a butterfly

24
00:01:32.300 --> 00:01:35.470 
and it again sees, oh well maybe
my last prediction model was

25
00:01:35.470 --> 00:01:39.120 
not the best one and it will
again look for the smallest or

26
00:01:39.120 --> 00:01:42.510 
the first half space it finds
that is consistent and maybe

27
00:01:42.510 --> 00:01:45.620 
its this one and if we assume that
this is the one we are looking for

28
00:01:45.850 --> 00:01:49.390 
then no matter which image it
will see in the future it will

29
00:01:49.390 --> 00:01:54.070 
never go away from this half space. So
it has found the right half space

30
00:01:54.240 --> 00:02:00.080 
and this works always because no
matter how we present the images

31
00:02:00.410 --> 00:02:06.230 
to the learner it will at some
point have seen enough images

32
00:02:06.390 --> 00:02:10.620 
to exclude all the half spaces
that were not correct that came

33
00:02:10.620 --> 00:02:14.090 
before the half space we are
looking for and then it will like

34
00:02:14.090 --> 00:02:18.960 
decide for the half space is correct
and will never change its mind again.

35
00:02:19.630 --> 00:02:24.600 
So this works and we have shown
that every like collection of

36
00:02:25.000 --> 00:02:31.030 
similar in the sense of uniformly innumerable
sets we call this hypothesis space

37
00:02:31.170 --> 00:02:34.060 
can be learned by a
learner from informants.

38
00:02:37.120 --> 00:02:42.960 
So neural networks are like the
standard way nowadays to approach

39
00:02:42.970 --> 00:02:45.160 
a lot of machine
learning tasks.

40
00:02:45.910 --> 00:02:50.010 
And as you have seen maybe
in the excursion on the

41
00:02:50.210 --> 00:02:54.250 
in the first week the neural
networks are not trained like

42
00:02:54.260 --> 00:02:59.070 
in full batch, so they can't I mean
you do not feed all the images

43
00:02:59.070 --> 00:03:03.440 
to the network and then adjust the w eights.
This is computationally expensive.

44
00:03:03.770 --> 00:03:08.070 
What we do is we feed only a part
of the images to the neural

45
00:03:08.070 --> 00:03:11.370 
network and then adjust the weights
with weights then for our

46
00:03:11.590 --> 00:03:12.950 
hypothesis.

47
00:03:16.340 --> 00:03:20.030 
So there are two kinds of learners,
the ones that use all training data

48
00:03:21.000 --> 00:03:25.770 
and the mini batch learners that
use of pre specified batch size k

49
00:03:26.120 --> 00:03:32.260 
number of inputs, in our case labeled
images, and compute the prediction model

50
00:03:32.570 --> 00:03:36.230 
from the last prediction model,
namely, the weights it had

51
00:03:36.240 --> 00:03:37.740 
before seeing
the images

52
00:03:38.350 --> 00:03:42.910 
and yeah the error that results
from seeing the images.

53
00:03:45.410 --> 00:03:49.320 
So let's define this in a
mathematical way. So we can

54
00:03:49.320 --> 00:03:52.860 
work with it and prove
things. A learner

55
00:03:52.860 --> 00:03:57.380 
M is called k-iterative which will
correspond to the mini batch learner

56
00:03:57.720 --> 00:04:02.900 
if its output only depends on the
last k inputs. So the last k images

57
00:04:03.620 --> 00:04:07.240 
if we write it in the informant
setting then its A(t-k)

58
00:04:07.240 --> 00:04:11.770 
till i of t-1 and the
corresponding prediction

59
00:04:11.770 --> 00:04:16.280 
model that was the output of m
after it had only seen the first

60
00:04:16.280 --> 00:04:18.570 
t-k number of images.

61
00:04:19.960 --> 00:04:24.010 
If k equals one so its an online
learner we call m iterative.

62
00:04:26.390 --> 00:04:31.910 
Ok so what is easy to see is that
if m is an iterative learner

63
00:04:31.910 --> 00:04:36.960 
so it only uses the last image,
then of course, I mean, we kind

64
00:04:36.960 --> 00:04:40.610 
of also have a k iterative
learner by a yeah

65
00:04:41.570 --> 00:04:48.440 
what do we do? Well we apply it
is k times. So if m after like

66
00:04:48.450 --> 00:04:52.770 
if we have a prediction model
where the learner coincide

67
00:04:53.460 --> 00:04:56.610 
then by the iterativeness they
will coincide after the

68
00:04:56.610 --> 00:05:00.050 
next image and after the next image
and after the next image and so on.

69
00:05:00.390 --> 00:05:04.280 
So they will coincide after k
images and this is exactly

70
00:05:04.280 --> 00:05:05.140 
what we wanted.

71
00:05:07.180 --> 00:05:10.270 
So how is it the other
way around? I mean

72
00:05:11.900 --> 00:05:14.950 
I have something I
wanted to know here is

73
00:05:14.950 --> 00:05:18.500 
I mean we're not talking about the
like the computational costs.

74
00:05:18.790 --> 00:05:23.180 
So of course it's like yeah
it may be a lot faster to

75
00:05:23.180 --> 00:05:26.800 
to not like update after
every image. But we're just

76
00:05:26.800 --> 00:05:30.360 
talking about questions of like
is there a machine learner

77
00:05:30.360 --> 00:05:34.890 
that can do the task at all? And
if there is an iterative one

78
00:05:34.890 --> 00:05:36.570 
then there is also a
k iterative one.

79
00:05:38.940 --> 00:05:43.210 
And how is it the other way around?
So if we have a k iterative

80
00:05:43.210 --> 00:05:46.710 
learner learning some hypothesis
space is there also an

81
00:05:46.720 --> 00:05:51.420 
iterative one doing that? And I mean
this is not as easy as the last lemma

82
00:05:51.840 --> 00:05:56.650 
because we cannot just use
the same learner but what

83
00:05:56.660 --> 00:06:00.440 
we can do and which is like a very
typical kind of argument is

84
00:06:00.680 --> 00:06:05.830 
we can use the k iterative learner
and with it we define another

85
00:06:05.830 --> 00:06:09.690 
one which we call N and this
will do the job for us.

86
00:06:09.800 --> 00:06:14.740 
So how does N work?
Well N just, well,

87
00:06:14.740 --> 00:06:18.730 
does almost the same things as M
does but we change the input.

88
00:06:19.310 --> 00:06:23.460 
So we repeat just every element k
times and ask the k iterative

89
00:06:23.460 --> 00:06:27.830 
learner what would you say
now? And this is what the

90
00:06:27.940 --> 00:06:31.100 
new learner says. And this
is of course iterative.

91
00:06:34.540 --> 00:06:39.530 
Ok so what we proved now that in
regard of pure learnability

92
00:06:40.050 --> 00:06:44.770 
it is not important which batch size we
choose. So everything that is learnable

93
00:06:44.960 --> 00:06:49.500 
with a very very large batch size is
also learnable in an online fashion,

94
00:06:50.230 --> 00:06:51.890 
just maybe not
as efficient.

95
00:06:55.790 --> 00:06:59.820 
Ok so you just saw in the last
slide a notation that was

96
00:06:59.820 --> 00:07:03.240 
maybe not familiar to you, so I
would just talk about this a bit.

97
00:07:04.350 --> 00:07:08.520 
So there are building
blocks to define a model.

98
00:07:08.670 --> 00:07:12.960 
One building block is for example
whether it is iterative or k-iterative

99
00:07:13.120 --> 00:07:16.940 
or whether it is a full batch which
we just will not denote at all.

100
00:07:17.080 --> 00:07:21.060 
So we always assume it uses all
information unless we state otherwise.

101
00:07:22.660 --> 00:07:28.760 
Then the presentation was by informant,
so by a finite number of labelled

102
00:07:29.160 --> 00:07:31.100 
binary labeled images

103
00:07:33.550 --> 00:07:38.580 
and we yeah we defined learning
success by eventually

104
00:07:38.770 --> 00:07:44.400 
settling on one correct hypothesis. And
this is often called explanatory learning

105
00:07:44.590 --> 00:07:48.510 
from which the abbreviation x
comes. And now we can just

106
00:07:48.510 --> 00:07:52.520 
stick this together and we have
a model. So this model is the

107
00:07:52.520 --> 00:07:57.030 
iterative learner from informant and
is successful if it eventually

108
00:07:57.210 --> 00:08:01.010 
settles for a correct hypothesis
no matter which informant

109
00:08:01.150 --> 00:08:03.600 
for the prediction
model to be learned I

110
00:08:04.110 --> 00:08:06.030 
yeah, I show to
the learner.

111
00:08:06.950 --> 00:08:13.490 
Ok so now we write these brackets
if we want to talk about

112
00:08:13.710 --> 00:08:19.210 
all classes or all hypothesis
spaces that are learnable by

113
00:08:19.210 --> 00:08:19.930 
such a learner.

114
00:08:22.560 --> 00:08:25.510 
Ok and we can do the same for
the k-iterative learners.

115
00:08:25.510 --> 00:08:29.310 
So this is the set of all hypothesis
spaces that is learnable

116
00:08:29.410 --> 00:08:34.630 
by a k-iterative learner from informant
and success being explanatorily defined.

117
00:08:35.590 --> 00:08:39.350 
And what we just showed on the last
slide with the two lemmas is

118
00:08:39.580 --> 00:08:40.530 
that this is the same.

119
00:08:41.380 --> 00:08:44.650 
So every hypothesis space learnable
by an iterative learner

120
00:08:44.800 --> 00:08:49.180 
is learnable by a k-iterative learner and
the other way around with the simulation.

121
00:08:50.660 --> 00:08:54.520 
Ok and because the building blocks
take just quite a lot of space

122
00:08:54.680 --> 00:08:58.800 
we will write this instead. So
we write it in the vertical

123
00:08:58.800 --> 00:09:02.380 
not in the vertical but in a
horizontal fashion. So yeah

124
00:09:02.500 --> 00:09:03.950 
this just means
the same thing.

125
00:09:07.890 --> 00:09:11.270 
Ok what did we prove? We proved every
hypothesis space is learnable

126
00:09:11.270 --> 00:09:13.690 
by enumeration with the
Occum's razor strategy.

127
00:09:14.530 --> 00:09:18.870 
We looked at different ways to
learn, one way is to consider all

128
00:09:18.870 --> 00:09:22.710 
information to produce a new
prediction model and one

129
00:09:22.710 --> 00:09:26.420 
way is to only use the last prediction
model and the current datum.

130
00:09:26.840 --> 00:09:28.930 
And there were other
versions in between

131
00:09:29.500 --> 00:09:32.540 
but we proved that the batch size
doesn't matter when we ask

132
00:09:32.550 --> 00:09:35.520 
when we talk about whether something
is learnable. So we will

133
00:09:35.520 --> 00:09:38.130 
always assume that the batch
size is one from now

134
00:09:39.330 --> 00:09:42.090 
and we introduced the notation
with the building blocks

135
00:09:42.780 --> 00:09:47.540 
and in the next video we will talk
about - is there an iterative learner

136
00:09:47.650 --> 00:09:49.700 
that learns the family
of half-spaces?
