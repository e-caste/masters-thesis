WEBVTT

1
00:00:00.840 --> 00:00:05.399 
So welcome, everybody, to a final presentation in our serious

2
00:00:05.400 --> 00:00:08.159 
future of computing power.

3
00:00:08.160 --> 00:00:12.689 
And we want to look beyond computer architecture and talk about a few trends

4
00:00:12.690 --> 00:00:13.690 
in computing.

5
00:00:14.760 --> 00:00:17.969 
I would like to talk about computing headaches.

6
00:00:17.970 --> 00:00:22.679 
So this is the question of implications our programs and our style of computing

7
00:00:22.680 --> 00:00:25.199 
has on daily life.

8
00:00:25.200 --> 00:00:29.819 
We will look a little bit at energy usage and how energy is becoming

9
00:00:29.820 --> 00:00:32.279 
a crucial part of computing.

10
00:00:32.280 --> 00:00:37.109 
And then we will figure that after talking about a couple of weeks

11
00:00:37.110 --> 00:00:41.609 
about hardware and computer architecture and means

12
00:00:41.610 --> 00:00:46.199 
of building reliable systems, we will discuss that

13
00:00:46.200 --> 00:00:50.849 
the focus needs to shift and needs to be shifted on to software and

14
00:00:50.850 --> 00:00:54.419 
how software will help building reliable systems.

15
00:00:54.420 --> 00:00:59.099 
So let's start talking about computing addicts and

16
00:00:59.100 --> 00:01:03.869 
we start out with a story. This can be found on New York Times, on the Internet.

17
00:01:03.870 --> 00:01:08.549 
And this is basically the story that instead of having a real human

18
00:01:08.550 --> 00:01:13.079 
boss, people at some companies like Uber, they experience

19
00:01:13.080 --> 00:01:17.429 
that their boss is just a computer program, an algorithm, basically.

20
00:01:17.430 --> 00:01:22.049 
And it's very difficult to find solutions

21
00:01:22.050 --> 00:01:23.669 
in conflicts.

22
00:01:25.260 --> 00:01:29.819 
On one hand side, data and algorithms appear to be objective and neutral

23
00:01:29.820 --> 00:01:34.289 
and even and actually much better than 10 every

24
00:01:34.290 --> 00:01:38.919 
human boss. But on the other side, it's just probabilities and statistics.

25
00:01:38.920 --> 00:01:43.409 
Right. And for a driver to situations like that that this

26
00:01:43.410 --> 00:01:47.999 
driver is having problems with his passengers maybe

27
00:01:48.000 --> 00:01:52.439 
and the system will offer that the passenger and the driver are never matched

28
00:01:52.440 --> 00:01:57.329 
again. But if there are too many problems, then the driver will get deactivated.

29
00:01:57.330 --> 00:02:01.540 
And he basically has a little means to complain and to fight against it.

30
00:02:03.960 --> 00:02:08.339 
There's another interesting article in The New York Times, and this is about Donald

31
00:02:08.340 --> 00:02:12.779 
Knuth, who turned 80 and gave an interview, and he is

32
00:02:12.780 --> 00:02:17.339 
known for the art of computer programing.

33
00:02:17.340 --> 00:02:22.049 
Six volumes of many algorithms are described

34
00:02:22.050 --> 00:02:26.639 
and people, computer scientists for many years relied on

35
00:02:26.640 --> 00:02:27.640 
the algorithms here.

36
00:02:29.580 --> 00:02:34.619 
If you read the interview, there is a discussion first that algorithms

37
00:02:34.620 --> 00:02:39.059 
that were known and important

38
00:02:39.060 --> 00:02:43.499 
to computer scientists, to people with an education in that area, to

39
00:02:43.500 --> 00:02:48.029 
specialists that algorithms nowadays are suddenly a topic that is addressed by

40
00:02:48.030 --> 00:02:50.819 
journalist that is addressed by everybody.

41
00:02:50.820 --> 00:02:55.589 
And people have maybe a different understanding what algorithms might be.

42
00:02:55.590 --> 00:03:01.439 
And the other thing is that algorithms are no longer

43
00:03:01.440 --> 00:03:06.029 
the way how programing works. But engineers

44
00:03:06.030 --> 00:03:10.389 
and scientists and artists, everybody is teaming up to solve real problems.

45
00:03:10.390 --> 00:03:14.969 
So computing that was confined in the box in a computer is

46
00:03:14.970 --> 00:03:18.659 
now suddenly having an impact on daily life.

47
00:03:18.660 --> 00:03:23.309 
But for the time being, the algorithms at least were written by

48
00:03:23.310 --> 00:03:27.809 
humans addressing difficult problems and

49
00:03:27.810 --> 00:03:32.339 
making good progress here, producing code with box

50
00:03:32.340 --> 00:03:35.859 
and biases. And this is troubling enough.

51
00:03:35.860 --> 00:03:40.739 
Right. But if you look at tomorrow and the shift, we are not going today,

52
00:03:40.740 --> 00:03:46.139 
then we are looking at a world where the algorithms are no longer written by humans,

53
00:03:46.140 --> 00:03:49.169 
but they are written by the machine as it learns.

54
00:03:49.170 --> 00:03:53.939 
So people talk about artificial intelligence, they are talking about self-learning

55
00:03:53.940 --> 00:03:58.559 
learning systems. And these systems are based on data.

56
00:03:58.560 --> 00:04:03.209 
And the problem is this with algorithms, programing languages that software engineers

57
00:04:03.210 --> 00:04:07.709 
and whole discipline that looks on how to produce

58
00:04:07.710 --> 00:04:10.679 
code with little errors.

59
00:04:10.680 --> 00:04:15.869 
With data, we are learning anew how to

60
00:04:15.870 --> 00:04:20.819 
judge biases and faults and problems in data.

61
00:04:20.820 --> 00:04:25.529 
And this is really a turning point and also a big problem

62
00:04:25.530 --> 00:04:26.530 
for future.

63
00:04:27.930 --> 00:04:28.930 
Why is that?

64
00:04:29.890 --> 00:04:33.749 
There's one famous example which I just want to bring up here, which is the trolley

65
00:04:33.750 --> 00:04:36.719 
problem. The trolley problem goes like that.

66
00:04:36.720 --> 00:04:41.699 
There's a trolley running and there's a problem with five people standing on the track

67
00:04:41.700 --> 00:04:44.669 
and the trolley might hit them and kill them, actually.

68
00:04:44.670 --> 00:04:49.259 
But there's a switch. And if you pull the lever, theswitch, then the trolley will go

69
00:04:49.260 --> 00:04:52.319 
to the other out and just hit one person.

70
00:04:52.320 --> 00:04:54.989 
So should you pull the lever?

71
00:04:54.990 --> 00:04:58.019 
And who should decide on pulling the lever?

72
00:05:00.420 --> 00:05:04.949 
Maybe people will judge and say, OK, it's better to have just one incident

73
00:05:04.950 --> 00:05:08.129 
instead of five, so you would pull the lever.

74
00:05:08.130 --> 00:05:12.659 
But this question can be rephrased in many disciplines.

75
00:05:12.660 --> 00:05:17.129 
It can be rephrased, phrased in the discipline of medicine, where the question is

76
00:05:17.130 --> 00:05:20.459 
whom to deal with first.

77
00:05:20.460 --> 00:05:24.540 
If there are many emergency room patients, for instance, or

78
00:05:25.620 --> 00:05:30.449 
if you look at self-driving cars or autonomous driving,

79
00:05:30.450 --> 00:05:34.769 
whom should you protect? The people on the street, the kids, the elder persons, the

80
00:05:34.770 --> 00:05:39.209 
driver. It's very difficult to put

81
00:05:39.210 --> 00:05:42.629 
decisions of that kind into computer systems.

82
00:05:42.630 --> 00:05:47.099 
And it's even more difficult if these decisions are based

83
00:05:47.100 --> 00:05:51.809 
on self learning systems and systems that are no longer

84
00:05:51.810 --> 00:05:56.609 
easy to comprehend. So message at this point, basically,

85
00:05:56.610 --> 00:06:01.259 
we need to look at tractability, at

86
00:06:01.260 --> 00:06:05.939 
explainability, at the means of understanding for the system,

87
00:06:05.940 --> 00:06:10.529 
future systems, at least to the same extent like we did over these

88
00:06:10.530 --> 00:06:15.269 
last couple of weeks with the computer architecture in the power

89
00:06:15.270 --> 00:06:17.069 
server landscape.

90
00:06:19.990 --> 00:06:24.519 
Once we starts thinking about the implications of computing on daily

91
00:06:24.520 --> 00:06:29.559 
life, you suddenly stumble across figures like this,

92
00:06:29.560 --> 00:06:34.119 
like, for instance, if you look at an office building, then the computer will account

93
00:06:34.120 --> 00:06:38.829 
for about 30 percent of the electricity used in these buildings.

94
00:06:38.830 --> 00:06:42.339 
This is an announcement from the Department of Energy in the US.

95
00:06:42.340 --> 00:06:47.439 
And if you look at the entire US, then about two percent

96
00:06:47.440 --> 00:06:51.819 
of the total electricity consumption goes into computing.

97
00:06:51.820 --> 00:06:56.349 
So if we are concerned about clean energy and about energy

98
00:06:56.350 --> 00:07:00.789 
usage in general, then we need to look at our data centers and need

99
00:07:00.790 --> 00:07:01.839 
to look at computing.

100
00:07:03.130 --> 00:07:07.719 
There's also the statement, right, that Spotify is using more

101
00:07:07.720 --> 00:07:13.289 
energy than the entire production of CDs

102
00:07:13.290 --> 00:07:17.919 
of compact discs in previous days has eaten

103
00:07:17.920 --> 00:07:22.359 
up in total. So service-based systems,

104
00:07:22.360 --> 00:07:27.099 
cloud computing, those might be actually not helping

105
00:07:27.100 --> 00:07:31.599 
saving energy, but they might be factors that contribute

106
00:07:31.600 --> 00:07:33.390 
to higher energy usage in computing.

107
00:07:34.550 --> 00:07:39.159 
What are the means to manage that? Energy usage, server consolidation, the question

108
00:07:39.160 --> 00:07:43.599 
of virtualization. Should you run many workloads in

109
00:07:43.600 --> 00:07:48.519 
cloud or should you try to do to put computing as effective

110
00:07:48.520 --> 00:07:50.769 
as massive as possible?

111
00:07:50.770 --> 00:07:53.709 
This is the rush to completion approach.

112
00:07:53.710 --> 00:07:58.239 
At least we should draw the conclusion that we need to

113
00:07:58.240 --> 00:08:00.399 
measure our energy footprint.

114
00:08:00.400 --> 00:08:04.570 
And this is an old figure that shows just a couple of

115
00:08:05.650 --> 00:08:08.349 
processes running on a Windows system.

116
00:08:08.350 --> 00:08:12.909 
And the message here is if you go with a standard operating

117
00:08:12.910 --> 00:08:17.379 
systems, then these systems will give you a very cost grained view

118
00:08:17.380 --> 00:08:20.529 
on what's going on in your computer.

119
00:08:21.850 --> 00:08:26.829 
In particular, this view will not address how new programing models

120
00:08:26.830 --> 00:08:31.629 
like virtualization, like dynamic programing approaches, how they

121
00:08:31.630 --> 00:08:36.189 
address energy usage, and also changes in software architecture, like Microsoft's

122
00:08:36.190 --> 00:08:40.808 
architectures, where you have oftentimes a distribution and also

123
00:08:40.809 --> 00:08:45.279 
smaller grained apps, maybe how they affect

124
00:08:45.280 --> 00:08:46.280 
energy usage.

125
00:08:47.740 --> 00:08:52.509 
Here at Hasso Plattner Institute, we do have the Futures SAP Lab, which is an

126
00:08:52.510 --> 00:08:57.009 
computing infrastructure, which is actually accessible also for projects from the

127
00:08:57.010 --> 00:09:01.749 
outside and within this future SAP lab that we have been running

128
00:09:01.750 --> 00:09:06.489 
projects that try to drill down and figure what

129
00:09:06.490 --> 00:09:11.169 
exact how power consumption goes on

130
00:09:11.170 --> 00:09:16.239 
on today's computers. And what we see here is, all right, there are

131
00:09:16.240 --> 00:09:21.219 
performance Komnas and it is very possible to figure

132
00:09:21.220 --> 00:09:25.539 
additional values about the behavior of a system.

133
00:09:25.540 --> 00:09:30.669 
We also see that there is an average usage

134
00:09:30.670 --> 00:09:35.469 
and then minimal usage and a maximum usage and they are differing by a factor

135
00:09:35.470 --> 00:09:40.179 
two or three. This is measurements based on internal servers,

136
00:09:40.180 --> 00:09:43.809 
but same thing could be done on the power architecture.

137
00:09:43.810 --> 00:09:48.399 
You could go even further. And again, Linux on an internal system

138
00:09:48.400 --> 00:09:53.049 
where we tried to break down the energy usage

139
00:09:53.050 --> 00:09:57.729 
to a single user. So it might be possible

140
00:09:57.730 --> 00:10:02.529 
and might be desirable in the future that we do resource management

141
00:10:02.530 --> 00:10:05.409 
based on the energy consumption of single users.

142
00:10:05.410 --> 00:10:10.119 
This is something that needs to be added to the operating systems and to the resource

143
00:10:10.120 --> 00:10:12.009 
management frameworks we have today.

144
00:10:14.530 --> 00:10:19.329 
Another screenshot from an HP system, in this case,

145
00:10:19.330 --> 00:10:24.429 
this is a blade system where we are able to install

146
00:10:24.430 --> 00:10:29.109 
power caps. So what we see here is a number of redundant power

147
00:10:29.110 --> 00:10:33.459 
supplies. Some of them are online, some of them are offline.

148
00:10:33.460 --> 00:10:37.470 
But we have placed a power cap at some

149
00:10:38.920 --> 00:10:42.489 
five kilowatts on this box.

150
00:10:42.490 --> 00:10:45.399 
So this might be something to discuss.

151
00:10:45.400 --> 00:10:50.199 
How would your programing model address the power consumption

152
00:10:50.200 --> 00:10:54.249 
and address also limits for power consumption?

153
00:10:54.250 --> 00:10:57.549 
And again, the need to measure right?

154
00:10:57.550 --> 00:11:00.139 
If you want to do something about power

155
00:11:02.470 --> 00:11:07.029 
consciousness and power usage in your system, you first need to

156
00:11:07.030 --> 00:11:11.679 
be able to measure again on the firmware

157
00:11:11.680 --> 00:11:16.449 
level, on the monitoring infrastructure level, many

158
00:11:16.450 --> 00:11:20.949 
values are available that are not exposed currently to the

159
00:11:20.950 --> 00:11:25.389 
operating system and to the processes running in the operating system.

160
00:11:25.390 --> 00:11:29.420 
And if you think further and ask yourself what is my favorite programing language

161
00:11:30.760 --> 00:11:34.000 
be it C, be it Java, be it Python.

162
00:11:35.200 --> 00:11:40.269 
Other means that I express power usage in my programing, expected power

163
00:11:40.270 --> 00:11:44.769 
usage. So something to address for

164
00:11:44.770 --> 00:11:45.770 
future research.

165
00:11:47.350 --> 00:11:49.299 
All right. Next aspect.

166
00:11:50.470 --> 00:11:54.669 
If we talk about server systems, we talk about reliable systems, we talk about

167
00:11:54.670 --> 00:11:59.619 
dependability. And this is a term that comes

168
00:11:59.620 --> 00:12:04.209 
basically from the computer architecture, from the hardware field

169
00:12:04.210 --> 00:12:09.159 
and the entire field of Fortran and computing, talks about

170
00:12:09.160 --> 00:12:13.839 
dependability attributes, about different means to achieve dependability, and also

171
00:12:13.840 --> 00:12:16.929 
about the threats to dependable systems.

172
00:12:16.930 --> 00:12:21.369 
It started out and how it can now be and has to be extended

173
00:12:21.370 --> 00:12:23.829 
towards software-based systems.

174
00:12:23.830 --> 00:12:29.099 
If you look at dependability research and traditionally

175
00:12:29.100 --> 00:12:33.819 
the mainframe systems to have a solution based systems where king

176
00:12:33.820 --> 00:12:38.919 
and the most important. And it's still that if you look at energy

177
00:12:38.920 --> 00:12:43.689 
production at power plants or if you look at space and air technology,

178
00:12:43.690 --> 00:12:47.409 
then we look at solutions.

179
00:12:47.410 --> 00:12:51.939 
However, there is a shift towards software in

180
00:12:51.940 --> 00:12:53.080 
most other areas.

181
00:12:54.100 --> 00:12:58.749 
This example here is high-performance computing, where you have large scale cluster

182
00:12:58.750 --> 00:13:03.189 
and distributed systems, where reliability is addressed

183
00:13:03.190 --> 00:13:05.169 
mainly in software.

184
00:13:05.170 --> 00:13:09.699 
And if you look at the server systems in our lab or in the data center or in

185
00:13:09.700 --> 00:13:14.499 
the cloud, even then, reliability is

186
00:13:14.500 --> 00:13:19.119 
mainly addressed by software having failover models that are

187
00:13:19.120 --> 00:13:23.539 
operating on a relatively coarse granular level.

188
00:13:25.930 --> 00:13:29.799 
If you look at the servers and the server architecture and we talked a lot about the

189
00:13:29.800 --> 00:13:34.599 
power systems, but also in Intel across the industry,

190
00:13:34.600 --> 00:13:39.519 
we have a trend and that trend is getting more complex.

191
00:13:39.520 --> 00:13:43.179 
So there are additional levels of caches and so forth.

192
00:13:43.180 --> 00:13:47.919 
And the other thing, this assumption that the word is symmetric

193
00:13:47.920 --> 00:13:52.689 
and every processor can talk to every memory and it's always taking

194
00:13:52.690 --> 00:13:57.139 
the same amount of time and it's having giving you the same bandwidth and

195
00:13:57.140 --> 00:14:02.559 
therefore this assumption no longer is true. We have

196
00:14:02.560 --> 00:14:08.289 
systems and this is an example of Silicon Graphics server system

197
00:14:08.290 --> 00:14:12.969 
where we have more elaborate topologies

198
00:14:12.970 --> 00:14:17.439 
and interconnection networks among processors and memory modules.

199
00:14:17.440 --> 00:14:22.149 
And people talk about NUMA, which means the non-uniform memory access

200
00:14:22.150 --> 00:14:26.619 
model. So depending on a placement of data,

201
00:14:26.620 --> 00:14:31.089 
it might be appropriate to use this particular processor

202
00:14:31.090 --> 00:14:35.499 
and this particular socket to process the data or use a different one.

203
00:14:35.500 --> 00:14:40.059 
And you might have big performance impact and performance differences actually,

204
00:14:40.060 --> 00:14:44.929 
by just placing the data on the wrong socket.

205
00:14:44.930 --> 00:14:47.559 
If you look at your favorite operating system,

206
00:14:49.360 --> 00:14:53.710 
are you able and how does the operating system address these new issues?

207
00:14:55.430 --> 00:14:59.479 
The simple answer today is basically first touch.

208
00:14:59.480 --> 00:15:04.269 
So if you have a processor that is allocating memory

209
00:15:04.270 --> 00:15:08.739 
like this, is this one here, then it will allocate the memory

210
00:15:08.740 --> 00:15:11.049 
close to this to this particular socket.

211
00:15:11.050 --> 00:15:16.499 
Right. And if. A thread that runs here later moves over to a different processor

212
00:15:16.500 --> 00:15:20.999 
because of affinity, because of scalping decisions, then still

213
00:15:21.000 --> 00:15:24.929 
the data needs to be accessed over here. And this is going to take a lot of time.

214
00:15:24.930 --> 00:15:29.060 
And this also has the impact of congestion on

215
00:15:30.150 --> 00:15:33.179 
result in interconnection network.

216
00:15:33.180 --> 00:15:37.619 
So a problem that needs to be addressed on all levels and

217
00:15:37.620 --> 00:15:39.539 
on all computer architectures.

218
00:15:39.540 --> 00:15:44.529 
If we look at the bigger service systems in the power range, for instance, that

219
00:15:44.530 --> 00:15:47.929 
E880, then

220
00:15:48.980 --> 00:15:53.659 
measurements on that system exhibit NUMA effects,

221
00:15:53.660 --> 00:15:56.569 
as well as on energy systems.

222
00:15:56.570 --> 00:16:01.669 
There are differences. You can hate certain situations because you have higher

223
00:16:01.670 --> 00:16:04.039 
bandwidth in the memory interconnect.

224
00:16:04.040 --> 00:16:07.909 
But typically there's is a tradeoff between bandwidth and latency.

225
00:16:07.910 --> 00:16:12.607 
And either way, these problems need to be addressed and in software.

226
00:16:14.840 --> 00:16:17.600 
The world is getting even more complex. If you look at.

227
00:16:18.770 --> 00:16:23.239 
The cost and we had several presentations and

228
00:16:23.240 --> 00:16:28.219 
discussions about copy the current accelerator process interface

229
00:16:28.220 --> 00:16:32.699 
and also about GPS and FPGA

230
00:16:32.700 --> 00:16:35.419 
s additional compute devices.

231
00:16:35.420 --> 00:16:39.919 
There's a clear trend towards a more heterogeneous world

232
00:16:39.920 --> 00:16:44.359 
of computing systems, and this means more

233
00:16:44.360 --> 00:16:46.999 
challenges in terms of resource management.

234
00:16:47.000 --> 00:16:51.889 
This resource management most likely will not be done automatically

235
00:16:51.890 --> 00:16:56.569 
by the operating system, but will need to be addressed in software as

236
00:16:56.570 --> 00:16:57.570 
well.

237
00:17:00.940 --> 00:17:05.469 
We talk about software every now and then and figure

238
00:17:05.470 --> 00:17:10.749 
that it's just getting more is piling up, the stack is getting bigger.

239
00:17:10.750 --> 00:17:12.369 
And this is

240
00:17:17.400 --> 00:17:22.368 
due to additional features in the software, like, for instance,

241
00:17:22.369 --> 00:17:27.239 
the management of NUMA characteristics, also like the management

242
00:17:27.240 --> 00:17:31.889 
of heterogeneous computing environments, but we also have effects

243
00:17:31.890 --> 00:17:36.419 
from the hardware arena, for instance, the decreasing

244
00:17:36.420 --> 00:17:39.419 
structural sizes, the massive memory increase.

245
00:17:39.420 --> 00:17:43.949 
And these effects lead to more foreclosures

246
00:17:43.950 --> 00:17:47.489 
and less error containment. So we have just more problems.

247
00:17:47.490 --> 00:17:51.629 
And this is because the systems are getting bigger and it's also because structures are

248
00:17:51.630 --> 00:17:56.129 
getting smaller. And the conclusion

249
00:17:56.130 --> 00:17:58.829 
from that, and this is also an outlook and future work.

250
00:17:58.830 --> 00:18:03.899 
Basically, the traditional hardware-based models, they need an update.

251
00:18:03.900 --> 00:18:08.364 
So we have more memory, we have increased density, higher data rates,

252
00:18:08.365 --> 00:18:12.839 
we do no longer have monolithic processes,

253
00:18:12.840 --> 00:18:17.189 
but many because we see the interconnect as a crucial component.

254
00:18:17.190 --> 00:18:21.959 
We have issues with fault in isolation because

255
00:18:21.960 --> 00:18:24.109 
not everything in a computer is redundant.

256
00:18:25.710 --> 00:18:28.530 
Things like the reactive fault tolerance get inappropriate.

257
00:18:30.780 --> 00:18:35.459 
People try to look at machine learning, at prediction,

258
00:18:35.460 --> 00:18:38.819 
at predictive models for fault tolerance.

259
00:18:38.820 --> 00:18:42.899 
We see the software stack expanding.

260
00:18:42.900 --> 00:18:46.109 
We talked about virtualization as a new layer.

261
00:18:46.110 --> 00:18:48.359 
You've understood that.

262
00:18:48.360 --> 00:18:51.660 
And power architecture has no basically

263
00:18:53.240 --> 00:18:57.749 
very exceptional cases only where you run the system without an hypervisor,

264
00:18:57.750 --> 00:19:02.399 
oftentimes you have to hypervisor as a basic component in your system.

265
00:19:02.400 --> 00:19:07.109 
And if you look at testing and software engineering

266
00:19:07.110 --> 00:19:11.549 
as an arena, we have very little to support for reliability

267
00:19:11.550 --> 00:19:14.069 
research and everything below.

268
00:19:14.070 --> 00:19:18.869 
The U.S. is not very accessible and it's not very

269
00:19:18.870 --> 00:19:22.109 
good connected to the software layers above.

270
00:19:25.700 --> 00:19:27.829 
At this point, I just want to.

271
00:19:27.830 --> 00:19:31.864 
Give a hint and suggest that you may have a look at our future SAP lab.

272
00:19:32.900 --> 00:19:37.429 
The Future SAP lab is an collaboration project at Hasso Plattner Institute, together

273
00:19:37.430 --> 00:19:42.319 
with industry. And the idea is that we use

274
00:19:42.320 --> 00:19:46.879 
silver systems for research and for instance, to look at

275
00:19:46.880 --> 00:19:52.369 
field prediction using monitoring on different levels,

276
00:19:52.370 --> 00:19:53.710 
proactive virtual machine migration

277
00:19:57.050 --> 00:20:01.489 
based on data analysis and data science, if you will, and also a platform

278
00:20:01.490 --> 00:20:03.820 
where we try to do fault injection and stress

279
00:20:06.380 --> 00:20:11.239 
testing for systems. So one specific experiment,

280
00:20:11.240 --> 00:20:16.069 
and this has been reported about and publications out

281
00:20:16.070 --> 00:20:21.199 
that is about fault injection into

282
00:20:21.200 --> 00:20:25.879 
X86 systems. So basically the idea is if you have a medical

283
00:20:25.880 --> 00:20:29.929 
system, you could use the one core to monitor the other core.

284
00:20:29.930 --> 00:20:34.279 
And now you have events.

285
00:20:34.280 --> 00:20:38.869 
If, for instance, you lower the supply voltage and you will see

286
00:20:38.870 --> 00:20:42.650 
cause failing and these events should be

287
00:20:43.880 --> 00:20:48.160 
fed into a software infrastructure that eventually will lead to reconfiguration

288
00:20:50.000 --> 00:20:55.399 
of the system. This is applicable to two major platforms, to all platforms, basically.

289
00:20:55.400 --> 00:20:59.899 
And if you look closer at performance counters then these

290
00:20:59.900 --> 00:21:04.549 
performance counters, they are quite expensive, but they are oftentimes not visible

291
00:21:04.550 --> 00:21:10.249 
and ignored. What we see here, for instance, is a situation where,

292
00:21:10.250 --> 00:21:14.929 
again, a couple of years ago, but, basically 15

293
00:21:14.930 --> 00:21:17.967 
minutes before a memory module

294
00:21:20.190 --> 00:21:24.899 
failed this memory module, given notification

295
00:21:24.900 --> 00:21:30.149 
and 15 minutes is quite some time, so it's quite possible to

296
00:21:30.150 --> 00:21:34.739 
migrate your software infrastructure to a different node if you see

297
00:21:34.740 --> 00:21:39.539 
problems coming. And here's another example where we have

298
00:21:39.540 --> 00:21:44.099 
errors that have been announced, the system has been

299
00:21:44.100 --> 00:21:46.409 
talking about before, basically.

300
00:21:46.410 --> 00:21:51.029 
And when does the system

301
00:21:51.030 --> 00:21:55.699 
administrator typically figure out that problems have

302
00:21:55.700 --> 00:21:57.809 
have arrived in the landscape?

303
00:21:57.810 --> 00:22:00.689 
This is when the system finished a reboot.

304
00:22:00.690 --> 00:22:05.489 
So a lot of information available, very little

305
00:22:05.490 --> 00:22:10.169 
usage of that information in today's management infrastructure.

306
00:22:10.170 --> 00:22:14.609 
And everybody is talking about machine learning as

307
00:22:14.610 --> 00:22:15.809 
the new big trend.

308
00:22:17.070 --> 00:22:21.779 
I address the question whether machine learning may be

309
00:22:21.780 --> 00:22:25.139 
unethical, depending on where you use your computers.

310
00:22:25.140 --> 00:22:30.269 
But if you want to use machine learning to do for prediction,

311
00:22:30.270 --> 00:22:34.079 
that might be actually a neat idea to to make the systems more reliable,

312
00:22:35.140 --> 00:22:36.140 
reliable.

313
00:22:37.080 --> 00:22:41.729 
And what we have developed here is a framework where we use hardware

314
00:22:41.730 --> 00:22:46.229 
failure predictors. We use failure predictors from the virtual machine monitor level,

315
00:22:46.230 --> 00:22:50.969 
from the operating systems and also from the applications to generate something called

316
00:22:50.970 --> 00:22:56.009 
inhealth indicator. And based on the health indicator, we will be able

317
00:22:56.010 --> 00:23:00.359 
to trigger migration decisions basically for virtual machines.

318
00:23:00.360 --> 00:23:05.819 
So let's assume we have all this data like using

319
00:23:05.820 --> 00:23:09.270 
Java monitoring infrastructures or applications of

320
00:23:10.320 --> 00:23:15.329 
infrastructures, using Detroiters or monitoring foreigners or using

321
00:23:15.330 --> 00:23:19.919 
VMware. We probe on the VM level and looking at the

322
00:23:19.920 --> 00:23:25.259 
performance kind of Sony machine check architecture that we see in

323
00:23:25.260 --> 00:23:29.970 
that that we first saw in the mainframe later in power, but also an internal audit.

324
00:23:30.990 --> 00:23:35.429 
And we use all these values to fit

325
00:23:35.430 --> 00:23:38.039 
it into a machine learning algorithm.

326
00:23:38.040 --> 00:23:40.679 
And then we generate a system health indicator.

327
00:23:40.680 --> 00:23:45.389 
And depending on the level of the system health care, we may trigger

328
00:23:45.390 --> 00:23:50.339 
a migration. And if you heard

329
00:23:50.340 --> 00:23:55.169 
about power, A.J., or even about the power of suspects on the mainframe,

330
00:23:55.170 --> 00:24:00.059 
then you know that there is varying levels of support for that particular

331
00:24:00.060 --> 00:24:04.949 
migration. And the power platform, again, is doing exceptionally

332
00:24:04.950 --> 00:24:09.719 
well with respect to hiding that that reconfiguration

333
00:24:09.720 --> 00:24:11.789 
process from the application from the user.

334
00:24:14.190 --> 00:24:18.899 
So what are the conclusions we see advances in the server technology.

335
00:24:18.900 --> 00:24:21.869 
We have many calls coming.

336
00:24:21.870 --> 00:24:23.789 
We have a tremendous amount of memory.

337
00:24:23.790 --> 00:24:28.229 
We have new technologies, but

338
00:24:28.230 --> 00:24:32.429 
reliability is the most sought after quality after all.

339
00:24:32.430 --> 00:24:36.929 
So our our reading is that in

340
00:24:36.930 --> 00:24:40.259 
the future, it will no longer be possible to write.

341
00:24:41.390 --> 00:24:46.269 
Canoe's style algorithms to address all these problems,

342
00:24:46.270 --> 00:24:50.769 
but we will need to use data science machine learning to use predictive

343
00:24:50.770 --> 00:24:55.839 
maintenance approaches to how to do something

344
00:24:55.840 --> 00:25:01.000 
about reliability and to increase the overall system's reliability.

345
00:25:02.350 --> 00:25:07.419 
And another important statement, we think that

346
00:25:07.420 --> 00:25:12.309 
new approaches in software engineering will be

347
00:25:12.310 --> 00:25:16.809 
needed. Microsoft's architectures are promising approach

348
00:25:16.810 --> 00:25:21.669 
that we see today, but the question of energy of our computing is

349
00:25:21.670 --> 00:25:26.049 
mostly unaddressed. So a field of open research

350
00:25:27.530 --> 00:25:31.389 
after all the conclusion on the outlook here.

351
00:25:31.390 --> 00:25:35.739 
So we see changes in system software coming on the horizon.

352
00:25:35.740 --> 00:25:40.419 
And these changes are mostly driven by hardware, by computer

353
00:25:40.420 --> 00:25:44.979 
architecture developments like new form factors, more dense

354
00:25:44.980 --> 00:25:48.970 
computing, new management interfaces like.

355
00:25:50.670 --> 00:25:55.169 
The decision, whether you put computation in

356
00:25:55.170 --> 00:26:00.359 
your local data center or outsourced to the cloud, so placement decisions like

357
00:26:00.360 --> 00:26:05.759 
new programing models that you use for GPU computing that you use for FPGA

358
00:26:05.760 --> 00:26:09.899 
and also the question of virtualization that brings new security problems visit.

359
00:26:09.900 --> 00:26:15.599 
So computer architecture is basically

360
00:26:15.600 --> 00:26:20.279 
the trigger, but that drives changes in many layers

361
00:26:20.280 --> 00:26:21.280 
of software.

362
00:26:24.960 --> 00:26:29.429 
Just kind of concludes the talk, but before finishing, I just

363
00:26:29.430 --> 00:26:30.750 
want to give two pointers.

364
00:26:32.220 --> 00:26:37.319 
We are happy that you developed and you detected openHPI

365
00:26:37.320 --> 00:26:40.079 
as a platform for this particular course.

366
00:26:40.080 --> 00:26:44.669 
There are more courses on openHPI, some of them in German, some

367
00:26:44.670 --> 00:26:47.700 
of them in English, some of them even in Chinese.

368
00:26:48.810 --> 00:26:53.009 
And we see our future of computing calls here.

369
00:26:53.010 --> 00:26:57.629 
But you are invited to find more courses

370
00:26:57.630 --> 00:27:02.249 
on openHPI. And the other thing, the future SOC lab.

371
00:27:02.250 --> 00:27:06.899 
Right? I mentioned that a couple of times, but that might be something that is, of all,

372
00:27:06.900 --> 00:27:12.179 
a closer look. So feel free to Hennan.

373
00:27:12.180 --> 00:27:18.239 
Project proposals. Resources on the future are basically available

374
00:27:18.240 --> 00:27:20.849 
to researchers from all over the world.

375
00:27:22.080 --> 00:27:26.519 
Do you have to go to this call for projects process and

376
00:27:26.520 --> 00:27:31.049 
hand hand in project proposals that will allow you and

377
00:27:31.050 --> 00:27:35.759 
give you access to the lab's resources for about six months

378
00:27:35.760 --> 00:27:40.629 
at this point in time? I just want to finish and have one single conclusion.

379
00:27:40.630 --> 00:27:45.209 
This is energy efficiency that will be the key for the data

380
00:27:45.210 --> 00:27:48.660 
center and maybe the most important aspect for years to come

381
00:27:50.340 --> 00:27:52.890 
at this point in time. I just want to say thank you for your attention.
