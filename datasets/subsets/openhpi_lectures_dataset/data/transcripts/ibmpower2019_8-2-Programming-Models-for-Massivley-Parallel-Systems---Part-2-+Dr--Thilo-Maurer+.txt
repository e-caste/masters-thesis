WEBVTT

1
00:00:00.930 --> 00:00:02.999 
OK, so in the second part

2
00:00:03.000 --> 00:00:05.009 
of this presentation, I will talk

3
00:00:05.010 --> 00:00:07.079 
about more practical aspects,

4
00:00:07.080 --> 00:00:09.119 
aspects of parallelising applications.

5
00:00:10.140 --> 00:00:12.459 
So when one starts

6
00:00:12.460 --> 00:00:14.699 
out to parallelise its application,

7
00:00:14.700 --> 00:00:17.399 
there are three different

8
00:00:17.400 --> 00:00:18.539 
ways of doing it.

9
00:00:19.920 --> 00:00:22.139 
The first thing, and that's usually

10
00:00:22.140 --> 00:00:24.359 
the most easiest thing is

11
00:00:24.360 --> 00:00:25.589 
to use something that's already

12
00:00:25.590 --> 00:00:28.259 
available, which means to use libraries

13
00:00:28.260 --> 00:00:29.880 
that at least to

14
00:00:30.960 --> 00:00:32.640 
a large extent, cover what

15
00:00:33.900 --> 00:00:35.309 
your application does.

16
00:00:35.310 --> 00:00:37.589 
Usually there are other programmers

17
00:00:37.590 --> 00:00:40.079 
that have run into the same problem

18
00:00:40.080 --> 00:00:42.239 
as oneself is

19
00:00:42.240 --> 00:00:45.029 
now about to tackle.

20
00:00:45.030 --> 00:00:47.729 
And so

21
00:00:47.730 --> 00:00:48.989 
this problem might actually already be

22
00:00:48.990 --> 00:00:51.119 
solved. And then the way to go

23
00:00:51.120 --> 00:00:52.829 
is to use libraries.

24
00:00:52.830 --> 00:00:54.029 
For example, there's a very good

25
00:00:54.030 --> 00:00:55.949 
library for deep learning, which is

26
00:00:55.950 --> 00:00:58.229 
called DNN, provided by Nvidia

27
00:00:58.230 --> 00:00:59.879 
and it implements pretty much

28
00:00:59.880 --> 00:01:02.849 
everything one needs to run.

29
00:01:02.850 --> 00:01:04.829 
Its your

30
00:01:04.830 --> 00:01:07.499 
deep neural network

31
00:01:07.500 --> 00:01:09.510 
calculations on a GPU.

32
00:01:11.550 --> 00:01:13.559 
And well, the second thing

33
00:01:13.560 --> 00:01:15.819 
that one

34
00:01:15.820 --> 00:01:17.709 
does when there's nothing readily

35
00:01:17.710 --> 00:01:19.749 
available like a library is

36
00:01:19.750 --> 00:01:21.789 
to actually modifies on application

37
00:01:21.790 --> 00:01:24.099 
in a more intense way,

38
00:01:24.100 --> 00:01:25.100 
but still,

39
00:01:26.340 --> 00:01:28.649 
which still produces portable code,

40
00:01:28.650 --> 00:01:30.719 
which means that it can run

41
00:01:30.720 --> 00:01:32.939 
on multiple architectures without

42
00:01:32.940 --> 00:01:35.039 
modification, let it be on a

43
00:01:35.040 --> 00:01:37.049 
regular CPU or a massively

44
00:01:37.050 --> 00:01:38.750 
parallel GPU,

45
00:01:40.080 --> 00:01:42.059 
and this method is by the use of

46
00:01:42.060 --> 00:01:43.089 
compiler directors.

47
00:01:43.090 --> 00:01:44.549 
They're relatively easy to learn

48
00:01:46.020 --> 00:01:47.020 
and

49
00:01:50.780 --> 00:01:53.150 
they help the compiler to

50
00:01:54.500 --> 00:01:56.659 
understand the standards, maybe

51
00:01:56.660 --> 00:01:58.090 
not the right word to

52
00:02:00.230 --> 00:02:02.659 
implement a way of

53
00:02:02.660 --> 00:02:04.849 
parallelising sections of code

54
00:02:04.850 --> 00:02:07.099 
or loops in a way, as you requested

55
00:02:07.100 --> 00:02:08.819 
by these compiler directives.

56
00:02:10.190 --> 00:02:12.259 
One thing is that when

57
00:02:12.260 --> 00:02:14.509 
you start mixing pragma languages

58
00:02:14.510 --> 00:02:15.849 
like OpenMP and OpenACC,

59
00:02:17.060 --> 00:02:19.159 
the application code may become

60
00:02:19.160 --> 00:02:21.439 
hard to read since

61
00:02:21.440 --> 00:02:23.599 
in the end you might have 50

62
00:02:23.600 --> 00:02:25.879 
percent lines by OpenMP

63
00:02:25.880 --> 00:02:28.429 
and 50 percent lines by OpenACC

64
00:02:28.430 --> 00:02:29.479 
and then still

65
00:02:30.590 --> 00:02:33.139 
like 50 percent of the

66
00:02:33.140 --> 00:02:35.419 
lines only from

67
00:02:35.420 --> 00:02:38.149 
your own portable code.

68
00:02:38.150 --> 00:02:40.129 
So what I want to

69
00:02:40.130 --> 00:02:42.199 
say is that your code

70
00:02:42.200 --> 00:02:44.359 
can become very convoluted

71
00:02:44.360 --> 00:02:46.520 
with the compiler directives and

72
00:02:48.320 --> 00:02:50.539 
the actual application code might

73
00:02:50.540 --> 00:02:53.329 
become not clear

74
00:02:53.330 --> 00:02:55.489 
to the third person that starts reading

75
00:02:55.490 --> 00:02:56.490 
your code.

76
00:02:57.710 --> 00:02:58.999 
And then the most

77
00:03:01.940 --> 00:03:04.039 
difficult, but also the

78
00:03:04.040 --> 00:03:06.349 
method that provides best performance

79
00:03:06.350 --> 00:03:08.539 
to your application

80
00:03:08.540 --> 00:03:10.609 
will be to use

81
00:03:10.610 --> 00:03:12.889 
extensions to programing languages

82
00:03:12.890 --> 00:03:14.719 
that are well suited for massively

83
00:03:14.720 --> 00:03:16.789 
parallel applications.

84
00:03:16.790 --> 00:03:18.830 
It's a huge effort to actually

85
00:03:20.120 --> 00:03:21.559 
realize such

86
00:03:23.120 --> 00:03:24.769 
implementations because someone usually

87
00:03:24.770 --> 00:03:26.899 
has to completely rewrite

88
00:03:26.900 --> 00:03:28.909 
his own application.

89
00:03:28.910 --> 00:03:31.159 
And examples for APIs that are used

90
00:03:31.160 --> 00:03:33.139 
here are, for example, MPI

91
00:03:33.140 --> 00:03:34.140 
and CUDA.

92
00:03:38.010 --> 00:03:40.619 
Now, let's talk a little bit about

93
00:03:40.620 --> 00:03:42.749 
these programing models,

94
00:03:42.750 --> 00:03:44.819 
so that's Pthread, that's

95
00:03:44.820 --> 00:03:45.870 
the most basic thing

96
00:03:47.070 --> 00:03:49.290 
in shared memory processing.

97
00:03:50.370 --> 00:03:51.719 
It's also very old.

98
00:03:53.520 --> 00:03:54.520 
It's

99
00:03:55.740 --> 00:03:57.809 
when people started building parallel

100
00:03:57.810 --> 00:03:58.860 
processes, meaning

101
00:04:00.480 --> 00:04:02.759 
CPU cores that could host

102
00:04:02.760 --> 00:04:03.760 
multiple threads.

103
00:04:06.150 --> 00:04:08.219 
It wasn't easy to

104
00:04:08.220 --> 00:04:09.749 
do this because every processor was

105
00:04:09.750 --> 00:04:11.759 
different, so people looked for

106
00:04:11.760 --> 00:04:14.579 
a standard. In 1995,

107
00:04:14.580 --> 00:04:17.189 
they created the so-called IEEE

108
00:04:17.190 --> 00:04:19.169 
POSIX standard, which

109
00:04:19.170 --> 00:04:21.599 
is short for portable operating system

110
00:04:21.600 --> 00:04:23.261 
interface and

111
00:04:25.100 --> 00:04:27.439 
allows a software

112
00:04:27.440 --> 00:04:29.719 
developer to define

113
00:04:29.720 --> 00:04:31.909 
threads within

114
00:04:31.910 --> 00:04:33.889 
a Linux

115
00:04:33.890 --> 00:04:36.199 
process that runs

116
00:04:36.200 --> 00:04:38.160 
concurrently to the main program.

117
00:04:40.010 --> 00:04:41.134 
There are a lot of

118
00:04:44.690 --> 00:04:46.999 
management calls

119
00:04:47.000 --> 00:04:49.519 
that we can do to create, terminate,

120
00:04:49.520 --> 00:04:51.739 
join and detach these threads,

121
00:04:51.740 --> 00:04:52.879 
and also there's a lot of

122
00:04:52.880 --> 00:04:55.129 
infrastructure to create sections

123
00:04:55.130 --> 00:04:57.229 
of mutual exclusion,

124
00:04:57.230 --> 00:04:58.230 
creating,

125
00:04:59.630 --> 00:05:01.790 
destroying and unlocking and locking.

126
00:05:03.650 --> 00:05:06.139 
And as I said earlier,

127
00:05:06.140 --> 00:05:08.719 
as being an SMP system,

128
00:05:08.720 --> 00:05:10.759 
the communication

129
00:05:10.760 --> 00:05:13.129 
between the

130
00:05:13.130 --> 00:05:14.600 
threads or in this case

131
00:05:15.980 --> 00:05:16.980 
the threads of the workers, in this case,

132
00:05:18.350 --> 00:05:20.779 
is by the use of common variables.

133
00:05:20.780 --> 00:05:22.789 
And these common

134
00:05:22.790 --> 00:05:24.749 
variables are often called condition

135
00:05:24.750 --> 00:05:26.929 
variables in this context, which means

136
00:05:26.930 --> 00:05:28.939 
that you write

137
00:05:28.940 --> 00:05:30.529 
something to these variables and then

138
00:05:30.530 --> 00:05:31.530 
the second thread will

139
00:05:32.570 --> 00:05:34.489 
notice the change and this event will

140
00:05:34.490 --> 00:05:36.469 
drive then the synchronization between

141
00:05:36.470 --> 00:05:37.849 
the tasks.

142
00:05:37.850 --> 00:05:39.619 
The largest system currently available

143
00:05:41.300 --> 00:05:43.459 
is the IBM E880 that can

144
00:05:43.460 --> 00:05:46.579 
host this massively

145
00:05:46.580 --> 00:05:47.580 
parallel

146
00:05:49.430 --> 00:05:51.859 
thread count and you can have up to

147
00:05:51.860 --> 00:05:54.019 
1500 parallel threads at the same

148
00:05:54.020 --> 00:05:55.519 
time in the shared memory system.

149
00:05:59.600 --> 00:06:02.689 
The next thing is

150
00:06:02.690 --> 00:06:04.399 
the next API that I want to talk about

151
00:06:04.400 --> 00:06:06.439 
is the message passing interface.

152
00:06:06.440 --> 00:06:08.599 
It's also extremely old and

153
00:06:08.600 --> 00:06:09.600 
foremost

154
00:06:11.360 --> 00:06:13.699 
used in high performance

155
00:06:13.700 --> 00:06:15.109 
computing.

156
00:06:15.110 --> 00:06:16.969 
Pretty much every high performance

157
00:06:16.970 --> 00:06:19.399 
computing application is using this API

158
00:06:19.400 --> 00:06:21.559 
to communicate between

159
00:06:21.560 --> 00:06:23.689 
its workers, which are here called the

160
00:06:23.690 --> 00:06:25.459 
ranks of the application.

161
00:06:25.460 --> 00:06:27.889 
So each process

162
00:06:27.890 --> 00:06:30.079 
is what they call a rank, and

163
00:06:30.080 --> 00:06:31.609 
these ranks then communicate

164
00:06:33.020 --> 00:06:34.020 
using,

165
00:06:35.330 --> 00:06:37.339 
communicate by sending messages to the

166
00:06:37.340 --> 00:06:39.859 
other rank

167
00:06:39.860 --> 00:06:41.089 
and then triggering

168
00:06:44.870 --> 00:06:46.999 
and processing by the reception

169
00:06:47.000 --> 00:06:48.259 
of such messages.

170
00:06:51.200 --> 00:06:53.359 
So MPI is not a product,

171
00:06:53.360 --> 00:06:54.860 
it's a standard,

172
00:06:56.300 --> 00:06:58.279 
which means that there is a vast

173
00:06:58.280 --> 00:07:00.079 
number of implementations of this

174
00:07:00.080 --> 00:07:01.129 
standard.

175
00:07:01.130 --> 00:07:03.259 
Each hardware vendor pretty much

176
00:07:03.260 --> 00:07:04.929 
has its own implementation.

177
00:07:04.930 --> 00:07:07.069 
For example, there's the IBM PE-MPI,

178
00:07:07.070 --> 00:07:08.719 
then there's Intel MPI and for

179
00:07:08.720 --> 00:07:10.440 
Microsoft, there's MPI.NET.

180
00:07:11.960 --> 00:07:13.669 
But there are also open implementations

181
00:07:13.670 --> 00:07:15.949 
of MPI and

182
00:07:15.950 --> 00:07:18.349 
the most commonly used today's

183
00:07:18.350 --> 00:07:19.350 
most likely OpenMPI.

184
00:07:21.830 --> 00:07:24.119 
And it also supports massive

185
00:07:24.120 --> 00:07:26.339 
parallelism, although it has not been

186
00:07:26.340 --> 00:07:28.829 
designed from the beginning to

187
00:07:28.830 --> 00:07:31.109 
be supportive

188
00:07:31.110 --> 00:07:32.250 
of massive parallelism,

189
00:07:33.540 --> 00:07:35.609 
since it has been developed

190
00:07:35.610 --> 00:07:37.589 
in an age where massively parallel

191
00:07:37.590 --> 00:07:39.059 
computing systems have not been

192
00:07:39.060 --> 00:07:41.029 
available, but

193
00:07:41.030 --> 00:07:43.079 
it's still used today in large

194
00:07:43.080 --> 00:07:45.299 
colocations of server so-called

195
00:07:45.300 --> 00:07:47.009 
clusters, for example, like the

196
00:07:47.010 --> 00:07:49.289 
BlueGene/Q, where it runs for

197
00:07:49.290 --> 00:07:50.909 
500 thousand ranks in parallel

198
00:07:50.910 --> 00:07:51.910 
successfully.

199
00:07:56.680 --> 00:07:58.479 
Now, how does that work?

200
00:07:58.480 --> 00:08:01.209 
As I said, each processor

201
00:08:01.210 --> 00:08:03.909 
is what they call a rank

202
00:08:03.910 --> 00:08:06.219 
and each rank has

203
00:08:06.220 --> 00:08:08.049 
a number, which is called the rank

204
00:08:08.050 --> 00:08:10.149 
number and rank number, then

205
00:08:10.150 --> 00:08:13.119 
decides what role this

206
00:08:13.120 --> 00:08:15.189 
subprogram takes in the

207
00:08:15.190 --> 00:08:16.779 
full task decomposition.

208
00:08:17.980 --> 00:08:19.959 
But the main but what are the main

209
00:08:19.960 --> 00:08:21.699 
things to understand is that actually

210
00:08:21.700 --> 00:08:23.889 
each of these ranks

211
00:08:23.890 --> 00:08:26.199 
executes the same program

212
00:08:26.200 --> 00:08:28.269 
code, not

213
00:08:28.270 --> 00:08:30.249 
meaning that it executes

214
00:08:30.250 --> 00:08:31.959 
the identical instructions at all

215
00:08:31.960 --> 00:08:32.859 
times.

216
00:08:32.860 --> 00:08:34.808 
But it executes the same application

217
00:08:34.809 --> 00:08:36.908 
code and only this rank number then

218
00:08:36.909 --> 00:08:39.609 
discriminates which rank fulfils

219
00:08:39.610 --> 00:08:40.610 
which tasks

220
00:08:41.620 --> 00:08:42.620 
in this

221
00:08:43.720 --> 00:08:45.699 
task decomposition that

222
00:08:45.700 --> 00:08:47.169 
I was showing earlier.

223
00:08:47.170 --> 00:08:49.006 
So earlier you were seeing this

224
00:08:51.580 --> 00:08:53.649 
acyclic directed graph where you had

225
00:08:53.650 --> 00:08:55.435 
the nodes connecting,

226
00:08:57.730 --> 00:08:59.199 
where you had the lines connecting the

227
00:08:59.200 --> 00:09:01.179 
work items that had to be done.

228
00:09:01.180 --> 00:09:02.650 
And you were seeing five

229
00:09:03.670 --> 00:09:06.459 
vertical groupings

230
00:09:06.460 --> 00:09:08.559 
of these dots.

231
00:09:09.850 --> 00:09:12.369 
And these groupings resemble

232
00:09:12.370 --> 00:09:14.499 
what is called a rank here.

233
00:09:14.500 --> 00:09:16.509 
So the

234
00:09:16.510 --> 00:09:19.479 
rank decides which tasks it executes

235
00:09:19.480 --> 00:09:22.089 
in the sequence as defined by

236
00:09:22.090 --> 00:09:23.184 
the directed acyclic graph.

237
00:09:25.740 --> 00:09:27.689 
Something else to understand here is

238
00:09:27.690 --> 00:09:30.029 
with the message passing

239
00:09:30.030 --> 00:09:32.159 
paradigm or with MPI

240
00:09:32.160 --> 00:09:34.439 
is that not every

241
00:09:34.440 --> 00:09:37.229 
rank sees or has

242
00:09:37.230 --> 00:09:39.059 
full access to all of the data that

243
00:09:39.060 --> 00:09:40.819 
needs to be processed.

244
00:09:40.820 --> 00:09:43.859 
Instead, the full set of data is

245
00:09:43.860 --> 00:09:46.139 
divided into a number

246
00:09:46.140 --> 00:09:47.220 
of chunks,

247
00:09:48.390 --> 00:09:50.369 
and each chunk is then attributed to a

248
00:09:50.370 --> 00:09:52.979 
single rank. So each rank just accesses

249
00:09:52.980 --> 00:09:56.159 
a chunk or a part of the whole data

250
00:09:56.160 --> 00:09:57.410 
and then also only

251
00:09:59.880 --> 00:10:01.639 
computes using this data.

252
00:10:03.330 --> 00:10:05.669 
And does its subtask using this data

253
00:10:05.670 --> 00:10:07.529 
and then communication takes place

254
00:10:07.530 --> 00:10:09.569 
after the subtask is done to

255
00:10:09.570 --> 00:10:11.339 
communicate the result back to all the

256
00:10:11.340 --> 00:10:12.840 
other ranks and

257
00:10:14.070 --> 00:10:16.139 
then to use these

258
00:10:22.870 --> 00:10:24.879 
reduced or partially fulfilled

259
00:10:24.880 --> 00:10:26.169 
subtasks to trigger

260
00:10:27.190 --> 00:10:29.679 
the following subtasks.

261
00:10:32.110 --> 00:10:34.569 
And the communication itself, as I said

262
00:10:34.570 --> 00:10:36.339 
in message passing, is usually done

263
00:10:36.340 --> 00:10:37.989 
explicitly by the programmer, which

264
00:10:37.990 --> 00:10:39.759 
means that the programmer will have to

265
00:10:39.760 --> 00:10:41.799 
take care of this orchestration of

266
00:10:41.800 --> 00:10:44.139 
what I just said, namely sending

267
00:10:44.140 --> 00:10:45.140 
data around,

268
00:10:46.210 --> 00:10:47.919 
running the subtasks, deciding which

269
00:10:47.920 --> 00:10:49.959 
subtask it's doing

270
00:10:49.960 --> 00:10:50.960 
and so on.

271
00:10:54.280 --> 00:10:56.123 
And then there's OpenMP,

272
00:10:57.790 --> 00:11:00.729 
which is essentially an extension or

273
00:11:00.730 --> 00:11:02.950 
simplification of the Pthreads model

274
00:11:04.060 --> 00:11:05.949 
in that sense, that the programmer

275
00:11:05.950 --> 00:11:08.109 
doesn't have to explicitly work

276
00:11:08.110 --> 00:11:10.089 
with all these threads,

277
00:11:10.090 --> 00:11:12.549 
but it can provide

278
00:11:12.550 --> 00:11:14.200 
annotations to the compiler

279
00:11:15.790 --> 00:11:18.339 
such that the compiler can parallelise

280
00:11:18.340 --> 00:11:19.450 
sections of code

281
00:11:21.760 --> 00:11:24.249 
onto multiple tasks in the way

282
00:11:24.250 --> 00:11:26.499 
the programmer said within

283
00:11:26.500 --> 00:11:28.719 
this short annotations, for example,

284
00:11:28.720 --> 00:11:31.299 
if you have a code block which does

285
00:11:31.300 --> 00:11:33.339 
a set of instructions, then you

286
00:11:33.340 --> 00:11:35.860 
can prefix that code block by pragma

287
00:11:36.880 --> 00:11:39.189 
omp parallel, and then the compiler

288
00:11:39.190 --> 00:11:41.199 
will generate a code such

289
00:11:41.200 --> 00:11:43.209 
that on

290
00:11:43.210 --> 00:11:45.639 
each thread this

291
00:11:45.640 --> 00:11:46.640 
single

292
00:11:47.900 --> 00:11:50.042 
block is executed

293
00:11:51.800 --> 00:11:53.899 
by each of the threads

294
00:11:53.900 --> 00:11:55.369 
and I mean actually by each of the

295
00:11:55.370 --> 00:11:56.929 
threads, they all do the same thing.

296
00:11:59.180 --> 00:12:01.169 
So, that can

297
00:12:01.170 --> 00:12:03.179 
be used for a lot of things,

298
00:12:03.180 --> 00:12:04.200 
but it's not maybe

299
00:12:05.430 --> 00:12:07.469 
the most frequently used

300
00:12:07.470 --> 00:12:09.239 
pragma that people use.

301
00:12:09.240 --> 00:12:11.099 
The most frequently used pragma would

302
00:12:11.100 --> 00:12:13.379 
be OpenMP parallel 4,

303
00:12:13.380 --> 00:12:14.669 
which means that you can,

304
00:12:16.110 --> 00:12:18.179 
which means that the compiler knows

305
00:12:18.180 --> 00:12:20.279 
what it sees at pragma that the

306
00:12:20.280 --> 00:12:22.379 
following for loop block

307
00:12:22.380 --> 00:12:24.720 
or loop block will be divided

308
00:12:25.860 --> 00:12:27.899 
between the

309
00:12:27.900 --> 00:12:29.909 
executing threads such as

310
00:12:29.910 --> 00:12:31.919 
the first, let's say,

311
00:12:31.920 --> 00:12:33.869 
five threads, get the first five

312
00:12:33.870 --> 00:12:36.539 
iterations and the next

313
00:12:36.540 --> 00:12:38.959 
five threads get the next 10 iterations

314
00:12:38.960 --> 00:12:41.219 
and so on. And there are different ways

315
00:12:41.220 --> 00:12:43.739 
of distributing which loops are

316
00:12:43.740 --> 00:12:46.649 
executed on which thread.

317
00:12:46.650 --> 00:12:49.199 
But what it doesn't do is

318
00:12:49.200 --> 00:12:51.239 
it doesn't do the task decomposition

319
00:12:51.240 --> 00:12:52.739 
for you in any way.

320
00:12:52.740 --> 00:12:53.999 
The compiler just can't do that.

321
00:12:54.000 --> 00:12:55.919 
It doesn't know from the code how to

322
00:12:55.920 --> 00:12:58.349 
generate this directed

323
00:12:58.350 --> 00:12:59.549 
acyclic graph.

324
00:12:59.550 --> 00:13:01.529 
And you can use but you

325
00:13:01.530 --> 00:13:03.449 
can use these pragmas to realize your

326
00:13:03.450 --> 00:13:05.459 
task decomposition.

327
00:13:05.460 --> 00:13:08.039 
OpenMP is also one of the most

328
00:13:08.040 --> 00:13:10.019 
successful languages for

329
00:13:10.020 --> 00:13:11.249 
doing multiprocessing.

330
00:13:12.540 --> 00:13:14.400 
It is an SMP system.

331
00:13:16.140 --> 00:13:18.389 
But in the past, people

332
00:13:18.390 --> 00:13:20.519 
have started to integrate

333
00:13:20.520 --> 00:13:21.677 
massively parallel

334
00:13:24.090 --> 00:13:25.934 
computer accelerators into OpenMP,

335
00:13:27.720 --> 00:13:29.999 
which need to

336
00:13:30.000 --> 00:13:32.039 
which in the past have

337
00:13:32.040 --> 00:13:34.019 
been it was

338
00:13:34.020 --> 00:13:36.209 
necessary to talk to them via a message

339
00:13:36.210 --> 00:13:37.049 
passing.

340
00:13:37.050 --> 00:13:38.050 
So,

341
00:13:39.210 --> 00:13:41.189 
for example, with GPUs, they're

342
00:13:41.190 --> 00:13:43.019 
attached to the host processor via

343
00:13:43.020 --> 00:13:45.299 
Express, PCI Express as a

344
00:13:45.300 --> 00:13:46.300 
message passing interface.

345
00:13:47.700 --> 00:13:49.559 
So the host processor needs to send the

346
00:13:49.560 --> 00:13:51.809 
request to the GPU and

347
00:13:51.810 --> 00:13:53.781 
then the GPU does its

348
00:13:53.782 --> 00:13:55.769 
task and sends the results

349
00:13:55.770 --> 00:13:56.770 
back to the host.

350
00:13:58.500 --> 00:14:00.569 
But this is kind of a conflict because

351
00:14:00.570 --> 00:14:03.329 
message passing and OpenMP, which is

352
00:14:03.330 --> 00:14:05.339 
a shared memory system, doesn't really

353
00:14:05.340 --> 00:14:06.509 
fit together.

354
00:14:06.510 --> 00:14:07.559 
But using this

355
00:14:09.750 --> 00:14:12.089 
pragma annotations

356
00:14:12.090 --> 00:14:14.309 
or compiler directives, it

357
00:14:14.310 --> 00:14:16.289 
is very easy for the compiler to

358
00:14:16.290 --> 00:14:18.539 
also hide that difficulty

359
00:14:18.540 --> 00:14:19.589 
away from him.

360
00:14:19.590 --> 00:14:21.629 
And the compiler will take care

361
00:14:21.630 --> 00:14:23.729 
of the complete communication

362
00:14:23.730 --> 00:14:25.289 
implicitly.

363
00:14:25.290 --> 00:14:26.290 
Still,

364
00:14:29.340 --> 00:14:31.559 
the user will have to take care of the

365
00:14:31.560 --> 00:14:33.539 
transfer of the data

366
00:14:33.540 --> 00:14:35.490 
by adding extra

367
00:14:37.800 --> 00:14:39.959 
compiler directives, and these

368
00:14:39.960 --> 00:14:42.149 
usually start with pragma

369
00:14:42.150 --> 00:14:45.209 
openMP device X, Y, Z

370
00:14:45.210 --> 00:14:47.219 
in order to discriminate the regular

371
00:14:47.220 --> 00:14:48.990 
SMP pragmas from the

372
00:14:50.280 --> 00:14:51.280 
device offload pragmas.

373
00:14:53.590 --> 00:14:54.590 
And then there's OpenACC,

374
00:14:56.170 --> 00:14:58.569 
which is also

375
00:14:58.570 --> 00:14:59.570 
a language.

376
00:15:02.390 --> 00:15:05.029 
Using this so-called

377
00:15:05.030 --> 00:15:07.009 
compiler directives approach,

378
00:15:07.010 --> 00:15:09.919 
but this time it's

379
00:15:09.920 --> 00:15:12.439 
accelerator driven

380
00:15:12.440 --> 00:15:14.449 
and it doesn't have to

381
00:15:14.450 --> 00:15:16.909 
do anything with the host processor

382
00:15:16.910 --> 00:15:19.279 
that is just hosting the

383
00:15:21.530 --> 00:15:22.530 
GPU device.

384
00:15:24.080 --> 00:15:25.849 
Which means that you can control

385
00:15:25.850 --> 00:15:27.949 
anything that the host does, but what

386
00:15:27.950 --> 00:15:29.989 
you can do is you can control from the

387
00:15:29.990 --> 00:15:31.729 
host what is being offloaded within

388
00:15:31.730 --> 00:15:33.859 
your program to the device, how

389
00:15:33.860 --> 00:15:36.259 
things are calculated and

390
00:15:36.260 --> 00:15:37.969 
how data is transferred back to the

391
00:15:37.970 --> 00:15:38.970 
host.

392
00:15:43.420 --> 00:15:44.889 
The compiler directives are very

393
00:15:44.890 --> 00:15:46.328 
similar to OpenMP,

394
00:15:47.560 --> 00:15:49.779 
as you can see on the right, they are

395
00:15:49.780 --> 00:15:52.059 
pretty much the same, just the.

396
00:15:53.820 --> 00:15:55.499 
The naming is a little bit different,

397
00:15:55.500 --> 00:15:56.970 
something that is

398
00:15:58.020 --> 00:16:00.539 
new or actually conceptually

399
00:16:00.540 --> 00:16:02.579 
different from OpenMP is that

400
00:16:02.580 --> 00:16:04.649 
there's a case

401
00:16:04.650 --> 00:16:06.709 
called pragma acc

402
00:16:06.710 --> 00:16:08.580 
kernels, and

403
00:16:11.360 --> 00:16:13.339 
that's a directive that allows

404
00:16:13.340 --> 00:16:14.389 
the compiler to

405
00:16:15.830 --> 00:16:19.039 
automatically infer what a potential

406
00:16:19.040 --> 00:16:21.649 
directed acyclic graph for the section

407
00:16:21.650 --> 00:16:23.479 
under inspection could potentially be

408
00:16:23.480 --> 00:16:25.519 
and come up with suggestions

409
00:16:25.520 --> 00:16:26.889 
and implement them.

410
00:16:26.890 --> 00:16:27.890 
And then the user

411
00:16:30.140 --> 00:16:31.639 
will receive a report on what has

412
00:16:31.640 --> 00:16:33.829 
parallelize and which way the user

413
00:16:33.830 --> 00:16:34.880 
can then use this

414
00:16:36.290 --> 00:16:38.179 
to either use it as it is or to do

415
00:16:39.470 --> 00:16:41.419 
improvements using

416
00:16:42.620 --> 00:16:44.222 
more involved

417
00:16:47.090 --> 00:16:48.853 
parallelization using multiple

418
00:16:51.440 --> 00:16:54.199 
separate pragma instructions.

419
00:16:54.200 --> 00:16:55.211 
One thing that's

420
00:16:57.750 --> 00:16:59.849 
nice is that OpenACC can

421
00:16:59.850 --> 00:17:01.289 
be used for multiple programming

422
00:17:01.290 --> 00:17:02.290 
languages

423
00:17:03.360 --> 00:17:05.519 
and that it's compatible with other

424
00:17:05.520 --> 00:17:07.769 
libraries that the same

425
00:17:07.770 --> 00:17:08.669 
vendor provides.

426
00:17:08.670 --> 00:17:10.769 
So there you can mix it with

427
00:17:10.770 --> 00:17:12.560 
libraries like cuBLAS, cuFFT,

428
00:17:14.890 --> 00:17:17.338 
cuSPARS matrix operations

429
00:17:17.339 --> 00:17:19.559 
and many more

430
00:17:19.560 --> 00:17:21.059 
since they come from the same vendor.

431
00:17:26.950 --> 00:17:28.599 
So, but

432
00:17:29.980 --> 00:17:32.139 
these are what I explained

433
00:17:32.140 --> 00:17:34.329 
are so far very parts, but

434
00:17:34.330 --> 00:17:36.369 
the thing is today you have to

435
00:17:36.370 --> 00:17:38.709 
use all of them in the same program

436
00:17:38.710 --> 00:17:40.359 
at the same time.

437
00:17:40.360 --> 00:17:43.029 
Today, HPC (high performance computing)

438
00:17:43.030 --> 00:17:45.969 
is hybrid and is what

439
00:17:45.970 --> 00:17:48.039 
programs have to use is a many-core

440
00:17:48.040 --> 00:17:49.480 
host that

441
00:17:50.890 --> 00:17:53.109 
eventually coherently attaches

442
00:17:53.110 --> 00:17:56.379 
a massively parallel accelerator

443
00:17:56.380 --> 00:17:57.380 
to massively accelerator

444
00:17:58.630 --> 00:18:00.609 
needs to either be programmed in OpenACC

445
00:18:00.610 --> 00:18:02.499 
or CUDA and many cohorts to be

446
00:18:02.500 --> 00:18:05.169 
programmed using openMP or Pthreads.

447
00:18:05.170 --> 00:18:07.659 
And that means that the typical app is

448
00:18:07.660 --> 00:18:09.759 
actually a very convoluted use of

449
00:18:09.760 --> 00:18:11.439 
all the models that are available.

450
00:18:13.180 --> 00:18:15.159 
And of course, in HPC

451
00:18:15.160 --> 00:18:16.539 
you have multiple hosts who will have

452
00:18:16.540 --> 00:18:18.699 
to use MPI as well.

453
00:18:18.700 --> 00:18:20.469 
So you pretty much need to use all of

454
00:18:20.470 --> 00:18:22.809 
what I have shown earlier.

455
00:18:25.060 --> 00:18:27.159 
There's a nice tutorial

456
00:18:27.160 --> 00:18:29.889 
around on the Web which

457
00:18:29.890 --> 00:18:32.109 
guides you through

458
00:18:32.110 --> 00:18:34.329 
six or seven steps

459
00:18:34.330 --> 00:18:36.309 
that show you how to do simple things

460
00:18:36.310 --> 00:18:38.469 
and OpenMP and

461
00:18:38.470 --> 00:18:40.509 
OpenACC and during the

462
00:18:40.510 --> 00:18:42.669 
steps where they quickly

463
00:18:42.670 --> 00:18:44.469 
become non-trivial and you will

464
00:18:44.470 --> 00:18:46.239 
actually have to do a lot of work to

465
00:18:46.240 --> 00:18:47.559 
parallelize the application and you

466
00:18:47.560 --> 00:18:49.989 
will actually realize what

467
00:18:49.990 --> 00:18:51.759 
are the typical problems and how does

468
00:18:51.760 --> 00:18:53.890 
it feel to to realize

469
00:18:54.940 --> 00:18:57.039 
task decompositions and in

470
00:18:57.040 --> 00:18:58.299 
implementing such using

471
00:18:59.530 --> 00:19:00.534 
OpenACC and OpenMP.

472
00:19:02.130 --> 00:19:03.690 
The next last thing is

473
00:19:05.280 --> 00:19:07.559 
CUDA. CUDA is great because it allows

474
00:19:07.560 --> 00:19:09.119 
the programmer to have the

475
00:19:12.830 --> 00:19:14.180 
greatest freedom in what

476
00:19:15.470 --> 00:19:17.629 
he can do with this massively

477
00:19:17.630 --> 00:19:18.770 
parallel architecture.

478
00:19:20.060 --> 00:19:22.099 
This is, as I mentioned earlier, an

479
00:19:22.100 --> 00:19:24.299 
extension to the C

480
00:19:24.300 --> 00:19:26.729 
and C++ programing languages.

481
00:19:26.730 --> 00:19:28.909 
It can also be used from Fortran

482
00:19:28.910 --> 00:19:30.979 
and Python, and it

483
00:19:30.980 --> 00:19:33.679 
exposes the complete GPU

484
00:19:33.680 --> 00:19:35.809 
to the programmer such that

485
00:19:35.810 --> 00:19:37.849 
he can make the best

486
00:19:37.850 --> 00:19:39.859 
use of all the resources that are that

487
00:19:39.860 --> 00:19:42.349 
are in this device.

488
00:19:42.350 --> 00:19:44.299 
CUDA has actually been short for

489
00:19:44.300 --> 00:19:46.519 
Compute Unified

490
00:19:46.520 --> 00:19:48.559 
Device Architecture, but they quickly

491
00:19:48.560 --> 00:19:50.239 
dropped this name because it was just,

492
00:19:50.240 --> 00:19:51.829 
you know, nobody could remember it.

493
00:19:57.440 --> 00:19:59.659 
It's actually a very small set

494
00:19:59.660 --> 00:20:03.169 
of extensions to C and C++

495
00:20:03.170 --> 00:20:05.389 
that allow you to

496
00:20:05.390 --> 00:20:07.010 
attach a GPU

497
00:20:08.510 --> 00:20:10.699 
that allows it to parallelise C and C++

498
00:20:10.700 --> 00:20:11.762 
applications onto GPUs.

499
00:20:14.030 --> 00:20:16.279 
So which means that most

500
00:20:16.280 --> 00:20:18.799 
of your regular code can stay

501
00:20:18.800 --> 00:20:20.839 
standard C, and just

502
00:20:20.840 --> 00:20:22.669 
from time to time, you will have to use

503
00:20:22.670 --> 00:20:24.659 
APIs to manage the devices to

504
00:20:24.660 --> 00:20:26.809 
the memory, the data transfers

505
00:20:26.810 --> 00:20:28.909 
and obviously also the kernels that

506
00:20:28.910 --> 00:20:30.799 
are run on the GPU.

507
00:20:33.650 --> 00:20:35.899 
CUDA has started out a long time

508
00:20:35.900 --> 00:20:38.269 
ago when the first GPUs

509
00:20:38.270 --> 00:20:40.309 
came on the market and at that point in

510
00:20:40.310 --> 00:20:42.919 
time there were no

511
00:20:42.920 --> 00:20:45.769 
shared memory accelerators available,

512
00:20:45.770 --> 00:20:48.019 
but today this has changed and with

513
00:20:48.020 --> 00:20:50.089 
NVLink it is now possible to

514
00:20:50.090 --> 00:20:52.669 
coherently couple accelerators

515
00:20:52.670 --> 00:20:53.810 
to the host processors,

516
00:20:55.190 --> 00:20:57.259 
which means that in principle,

517
00:20:57.260 --> 00:20:59.299 
you can now see the set of CPU

518
00:20:59.300 --> 00:21:01.549 
plus GPU as a single shared memory

519
00:21:01.550 --> 00:21:03.919 
system and CUDA also

520
00:21:03.920 --> 00:21:06.769 
now embraces this approach.

521
00:21:06.770 --> 00:21:08.929 
The latest version is

522
00:21:08.930 --> 00:21:11.089 
CUDA6 and 7, 8 and

523
00:21:11.090 --> 00:21:12.368 
9 such that the user can

524
00:21:14.470 --> 00:21:15.470 
remove

525
00:21:16.480 --> 00:21:18.729 
lots of his added

526
00:21:18.730 --> 00:21:20.500 
GPU code

527
00:21:21.550 --> 00:21:23.260 
such that the application looks,

528
00:21:24.360 --> 00:21:25.900 
application code feels much more,

529
00:21:27.010 --> 00:21:28.010 
well,

530
00:21:31.130 --> 00:21:32.130 
much more simple

531
00:21:34.280 --> 00:21:36.289 
like it, and I mean like it was in

532
00:21:36.290 --> 00:21:39.079 
the beginning before he added his extra

533
00:21:39.080 --> 00:21:41.119 
CUDA code to support the GPU.

534
00:21:44.070 --> 00:21:46.379 
And the last thing I want to talk today

535
00:21:46.380 --> 00:21:48.509 
is about Apache

536
00:21:48.510 --> 00:21:50.789 
Spark, which is an open-source

537
00:21:50.790 --> 00:21:52.619 
framework for cluster computing that

538
00:21:52.620 --> 00:21:54.599 
has been designed to

539
00:21:54.600 --> 00:21:56.579 
to cope with problems in

540
00:21:56.580 --> 00:21:57.580 
big data.

541
00:21:59.520 --> 00:22:00.520 
And

542
00:22:01.590 --> 00:22:04.049 
the big data set here

543
00:22:04.050 --> 00:22:06.149 
is being managed by an abstraction that

544
00:22:06.150 --> 00:22:08.189 
is called a Resilient Distributed

545
00:22:08.190 --> 00:22:10.379 
Dataset that is cared

546
00:22:10.380 --> 00:22:13.229 
about by the Spark runtime

547
00:22:13.230 --> 00:22:15.749 
such that the programmer just has to

548
00:22:15.750 --> 00:22:17.969 
specify rather high-level

549
00:22:19.470 --> 00:22:20.579 
requests.

550
00:22:20.580 --> 00:22:22.619 
And the parallelisation will then

551
00:22:22.620 --> 00:22:24.749 
be done

552
00:22:24.750 --> 00:22:26.969 
implicitly using the Spark

553
00:22:26.970 --> 00:22:28.320 
framework usage

554
00:22:29.560 --> 00:22:31.049 
message passing under the hood.

555
00:22:32.640 --> 00:22:34.199 
There are a lot of libraries that can

556
00:22:34.200 --> 00:22:36.239 
extend this basic

557
00:22:36.240 --> 00:22:38.579 
Spark core to the programmers'

558
00:22:38.580 --> 00:22:40.709 
needs, let it be for database's

559
00:22:40.710 --> 00:22:42.959 
likes Spark SQL or for machine

560
00:22:42.960 --> 00:22:45.569 
learning using MLlib or

561
00:22:45.570 --> 00:22:47.640 
for graph processing using

562
00:22:49.710 --> 00:22:50.710 
GraphML.

563
00:22:51.450 --> 00:22:53.759 
The user can use multiple programing

564
00:22:53.760 --> 00:22:56.009 
languages to attach to

565
00:22:56.010 --> 00:22:58.139 
Spark servers which

566
00:22:58.140 --> 00:22:59.999 
are themselves implemented using Scala,

567
00:23:00.000 --> 00:23:02.309 
which is a functional language

568
00:23:02.310 --> 00:23:03.569 
and only using this functional

569
00:23:03.570 --> 00:23:05.249 
language, it will be possible to

570
00:23:06.870 --> 00:23:07.870 
realize

571
00:23:11.530 --> 00:23:13.819 
involved and still

572
00:23:13.820 --> 00:23:14.820 
parallelizable

573
00:23:17.870 --> 00:23:20.089 
algorithms since only with

574
00:23:20.090 --> 00:23:22.549 
a functional approach of Scala compiler

575
00:23:22.550 --> 00:23:24.559 
or the executing runtime will be

576
00:23:24.560 --> 00:23:27.439 
able to decompose

577
00:23:27.440 --> 00:23:29.719 
to at least partly decompose

578
00:23:29.720 --> 00:23:30.949 
your code into a

579
00:23:33.860 --> 00:23:36.889 
highly efficient task decomposition.

580
00:23:36.890 --> 00:23:38.299 
It's currently very efficient on

581
00:23:38.300 --> 00:23:40.579 
many-core and cluster hardware,

582
00:23:40.580 --> 00:23:41.580 
since

583
00:23:44.690 --> 00:23:46.939 
Scala is intrinsically

584
00:23:46.940 --> 00:23:49.039 
data-parallel and not compute

585
00:23:49.040 --> 00:23:51.169 
parallel, which means that

586
00:23:51.170 --> 00:23:53.449 
typical GPUs, which suffer

587
00:23:53.450 --> 00:23:54.450 
from

588
00:23:56.750 --> 00:23:57.750 
divergence

589
00:23:58.880 --> 00:24:00.979 
will not

590
00:24:00.980 --> 00:24:03.199 
be the best thing to execute such

591
00:24:03.200 --> 00:24:04.200 
code.

592
00:24:05.270 --> 00:24:07.489 
That brings the question whether this

593
00:24:07.490 --> 00:24:08.899 
can be a framework that could be used

594
00:24:08.900 --> 00:24:10.789 
for massive parallelism?

595
00:24:10.790 --> 00:24:11.790 
Well,

596
00:24:12.950 --> 00:24:14.989 
that we will

597
00:24:14.990 --> 00:24:16.279 
see that in the future.

598
00:24:16.280 --> 00:24:18.289 
As as it seems right now,

599
00:24:18.290 --> 00:24:20.030 
massive parallel systems are mainly

600
00:24:22.280 --> 00:24:23.280 
driven by

601
00:24:24.480 --> 00:24:25.760 
GPU developments

602
00:24:27.920 --> 00:24:30.043 
and these GPUs are

603
00:24:31.270 --> 00:24:33.579 
in a sense, as I explained

604
00:24:33.580 --> 00:24:35.649 
in the first part of this talk,

605
00:24:35.650 --> 00:24:37.869 
mainly governed by weaker

606
00:24:37.870 --> 00:24:39.879 
execution units,

607
00:24:39.880 --> 00:24:41.499 
which doesn't really match together

608
00:24:41.500 --> 00:24:44.739 
with the way Spark is implemented,

609
00:24:44.740 --> 00:24:47.139 
where the granularity

610
00:24:47.140 --> 00:24:48.369 
isn't that fine.

611
00:24:52.030 --> 00:24:54.549 
So I talked a lot

612
00:24:54.550 --> 00:24:56.529 
of things in these two

613
00:24:56.530 --> 00:24:58.179 
presentations, and if you're curious,

614
00:24:58.180 --> 00:25:00.309 
you can go into these details and

615
00:25:00.310 --> 00:25:02.559 
do some further reading to understand

616
00:25:02.560 --> 00:25:04.599 
how it actually does feel when you

617
00:25:06.250 --> 00:25:08.589 
use these techniques that I described.
