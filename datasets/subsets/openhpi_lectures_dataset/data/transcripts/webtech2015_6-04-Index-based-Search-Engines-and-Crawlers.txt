WEBVTT

1
00:00:05.060 --> 00:00:07.900 
The most powerful search engines are index based.

2
00:00:08.850 --> 00:00:11.910 
So now we want to speak about index based search engines

3
00:00:12.350 --> 00:00:18.020 
and in this context we have to discuss also crawlers, see how they work,

4
00:00:18.240 --> 00:00:19.740 
crawlers which provide the

5
00:00:20.500 --> 00:00:24.040 
information, which collect the documents

6
00:00:24.490 --> 00:00:25.620 
for the search machine.

7
00:00:27.120 --> 00:00:30.650 
So, today the most popular search engines

8
00:00:32.220 --> 00:00:39.270 
are Google and Bing and these search machines both are index based.

9
00:00:40.170 --> 00:00:47.670 
Index based, so working automatically and having the most complete

10
00:00:48.100 --> 00:00:51.360 
collection of web document data.

11
00:00:52.580 --> 00:00:59.260 
An index, so the 'index based' comes from the index. Index is a data structure

12
00:00:59.770 --> 00:01:05.950 
to map descriptors to document. Descriptors that are keywords

13
00:01:05.950 --> 00:01:11.190 
that are relevant terms and later on the work of the search machine is

14
00:01:11.590 --> 00:01:14.760 
the search machine collects all documents,

15
00:01:15.220 --> 00:01:19.370 
collects as much document as possible from the web then the

16
00:01:19.370 --> 00:01:21.600 
documents are analyzed and

17
00:01:23.230 --> 00:01:26.610 
the result of the analysis of the web document is

18
00:01:26.730 --> 00:01:31.370 
that the document is described by a number of so called descriptors.

19
00:01:32.140 --> 00:01:35.760 
And now when a user comes and puts a query

20
00:01:36.350 --> 00:01:37.990 
into the search engine, the query

21
00:01:39.460 --> 00:01:41.650 
is matched with these descriptive

22
00:01:42.360 --> 00:01:45.470 
and the better the match is for a document,

23
00:01:46.120 --> 00:01:50.980 
the more likely it is that it is to be presented to the user.

24
00:01:52.260 --> 00:01:58.750 
Basic functions of such index based search engines are first data collection.

25
00:02:00.060 --> 00:02:05.800 
The search machine the better, the more data it has in its collection.

26
00:02:07.610 --> 00:02:11.630 
It needs a collection. So typically a search machine does not

27
00:02:11.630 --> 00:02:16.030 
start when the user put us the query does not start to

28
00:02:16.660 --> 00:02:21.470 
go through the internet and like to find pages that would

29
00:02:22.410 --> 00:02:28.630 
last much too long . So the search machine starts before user is

30
00:02:28.630 --> 00:02:31.990 
asking to collect data and to analyze data of the web

31
00:02:33.040 --> 00:02:36.370 
and this is a first phase the

32
00:02:37.110 --> 00:02:42.180 
search engine has to do, the more the larger the data collection is,

33
00:02:42.500 --> 00:02:46.030 
the better the search machine is able to

34
00:02:46.450 --> 00:02:53.690 
give relevant information to query a user. Then the second step, if the data in

35
00:02:53.810 --> 00:02:56.460 
the pages of the web documents are collected

36
00:02:56.930 --> 00:03:01.770 
the documents need to be analyzed and reviewed.

37
00:03:03.030 --> 00:03:11.260 
Third step is the index needs to be generated. That means the

38
00:03:11.650 --> 00:03:17.680 
description of the web pages and these descriptors this index

39
00:03:18.090 --> 00:03:24.380 
needs not only to be established, but needs also to be managed. And then

40
00:03:24.990 --> 00:03:30.030 
and this is a strong sense. This is the task of the search machine

41
00:03:30.190 --> 00:03:35.650 
is to answer queries. To answer queries and to give all the

42
00:03:36.670 --> 00:03:42.470 
relevant evaluation in the answer so that

43
00:03:42.960 --> 00:03:48.700 
the higher the relevance of a document is, the better this document should fit

44
00:03:48.910 --> 00:03:50.140 
to the query of the user.

45
00:03:51.220 --> 00:03:56.230 
Let's start to consider the first step. Let's start to have

46
00:03:56.230 --> 00:03:57.690 
a look to data collection.

47
00:03:58.590 --> 00:04:02.190 
How search machines collect data.

48
00:04:03.120 --> 00:04:06.150 
So the first question we have to ask what kind of data

49
00:04:06.580 --> 00:04:11.990 
because there is a large diversity of data in the web. There are

50
00:04:13.510 --> 00:04:19.910 
static html pages. But there are and become more and more important

51
00:04:20.060 --> 00:04:23.390 
dynamically generated html document.

52
00:04:24.090 --> 00:04:25.520 
We have spoken about  the

53
00:04:26.250 --> 00:04:30.850 
documents that are generated to fit exactly the wishes

54
00:04:31.370 --> 00:04:35.170 
of the user. There are multimedia data.

55
00:04:35.970 --> 00:04:41.860 
There are text, there are images, photos, there are audio, data, there are video files,

56
00:04:42.010 --> 00:04:44.460 
all different types of data

57
00:04:45.120 --> 00:04:50.190 
that have to be differently treated in the face of analysis. There

58
00:04:51.580 --> 00:04:57.820 
are data in different formats, for example postscript, pdf, word, powerpoint,

59
00:04:58.110 --> 00:05:03.260 
to mention only a few of the popular formats that are available in the web.

60
00:05:04.240 --> 00:05:08.100 
There is source code or source code inside a

61
00:05:09.170 --> 00:05:12.630 
webpage for example for a client that application

62
00:05:14.470 --> 00:05:20.210 
and many many more. So, first question one has to answer how to deal

63
00:05:20.410 --> 00:05:23.900 
with the diversity of data ,data types in the web.

64
00:05:25.390 --> 00:05:30.900 
But it is another question connected to the problem of collecting data

65
00:05:32.010 --> 00:05:35.760 
for a search machine. When the data should be selected.

66
00:05:36.350 --> 00:05:42.750 
The web lives. So in every moment new data new pages are published in the web.

67
00:05:43.330 --> 00:05:46.570 
Old pages are modified.

68
00:05:47.390 --> 00:05:52.410 
Pages are withdrawn from the web. So data and documents in the

69
00:05:52.410 --> 00:05:57.480 
knowledge space of a web usually have only a short lifetime.

70
00:05:58.380 --> 00:06:01.700 
There are different types, for example if you consider

71
00:06:02.740 --> 00:06:04.620 
web pages that provide news

72
00:06:05.220 --> 00:06:07.820 
its very short lifetime. If you

73
00:06:09.080 --> 00:06:16.350 
have a longer lifetime. So the machine has to deal with this aspect.

74
00:06:17.840 --> 00:06:25.200 
The pages are subject to constant change. So if a machine once

75
00:06:26.860 --> 00:06:29.610 
took a web document into its collection

76
00:06:30.310 --> 00:06:33.150 
and start to analyze this document,

77
00:06:33.990 --> 00:06:39.330 
the result is not true forever, because this document in the web

78
00:06:39.550 --> 00:06:44.280 
can be changed. So if the user puts a query and the search machine

79
00:06:44.280 --> 00:06:46.180 
provides the old page,

80
00:06:46.930 --> 00:06:51.410 
the user is unhappy, the search machine is not doing a good job.

81
00:06:52.830 --> 00:06:58.270 
The documents in the web are interconnected with each other. They are

82
00:06:58.490 --> 00:07:03.900 
connected via links and its possible if there is one document

83
00:07:03.920 --> 00:07:05.810 
with a link to another document,

84
00:07:06.440 --> 00:07:10.260 
that the other document is changed or the other document is withdrawn.

85
00:07:10.370 --> 00:07:14.870 
Then the link goes into the empty and

86
00:07:15.500 --> 00:07:18.360 
failure, an error message is sent back.

87
00:07:18.970 --> 00:07:23.510 
So you see it's problematic to discuss when

88
00:07:23.950 --> 00:07:25.560 
data need to be collected.

89
00:07:26.190 --> 00:07:29.480 
It is clear that the captured database

90
00:07:30.430 --> 00:07:35.500 
the captured data collection of the search machine must be maintained

91
00:07:35.900 --> 00:07:38.360 
at periodic intervals.

92
00:07:39.040 --> 00:07:40.780 
One has to think about the

93
00:07:42.890 --> 00:07:46.430 
size of the periodic intervals.

94
00:07:47.900 --> 00:07:50.590 
It can depend on the type of the page

95
00:07:51.020 --> 00:07:55.490 
but on the other side there are so many information in the web

96
00:07:56.020 --> 00:08:01.020 
if the search machine has not only to collect the data once,

97
00:08:01.410 --> 00:08:05.380 
if the search machine has to do this periodically you see

98
00:08:05.380 --> 00:08:09.090 
it's a big problem and a big challenge.

99
00:08:10.460 --> 00:08:14.960 
Speaking about data collection, we have also to discuss how

100
00:08:14.960 --> 00:08:18.630 
this data can be collected. So the data from the web

101
00:08:19.370 --> 00:08:24.690 
in the index based search machine are collected by means of special

102
00:08:25.110 --> 00:08:27.950 
autonomous working software tools.

103
00:08:28.600 --> 00:08:33.560 
And these as the name of these software tools are web robots

104
00:08:33.610 --> 00:08:37.040 
or robots or web crawler or crawler.

105
00:08:38.070 --> 00:08:40.840 
These are very specialized software, these

106
00:08:41.520 --> 00:08:46.970 
autonomously walk through the world wide web. Walk means it follows

107
00:08:47.480 --> 00:08:52.750 
from, it stops from one document to another following the links

108
00:08:52.890 --> 00:08:57.390 
inside the document and collecting the data

109
00:08:58.440 --> 00:09:03.280 
found in this way. So we have our crawler our robot

110
00:09:03.780 --> 00:09:09.050 
and he is walking through the web. The web here is shown as a graph.

111
00:09:09.240 --> 00:09:13.410 
These are the documents and the connection between two documents

112
00:09:14.160 --> 00:09:16.370 
are provided by links. So if

113
00:09:17.310 --> 00:09:20.990 
the software is investigating this document,

114
00:09:21.620 --> 00:09:26.890 
then it can analyze in which places are the links and the links

115
00:09:27.950 --> 00:09:32.320 
behind the links there are urls of other documents and the

116
00:09:32.320 --> 00:09:34.280 
machine can continue to

117
00:09:34.960 --> 00:09:39.140 
to visit and collect this document, can go and

118
00:09:39.690 --> 00:09:44.020 
visit and collect this document and in this way

119
00:09:44.580 --> 00:09:48.480 
the web can be traversed by such a software tool.

120
00:09:49.900 --> 00:09:54.540 
So the crawler can automatically find web pages and documents in the web

121
00:09:55.420 --> 00:09:59.780 
simply by following the urls behind the links

122
00:10:00.360 --> 00:10:03.840 
inside the visited document.

123
00:10:05.710 --> 00:10:13.030 
The crawler moves in this way through the web and collects

124
00:10:13.130 --> 00:10:14.980 
documents. And the documents

125
00:10:15.820 --> 00:10:21.780 
the crawler found are stored in a database typically a distributed database.

126
00:10:23.480 --> 00:10:26.370 
The documents that are already stored in the database

127
00:10:26.830 --> 00:10:31.900 
must be periodically checked, must be periodically checked for consistency

128
00:10:32.250 --> 00:10:37.040 
for changes or for deletion. We discussed this already.

129
00:10:38.560 --> 00:10:43.060 
Let's have a closer look how such crawler are working. Crawler

130
00:10:43.290 --> 00:10:47.460 
that collect data, that collect web documents for search machine.

131
00:10:49.780 --> 00:10:56.450 
There is starting initialized queue, and in this queue there are urls.

132
00:10:56.970 --> 00:11:00.540 
Typically the else are randomly chosen.

133
00:11:01.500 --> 00:11:03.840 
Then the crawler

134
00:11:07.240 --> 00:11:12.960 
request this document and analyze this document and found urls

135
00:11:12.960 --> 00:11:15.710 
in this document. And then

136
00:11:17.980 --> 00:11:24.150 
the urls the crawler found in the loaded document are placed in the queue.

137
00:11:25.490 --> 00:11:27.360 
And the queue that

138
00:11:28.480 --> 00:11:30.310 
gives the crawler the

139
00:11:31.320 --> 00:11:36.790 
goals for where he has to load the next document.

140
00:11:38.010 --> 00:11:43.700 
So it is important if a document is loaded to find all the

141
00:11:43.700 --> 00:11:50.540 
hyperlinks in that document and then to go step by step to

142
00:11:51.060 --> 00:11:58.420 
the urls and investigating, analyzing see a document with this url.

143
00:12:00.330 --> 00:12:06.980 
And then the document has to be analyzed and has to be saved in a database

144
00:12:07.390 --> 00:12:09.580 
and then this has to be done

145
00:12:10.510 --> 00:12:11.440 


146
00:12:12.930 --> 00:12:17.160 
the crawler has to take the next url and has to follow the cycle

147
00:12:17.670 --> 00:12:23.810 
to crawl as much as possible through the documents placed in the web.

148
00:12:24.450 --> 00:12:29.030 
So the simple idea is the autonomous software, this web crawler,

149
00:12:29.310 --> 00:12:37.270 
they follow different urls and the urls are received out of the documents

150
00:12:37.390 --> 00:12:39.090 
that are visited.

151
00:12:41.860 --> 00:12:44.360 
Of course the crawler works

152
00:12:44.970 --> 00:12:50.500 
in the client server following the client server principle. We have here

153
00:12:50.760 --> 00:12:54.940 
the crawler and the web crawler wants to

154
00:12:55.620 --> 00:13:00.480 
investigate, wants to visit a new resource in the web, a new document.

155
00:13:01.000 --> 00:13:04.350 
Then the server follows a url

156
00:13:05.680 --> 00:13:11.010 
sends a request. It as an http request, the hypertext transfer request

157
00:13:11.180 --> 00:13:13.660 
simply as we know this from the communication

158
00:13:14.370 --> 00:13:17.480 


159
00:13:17.480 --> 00:13:19.640 
protocol in the world wide web. The

160
00:13:20.540 --> 00:13:23.900 
request the document with the document which is

161
00:13:24.380 --> 00:13:29.550 
from the server, the document is placed and then the server

162
00:13:29.550 --> 00:13:34.750 
sends back the document within an http response and then the

163
00:13:34.750 --> 00:13:40.890 
crawler can start to analyze this document to extract the urls in that document

164
00:13:41.060 --> 00:13:43.610 
and then go in the next cycle

165
00:13:44.390 --> 00:13:48.230 
eventually to another server to receive a new document.

166
00:13:49.920 --> 00:13:54.190 
so this was the first step, the first phase of an index based search machine

167
00:13:54.470 --> 00:13:56.180 
how to collect data.

168
00:13:57.430 --> 00:14:03.760 
Now the data the found web pages need to be analyzed and need to be evaluated.

169
00:14:04.510 --> 00:14:05.560 
The content

170
00:14:06.880 --> 00:14:11.190 
the search machine has to find out what is the content of the document

171
00:14:11.880 --> 00:14:17.630 
and also here this needs to be done automatically. So software tools

172
00:14:17.840 --> 00:14:20.940 
analyze and evaluate a document's content.

173
00:14:21.500 --> 00:14:28.880 
And this software tools are so called information retrieval systems.

174
00:14:29.570 --> 00:14:31.870 
Information retrieval systems are

175
00:14:32.540 --> 00:14:36.380 
developed and investigated in computer science, they do not

176
00:14:36.380 --> 00:14:39.960 
only play a role in the web but also in other areas and the

177
00:14:39.960 --> 00:14:45.150 
idea is how to get out information of a huge amount of data.

178
00:14:46.440 --> 00:14:50.240 
So the information retrieval system now

179
00:14:50.760 --> 00:14:57.620 
has to identify the key content in the examined document.

180
00:14:58.430 --> 00:15:03.380 
It has to describe the document by means of the descriptors,

181
00:15:04.200 --> 00:15:08.780 
to make it easier for the later for the search machine, to match

182
00:15:09.160 --> 00:15:14.740 
a query to find out what are the most relevant documents in the data collection

183
00:15:15.240 --> 00:15:18.490 
that match a query of a customer.

184
00:15:19.840 --> 00:15:24.100 
So the analyzed documents are stored in a database and they

185
00:15:24.110 --> 00:15:29.230 
are stored according to the identified key contents, the keywords.

186
00:15:31.560 --> 00:15:36.220 
The individual documents are weighted by their relevance.

187
00:15:36.880 --> 00:15:40.080 
Later we will see how this relevance can be

188
00:15:40.570 --> 00:15:44.370 
computed and how this can be done by software automatically.

189
00:15:45.300 --> 00:15:50.090 
So the methods for creating and searching such a database are

190
00:15:50.090 --> 00:15:52.170 
referred to as indexing

191
00:15:52.920 --> 00:15:58.700 
and this is where the index based search machine have their name from.

192
00:16:00.630 --> 00:16:06.540 
Now, next step. Next task for a search machine to answer queries.

193
00:16:07.770 --> 00:16:11.980 
So, search is done. You notice by a user by

194
00:16:12.940 --> 00:16:18.330 
entering one or more search terms. Search terms are query or queries

195
00:16:19.620 --> 00:16:23.030 
and now the search machine has to

196
00:16:23.680 --> 00:16:27.840 
fulfill the task to automatically compute

197
00:16:28.260 --> 00:16:34.420 
the similarity between the query and between the documents in the database.

198
00:16:35.780 --> 00:16:40.780 
You remember the documents in the database, we have already characterized by

199
00:16:41.430 --> 00:16:46.320 
descriptors. Now what needs to be done is to find out the comparison

200
00:16:46.760 --> 00:16:52.040 
to compute the comparison between the query notion and the description notions.

201
00:16:53.120 --> 00:16:58.020 
And the better, the higher the similarity is

202
00:16:58.520 --> 00:17:04.290 
the more important it is for the search machine to send this

203
00:17:04.290 --> 00:17:09.500 
document, to provide this document as an answer to the query of the user.

204
00:17:11.390 --> 00:17:16.690 
The selection of the results or resulting documents that is

205
00:17:16.690 --> 00:17:20.940 
performed by so called query processors

206
00:17:21.430 --> 00:17:25.670 
and if one looks in more detail such query processors in the

207
00:17:26.670 --> 00:17:30.720 
stronger sense that are actually the search engines.

208
00:17:32.110 --> 00:17:36.620 
So found documents then after in this by this very processor

209
00:17:36.710 --> 00:17:39.160 
are presented to the user. You know

210
00:17:39.810 --> 00:17:44.630 
the google interface. Here is the keyword. The user query is given in

211
00:17:44.990 --> 00:17:51.320 
and then you get number of documents. I put in the notion openHPI

212
00:17:52.630 --> 00:17:58.380 
and then we got number of documents which are important in

213
00:17:58.380 --> 00:18:04.950 
the context of openHPI which explain what opeHPI is and the results

214
00:18:05.150 --> 00:18:12.350 
are presented in a list sequence and the machine likes to present

215
00:18:13.150 --> 00:18:17.770 
the most, the first result should be the most relevant,

216
00:18:17.950 --> 00:18:23.650 
second the second most relevant document. But here are not humans

217
00:18:23.650 --> 00:18:28.010 
behind it. This is all done by software all done by

218
00:18:28.540 --> 00:18:29.650 
automatic tools.

219
00:18:32.760 --> 00:18:37.560 
What is the advantage of this index based search machines? Of course

220
00:18:38.050 --> 00:18:42.860 
the data collection. The larger the collection is the

221
00:18:44.390 --> 00:18:50.560 
better is the possibility that the search machine is able to answer

222
00:18:50.690 --> 00:18:55.660 
to provide for a query really relevant information.

223
00:18:56.590 --> 00:19:02.070 
Sos automatic data collection by means of crawlers although the efficient creation

224
00:19:02.420 --> 00:19:07.120 
of a huge corpus of a largely up to date and complete data set.

225
00:19:07.730 --> 00:19:12.190 
Compared for example with web catalogs where human editors

226
00:19:12.850 --> 00:19:17.470 
are performing this analysis and evaluation of the data,

227
00:19:18.530 --> 00:19:24.040 
this is much these index based automatic approach is much more powerful

228
00:19:24.280 --> 00:19:28.300 
and able to deal with much more documents than

229
00:19:28.750 --> 00:19:30.190 
other types of machines.

230
00:19:30.950 --> 00:19:36.330 
But there are also some disadvantage. And of course the disadvantage depends

231
00:19:36.490 --> 00:19:39.860 
on the fact that not human editors

232
00:19:41.020 --> 00:19:45.540 
analyze and evaluate the document and the relevance of a document.

233
00:19:45.970 --> 00:19:49.910 
It is done automatically so the accuracy

234
00:19:50.410 --> 00:19:53.980 
depends on the algorithms that are used.

235
00:19:55.810 --> 00:20:00.440 
It is astonishing how powerful that search algorithms are.

236
00:20:01.330 --> 00:20:07.250 
I guess you are in most case satisfied with the results you get when you

237
00:20:07.890 --> 00:20:09.270 
put a query to google.

238
00:20:10.980 --> 00:20:15.440 
This is an index based machine, but sometimes you see

239
00:20:15.930 --> 00:20:20.070 
the selection was done, the evaluation was done by a machine

240
00:20:20.230 --> 00:20:21.490 
and not by a human.

241
00:20:22.990 --> 00:20:26.640 
So the analyzes and the evaluation of documents are done by

242
00:20:26.640 --> 00:20:30.460 
machines and not by humans, is a relevance

243
00:20:31.520 --> 00:20:33.390 
evaluation that means

244
00:20:34.610 --> 00:20:39.100 
is a document more important to show if a user asks the query, then

245
00:20:39.110 --> 00:20:44.740 
to decide which of the documents are most relevant to this query.

246
00:20:45.420 --> 00:20:49.950 
All this is done by a algorithms, is done by machines.

247
00:20:50.470 --> 00:20:57.210 
So humans often are better able to make such a evaluation. So

248
00:20:57.210 --> 00:21:01.340 
one can diseases as a disadvantage but also here we see in practice

249
00:21:01.870 --> 00:21:03.210 
that it works quite well.

250
00:21:03.940 --> 00:21:07.390 
Then this automatic relevance assessment

251
00:21:07.810 --> 00:21:12.090 
that can lead to qualitative inferior results.

252
00:21:12.760 --> 00:21:17.020 
Later in this week when we speak about semantic web we will

253
00:21:17.020 --> 00:21:23.600 
see in which points this automatic approach particularly is weak.
