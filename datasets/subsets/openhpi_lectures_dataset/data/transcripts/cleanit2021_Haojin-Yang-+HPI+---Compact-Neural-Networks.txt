WEBVTT

1
00:00:00.980 --> 00:00:04.650 
Hello my name is Haojin Yang,
I'm a senior researcher from

2
00:00:04.650 --> 00:00:07.310 
the chair of Internet
Technology and Systems.

3
00:00:07.760 --> 00:00:12.070 
Today I would like to introduce the
topic of Deep Model Compression

4
00:00:12.360 --> 00:00:13.860 
and Compact
Network Design.

5
00:00:16.730 --> 00:00:19.330 
The current success
of AI technology

6
00:00:20.070 --> 00:00:22.980 
can be traced back
to the year 2006.

7
00:00:23.680 --> 00:00:25.070 
The deep neural
networks

8
00:00:26.360 --> 00:00:30.510 
achieved human-level performance so
far in some recognition tasks.

9
00:00:31.170 --> 00:00:35.430 
So first the speech recognition
and then the image recognition.

10
00:00:35.950 --> 00:00:39.410 
After that more and
more AI applications

11
00:00:39.870 --> 00:00:45.210 
have been developed, for instance beating
a human in strategy games like Go

12
00:00:45.750 --> 00:00:50.410 
or the chatbots - they are the
regular assistance system for

13
00:00:50.410 --> 00:00:53.190 
the social communication
and retail systems.

14
00:00:54.120 --> 00:00:59.700 
The practicality of machine translation
is far greater than before.

15
00:01:00.490 --> 00:01:05.290 
Autonomous driving and the machine
learning best disease classification

16
00:01:05.440 --> 00:01:07.810 
having gotten so much
attention like now.

17
00:01:11.990 --> 00:01:18.450 
With unprecedented opportunities, AI also
faces many challenges. While actually

18
00:01:18.670 --> 00:01:21.460 
AI computation has entered
the second half.

19
00:01:22.570 --> 00:01:24.050 
How to better implement

20
00:01:25.860 --> 00:01:29.710 
and adapt the AI technology to
the application scenarios has

21
00:01:29.710 --> 00:01:34.740 
become the focus of the industry,
and how to make AI beyond just

22
00:01:34.740 --> 00:01:39.290 
an excellent game player but
further become an inclusive

23
00:01:39.300 --> 00:01:43.530 
technology in our daily life, which
is actually more important.

24
00:01:44.540 --> 00:01:47.140 
For the research
community, we should

25
00:01:47.830 --> 00:01:51.630 
continue to make breakthroughs
and fulfil the promises, for

26
00:01:51.630 --> 00:01:53.830 
example, in the
autonomous driving

27
00:01:54.650 --> 00:01:57.580 
which received a lot of
attention and investment.

28
00:01:58.980 --> 00:02:03.310 
On the other hand, AI computing
brings more carbon emissions.

29
00:02:04.230 --> 00:02:08.330 
For instance the most recent google
model the switch transformer

30
00:02:08.330 --> 00:02:11.880 
model has 1600
billion parameters

31
00:02:12.630 --> 00:02:16.960 
and training such a model requires
thousands of GPU's working on for

32
00:02:17.470 --> 00:02:21.490 
several weeks and it costs hundreds
of thousands of US Dollars.

33
00:02:22.870 --> 00:02:27.260 
The carbon emission resulting in the
training process is equivalent

34
00:02:27.270 --> 00:02:31.900 
to a large number of round-trip
flights from San Francisco

35
00:02:32.090 --> 00:02:37.500 
to New York city. Therefore it is
especially important to improve

36
00:02:37.500 --> 00:02:41.870 
the efficiency of AI models.
And we should reduce

37
00:02:42.290 --> 00:02:45.810 
or even decouple the strong
dependence on the high performance

38
00:02:45.810 --> 00:02:46.830 
computing hardware.

39
00:02:51.470 --> 00:02:56.940 
Deep-learning owes much of its success
to the ability to train large models

40
00:02:57.150 --> 00:03:00.450 
by leveraging data sets with
ever-increasing size.

41
00:03:01.920 --> 00:03:06.560 
The best performing models from
computer vision and ARP applications

42
00:03:07.100 --> 00:03:12.080 
tend to have tens to hundreds of layers
and hundreds of millions of parameters.

43
00:03:13.390 --> 00:03:19.380 
However, recently an increasing number of
applications, including autonomous driving,

44
00:03:19.530 --> 00:03:23.870 
assistant app, and surveillance app
demand machine learning models which

45
00:03:23.980 --> 00:03:28.180 
can also be deployed on the low
power and low resource devices,

46
00:03:29.010 --> 00:03:32.220 
which usually have strong
latency requirements.

47
00:03:33.290 --> 00:03:37.120 
Therefore in such applications
the notion model compression

48
00:03:37.350 --> 00:03:39.040 
has gained popularity.

49
00:03:41.370 --> 00:03:44.690 
The goal of model compression
is very simple,

50
00:03:45.160 --> 00:03:49.860 
simply to take a large reference
neural network and output a smaller

51
00:03:50.060 --> 00:03:55.480 
less expensive compressed network that is
functionally equivalent to the reference.

52
00:03:56.770 --> 00:04:01.830 
Commonly used technologies for model
compression includes compact network design

53
00:04:02.340 --> 00:04:06.340 
that is to design more efficient
network architecture.

54
00:04:07.080 --> 00:04:11.630 
Knowledge desolation which introduces
a cumbersome teacher model

55
00:04:11.820 --> 00:04:17.470 
and a more efficient compact student
model. The training of the student model

56
00:04:17.770 --> 00:04:21.670 
can be done based on the transfer of
knowledge extracted from the teacher.

57
00:04:22.770 --> 00:04:27.180 
Pruning techniques are intended
to remove the neurons

58
00:04:27.800 --> 00:04:31.460 
which relative weekly waited
for the final predictions.

59
00:04:32.120 --> 00:04:36.180 
We can also apply quantization
and binarization techniques

60
00:04:36.410 --> 00:04:39.960 
on the model parameters to further
reduce the memory and the

61
00:04:39.960 --> 00:04:41.510 
computation overhead.

62
00:04:46.570 --> 00:04:51.330 
Image classification models serve as the
cornerstone for the current AI wave,

63
00:04:52.250 --> 00:04:57.410 
from the menu design - designing the
network like AlexNet and ResNet,

64
00:04:58.250 --> 00:05:01.220 
into the very recent neural
architecture search.

65
00:05:02.660 --> 00:05:06.560 
A large number of various neural
network architectures have evolved

66
00:05:06.560 --> 00:05:10.050 
in the past few years, just like
tony stark's armoured suits.

67
00:05:11.020 --> 00:05:14.360 
Here we can see that the
MobileNet version 3 from google

68
00:05:14.590 --> 00:05:19.810 
is probably the most popular backbone
model used in mobile AI applications,

69
00:05:20.880 --> 00:05:25.990 
and it combines the handcrafted
design of the basic building blocks

70
00:05:26.270 --> 00:05:28.290 
and the automatic
search function.

71
00:05:32.250 --> 00:05:35.320 
This slide shows the trends
of the deep model design.

72
00:05:36.420 --> 00:05:41.950 
The histogram shows the computation
complexity of the deep model, for example,

73
00:05:42.100 --> 00:05:47.040 
for simple inference a
ResNet 152 model requires

74
00:05:47.040 --> 00:05:52.060 
3870 million multiple
accumulative operations.

75
00:05:53.100 --> 00:05:55.020 
And we can see
the line chart

76
00:05:55.900 --> 00:05:58.980 
shows the top one classification
accuracy of imagenets,

77
00:05:59.870 --> 00:06:02.860 
and from the year
2015 to 2019

78
00:06:03.590 --> 00:06:09.040 
the decrease of model complexity is
already more than an order of magnitude

79
00:06:09.630 --> 00:06:12.230 
and accuracy -
classification accuracy

80
00:06:12.860 --> 00:06:14.180 
is maintained
very well.

81
00:06:15.360 --> 00:06:18.590 
We predict that this
trend will continue

82
00:06:20.780 --> 00:06:23.430 
and the complexity will
be further reduced

83
00:06:23.970 --> 00:06:25.620 
in the future
several years.

84
00:06:29.830 --> 00:06:34.060 
Back to the current compact network
which is based on the handcrafted

85
00:06:34.060 --> 00:06:39.490 
design of basic blocks incorporated with the
whole network architecture construction.

86
00:06:40.340 --> 00:06:44.010 
On the left side of this slide,
I demonstrate several designs

87
00:06:44.010 --> 00:06:49.110 
of state-of-the-art networks which all
consist of different convolution

88
00:06:49.320 --> 00:06:50.930 
layers with different

89
00:06:51.570 --> 00:06:53.180 
nonlinear activation
function

90
00:06:54.100 --> 00:06:56.150 
being constructed just
in a different way.

91
00:06:56.770 --> 00:07:00.660 
And on the right side, the whole
network architecture is often

92
00:07:00.660 --> 00:07:04.460 
based on several stages which are
represented by different colors.

93
00:07:05.220 --> 00:07:10.150 
Each of those stages, in turn, consists
of several repeating basic blocks,

94
00:07:10.850 --> 00:07:15.430 
and we can see that here there are a
lot of configuration possibilities,

95
00:07:16.530 --> 00:07:18.830 
for example, the
configuration factor like

96
00:07:19.330 --> 00:07:22.590 
size of the convolution - kernel
size of the convolution layer

97
00:07:22.600 --> 00:07:26.690 
in a certain stage and the number
of filters filter channels,

98
00:07:27.210 --> 00:07:30.690 
how many blocks should be used in
a certain stage and how many

99
00:07:30.690 --> 00:07:35.120 
states should be used in total
and which kind of nonlinear

100
00:07:35.120 --> 00:07:38.440 
activation function should be used
for a certain convolution layer

101
00:07:38.830 --> 00:07:40.340 
and so on and so forth.

102
00:07:41.700 --> 00:07:45.410 
All of those configuration
parameters can be determined

103
00:07:45.830 --> 00:07:50.690 
by using our experience by hand or
we can apply the computation power

104
00:07:50.890 --> 00:07:52.720 
with automatic
search method.

105
00:07:54.890 --> 00:07:59.070 
Neural architecture search has
proved to exceed the performance

106
00:07:59.390 --> 00:08:02.220 
of the manual designed
network architecture.

107
00:08:03.680 --> 00:08:07.550 
The afore-mentioned needs a large
amount of configurations. We could

108
00:08:07.550 --> 00:08:12.990 
apply them to build a search space and
we pick an architecture from the space

109
00:08:13.670 --> 00:08:17.680 
according to a search principle
and we can further train it

110
00:08:18.120 --> 00:08:23.140 
and evaluate according to a metric.
The commonly used metric includes

111
00:08:23.530 --> 00:08:27.680 
inference speed, model accuracy,
and computation complexity.

112
00:08:28.760 --> 00:08:32.260 
The evaluation result will
return to the search strategy

113
00:08:32.450 --> 00:08:36.910 
and we'll start the next round
of search process and pick a

114
00:08:37.740 --> 00:08:43.990 
next architecture based on our response
from the performance evaluation.

115
00:08:44.660 --> 00:08:48.040 
And the process repeats until we
can reach a stop condition.

116
00:08:48.720 --> 00:08:53.590 
Either we can find an architecture
satisfying all the performance evaluation

117
00:08:53.900 --> 00:08:57.170 
requirements or the search
process just fails.

118
00:08:59.850 --> 00:09:04.610 
State-of-the-art compact network are
almost all based on the combination

119
00:09:04.820 --> 00:09:08.510 
of handcrafted block design and
neural architecture search.

120
00:09:09.180 --> 00:09:12.990 
In the recent 3 years, there's
been a lot of research effort

121
00:09:13.230 --> 00:09:17.580 
made on this topic and of
course, there are some

122
00:09:17.700 --> 00:09:22.360 
very promising research directions. For
example, the hardware aware search,

123
00:09:22.830 --> 00:09:27.170 
it means that if you place the
most computation expensive part

124
00:09:27.230 --> 00:09:33.490 
like training and searching on a cloud by leveraging
the high-performance computation hardware

125
00:09:34.040 --> 00:09:37.410 
and perform the evaluation
process directly on the

126
00:09:38.080 --> 00:09:42.440 
target hardware, for example, handheld
devices or the mobile phone.

127
00:09:44.850 --> 00:09:48.910 
Another issue is that the
computation resource often fail to

128
00:09:48.970 --> 00:09:51.040 
explore a very large
search space.

129
00:09:52.530 --> 00:09:57.520 
To deal with that, one
possible direction is to

130
00:09:57.520 --> 00:10:01.930 
apply a proxy metric for the neural
architecture search, it means

131
00:10:01.930 --> 00:10:06.360 
that we will replace the model
accuracy by this proxy metric.

132
00:10:07.810 --> 00:10:11.530 
We probably don't need to train the
model completely or we even don't,

133
00:10:11.740 --> 00:10:13.640 
probably don't need to
train the model at all.

134
00:10:14.660 --> 00:10:17.840 
For example, we can apply
the convergence speed

135
00:10:18.610 --> 00:10:21.770 
to measure the quality of the
neural network architecture

136
00:10:22.250 --> 00:10:24.970 
where they proved to be
positively related.

137
00:10:27.190 --> 00:10:33.040 
Okay these are the most relevant
references to this video and thank

138
00:10:33.040 --> 00:10:33.740 
you for watching.
