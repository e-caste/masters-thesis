WEBVTT

1
00:00:00.840 --> 00:00:02.060 
Hello and welcome.

2
00:00:04.540 --> 00:00:12.810 
The limitation of perceptron for nonlinear problems has been overcome by adding a nonlinear activation function to the neuron

3
00:00:12.810 --> 00:00:13.359 
layer.

4
00:00:14.339 --> 00:00:18.609 
This figure depicts a commonly used nonlinear activation function.

5
00:00:18.879 --> 00:00:20.059 
The sigmoid function.

6
00:00:21.739 --> 00:00:26.500 
It is applied on top of the linear combination of the weighted inputs.

7
00:00:27.530 --> 00:00:31.320 
Sigmoid function has several beneficial characteristics.

8
00:00:31.839 --> 00:00:39.990 
For example, it's a continuous function so it is differentiable and it's mono atomically increasing.

9
00:00:40.000 --> 00:00:49.479 
So calculating the derivative of the sigmoid function is very fast and relatively easy, moreover, it mimics the excitement

10
00:00:49.479 --> 00:00:53.450 
and inhibition behavior of the biological neurons

11
00:00:53.460 --> 00:00:57.539 
in a reasonable way.

12
00:00:57.539 --> 00:01:04.760 
It is recognized that the multi layer perceptron doesn't have the inherent flows of its single layer version.

13
00:01:05.640 --> 00:01:16.829 
So this error was corrected in the expanded edition of Minsky's book. As can be seen from the label from the table perceptron

14
00:01:16.840 --> 00:01:18.329 
with single hidden layer

15
00:01:18.340 --> 00:01:28.200 
and the sigmoid function can successfully solve the problem at the number of hidden layers increases the convex area can

16
00:01:28.200 --> 00:01:34.859 
form any arbitrary shape, so that any complicated classification problems can be solved.

17
00:01:35.540 --> 00:01:44.260 
Hornik and Cybenko proved that multi layer feed forward network can consider it as a universal approximator.

18
00:01:44.739 --> 00:01:53.829 
That is if the number of hidden neuron is large enough and its output is followed by a sigmoid activation function, then

19
00:01:53.840 --> 00:02:03.459 
it can approximate any mass function from an infinite space to another infinite space with any desired accuracy.

20
00:02:04.439 --> 00:02:08.159 
So the multi layer perceptron is a good classifier.

21
00:02:08.169 --> 00:02:12.560 
But the problem is how to adjust the ways of the hidden layers.

22
00:02:13.840 --> 00:02:18.550 
For the neural knows of hidden layers, they don't have expected output.

23
00:02:19.340 --> 00:02:25.849 
So we cannot directly use learning rules like Hebbian rule of standard perceptrons for the training.

24
00:02:27.740 --> 00:02:38.870 
This problem has been solved by using back propagation algorithm proposed by Rummelhart, Hinton and Williams in 1986. This

25
00:02:38.870 --> 00:02:42.949 
paper published in nature has gained great impact.

26
00:02:43.460 --> 00:02:46.419 
It suggests a solution to update weights

27
00:02:46.430 --> 00:02:48.159 
using gradient descent.

28
00:02:48.840 --> 00:02:53.550 
Back propagation is very efficient for calculating the loss gradient.

29
00:02:56.139 --> 00:02:59.860 
This is a typical artificial neural network with two hidden layers.

30
00:03:00.340 --> 00:03:03.860 
The input to the network is the role image data.

31
00:03:03.900 --> 00:03:08.259 
The output is a probability distribution of each class.

32
00:03:08.939 --> 00:03:16.949 
We compare the prediction output with the true labels or we also call it ground truth and calculate the error.

33
00:03:17.439 --> 00:03:25.560 
The error measures the magnitude of the difference between the prediction result and the true labels.

34
00:03:26.240 --> 00:03:33.159 
The whole idea of training is to adjust the ways a little bit to make the error as low as possible.

35
00:03:34.039 --> 00:03:43.289 
Now the problem of doing this each time we go back and calculate the error, we have to multiply all of those ways by all

36
00:03:43.289 --> 00:03:50.250 
the neuron values at each layer and we have to do that again and again once for each weight.

37
00:03:50.939 --> 00:04:00.039 
This takes forever in computing terms and on computing scale is not a practical way.

38
00:04:00.039 --> 00:04:04.860 
Luckily there's the insights that lead us to do this in a very reasonable way.

39
00:04:05.240 --> 00:04:15.590 
We can calculate the slope directly, more precisely calculate the gradient. We can figure out the direction that we need

40
00:04:15.590 --> 00:04:23.970 
to adjust the weight without going all the way back through our neural network and recalculating. This is the function for

41
00:04:23.970 --> 00:04:25.060 
way to update.

42
00:04:25.740 --> 00:04:32.899 
So in the mathematical representation in the form theta represents ways or bias.

43
00:04:32.910 --> 00:04:35.360 
It is a learning rate.

44
00:04:36.050 --> 00:04:43.259 
A J is gradient of loss function with respect to the specific weight parameter.

45
00:04:44.139 --> 00:04:49.449 
Those mountains and valleys in the graphic represent the surface of loss function.

46
00:04:50.040 --> 00:04:58.959 
We can imagine minimizing the prediction error is like rolling down to the bottom of the valley.

47
00:05:00.040 --> 00:05:10.649 
But the whole process is performed step by step and this step size controlled by and the speed of changing the parameter

48
00:05:10.649 --> 00:05:14.500 
is controlled by the step size which is the parameter eta.

49
00:05:14.509 --> 00:05:22.230 
And this method called gradient descent which is the most popular algorithm for training artificial neural networks.

50
00:05:22.240 --> 00:05:24.540 
Now,

51
00:05:24.540 --> 00:05:29.060 
here's the example showing you how the gradient descent works.

52
00:05:29.639 --> 00:05:36.569 
The optimal objective optimization is to adjust the parameter theta to minimize the cost function j.

53
00:05:36.569 --> 00:05:36.959 


54
00:05:39.740 --> 00:05:41.870 
So here the update of theta.

55
00:05:43.839 --> 00:05:54.060 
It's according to this formula, the new theta equals the old theta minus eta times derivative of j.

56
00:05:54.439 --> 00:05:56.009 
With respect to theta.

57
00:05:56.040 --> 00:05:57.230 
For example,

58
00:05:57.230 --> 00:06:02.639 
now we have parameter set to two

59
00:06:02.639 --> 00:06:12.860 
and theta 2 has been randomly initialized. Within calculate the derivative which is actually the tangent at this position.

60
00:06:13.439 --> 00:06:23.759 
And here we will have a negative tangent value and if theta has minus a negative value, it will become bigger

61
00:06:24.240 --> 00:06:26.639 
and thus moved to the right direction.

62
00:06:26.810 --> 00:06:32.660 
If you look at this coordinate system which is closer to the bottom of the valley.

63
00:06:33.540 --> 00:06:41.939 
So with stochastic gradient descent method, we can gradually approach the local minimum.

64
00:06:41.939 --> 00:06:44.740 
To update the way to deeper inside the network.

65
00:06:44.790 --> 00:06:48.860 
We pass the arrow layer by layer by applying the chain rule.

66
00:06:49.639 --> 00:06:58.529 
So in short a derivative times another derivative I assume that you are already familiar with the chain rules.

67
00:06:58.540 --> 00:07:02.259 
Otherwise you may want to recap calculus a little bit.

68
00:07:03.040 --> 00:07:12.439 
Nevertheless, here is a very simple example. A neural network with just one input, one hidden neuron and one single output

69
00:07:12.439 --> 00:07:12.959 
neuron.

70
00:07:13.730 --> 00:07:15.860 
There is a single way to put connection.

71
00:07:16.240 --> 00:07:23.949 
So for adjusting the layer, we can easily calculate the derivative of the output with respect to the corresponding weight.

72
00:07:24.939 --> 00:07:32.949 
In this case we can calculate the derivative of age with respect to w1, the weight one equals x.

73
00:07:33.740 --> 00:07:35.899 
And the derivative of y.

74
00:07:35.910 --> 00:07:40.269 
With respect to the hidden note age equals w2.

75
00:07:40.269 --> 00:07:40.750 


76
00:07:42.199 --> 00:07:45.850 
Then if you want to know how much the output y changed.

77
00:07:45.939 --> 00:07:56.470 
If the w1 change, we can just calculate the derivative dy by respect to the input way w1 which equals

78
00:07:56.470 --> 00:07:57.949 
w2 times x.

79
00:07:59.740 --> 00:08:06.550 
Finally a recall the two results we already have and we multiply them together.

80
00:08:06.939 --> 00:08:12.500 
Thus we can calculate the slope of each tiny step and multiply them together.

81
00:08:12.529 --> 00:08:20.139 
This is the chaining used for multi layer neural networks.

82
00:08:20.139 --> 00:08:22.470 
For more complicated networks,

83
00:08:22.480 --> 00:08:25.069 
if we want to know how much the error

84
00:08:25.069 --> 00:08:25.149 


85
00:08:25.149 --> 00:08:25.430 


86
00:08:25.430 --> 00:08:26.100 
c changed.

87
00:08:26.110 --> 00:08:32.070 
If we adjust a parameter deeper inside the network, for example the bias B.

88
00:08:32.070 --> 00:08:32.559 
One.

89
00:08:33.039 --> 00:08:40.559 
We just calculate the derivative of each tiny step all the way back and then multiply them all together.

90
00:08:41.139 --> 00:08:46.279 
There seems to be a simple and elegant method for multi layer neural networks.

91
00:08:53.440 --> 00:09:01.659 
However, remember that the whole field of AI is based on the understanding of how our brain works, right?

92
00:09:02.340 --> 00:09:08.419 
But back propagation is not strictly in accordance with the biological neuron.

93
00:09:08.429 --> 00:09:16.950 
So we cannot find similar mechanisms in human brain, human neuron system and it lacks of data and the powerful hardware at

94
00:09:16.950 --> 00:09:19.610 
that time that people couldn't train

95
00:09:19.620 --> 00:09:21.559 
deep neural network at that time.

96
00:09:22.440 --> 00:09:27.710 
On the other hand, other machine learning methods have emerged in 1990s.

97
00:09:27.720 --> 00:09:36.789 
For example, the support vector machine kernel machine, which is a strong convex approach, shown better performance in a

98
00:09:36.799 --> 00:09:39.149 
large variety of applications.

99
00:09:39.240 --> 00:09:44.159 
So strong convex approach means that there is a global optimization minimum.

100
00:09:44.740 --> 00:09:54.110 
UOn the contrary we know that neural network using stochastic gradient descent optimization is a non convex optimization

101
00:09:54.110 --> 00:09:54.860 
problem.

102
00:09:54.870 --> 00:09:58.559 
It may suffer from the local minimum or set of points.

103
00:09:59.340 --> 00:10:07.100 
Therefore, artificial neural networks are theoretically not beautiful enough and it doesn't work better in practice at while

104
00:10:07.110 --> 00:10:07.950 
at that time.

105
00:10:09.639 --> 00:10:16.750 
So the second AI winter occurred in almost the same period of the rise of support vector machine.

106
00:10:20.039 --> 00:10:21.059 
Thank you for watching.
