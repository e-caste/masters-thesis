WEBVTT

1
00:00:01.010 --> 00:00:04.740 
hello my name is patrick crain. and i'm a phd student at the

2
00:00:04.740 --> 00:00:06.490 
university of illinois at urbana champaign.

3
00:00:07.130 --> 00:00:09.770 
today i will be sharing the results of an experiment on how

4
00:00:09.770 --> 00:00:12.190 
scores and written comments influence feedback perceptions

5
00:00:12.190 --> 00:00:13.900 
and revisions to open ended projects.

6
00:00:15.450 --> 00:00:17.960 
here is an example of an open ended creative project shared

7
00:00:17.960 --> 00:00:19.770 
with an online feedback exchange community.

8
00:00:20.640 --> 00:00:22.810 
the creator of this project received constructive comments

9
00:00:22.810 --> 00:00:25.070 
offering insight suggestions for improving their work.

10
00:00:26.010 --> 00:00:28.520 
both the comments and the design itself include some feedback

11
00:00:28.520 --> 00:00:31.190 
in the form of votes used primarily within the community to

12
00:00:31.190 --> 00:00:33.920 
increase the visibility of the highest quality designs and feedback.

13
00:00:34.760 --> 00:00:38.260 
a similar coupling of summit and constructive feedback is often present in classrooms

14
00:00:38.460 --> 00:00:41.420 
in the form of instructor grades and comments on open ended assignments.

15
00:00:42.210 --> 00:00:44.860 
despite the prevalence of this pattern. it is not well understood

16
00:00:44.860 --> 00:00:47.310 
how summative feedback gives context to constructive feedback

17
00:00:47.320 --> 00:00:50.380 
or influences subsequent revisions to in-progress open ended work.

18
00:00:51.670 --> 00:00:55.560 
additionally increased participation in online feedback exchange in learning communities

19
00:00:55.670 --> 00:00:58.280 
has necessitated a move towards automated assessment techniques.

20
00:00:58.920 --> 00:01:01.160 
these techniques typically involve scoring several smaller

21
00:01:01.160 --> 00:01:04.110 
components of open ended projects selecting pre authored comments

22
00:01:04.110 --> 00:01:06.680 
for each component from a pool based on that component's score

23
00:01:06.960 --> 00:01:09.040 
and showing these comments and scores to the projects creator.

24
00:01:10.210 --> 00:01:12.320 
while these techniques assume both comments and scores are

25
00:01:12.320 --> 00:01:14.930 
valuable to creators of open ended projects. little research

26
00:01:14.930 --> 00:01:16.980 
has considered the individual roles of written comments and

27
00:01:16.980 --> 00:01:20.240 
quality scores in motivating revisions. a deeper understanding

28
00:01:20.240 --> 00:01:22.410 
of how constructive and some of the feedback in operate

29
00:01:22.410 --> 00:01:25.040 
is essential to informing the design of future scalable automated

30
00:01:25.040 --> 00:01:26.660 
assessment systems for open ended work.

31
00:01:28.340 --> 00:01:31.130 
with this in mind we designed an experiment to answer the question

32
00:01:31.130 --> 00:01:33.680 
of how quality scores in pre authored written comments influence

33
00:01:33.680 --> 00:01:36.640 
creators' perceptions of feedback and subsequent revisions to their work.

34
00:01:37.610 --> 00:01:40.490 
our experiment included three hypotheses. for brevity i will

35
00:01:40.490 --> 00:01:41.550 
discuss two of these today.

36
00:01:42.530 --> 00:01:45.740 
first we hypothesize that showing written comments with or without a score

37
00:01:45.960 --> 00:01:48.910 
would prompt higher task satisfaction and higher feedback satisfaction

38
00:01:48.990 --> 00:01:50.120 
than showing only as for.

39
00:01:50.810 --> 00:01:53.530 
second we hypothesize that showing a score would elicit more

40
00:01:53.530 --> 00:01:56.110 
revision effort and greater improvement than not showing a score

41
00:01:56.360 --> 00:01:58.220 
regardless of the presence of written comments.

42
00:01:59.900 --> 00:02:02.310 
to test these hypotheses we recruited four hundred forty one

43
00:02:02.310 --> 00:02:04.250 
participants to complete a creative writing task.

44
00:02:05.040 --> 00:02:07.330 
creative writing was chosen as a representative of a class

45
00:02:07.330 --> 00:02:09.810 
of domains where open-ended projects are iteratively revised

46
00:02:09.810 --> 00:02:10.810 
with respect to feedback.

47
00:02:11.770 --> 00:02:14.240 
after completing a short background survey participants were

48
00:02:14.240 --> 00:02:16.530 
asked to compose an original creative story of approximately

49
00:02:16.530 --> 00:02:18.790 
two hundred fifty words based on a visual writing problem.

50
00:02:19.890 --> 00:02:22.460 
each story was evaluated and assigned to one of four feedback

51
00:02:22.460 --> 00:02:25.580 
conditions wherein participants were shown either pre authored constructive comments.

52
00:02:25.900 --> 00:02:29.880 
a qualities for both comments and a score or no feedback on their story.

53
00:02:30.720 --> 00:02:32.840 
these feedback conditions comprise the spectrum of feedback

54
00:02:32.840 --> 00:02:36.350 
presentations commonly used within classrooms
and online feedback exchange platforms.

55
00:02:37.720 --> 00:02:39.680 
participants were asked to revise their stories with respect

56
00:02:39.680 --> 00:02:42.310 
to the feedback they received or based on self-reflection if

57
00:02:42.310 --> 00:02:43.450 
they were not assigned feedback.

58
00:02:44.370 --> 00:02:46.920 
following these revisions participants were given a post-test

59
00:02:46.920 --> 00:02:49.350 
survey inquiring about their satisfaction with the feedback

60
00:02:49.350 --> 00:02:51.810 
and story writing experience. the amount of effort they put

61
00:02:51.810 --> 00:02:54.520 
into writing or revising their stories and the types of revisions

62
00:02:54.520 --> 00:02:55.620 
they made to their stories.

63
00:02:56.470 --> 00:02:58.950 
finally participants' revised stories were re-evaluated to

64
00:02:58.950 --> 00:03:01.350 
determine their degree of improvement since their initial drafts.

65
00:03:03.470 --> 00:03:05.890 
to mirror the feedback produced by existing automated assessment

66
00:03:05.890 --> 00:03:08.770 
systems for open ended work, we collaborated with two graduate

67
00:03:08.770 --> 00:03:10.920 
students from our university's writing department to refine

68
00:03:10.920 --> 00:03:11.910 
a creative writing rubric.

69
00:03:12.570 --> 00:03:15.890 
the rubric evaluated stories for narrative cohesion character development

70
00:03:16.100 --> 00:03:19.360 
use of imagery variety and sentence structure use of proper

71
00:03:19.360 --> 00:03:21.820 
writing mechanics and adherence to task instructions.

72
00:03:22.660 --> 00:03:24.790 
the team iteratively developed a piece of written feedback

73
00:03:24.790 --> 00:03:27.920 
corresponding to each possible score and dimension
on the writing rubric.

74
00:03:28.700 --> 00:03:31.160 
following the writing phase of our study the team evaluated

75
00:03:31.160 --> 00:03:33.450 
each of the participant's stories according to the writing rubric.

76
00:03:34.170 --> 00:03:36.640 
a script automatically assign three comments from the feedback

77
00:03:36.640 --> 00:03:39.480 
pool to each story based on the lowest scoring
dimension of those stories.

78
00:03:40.460 --> 00:03:42.560 
these comments were shown to participants in the comments plus

79
00:03:42.560 --> 00:03:45.580 
score and comments only feedback conditions while the combined

80
00:03:45.580 --> 00:03:47.450 
rubric score was shown to participants in the comments plus

81
00:03:47.450 --> 00:03:48.870 
score and score only conditions.

82
00:03:51.470 --> 00:03:53.790 
in our experiment we found that participants were significantly

83
00:03:53.790 --> 00:03:56.100 
more satisfied with the feedback they received, if that feedback

84
00:03:56.100 --> 00:03:58.740 
contained comments than if it consisted of only a score.

85
00:03:59.610 --> 00:04:02.060 
satisfaction with the writing task was positively correlated

86
00:04:02.060 --> 00:04:04.370 
with initial test performance among those who were shown a score

87
00:04:04.570 --> 00:04:06.990 
suggesting scores may play a role in incentivizing revision.

88
00:04:07.860 --> 00:04:10.590 
however, participants who received scores did not invest any

89
00:04:10.590 --> 00:04:12.650 
more effort in their stories or produce stronger revisions

90
00:04:12.650 --> 00:04:13.860 
than those who did not receive scores.

91
00:04:14.550 --> 00:04:17.150 
additionally participants who received both comments and scores

92
00:04:17.150 --> 00:04:20.260 
made no more deep revisions to their stories than
those who received only comments.

93
00:04:20.930 --> 00:04:23.100 
these findings suggest quality scores may undermine effort

94
00:04:23.100 --> 00:04:24.900 
by signaling minimal room for improvement.

95
00:04:25.920 --> 00:04:28.580 
finally we found a negative correlation between initial draft

96
00:04:28.580 --> 00:04:31.670 
quality and revision quality. this could reflect diminishing

97
00:04:31.670 --> 00:04:34.330 
returns from feedback on nearly complete open ended projects

98
00:04:34.480 --> 00:04:36.790 
which may be exacerbated by the presence of a quality score.

99
00:04:39.090 --> 00:04:41.670 
based on our findings we offer two recommendations for designers

100
00:04:41.670 --> 00:04:44.140 
of feedback exchange platforms incorporating automated assessments

101
00:04:44.140 --> 00:04:47.360 
of open ended work. we discuss additional implications in our paper.

102
00:04:48.900 --> 00:04:51.650 
given the lack of demonstrable utility of quality scores.

103
00:04:51.650 --> 00:04:53.870 
our first recommendation is to refrain from showing quality scores

104
00:04:53.870 --> 00:04:56.440 
for in-progress open ended work using them only for selecting

105
00:04:56.440 --> 00:04:57.840 
appropriate prix author comments.

106
00:04:58.790 --> 00:05:01.490 
alternately a feedback exchange platform might let creators

107
00:05:01.490 --> 00:05:03.320 
decide for themselves whether they'd like to receive quality

108
00:05:03.320 --> 00:05:04.230 
scores on their work.

109
00:05:05.420 --> 00:05:07.960 
our second recommendation is to prioritize the personalization

110
00:05:07.960 --> 00:05:09.480 
of constructive comments as much as possible,

111
00:05:10.520 --> 00:05:13.650 
although automated assessments inherently lack some level of

112
00:05:13.860 --> 00:05:16.210 
personalization techniques for repurposing existing feedback or crowdsourcing.

113
00:05:16.210 --> 00:05:19.140 
feedback could enable future automated assessment systems to

114
00:05:19.140 --> 00:05:21.790 
offer a deeper degree of personalization and elicit stronger

115
00:05:21.790 --> 00:05:23.030 
revisions to open-ended work.

116
00:05:24.930 --> 00:05:28.430 
we see many directions for future research,
i will briefly discuss three of them.

117
00:05:29.450 --> 00:05:31.750 
it would be valuable to explore how alternative forms of some

118
00:05:31.750 --> 00:05:34.050 
of the feedback influence perceptions of constructive feedback

119
00:05:34.050 --> 00:05:35.330 
and subsequent revision behavior.

120
00:05:36.100 --> 00:05:38.330 
two examples might be showing scores indicating how much a

121
00:05:38.330 --> 00:05:41.190 
project has improved since the last draft or scores indicating.

122
00:05:41.190 --> 00:05:43.590 
the priority of addressing individual constructive comments.

123
00:05:44.570 --> 00:05:47.280 
given our experiment only allowed for a single round of feedback

124
00:05:47.510 --> 00:05:49.940 
future research should also investigate how scores in comments

125
00:05:49.950 --> 00:05:52.350 
influence longer iterative processes consisting of three or

126
00:05:52.350 --> 00:05:53.640 
more feedback revision cycles.

127
00:05:54.620 --> 00:05:57.260 
finally our findings should be field-tested in a classroom

128
00:05:57.260 --> 00:06:00.070 
or online feedback exchange community by comparing how constructive

129
00:06:00.070 --> 00:06:02.490 
feedback within without qualities for is influences feedback

130
00:06:02.490 --> 00:06:03.750 
perceptions and revision behavior.

131
00:06:05.710 --> 00:06:07.910 
to summarize while constructive and summative feedback are

132
00:06:07.910 --> 00:06:10.580 
critical for scaleable automated assessments of open-ended work.

133
00:06:10.720 --> 00:06:12.840 
there has been little research investigating how these components

134
00:06:12.840 --> 00:06:15.410 
interact to affect perceptions of feedback and revision behavior.

135
00:06:16.080 --> 00:06:18.870 
in our experiment we found that task satisfaction was correlated

136
00:06:18.870 --> 00:06:21.730 
with performance on an initial story draft
when the quality score was present.

137
00:06:22.410 --> 00:06:25.190 
we did not otherwise find any significant benefits to showing a score

138
00:06:25.340 --> 00:06:28.140 
and found that in some cases scores undermined revision depth an effort.

139
00:06:28.940 --> 00:06:31.530 
to instructors and designers of online feedback exchange platforms.

140
00:06:31.670 --> 00:06:34.100 
we recommend exploring alternative forms of summit feedback,

141
00:06:34.110 --> 00:06:37.040 
such as improvement scores or priority scores to incentivize revision.

142
00:06:37.630 --> 00:06:41.260 
we also recommend utilizing techniques such
as repurposing or crowdsourcing feedback

143
00:06:41.380 --> 00:06:44.030 
to further personalize automated assessments and help improve

144
00:06:44.030 --> 00:06:45.930 
the quality of revisions to open-ended work.

145
00:06:46.450 --> 00:06:47.310 
thank you for your time.
