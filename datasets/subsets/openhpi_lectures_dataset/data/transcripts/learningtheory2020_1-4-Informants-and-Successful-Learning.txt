WEBVTT

1
00:00:00.530 --> 00:00:03.790 
Hello and welcome to the last
video in this first week of the

2
00:00:03.790 --> 00:00:05.650 
course Computational
Learning Theory.

3
00:00:06.650 --> 00:00:11.370 
As I promised in the last video I
will clarify what learns means

4
00:00:11.630 --> 00:00:15.410 
and what of sequences from
increasing lengths.

5
00:00:18.320 --> 00:00:21.880 
Ok so I want to rephrase this with
the abbreviations we learned.

6
00:00:22.560 --> 00:00:26.160 
So the question is is there
a machine M or that

7
00:00:26.170 --> 00:00:31.600 
learns a predictor f in the hypothesis
space from suitable sequences

8
00:00:31.870 --> 00:00:36.960 
tow in t of increasing length.
Ok so now we want to learn

9
00:00:36.960 --> 00:00:42.390 
a specific predictor f and this is valid
because we assumed realizability

10
00:00:42.530 --> 00:00:46.410 
so we assume that the data
we get fits a predictor.

11
00:00:46.840 --> 00:00:50.150 
And this is also meant by
suitable. Let's formalize this.

12
00:00:51.670 --> 00:00:55.680 
We fix some predictor f in the
hypothesis space and we say

13
00:00:55.680 --> 00:01:00.170 
that some function I is called
an informant for this f

14
00:01:00.810 --> 00:01:06.970 
if it is a map a function that maps
each time to a labeled binary labeled

15
00:01:07.090 --> 00:01:07.870 
natural number.

16
00:01:09.470 --> 00:01:13.270 
And this labeling or this map
must work in the following way

17
00:01:13.500 --> 00:01:17.230 
where there has to be some
surjection, so a map from

18
00:01:17.230 --> 00:01:21.110 
natural numbers to natural numbers
that meets every natural number

19
00:01:21.730 --> 00:01:27.000 
such that at time t the
informant has the value

20
00:01:27.450 --> 00:01:32.110 
of this function at time t and
the according label when

21
00:01:32.110 --> 00:01:35.090 
we ask f what the label
of N of t should be.

22
00:01:37.410 --> 00:01:41.230 
For our image classification
example this looks as follows:

23
00:01:42.220 --> 00:01:47.800 
the informant first outputs the
natural number corresponding

24
00:01:47.800 --> 00:01:50.070 
to the first image
with a label one,

25
00:01:50.760 --> 00:01:54.420 
then the natural number corresponding to
the second image with the label one

26
00:01:55.010 --> 00:02:01.390 
and so on. So we were right for
example that i until length 0

27
00:02:01.390 --> 00:02:02.960 
well it is the
empty string. Ok.

28
00:02:03.790 --> 00:02:09.140 
But the informant until length
one this is the first

29
00:02:09.150 --> 00:02:13.180 
or the natural number given by the
first image and a label one.

30
00:02:15.310 --> 00:02:19.850 
When I look at the informant until length
two it gives us a sequence of two

31
00:02:20.030 --> 00:02:22.910 
labeled natural numbers namely
the ones according to the first

32
00:02:22.910 --> 00:02:26.150 
image and the one according to
the second image both with the

33
00:02:26.150 --> 00:02:28.830 
label one because there is
a butterfly on the image.

34
00:02:30.380 --> 00:02:35.170 
Ok but I mean this is now doesn't
really need this surjection

35
00:02:35.810 --> 00:02:40.290 
definition because every image
occurs once and this looks

36
00:02:40.290 --> 00:02:44.320 
quite straightforward but we also want to
allow representations like this one.

37
00:02:44.510 --> 00:02:47.980 
So there is an image that for example
the first informant hasn't shown

38
00:02:48.150 --> 00:02:52.550 
us yet but the second informant it
shows it as the first picture

39
00:02:52.740 --> 00:02:56.400 
and it also shows it twice and
as you can see it shows it

40
00:02:56.410 --> 00:02:59.790 
three times namely also
on position six again.

41
00:03:00.660 --> 00:03:05.540 
So the ordering is not fixed and we
also don't forbid repetitions.

42
00:03:06.850 --> 00:03:11.840 
Just a note on the side the practical
algorithms often don't care about repetitions

43
00:03:12.070 --> 00:03:15.620 
so you either remove them early
in the preprocessing or the

44
00:03:15.620 --> 00:03:17.180 
algorithm just
doesn't care.

45
00:03:20.150 --> 00:03:23.240 
Ok so what does learning now mean?
Now we have all the ingredients

46
00:03:23.240 --> 00:03:24.390 
we need to define it.

47
00:03:25.120 --> 00:03:29.700 
We have an informant for f, so we
have some data representation

48
00:03:29.890 --> 00:03:31.310 
that fits our predictor

49
00:03:32.630 --> 00:03:37.340 
and we have a learner and we say
that this learner which maps

50
00:03:37.480 --> 00:03:41.600 
finite sequences of binary labeled
natural numbers to natural

51
00:03:41.600 --> 00:03:43.650 
numbers which correspond
to a predictor,

52
00:03:44.280 --> 00:03:48.310 
well this machine learner is
successful on this presentation i

53
00:03:49.020 --> 00:03:51.970 
if it eventually
settles on some i

54
00:03:52.720 --> 00:03:55.930 
such that f i is exactly
that f we want to learn.

55
00:03:56.840 --> 00:04:02.830 
And eventually settled means that
at some point it has found an i

56
00:04:03.210 --> 00:04:07.070 
which is correct and it doesn't
change its mind anymore, so

57
00:04:07.070 --> 00:04:10.700 
if it sees more and more and more
images it will not change its mind.

58
00:04:11.250 --> 00:04:16.070 
How does it look like in our example?
Well again the learner gets

59
00:04:16.780 --> 00:04:19.880 
receives a representation
of the first image

60
00:04:20.370 --> 00:04:24.700 
and maybe of the second image
and maybe a third image and

61
00:04:24.790 --> 00:04:29.780 
I mean it can also already make
the output some hypothesis

62
00:04:29.990 --> 00:04:33.290 
some i which corresponds to a
predictor earlier but at this

63
00:04:33.290 --> 00:04:37.180 
point it's like a the first
interesting point so there it may

64
00:04:37.190 --> 00:04:38.940 
output this predictor

65
00:04:40.810 --> 00:04:46.350 
but yeah it might see or it will see
more pictures that are labelled

66
00:04:46.480 --> 00:04:50.390 
for example this one and then as
you can see that the output

67
00:04:50.400 --> 00:04:55.450 
that it made after three images after
having seen three labelled nature numbers

68
00:04:55.810 --> 00:05:01.000 
does not fit this situation. So
it might change its mind and

69
00:05:01.000 --> 00:05:04.080 
say well maybe it's
this half-space.

70
00:05:05.070 --> 00:05:07.540 
And then again it
sees more examples

71
00:05:08.200 --> 00:05:11.830 
but this does not contradict the
half-space it has seen so far

72
00:05:12.340 --> 00:05:15.470 
so it might just make
the same guess again.

73
00:05:16.390 --> 00:05:20.800 
And well if we assume that this
is the right one then no matter

74
00:05:20.800 --> 00:05:24.160 
which image which labelled image
the machine learner will

75
00:05:24.160 --> 00:05:28.880 
see it will never change its mind
and so the convergence time was

76
00:05:28.990 --> 00:05:33.130 
like already after four images
where the other images were

77
00:05:33.130 --> 00:05:37.040 
chosen in such a way it was lucky
with the representation that

78
00:05:37.040 --> 00:05:39.150 
it already gets the
right half space.

79
00:05:40.130 --> 00:05:43.270 
Ok so I just want
to remind you of

80
00:05:45.020 --> 00:05:49.500 
well I mean there is not the unique
informant for the situation

81
00:05:49.500 --> 00:05:53.560 
and we don't really know which
representation we receive.

82
00:05:54.540 --> 00:05:59.210 
So what we will require is
we will say m learns f

83
00:06:01.200 --> 00:06:05.400 
if it is successful on every
informant i. So also on the second

84
00:06:05.400 --> 00:06:09.530 
one I showed you where the pictures
come in a completely different order.

85
00:06:14.630 --> 00:06:16.920 
So our question now
comes down to

86
00:06:17.730 --> 00:06:23.390 
is there a machine learner m that learns
a predictor f from suitable sequences

87
00:06:23.570 --> 00:06:24.810 
of increasing length.

88
00:06:26.140 --> 00:06:29.680 
And well of increasing length
now means that we have

89
00:06:29.680 --> 00:06:33.730 
this informant and we see longer
and longer and longer initial

90
00:06:33.730 --> 00:06:35.320 
segments of this
informant.

91
00:06:41.450 --> 00:06:44.770 
Ok but well I mean the answer
to this question obviously

92
00:06:44.770 --> 00:06:48.980 
is yes because the learner can
just always say f I mean

93
00:06:48.980 --> 00:06:52.320 
it can always output the same
half-space if you want

94
00:06:52.320 --> 00:06:53.850 
to look at it in
our example.

95
00:06:54.720 --> 00:06:58.250 
This is not what we want. I mean what we
want is we want some machine learner

96
00:06:58.430 --> 00:07:02.000 
that is suitable for more than
one task. I mean they might be

97
00:07:02.000 --> 00:07:04.950 
similar the task because we have
to adjust the machine learner

98
00:07:04.950 --> 00:07:08.370 
in some way but we don't only
want to solve one task, we want

99
00:07:08.370 --> 00:07:09.750 
to solve multiple
tasks.

100
00:07:11.140 --> 00:07:13.850 
Ok so the last
definition we need

101
00:07:14.720 --> 00:07:21.810 
is m learns the hypothesis space f
if it learns every predictor in f.

102
00:07:23.820 --> 00:07:28.030 
So the question we want to answer
is now as short as this namely

103
00:07:28.210 --> 00:07:32.100 
is there a machine learner that
learns the whole hypothesis space?

104
00:07:32.860 --> 00:07:38.510 
And we will answer well this
question for the half-spaces soon.

105
00:07:40.860 --> 00:07:42.530 
Ok so let's look
at it again.

106
00:07:43.330 --> 00:07:46.960 
We have the binary classification
setting, we have the images

107
00:07:46.960 --> 00:07:51.260 
which are embedded in some high
dimensional space, they we assume

108
00:07:51.260 --> 00:07:56.520 
they are separable by a hyperplane,
by a linear function,

109
00:07:58.640 --> 00:08:04.340 
and we have a predictors that are pre-specified
elements of this hypothesis space

110
00:08:04.580 --> 00:08:09.630 
and now we want to find out whether
there is a machine learner that learns

111
00:08:09.840 --> 00:08:11.650 
this hypothesis space.

112
00:08:12.670 --> 00:08:15.140 
And we are going to discuss
this in the next week.
