WEBVTT

1
00:00:00.940 --> 00:00:03.750 
Hello and welcome. In this video,

2
00:00:03.759 --> 00:00:11.460 
we will further recap the most commonly used method for neural network optimization - stochastic gradient descent.

3
00:00:13.839 --> 00:00:22.660 
Generally speaking, gradient descent is to slightly change the parameters to see how the laws of a training set will change.

4
00:00:23.339 --> 00:00:26.850 
Then adjust the parameters to reduce the loss.

5
00:00:28.239 --> 00:00:31.370 
Stochastic gradient descent or in short, SGD.

6
00:00:31.370 --> 00:00:31.750 


7
00:00:32.840 --> 00:00:37.460 
Is a stochastic approximation of the gradient descent method.

8
00:00:37.840 --> 00:00:48.030 
And it is an iterative method for minimizing and objective function. In deep learning, SGD normally means mini batch gradient

9
00:00:48.030 --> 00:00:54.369 
descent. So it performs an update for very mini batch for every mini batch.

10
00:00:54.380 --> 00:01:03.369 
So a mini batch of samples of end training samples normally. Here, N is the batch size. Mini batch

11
00:01:03.380 --> 00:01:04.069 
SGD

12
00:01:04.069 --> 00:01:08.359 
has shown great performance in many practical applications.

13
00:01:08.840 --> 00:01:14.150 
So this method can effectively reduce the variance of the parameter updates.

14
00:01:16.140 --> 00:01:20.269 
So let's take a closer look at the mathematical form of SGD

15
00:01:20.269 --> 00:01:21.750 
briefly.

16
00:01:22.140 --> 00:01:32.290 
So in this form, mathematical function theta represent the weight parameters, could be weight or bias and

17
00:01:32.290 --> 00:01:37.989 
function j theta is a cost function with respect to the input

18
00:01:38.000 --> 00:01:38.510 
x

19
00:01:38.519 --> 00:01:42.890 
And the label target level y, theta

20
00:01:42.890 --> 00:01:46.390 
j is the derivative of cost function j.

21
00:01:46.400 --> 00:01:50.659 
And it can also be represented in another mathematical form.

22
00:01:52.040 --> 00:01:57.219 
So the optimization outline can be summarized in using two steps.

23
00:01:57.230 --> 00:02:06.920 
First review, initialize all the weight parameter theta, for example using Gaussian distribution and we will keep changing

24
00:02:06.920 --> 00:02:17.150 
theta small, change slowly changing Ceta to reduce the cost function j and to end up at a small minimum.

25
00:02:18.939 --> 00:02:23.949 
The optimization objective can be described by this mathematical function.

26
00:02:26.039 --> 00:02:33.759 
So as already mentioned, the mountains and valleys in the figure represent the surface of loss function.

27
00:02:34.560 --> 00:02:34.780 
SGD

28
00:02:34.780 --> 00:02:34.969 


29
00:02:34.969 --> 00:02:35.240 


30
00:02:35.240 --> 00:02:38.879 
is like rolling down to the bottom of the valley.

31
00:02:39.389 --> 00:02:42.659 
The whole process is performed step by step.

32
00:02:43.139 --> 00:02:52.770 
The parameter eta represents the learning rate which controls the step size of the movement and the neural network optimization

33
00:02:52.780 --> 00:02:55.009 
is a non convex process.

34
00:02:55.020 --> 00:03:00.280 
As you can see there are many values in the lost landscape so SDG

35
00:03:00.280 --> 00:03:00.449 


36
00:03:00.449 --> 00:03:00.810 


37
00:03:00.819 --> 00:03:03.560 
can only converge to a local minimum.

38
00:03:06.039 --> 00:03:08.770 
This simple example will show you how the SGD.

39
00:03:08.770 --> 00:03:08.939 


40
00:03:08.939 --> 00:03:09.689 
works.

41
00:03:11.139 --> 00:03:14.849 
The optimization objective is to adjust the parameter theta

42
00:03:14.849 --> 00:03:16.789 
to minimize the cost function j.

43
00:03:16.789 --> 00:03:17.099 


44
00:03:17.099 --> 00:03:24.750 
As already mentioned. And the update of theta is according to the derivative and the learning rate.

45
00:03:25.439 --> 00:03:31.069 
So the new theta equals theta minus eta times the derivative of j

46
00:03:31.080 --> 00:03:32.550 
with respect to theta.

47
00:03:33.840 --> 00:03:35.120 
For example,

48
00:03:35.169 --> 00:03:36.610 
now we have a parameter

49
00:03:36.610 --> 00:03:39.860 
theta one which has been randomly initialized.

50
00:03:40.340 --> 00:03:48.120 
We calculate the derivative which is actually the tangent at the current position along the slope of the curve.

51
00:03:48.129 --> 00:03:56.050 
And in this case we will have a positive tangent value. Because eta is always positive,

52
00:03:56.060 --> 00:04:06.090 
So if theta minus a positive value, it will become smaller and thus move to the left along the horizontal axis.

53
00:04:06.099 --> 00:04:11.860 
We can see that after this step we are closer to the bottom of the valley.

54
00:04:14.740 --> 00:04:20.490 
On the other hand, if our current position is on the left hand side of the curve,

55
00:04:21.139 --> 00:04:23.759 
now we have another parameter theta2.

56
00:04:24.339 --> 00:04:33.819 
We also calculate the tangent at this position and here we will have a negative tangent value and eta is always positive as

57
00:04:33.819 --> 00:04:34.600 
zero.

58
00:04:34.610 --> 00:04:46.189 
So if theta minus a negative value, it view becomes bigger and that move to the right direction which is also closer to

59
00:04:46.189 --> 00:04:47.959 
the bottom of the valley.

60
00:04:49.139 --> 00:04:57.449 
Now we can see with stochastic gradient descent method, we can gradually approach the local minimum.

61
00:05:00.540 --> 00:05:05.959 
Once we reach the bottom of the valley, the derivative here is close to zero.

62
00:05:06.339 --> 00:05:11.040 
So, we are not able to update the theta anymore. As mentioned before,

63
00:05:11.050 --> 00:05:18.720 
once we reach the local minimum or zero point, it is difficult to continue to update the ways using the standard SGD

64
00:05:18.720 --> 00:05:18.889 


65
00:05:18.889 --> 00:05:19.040 


66
00:05:19.040 --> 00:05:21.839 
method.

67
00:05:21.839 --> 00:05:32.000 
Regarding the learning rate parameter eta, we can easily find out from the figure if the learning rate is too small. The gradient

68
00:05:32.000 --> 00:05:39.459 
descent can be very slow this process, because we need more steps to reach the local minimum.

69
00:05:42.040 --> 00:05:51.040 
On the contrary, if the learning rate is too large, the gradient descent can overshoot minimum and training process may fail

70
00:05:51.040 --> 00:06:00.220 
to converge or even diverge. Moreover, gradient descent can converge to a local minimum even when the learning rate is fixed

71
00:06:00.740 --> 00:06:08.459 
because the derivative term will get smaller and smaller during the training. As the

72
00:06:08.470 --> 00:06:15.639 
the prediction laws of your neural network is getting smaller.

73
00:06:15.639 --> 00:06:22.629 
In the next video, we will use some example to show you how the computation graph work in the neural network.

74
00:06:22.639 --> 00:06:26.160 
And it is also the basic of the back propagation.

75
00:06:26.639 --> 00:06:28.160 
Thank you for watching the video.
