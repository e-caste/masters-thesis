WEBVTT

1
00:00:00.000 --> 00:00:04.320 
Hello and welcome to the third video of
the MOOC computational learning theory.

2
00:00:04.910 --> 00:00:09.870 
As I promised you this video is about
the range of the machine learner.

3
00:00:11.390 --> 00:00:14.560 
So we still want to know whether
there is a machine learner

4
00:00:14.880 --> 00:00:17.240 
that performs well
on a given task.

5
00:00:19.600 --> 00:00:23.660 
Ok so I want to rephrase the
question. So now it is - can

6
00:00:23.660 --> 00:00:27.230 
machine learner learn a
well performing predictor

7
00:00:27.720 --> 00:00:31.710 
from sequences of increasing
length? So what I did is

8
00:00:31.710 --> 00:00:37.830 
I added the set calligraphic f and what this
is is it is a prefixed hypothesis space.

9
00:00:38.040 --> 00:00:42.350 
So we in advance we already say
that the predictor needs to

10
00:00:42.350 --> 00:00:46.720 
be of a certain type. Every year in
practice machine learner does that.

11
00:00:46.900 --> 00:00:51.640 
So we don't lose anything by doing
it. Ok but what is f? Well

12
00:00:52.590 --> 00:00:57.360 
a set of predictors fi
labelled by natural numbers

13
00:00:57.960 --> 00:01:02.360 
is called the hypothesis space
if they are all similar

14
00:01:02.360 --> 00:01:06.960 
in a certain way, namely, there
is a computer program which on

15
00:01:06.970 --> 00:01:11.290 
inputs i, so i was the index
of the predictor, and n

16
00:01:12.150 --> 00:01:15.030 
both natural numbers
returns the number one

17
00:01:15.550 --> 00:01:21.960 
if and only if the i's predictor labels
the natural number n with one.

18
00:01:22.450 --> 00:01:27.940 
So it says yes on the image
encoded by n, for example.

19
00:01:30.460 --> 00:01:33.490 
Ok so and the machine learner now
we can define it because we

20
00:01:33.490 --> 00:01:37.680 
know the domain and the range is
a function that maps finite

21
00:01:37.680 --> 00:01:40.800 
labeled sequences of natural
numbers to natural numbers.

22
00:01:41.220 --> 00:01:44.960 
And the output natural number is
interpreted with respect to

23
00:01:44.960 --> 00:01:47.720 
this prefixed
hypothesis space f.

24
00:01:49.660 --> 00:01:54.710 
So let's look at the classification
with an SVM and perceptron as we did

25
00:01:54.880 --> 00:01:59.170 
in the first video with the images
containing butterflies or not.

26
00:02:01.260 --> 00:02:05.410 
So we have the training data,
the input sequence which now

27
00:02:05.410 --> 00:02:08.930 
i again write as a victor
because we have not encoded

28
00:02:08.930 --> 00:02:10.410 
it into a natural
number here.

29
00:02:11.140 --> 00:02:16.090 
So these have representations in some
may be very high dimensional space

30
00:02:16.330 --> 00:02:19.960 
like when drawing it it's easy
when I used two dimensions, so

31
00:02:19.960 --> 00:02:23.920 
I hope you'll forgive me that I did
that here. So I have a representation

32
00:02:23.930 --> 00:02:27.050 
of this first image in this
two dimensional plane.

33
00:02:27.550 --> 00:02:29.730 
Imagine it to be very
high dimensional please

34
00:02:30.300 --> 00:02:35.320 
and I do the same with the second
image and with the third image

35
00:02:36.390 --> 00:02:41.060 
and with a fourth image and so
on. So I find representations

36
00:02:41.330 --> 00:02:45.060 
like I have a general procedure to
find representation for an image

37
00:02:45.230 --> 00:02:46.200 
in this space.

38
00:02:47.960 --> 00:02:51.960 
And what they give you the SVM
or the perceptron is they

39
00:02:51.960 --> 00:02:56.200 
give you a half space. So they
give you basically a line and

40
00:02:56.210 --> 00:03:00.280 
one side of it which has to be
specified and what you do is

41
00:03:00.280 --> 00:03:03.920 
when you get any image you find
its representation in this

42
00:03:03.930 --> 00:03:07.280 
probably high-dimensional
space and with this

43
00:03:07.280 --> 00:03:10.240 
half-space that you already have,
which is your predictor, you

44
00:03:10.240 --> 00:03:13.260 
will predict the answer. So in
this case you would predict no,

45
00:03:14.330 --> 00:03:17.180 
because it's not in this
busy part half-space.

46
00:03:18.560 --> 00:03:22.320 
So what is always done when
you theoretically analyze

47
00:03:23.180 --> 00:03:27.770 
machine learning is that
you assume that your data

48
00:03:28.180 --> 00:03:33.670 
can be realized by one predictor in
your prefixed hypothesis space.

49
00:03:34.030 --> 00:03:39.720 
So in our case as we only look at half
spaces and we want to separate with them,

50
00:03:39.960 --> 00:03:42.620 
we assume that the data
is linear separable.

51
00:03:43.050 --> 00:03:47.010 
Of course this is problematic
but this is like yeah

52
00:03:47.700 --> 00:03:48.510 
is done a lot.

53
00:03:50.080 --> 00:03:54.120 
Okay so many learning algorithms use
half-spaces as prediction models

54
00:03:54.310 --> 00:03:58.270 
and sometimes you will tell
you will agree if I say well

55
00:03:58.270 --> 00:04:01.770 
maybe it's not linear separable
but with a kernel trick so

56
00:04:01.770 --> 00:04:06.470 
with the kernel function you
can also learn a lot of

57
00:04:06.480 --> 00:04:10.150 
binary classification tasks that
are not linearly separable.

58
00:04:10.420 --> 00:04:13.820 
And this is like where this
picture that you maybe know from

59
00:04:13.820 --> 00:04:18.230 
the title picture of the course comes
from. So this is a non linear

60
00:04:18.540 --> 00:04:27.670 
predictor. So I mean I claimed now that
the half spaces for hypothesis space

61
00:04:23.010 --> 00:04:27.670 
So I mean I claimed now that the
half spaces for hypothesis space

62
00:04:27.840 --> 00:04:32.700 
but I want to go into more detail
here and well for answering

63
00:04:32.700 --> 00:04:37.570 
this question we have to figure out how
does this uniform decision program

64
00:04:37.700 --> 00:04:40.300 
that I talked about earlier,
you remember I said

65
00:04:40.740 --> 00:04:44.630 
the predictors all have to be
similar in some way like in a

66
00:04:44.630 --> 00:04:45.600 
computable way.

67
00:04:47.820 --> 00:04:51.250 
Ok let's look at the half spaces
a little bit more detail first.

68
00:04:51.640 --> 00:04:55.870 
So for example the line I drew
on the last slide has the

69
00:04:55.870 --> 00:04:59.460 
following equation. So x2 which
is like in two dimensional

70
00:04:59.460 --> 00:05:03.590 
setting often called y equals
two thirds of x1 which is

71
00:05:03.590 --> 00:05:06.020 
then often called
only x plus four.

72
00:05:07.920 --> 00:05:13.020 
Ok so I can like do some equivalent
transformations and then I obtain

73
00:05:13.220 --> 00:05:16.390 
the normal form which is
twelve equals minus two x1

74
00:05:16.390 --> 00:05:17.680 
plus three x2.

75
00:05:19.190 --> 00:05:22.580 
So I have a normal vector which
is easily spotted from the

76
00:05:22.580 --> 00:05:27.110 
normal form which is -2, 3 in this
point outside the half-space.

77
00:05:28.280 --> 00:05:30.100 
So it gives me
the direction.

78
00:05:31.890 --> 00:05:36.940 
Ok so if I just multiply by minus
one I obtain another half-space

79
00:05:37.220 --> 00:05:42.630 
named the one that is a yeah just the opposite.
So the line belongs to the half-space

80
00:05:42.840 --> 00:05:46.720 
and all above it in the,
yeah yellow case.

81
00:05:48.790 --> 00:05:53.020 
Ok so what we will do is we will
consider all half spaces given

82
00:05:53.020 --> 00:05:56.980 
by such a normal form with integer
values for the parameters.

83
00:06:00.040 --> 00:06:02.960 
Now why does this form
hypothesis space?

84
00:06:03.630 --> 00:06:08.000 
Well let's look at a possible
procedure for deciding whether

85
00:06:08.000 --> 00:06:09.530 
something is in
the half-space.

86
00:06:10.580 --> 00:06:15.010 
Again we have a0 equals twelve, a1
equals minus two, a2 equals three

87
00:06:15.120 --> 00:06:19.230 
and we have some vector I
just arbitrarily chose -2,7

88
00:06:19.340 --> 00:06:25.380 
and now the computer can easily
compute whether this point -2,7

89
00:06:25.680 --> 00:06:29.540 
belongs to the half-space
which I called here l12

90
00:06:29.540 --> 00:06:32.270 
-2,3 because these other
values of the parameters.

91
00:06:32.980 --> 00:06:36.900 
Ok so just insert the values, compute
whether twelve is greater or equal

92
00:06:37.140 --> 00:06:41.870 
the result of -2 times
-2 plus 3 times 7

93
00:06:42.030 --> 00:06:43.370 
and you will know
the answer.

94
00:06:46.550 --> 00:06:51.140 
Well I mean that doesn't really
depend on the values of a2, a1,

95
00:06:51.800 --> 00:06:55.920 
a0 and on x1 and x2. So we
will just do the same thing

96
00:06:55.920 --> 00:06:58.390 
no matter which concrete
values we have.

97
00:07:00.090 --> 00:07:03.630 
And this is a decision procedure
that is in this sense universal

98
00:07:03.630 --> 00:07:07.390 
in these inputs and this is
exactly what we are looking for,

99
00:07:07.910 --> 00:07:11.470 
in order to show that something
is a valid hypothesis space.

100
00:07:11.620 --> 00:07:16.340 
But well something is missing
here because these are all

101
00:07:16.340 --> 00:07:21.560 
integer values and we talked about natural
numbers when we defined hypothesis spaces.

102
00:07:21.800 --> 00:07:25.330 
So we still have to fix something
here. Ok let's do that now.

103
00:07:27.660 --> 00:07:32.840 
Well integers are hmm well
I mean we start with

104
00:07:32.850 --> 00:07:34.510 
zero and map it to zero

105
00:07:35.190 --> 00:07:39.100 
and then -1 gets
mapped to 1, 1 to 2,

106
00:07:39.100 --> 00:07:43.920 
-2 to 3 and so on. You can read
the table. So we have a way to

107
00:07:44.120 --> 00:07:46.410 
well to enumerate
the integers.

108
00:07:47.200 --> 00:07:51.060 
And the computer can just remember
that enumeration and use it

109
00:07:53.010 --> 00:07:57.740 
and so with a merge method
we can do the following.

110
00:07:58.210 --> 00:08:01.860 
We have inputs i and
n, let's assume.

111
00:08:02.800 --> 00:08:06.890 
From i and n we can reconstruct,
with the merge method we

112
00:08:06.900 --> 00:08:11.280 
will talk about a bit more in
the exercises. i0, i1 and

113
00:08:11.280 --> 00:08:15.700 
i2, so three natural numbers, a
vector of three natural numbers

114
00:08:15.830 --> 00:08:17.470 
that was unmerged
from i

115
00:08:18.750 --> 00:08:22.890 
and n1 n2 unmerged from n
and then with the like

116
00:08:22.890 --> 00:08:26.120 
with the table you just saw above
the computer can just compute

117
00:08:26.330 --> 00:08:30.880 
vector of three integers
a0 a1 a2 and 2

118
00:08:30.880 --> 00:08:35.210 
enters x1 x2 and now it can
run the uniform decision

119
00:08:35.210 --> 00:08:38.730 
procedure that we talked about
in the last slide and decide

120
00:08:38.780 --> 00:08:42.830 
whether this point x1 x2 it's
in the half-space defined

121
00:08:42.830 --> 00:08:44.640 
by a0 a1 and a2.

122
00:08:46.130 --> 00:08:50.370 
And then the computer returns
yes or one if the answer

123
00:08:50.380 --> 00:08:54.610 
is yes the point is inside and
zero it says no otherwise.

124
00:08:55.210 --> 00:08:57.680 
And this exactlydefines
fi of n.

125
00:09:02.090 --> 00:09:06.140 
Ok so what did we do? We defined
the hypothesis space to be a set

126
00:09:06.520 --> 00:09:11.550 
of predictors and we yeah we
require to the attribute computer

127
00:09:11.550 --> 00:09:16.240 
program in which in a uniform way
decides whether something is

128
00:09:16.460 --> 00:09:22.120 
in the i's hypothesis or the set
defined by the i's hypothesis.

129
00:09:22.720 --> 00:09:26.660 
In the machine learner now for
every finite input sequence

130
00:09:26.670 --> 00:09:30.740 
of labeled natural numbers gives
us an extra number and this

131
00:09:30.750 --> 00:09:34.840 
is the current predictor, its
things fits the data best.

132
00:09:37.220 --> 00:09:42.580 
So this what we checked is also
suitable for our situation

133
00:09:42.580 --> 00:09:46.830 
in which we binary labeled images
with half spaces because

134
00:09:46.830 --> 00:09:50.560 
the set of half spaces is a
valid hypothesis space.

135
00:09:52.080 --> 00:09:56.300 
And there are lots of hypothesis bases
one could think about. For example also

136
00:09:56.530 --> 00:10:01.310 
the set of weight vectors for a fixed
architecture of a neural network.

137
00:10:02.930 --> 00:10:07.370 
Ok so in this video we clarified what
the range of the machine learner is

138
00:10:08.480 --> 00:10:11.980 
and in the next video we will
clarify what learning actually

139
00:10:11.980 --> 00:10:16.910 
means and what this like phrase from
sequences of increasing length

140
00:10:17.340 --> 00:10:18.940 
yeah how this is
formalized.
