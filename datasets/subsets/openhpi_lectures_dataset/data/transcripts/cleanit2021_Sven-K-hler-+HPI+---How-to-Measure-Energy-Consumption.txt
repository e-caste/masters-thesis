WEBVTT

1
00:00:00.850 --> 00:00:06.030 
Hello and welcome to energy-aware resource
management in Heterogeneous Computing Systems,

2
00:00:06.310 --> 00:00:10.310 
part of the Clean IT firm here
at Hasso Plattner Institute.

3
00:00:11.240 --> 00:00:15.820 
In the next few videos, my
colleagues, Max, Lukas, and I are

4
00:00:15.820 --> 00:00:19.990 
going to show you our approach
towards energy-aware computing,

5
00:00:20.390 --> 00:00:24.790 
and our research. We are part of the
Operating Systems and Middleware group

6
00:00:24.950 --> 00:00:29.300 
led by Professor Andreas Polze and
we try to understand energy demand

7
00:00:29.450 --> 00:00:31.180 
as an

8
00:00:31.810 --> 00:00:34.510 
operating resource of
our computer systems.

9
00:00:35.290 --> 00:00:41.090 
The job of an operating system is
to control operation in resources

10
00:00:41.590 --> 00:00:45.480 
and it also is the contact zone
between hard and software.

11
00:00:45.970 --> 00:00:49.250 
So, we can try to optimize
your energy demand

12
00:00:49.690 --> 00:00:54.030 
even after you've finished writing your
code and optimizing your algorithms.

13
00:00:54.820 --> 00:01:00.460 
And we need to understand that
energy can change over time

14
00:01:01.240 --> 00:01:05.210 
and the energy demand of the
entire system depending not only

15
00:01:05.210 --> 00:01:09.330 
on what other software is running
on that computer but also on

16
00:01:10.230 --> 00:01:12.390 
what is available on
the energy market.

17
00:01:13.220 --> 00:01:17.550 
The reason for this is simply the
growth of renewable energies.

18
00:01:18.590 --> 00:01:22.620 
Renewable energies in 2020
made about one half of

19
00:01:22.640 --> 00:01:25.570 
the entire energy production
here in Germany.

20
00:01:26.350 --> 00:01:31.710 
And as you can see out of
this, the highest amount of

21
00:01:31.710 --> 00:01:34.900 
renewable energy was actually
wind and solar power.

22
00:01:35.480 --> 00:01:40.670 
Those are considered volatile sources as
their availability changes over time

23
00:01:41.080 --> 00:01:44.970 
and as this production is then
fed into the European grid

24
00:01:44.990 --> 00:01:50.510 
and the energy market, in the energy market
you can see that the fluctuating energy

25
00:01:50.620 --> 00:01:53.540 
supply actually leads
to fluctuating

26
00:01:54.190 --> 00:01:58.630 
energy prices. Sometimes it
might even happen that

27
00:01:59.100 --> 00:02:03.150 
there's an overproduction, so you
actually have a negative price

28
00:02:03.430 --> 00:02:08.150 
in the energy market so that big
consumers are incentivised to

29
00:02:08.450 --> 00:02:11.530 
consume more and more energy
to stabilize the system.

30
00:02:11.980 --> 00:02:16.460 
This, for example, happened
here in week 43 of 2017

31
00:02:17.030 --> 00:02:22.130 
and we know that for a computing
lab there are very interesting

32
00:02:22.130 --> 00:02:26.810 
implications if we're taking this
changing price. So, you can say

33
00:02:27.160 --> 00:02:32.970 
that if we have a certain excess
demand, so we have not enough

34
00:02:33.440 --> 00:02:38.550 
supply generated by our power sources,
we maybe want to move down our

35
00:02:39.050 --> 00:02:40.710 
energy consumption
in our data lab

36
00:02:41.430 --> 00:02:45.390 
back into a certain target zone
and leave the penalty zones as

37
00:02:45.670 --> 00:02:53.520 
displayed up here. Furthermore, you also,
if there's a surplus of production,

38
00:02:53.790 --> 00:02:56.680 
maybe want to increase the
demand by computing more.

39
00:02:57.460 --> 00:03:01.470 
So, how can we control the
energy consumption in our

40
00:03:01.590 --> 00:03:03.860 
data centre, of course,

41
00:03:04.730 --> 00:03:06.350 
there's always a
good idea to

42
00:03:07.040 --> 00:03:12.140 
break down your options. The first option
is to work differently, so to use

43
00:03:12.280 --> 00:03:14.820 
more of your computer
resources.

44
00:03:15.530 --> 00:03:20.970 
This is commonly used already to scale your
operations like changing the CPU frequency or

45
00:03:21.260 --> 00:03:24.910 
using more computer resources of
the same type, so scaling out.

46
00:03:26.070 --> 00:03:29.300 
This is already happening and
this is well researched.

47
00:03:29.970 --> 00:03:34.440 
Another idea which is more important
from an operating system perspective

48
00:03:34.730 --> 00:03:40.350 
is to work at another time so you
want to try to pick or defer jobs

49
00:03:40.590 --> 00:03:44.090 
for a matching energy profile. First,
of course, you have to understand

50
00:03:44.090 --> 00:03:47.670 
what the energy profile of a
current job is and then move it

51
00:03:47.670 --> 00:03:49.410 
to a time when it's
more suitable.

52
00:03:50.110 --> 00:03:53.380 
This is very popular with
embedded systems. So,

53
00:03:53.880 --> 00:03:58.690 
imagine that your phone is deferring
some back up until your back

54
00:03:58.970 --> 00:04:02.620 
to energy socket, with your phone
connected to your energy socket.

55
00:04:03.350 --> 00:04:04.690 
And this is also
interesting

56
00:04:05.480 --> 00:04:10.610 
to run your updates or your backups
on your big computer systems

57
00:04:10.670 --> 00:04:12.410 
when there's more
energy available.

58
00:04:13.390 --> 00:04:16.030 
And a third option, of
course, is to work

59
00:04:16.630 --> 00:04:19.980 
elsewhere, so to work on
other hardware components

60
00:04:20.470 --> 00:04:22.380 
or on a different
hardware component

61
00:04:23.010 --> 00:04:27.960 
with another energy profile, this is
the main focus of our research here

62
00:04:28.090 --> 00:04:30.590 
at Operating Systems and
Middleware group at HPI.

63
00:04:32.590 --> 00:04:38.050 
Let me explain what kind of different
hardware classes I mean. So, for example,

64
00:04:38.190 --> 00:04:42.900 
you all know these general-purpose
CPU's which are extremely flexible

65
00:04:43.240 --> 00:04:46.790 
but they are not so specialized
or efficient for certain tasks

66
00:04:46.890 --> 00:04:48.570 
they are really
general purpose.

67
00:04:49.530 --> 00:04:54.220 
Maybe less familiar but still also
very popular are GPU's, so graphics

68
00:04:54.640 --> 00:04:58.210 
processing units which have a
typically very high throughput

69
00:04:58.420 --> 00:05:02.050 
but they're only suitable for certain
data or parallel workloads.

70
00:05:02.730 --> 00:05:05.710 
And maybe not so
familiar to you are

71
00:05:06.270 --> 00:05:12.080 
Field Programmable Gate Arrays or FPGA's
for short which can be highly optimized

72
00:05:12.180 --> 00:05:14.890 
very energy efficient but
difficult to program.

73
00:05:15.500 --> 00:05:19.100 
My colleagues in the next few clips
are going to focus on GPU's

74
00:05:19.100 --> 00:05:25.740 
and FPGA's in detail but first, I want to
stress how we can actually get information

75
00:05:25.920 --> 00:05:28.730 
how much energy our
program is consuming.

76
00:05:29.170 --> 00:05:33.410 
The problem is for those highly
different hardware classes

77
00:05:33.620 --> 00:05:38.450 
and for each class from different hardware
windows, there's no standardized interface

78
00:05:38.760 --> 00:05:41.790 
to access energy information
of what's going on.

79
00:05:42.260 --> 00:05:47.540 
Many of them have built-in counters but
they differ from platform to platform,

80
00:05:48.380 --> 00:05:49.720 
that's why we
started with

81
00:05:50.390 --> 00:05:55.420 
showing you how to get information
on what's going on on your system.

82
00:05:56.210 --> 00:06:00.010 
Here you can see a diagram
of different power draws

83
00:06:00.580 --> 00:06:04.080 
of a computer platform, we're going
to show you in a few minutes.

84
00:06:04.660 --> 00:06:07.540 
You can see the graphs
changing over time

85
00:06:08.230 --> 00:06:12.190 
as we're doing computation and
as I'm starting a job on the

86
00:06:12.190 --> 00:06:15.030 
system which is a physics
simulation. We're simulating heat

87
00:06:15.220 --> 00:06:17.440 
moving in a 2D
field and it's

88
00:06:17.850 --> 00:06:19.830 
comparable to your
climate simulation.

89
00:06:21.010 --> 00:06:26.420 
As this runs you see that the power draw of
the different components of the system

90
00:06:26.560 --> 00:06:29.770 
changed over time and
our tool "Pinpoint",

91
00:06:30.300 --> 00:06:34.500 
after the simulation has finished,
can tell you how much energy

92
00:06:34.510 --> 00:06:39.250 
which is the integral over the different
power samples, has actually been used

93
00:06:39.440 --> 00:06:40.660 
by those components.

94
00:06:42.170 --> 00:06:45.710 
So, what can we learn
from those diagrams?

95
00:06:46.640 --> 00:06:50.850 
The experiment was run on an Nvidia
Jetson TX2 development board,

96
00:06:51.110 --> 00:06:53.750 
this is comparable to
what you find in your

97
00:06:54.160 --> 00:06:58.540 
gaming engine or in your gaming
platform or in your automotive

98
00:06:58.990 --> 00:07:04.700 
system. And we took now this heat
map simulation and implemented it

99
00:07:04.920 --> 00:07:09.930 
with two different approaches
one being GPU based with CUDA

100
00:07:10.120 --> 00:07:14.150 
and the other one here on the left
side being CPU based with OpenMP.

101
00:07:14.990 --> 00:07:20.580 
And now we can compare the different energy
consumption by those two implementations

102
00:07:20.830 --> 00:07:26.150 
We find that the CPU based version
used 96 joules to compute

103
00:07:26.800 --> 00:07:29.510 
and the GPU version
only 87 joules.

104
00:07:29.940 --> 00:07:33.750 
So, this is a good thing now we
know on this particular platform

105
00:07:33.930 --> 00:07:38.290 
we should try to pick the GPU based
implementation if we want to save energy.

106
00:07:39.360 --> 00:07:41.690 
But there's more to this
diagram we can learn here.

107
00:07:42.350 --> 00:07:46.480 
Actually, if we look at the CPU
implementation we see a very high jitter

108
00:07:46.800 --> 00:07:50.810 
compared to the GPU version
within the power draws.

109
00:07:51.590 --> 00:07:55.530 
We can see that the CPU was
actually running too hot and was

110
00:07:55.530 --> 00:07:59.910 
clocked down after some time and
we see that the processing

111
00:07:59.910 --> 00:08:02.610 
speed of the CPU
was actually then

112
00:08:03.270 --> 00:08:07.040 
the main culprit in limiting
the energy supply

113
00:08:07.870 --> 00:08:14.200 
of the memory system. So, as the CPU was
jittering the memory did likewise.

114
00:08:15.460 --> 00:08:19.180 
Not so on the GPU version on the
right side of this picture,

115
00:08:19.470 --> 00:08:23.370 
here we can see a very smooth
line, the GPU simply maxed out

116
00:08:23.580 --> 00:08:28.970 
and also the interlink between the memory
system and the GPU was then saturated.

117
00:08:29.170 --> 00:08:32.760 
We don't see that much jitter but
we see that potentially the

118
00:08:32.760 --> 00:08:34.420 
memory could go
even further.

119
00:08:35.610 --> 00:08:38.560 
A second aspect we can see here
in this picture is that the

120
00:08:38.560 --> 00:08:42.090 
slopes of this graph are
actually somehow curved.

121
00:08:42.830 --> 00:08:48.450 
This is because the sensors built into
the support are actually not telling us

122
00:08:48.610 --> 00:08:53.330 
the entire truth. You see, that they're
averaging overtime to smooth out

123
00:08:53.620 --> 00:08:57.420 
certain problems in the
measurements. So,

124
00:08:58.440 --> 00:09:01.960 
we then thought maybe it's a
good idea to not only rely on

125
00:09:01.960 --> 00:09:06.610 
the internal measurements but
also try to connect a very fine

126
00:09:06.610 --> 00:09:10.280 
grained smart meter to the outside and
see what's actually going on there.

127
00:09:10.880 --> 00:09:14.310 
It says it has a certain latency
as you can see in the diagram

128
00:09:14.480 --> 00:09:19.590 
but it has very steep slopes and not
that much curved and we can also find

129
00:09:20.040 --> 00:09:25.300 
that there's about two point five what
discrepancy between the internal readings

130
00:09:25.550 --> 00:09:29.710 
and our external measurements.
We attribute those to aspects

131
00:09:29.710 --> 00:09:35.320 
like the carrier board itself, or the
power lost in the power supply module

132
00:09:37.130 --> 00:09:40.580 
of our development board.
So, we now know that

133
00:09:42.110 --> 00:09:45.170 
those diagrams were kind of
telling the truth but of course,

134
00:09:45.170 --> 00:09:49.920 
we can now start to also factor in
missing aspects of the energy

135
00:09:49.920 --> 00:09:51.540 
measurement so far
in our program.

136
00:09:52.850 --> 00:09:56.530 
But our tool does not only stop
there, but we can also now move

137
00:09:56.570 --> 00:10:01.560 
to an entirely different platform, that's
why we move to an open power server system

138
00:10:01.740 --> 00:10:05.730 
with an NVIDIA video graphics card
attached inside via the PCI.

139
00:10:06.440 --> 00:10:11.760 
Here we are in the very same simulation
again CPU-based and GPU-based.

140
00:10:12.140 --> 00:10:15.440 
And now we can see that on
this particular system

141
00:10:16.050 --> 00:10:18.700 
the energy consumption
of the OpenMP

142
00:10:19.530 --> 00:10:22.410 
implementation was actually
about 500 joules less.

143
00:10:23.550 --> 00:10:27.350 
Of course, taking into effect that
the entire system was using

144
00:10:27.350 --> 00:10:28.720 
way more energy than a

145
00:10:29.480 --> 00:10:33.920 
small system. We now know that if you
have this particular server running

146
00:10:34.190 --> 00:10:40.260 
you should favour choosing a job implemented
with OpenMP over the job implemented

147
00:10:40.360 --> 00:10:46.160 
in CUDA which is a nice thing to know if
you want to make decisions as a runtime

148
00:10:46.320 --> 00:10:50.190 
or operating system if several
implementations are available.

149
00:10:52.610 --> 00:10:57.100 
The tool is called "Pinpoint" and
for those familiar with the Perf

150
00:10:57.690 --> 00:11:02.660 
utility on Linux, it looks a little bit
like this, it was our inspiration.

151
00:11:03.110 --> 00:11:06.410 
And now we have a cross-platform
tool that can measure the

152
00:11:06.410 --> 00:11:08.770 
energy consumption of
your applications.

153
00:11:09.330 --> 00:11:11.790 
You can simply plug
in new data sources

154
00:11:12.210 --> 00:11:15.150 
and you can use it for your own
projects. If you want to you

155
00:11:15.150 --> 00:11:19.130 
can check it out on our
Github page. And now

156
00:11:20.910 --> 00:11:23.620 
you can really start
looking into what

157
00:11:24.580 --> 00:11:28.740 
energy different components of your
computer are using with your programs.

158
00:11:29.250 --> 00:11:34.090 
We encourage you to do so because let
me close with a final side note,

159
00:11:34.440 --> 00:11:41.210 
we often try to think that saving
computing time means saving energy,

160
00:11:42.130 --> 00:11:47.400 
the truth is it's not always this
way. So, our co-authors and

161
00:11:47.400 --> 00:11:52.020 
friends from both home and Erlangen did
a few years ago an interesting study

162
00:11:52.190 --> 00:11:57.500 
where they took a real-time benchmark
suite and compiled it for

163
00:11:58.060 --> 00:12:01.990 
our microcontroller and compared
ten different settings,

164
00:12:03.340 --> 00:12:06.480 
meaning the low power mode
and the normal power mode.

165
00:12:07.150 --> 00:12:11.870 
It turned out that although
the run time did of course

166
00:12:11.870 --> 00:12:14.590 
decrease when the CPU
was not troubled.

167
00:12:15.360 --> 00:12:20.220 
The energy demand did not
equally decrease in some cases

168
00:12:20.880 --> 00:12:24.990 
the energy demand
even rose for

169
00:12:25.870 --> 00:12:27.430 
computing the
entire benchmark.

170
00:12:28.910 --> 00:12:32.710 
Of course, this somehow makes sense
if you imagine a car driving

171
00:12:32.710 --> 00:12:35.890 
at 130 kilometres per
hour and another one

172
00:12:36.370 --> 00:12:40.590 
just driving 100 kilometres per hour
both travelling the same distance,

173
00:12:41.160 --> 00:12:45.410 
depending on factors like air
resistance, the traffic, how the

174
00:12:45.410 --> 00:12:47.840 
engine is built, and
so on, it takes

175
00:12:49.300 --> 00:12:52.440 
different amounts of energy and
sometimes the faster driving car

176
00:12:52.720 --> 00:12:57.020 
of course, would maybe have
consumed more gasoline at the end

177
00:12:57.320 --> 00:12:58.560 
than the slower
driving car.

178
00:12:59.850 --> 00:13:03.120 
The authors also then went on and
compared different compilers,

179
00:13:03.140 --> 00:13:07.870 
so, GCC and Klang and found that also
there it was possible to save energy

180
00:13:08.290 --> 00:13:14.160 
by just using a different compiler
and that speed up did not correlate

181
00:13:14.580 --> 00:13:16.640 
linearly with the
saved run time.

182
00:13:18.120 --> 00:13:25.590 
So, if you want to save energy, start measuring
and start measuring the right things

183
00:13:25.990 --> 00:13:30.270 
and then on a case by case scenario
pick the right execution

184
00:13:30.270 --> 00:13:33.850 
environment and built
environment for your workload.

185
00:13:34.650 --> 00:13:40.310 
So, you can start saving energy or
to stay in the picture of mobility

186
00:13:40.780 --> 00:13:44.980 
if you know that the weather's right
and the distance is ok maybe

187
00:13:45.250 --> 00:13:48.040 
start to think about,
maybe to pick a bike

188
00:13:48.580 --> 00:13:50.590 
over the car to get
to your target.

189
00:13:51.870 --> 00:13:57.430 
So, to sum up, computers nowadays have
different components being built-in

190
00:13:58.130 --> 00:14:01.870 
and the application should support
different hardware targets

191
00:14:02.560 --> 00:14:07.180 
and now with the right tooling and infrastructure,
we can start making placement decisions

192
00:14:07.470 --> 00:14:12.450 
on which hardware to run our software
on in order to save energy.

193
00:14:13.300 --> 00:14:15.370 
And if you want to
actually quantify this

194
00:14:16.240 --> 00:14:19.260 
start making measurements
and start then to

195
00:14:19.730 --> 00:14:23.950 
prove your application not only
for performance but also for

196
00:14:23.950 --> 00:14:24.840 
energy efficiency.
