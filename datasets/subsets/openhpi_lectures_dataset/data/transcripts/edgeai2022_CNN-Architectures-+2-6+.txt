WEBVTT

1
00:00:00.740 --> 00:00:05.580 
Hello and welcome in the last video we talked about AlexNet.

2
00:00:06.339 --> 00:00:13.230 
Today we will continue to review other ConvNets architectures

3
00:00:13.230 --> 00:00:17.010 
After AlexNet won the ImageNet challenge in 2012,

4
00:00:17.600 --> 00:00:20.449 
many works have been improved based on it.

5
00:00:21.239 --> 00:00:31.370 
For example, The ZFNet change the kernel size of the first convolutional layer to 7 x 7, and increased the number of

6
00:00:31.370 --> 00:00:35.250 
channels for the 3rd, 4th and 5th convolutional layer.

7
00:00:35.939 --> 00:00:46.539 
This simple change resulted in an accuracy increase of nearly 5% but at the same time it also increases the computation

8
00:00:46.539 --> 00:00:48.359 
overhead of the network.

9
00:00:48.939 --> 00:00:58.640 
Nevertheless, it indicated that AlexNet is far from reaching the upper limit of the accuracy.

10
00:00:58.640 --> 00:01:01.460 
Time has come to 2014.

11
00:01:02.039 --> 00:01:06.760 
This year the ImageNet challenge ushered in two winners.

12
00:01:07.640 --> 00:01:16.670 
I want to first introduce VGG-Net proposed by the Visual Geometry Group of Oxford University and this is the network with 19

13
00:01:16.670 --> 00:01:17.260 
layers.

14
00:01:20.040 --> 00:01:29.060 
VGG-Net is the runner up in the image classification task and the winner of localization task in the ImageNet challenge

15
00:01:29.060 --> 00:01:29.659 
that year.

16
00:01:30.140 --> 00:01:31.709 
Similar to AlexNet,

17
00:01:31.939 --> 00:01:35.959 
VGG-Net is also gradually shrinking the resolution.

18
00:01:36.340 --> 00:01:47.359 
It means a spatial dimension of the intermediate feature maps for achieving larger receptive field and while increasing the

19
00:01:47.359 --> 00:01:49.859 
number of the weight channels.

20
00:01:56.280 --> 00:01:56.760 
VGG-Net strictly used 3 x 3 filters with stride = 1 and pad = 1.

21
00:01:57.140 --> 00:02:06.189 
A more fine grained feature learning process is gradually expanded from a small receptive field to the global context to

22
00:02:06.189 --> 00:02:14.360 
the larger one, so increasing the channel will keep the similar information capacity of the network at each stage.

23
00:02:17.939 --> 00:02:27.379 
A notable feature is that the VGG-Net uses three consecutive 3 x 3 convolutions instead of a 7 x 7 convolutional

24
00:02:27.379 --> 00:02:34.159 
kernel. So the size of the receptive field achieved by these two methods is the same.

25
00:02:34.639 --> 00:02:41.659 
However, due to the more convolutional layers have been used, it increases the non linearity.

26
00:02:42.039 --> 00:02:48.069 
In addition, the small convolutional Kernel effectively reduces the number of parameters.

27
00:02:48.080 --> 00:02:54.250 
So this kind of design can reduce 45% of the parameters.

28
00:02:58.060 --> 00:03:06.430 
VGG-Net became the most accurate open source model and swept the academia. It became the most popular backbone model

29
00:03:06.430 --> 00:03:12.060 
before ResNet appeared and brought more than 60,000 citations.

30
00:03:12.439 --> 00:03:18.860 
It has much more parameters than AlexNet but half its error rate on the ImageNet challenge.

31
00:03:19.639 --> 00:03:29.870 
It once again confirmed an intuition about deep neural networks that ConvNets need to have a deeper network to learn hierarchical

32
00:03:29.870 --> 00:03:32.259 
representations of visual data.

33
00:03:33.240 --> 00:03:42.560 
Furthermore, the VGGNet first time introduced the modular design concept with stages and blocks in the deep network.

34
00:03:43.139 --> 00:03:52.039 
This design principle has become a common standard in the community.

35
00:03:52.039 --> 00:03:58.990 
GoogleNet or Google LeNet is a deep network structure proposed by Google researchers.

36
00:03:59.000 --> 00:04:02.150 
The name is to pay tribute to LeNet.

37
00:04:03.120 --> 00:04:12.469 
The GoogLeNet team proposed the inception network structure to construct a basic neuron module and to build a sparse and

38
00:04:12.469 --> 00:04:14.050 
high performance model.

39
00:04:15.139 --> 00:04:19.279 
GoogleNet is the winner of the ImageNet classification challenge

40
00:04:19.279 --> 00:04:30.660 
in 2014, it has made a bolder network structure attempt. Although there are 22 layers, the size of GoogleNet is much smaller

41
00:04:30.660 --> 00:04:32.860 
than AlexNet and VGGNet.

42
00:04:33.639 --> 00:04:42.959 
GoogLeNet has five million parameters about 12 times less than AlexNet and 27 times less than VGG19 network.

43
00:04:43.740 --> 00:04:52.709 
Therefore, when the memory or computing resources are limited, GoogLeNet is obviously a better choice and the accuracy of GoogLeNet

44
00:04:52.709 --> 00:04:54.259 
is even better.

45
00:04:56.839 --> 00:05:06.449 
Generally speaking, the most direct way to improve network performance is to increase network depth and width. Depth refers

46
00:05:06.459 --> 00:05:11.759 
to the number of network layers and width refers to the number of neurons.

47
00:05:12.240 --> 00:05:21.449 
However, this method has a problem of easy overfitting, increase computation overhead and the gradient vanishing problem.

48
00:05:22.240 --> 00:05:31.290 
The way to solve this problem is to reduce the parameters while increasing the depth and width. To reduce parameter, it is

49
00:05:31.290 --> 00:05:37.949 
straightforward to think of tuning full connections into sparse connections.

50
00:05:37.959 --> 00:05:48.500 
However, in terms of implementation, the actual computation efficiency will not be much improved after the full connection

51
00:05:48.500 --> 00:05:57.980 
becomes sparse because the most modern hardware is optimized for the dense matrix computation.

52
00:05:57.990 --> 00:06:06.839 
Although the sparse matrix has a small amount of data and but the current hardware doesn't

53
00:06:06.839 --> 00:06:11.160 
really fully support this kind of data structure.

54
00:06:12.139 --> 00:06:21.730 
Therefore, Google team proposed inception module, a sparse network structure that can generate dense data which can increase

55
00:06:21.730 --> 00:06:28.360 
the performance of the neural network and ensure the efficiency of the use of computing resources.

56
00:06:29.040 --> 00:06:31.259 
So let's take a closer look.

57
00:06:32.139 --> 00:06:35.759 
GoogleNet is proposed of three basic components.

58
00:06:36.620 --> 00:06:45.259 
The stem does preliminary processing of the image data and enter three stacked inception groups.

59
00:06:45.939 --> 00:06:52.959 
Each group consists of three inception modules and each group corresponds to a classification head.

60
00:06:53.540 --> 00:07:02.949 
So GoogleNet has totally three classification heads and the joint optimization of those three heads can be considered as

61
00:07:02.949 --> 00:07:09.550 
multi task learning, which helps to improve the generalization ability of the whole network.

62
00:07:10.639 --> 00:07:15.759 
So the design of the classification head and stem is more conventional.

63
00:07:15.769 --> 00:07:19.759 
So let's take a closer look at the inception module.

64
00:07:22.939 --> 00:07:29.259 
The inception module applies parallel future operations on the input from the previous layer.

65
00:07:30.139 --> 00:07:34.480 
Multiple receptive field size are used for convolution. Here,

66
00:07:34.480 --> 00:07:44.089 
in this case they are using 1x1, 3x3 and 5x5 convolutional kernel and 3x3 pooling is also

67
00:07:44.089 --> 00:07:48.199 
used to preserve the depth of the input.

68
00:07:49.339 --> 00:07:59.439 
On the one hand it increases the width of the network and on the other it also increases the adaptability of the network

69
00:07:59.449 --> 00:08:00.860 
to different skills.

70
00:08:01.439 --> 00:08:06.670 
The problem here is that pooling layer preserves the depth of the input layer.

71
00:08:07.339 --> 00:08:14.449 
This means that the total depth at the output of such module will always become bigger.

72
00:08:14.939 --> 00:08:20.459 
It results in a huge computation overhead and increase.

73
00:08:21.209 --> 00:08:25.550 
To improve this problem, the bottleneck design has been proposed.

74
00:08:26.339 --> 00:08:34.649 
Generally speaking using bottleneck design, we can keep the width and height unchanged, and only reduce the depth.

75
00:08:35.139 --> 00:08:44.399 
And this can be easily achieved by using a 1x1 convolution with stride=1 and we just reduce the filter numbers

76
00:08:44.409 --> 00:08:51.639 
as shown in this example.

77
00:08:51.639 --> 00:09:00.440 
And we can see in the figure a 1x1 convolution kernel is added before each 3x3 and 5x5 convolution

78
00:09:00.450 --> 00:09:04.350 
and also added after a max pooling.

79
00:09:04.360 --> 00:09:12.850 
This effectively reduces the number of channels during the concatenation and this design will now decrease accuracy.

80
00:09:13.440 --> 00:09:20.960 
You can see that the current computation complexity of this module has been reduced by about 58%.

81
00:09:22.039 --> 00:09:31.399 
The 1x1 convolution kernel is exactly the same as a normal convolution kernel, except that it's no longer learns

82
00:09:31.399 --> 00:09:38.169 
the local area and does not consider the correlation between pixels. And the 1x1 convolution

83
00:09:38.179 --> 00:09:46.440 
is thus integrating the information of different channels.

84
00:09:46.440 --> 00:09:52.049 
Okay, thank you for watching the video and we will continue our discussion in the next video.
