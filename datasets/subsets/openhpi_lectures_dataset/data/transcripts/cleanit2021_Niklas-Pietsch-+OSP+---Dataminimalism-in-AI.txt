WEBVTT

1
00:00:00.630 --> 00:00:04.570 
Hi my name is Niklas Pietsch and
I'm working as data scientist

2
00:00:04.580 --> 00:00:07.540 
at the OTTO group Solution
Provider, OSP.

3
00:00:08.380 --> 00:00:11.930 
In this video, I will present you the
results of a research project about

4
00:00:12.730 --> 00:00:14.790 
data minimalism in
AI applications.

5
00:00:15.770 --> 00:00:19.610 
This project is a joint
project of OSP and OTTO,

6
00:00:20.310 --> 00:00:22.410 
both of which are members
of the OTTO group,

7
00:00:22.960 --> 00:00:25.130 
one of the biggest e-commerce
companies in the world.

8
00:00:26.910 --> 00:00:31.930 
In recognition of our responsibility and
commitment to people and the planet,

9
00:00:32.640 --> 00:00:36.270 
the OTTO group's vision as
responsible commerce that inspire us.

10
00:00:37.590 --> 00:00:41.570 
This vision is the common thread
that has run through the OTTO

11
00:00:41.570 --> 00:00:43.700 
group for decades over
the generations.

12
00:00:44.830 --> 00:00:46.830 
Already in 1986,

13
00:00:47.440 --> 00:00:51.050 
Dr. Michael Otto has put the topic
environmental protection and

14
00:00:51.060 --> 00:00:54.180 
working in a sustainable way to
the heart of the core business.

15
00:00:55.420 --> 00:00:59.160 
Since then it's anger and the goals
and the strategy of the OTTO group.

16
00:01:01.550 --> 00:01:04.760 
OSP is the IT service
provider of the OTTO group

17
00:01:05.670 --> 00:01:08.680 
founded in nineteen ninety one. We
have thirty years of experience

18
00:01:09.160 --> 00:01:11.050 
in IT for retail
and logistics

19
00:01:12.240 --> 00:01:15.050 
with about three hundred
fifty colleagues. We

20
00:01:15.470 --> 00:01:18.630 
operate internationally
at five locations and

21
00:01:19.030 --> 00:01:22.310 
focus on software development
and big data analysis

22
00:01:22.900 --> 00:01:25.970 
for clients from the retail
and logistics industry.

23
00:01:27.850 --> 00:01:31.230 
As member of the OTTO group family,
we daily seek for opportunities to

24
00:01:31.720 --> 00:01:36.040 
apply our know-how and expertise
to reduce the CO2 footprint

25
00:01:36.740 --> 00:01:38.140 
for our clients
worldwide.

26
00:01:39.410 --> 00:01:40.610 
Among these efforts we

27
00:01:41.220 --> 00:01:45.320 
also explore and implement the
concept of data minimalism to

28
00:01:45.320 --> 00:01:47.440 
large scale AI
applications.

29
00:01:49.560 --> 00:01:53.240 
Why most companies try to collect
as much data as possible

30
00:01:53.720 --> 00:01:57.790 
whether they need it or not - the basic
idea of data minimalism is to

31
00:01:58.240 --> 00:02:02.130 
only use that data that really has
value for business decisions.

32
00:02:03.470 --> 00:02:05.430 
This is beneficial
in many ways.

33
00:02:06.530 --> 00:02:09.160 
Using less data means that you
need less hardware to store

34
00:02:09.160 --> 00:02:11.440 
the data and less computing
time to process the data.

35
00:02:12.740 --> 00:02:16.930 
Therefore data minimalism allows for
reduction of power usage and by this

36
00:02:17.350 --> 00:02:20.000 
for a significant reduction
of carbon emissions.

37
00:02:21.280 --> 00:02:25.970 
Nowadays data traffic actually causes
about as much emissions as air traffic.

38
00:02:27.330 --> 00:02:31.960 
It definitely minimalism has an
economical and societal benefits.

39
00:02:32.690 --> 00:02:37.530 
It reduces monetary cost, allows
your applications to run faster

40
00:02:37.530 --> 00:02:42.190 
and to scale better and lowers
your security and privacy risks.

41
00:02:43.560 --> 00:02:45.350 
As I will show you
later in this video,

42
00:02:46.240 --> 00:02:48.930 
data minimalism
can even improve

43
00:02:49.650 --> 00:02:52.580 
the performance of large
scale AI applications.

44
00:02:55.470 --> 00:02:59.870 
In our research, we apply the
concept of data minimalism

45
00:03:00.380 --> 00:03:03.420 
to machine learning systems that
computes product recommendations.

46
00:03:05.260 --> 00:03:08.430 
Given the product user of an
online shop is currently viewing,

47
00:03:08.860 --> 00:03:12.310 
such an application computes possible
alternatives for that product

48
00:03:13.490 --> 00:03:14.960 
which are offered
to the user

49
00:03:15.670 --> 00:03:17.240 
in order to help him
make decisions.

50
00:03:19.090 --> 00:03:20.570 
As you know,

51
00:03:21.190 --> 00:03:24.640 
such recommendations are typically
displayed at the bottom

52
00:03:24.640 --> 00:03:27.620 
of a product pit within a
recommendation cinema

53
00:03:28.150 --> 00:03:30.310 
that states things like

54
00:03:31.050 --> 00:03:35.560 
you also might like or customer
support that item also bought that.

55
00:03:37.780 --> 00:03:40.510 
Input of our system
re the user sessions.

56
00:03:41.300 --> 00:03:45.460 
These other chronologically
sorted clicks

57
00:03:45.460 --> 00:03:48.090 
on product pages
by one user.

58
00:03:48.940 --> 00:03:52.250 
Every time you visit a thirty
or different online shop and

59
00:03:52.250 --> 00:03:56.220 
click through different product pages
you're creating such a session.

60
00:03:57.610 --> 00:03:59.610 
The sessions then are
processed by the

61
00:04:00.690 --> 00:04:02.310 
word2vec algorithm.

62
00:04:03.060 --> 00:04:04.280 
This is a machine
learning

63
00:04:04.940 --> 00:04:08.850 
algorithm that uses a single network
and represents the products

64
00:04:08.850 --> 00:04:10.390 
that occur in our
sessions as

65
00:04:11.450 --> 00:04:15.520 
vectors in a vector space as
illustrated in the figure in the

66
00:04:15.520 --> 00:04:16.510 
middle of this slide.

67
00:04:18.010 --> 00:04:22.740 
The key to this algorithm now is
that products that occur in

68
00:04:22.750 --> 00:04:24.550 
a similar user
journey context

69
00:04:25.580 --> 00:04:28.840 
and therefore may serve as good
recommendations for each other

70
00:04:29.490 --> 00:04:34.040 
are represented by vectors that are
close to each other in vector space.

71
00:04:35.240 --> 00:04:39.970 
Therefore based on the distances
of vectors in vector space,

72
00:04:40.560 --> 00:04:43.440 
we can compute a list of
good recommendations

73
00:04:43.930 --> 00:04:45.240 
for each of our product.

74
00:04:47.350 --> 00:04:50.110 
We quantify the performance
of our recommender system

75
00:04:51.040 --> 00:04:53.290 
using different key
performance indicators.

76
00:04:53.700 --> 00:04:57.170 
One of these widely used
quantity in e-commerce

77
00:04:57.790 --> 00:04:59.290 
is the conversion rate

78
00:05:00.240 --> 00:05:03.100 
defined as the percentage of customers
who bought a recommendation.

79
00:05:06.090 --> 00:05:09.390 
The goal of our data minimalistic
approach now is to

80
00:05:09.850 --> 00:05:13.790 
reduce the amount of data that be
used for the training of our system

81
00:05:14.280 --> 00:05:18.780 
to that minimum that is needed in order
to obtain the system performance,

82
00:05:21.100 --> 00:05:24.550 
to get a first indicator on how
much data we need we

83
00:05:26.040 --> 00:05:28.990 
performed an experiment where
we step by step increased

84
00:05:29.450 --> 00:05:32.330 
the amount of data used for
the training of the system.

85
00:05:33.050 --> 00:05:37.860 
At each step, we computed
the top five recommendations

86
00:05:39.050 --> 00:05:45.110 
and based on these, we quantified
the performance of the system

87
00:05:45.390 --> 00:05:46.930 
calculating the
conversion rate.

88
00:05:48.420 --> 00:05:53.400 
By this we can map the data
volume to the system's performance.

89
00:05:55.980 --> 00:05:59.330 
On this slide I'd like to show you some
of the results of this experiment.

90
00:05:59.990 --> 00:06:03.900 
In this figure the X axis
represents the amount of data

91
00:06:04.610 --> 00:06:09.540 
in percentage of the total amount of data
that was available for this experiment.

92
00:06:10.830 --> 00:06:13.020 
This maximum
amount of data

93
00:06:13.700 --> 00:06:16.250 
comprises the user sessions
of three hundred days

94
00:06:16.790 --> 00:06:19.350 
which occupy about nine
TB of disk space.

95
00:06:20.850 --> 00:06:25.260 
The red line represents
the computing time,

96
00:06:25.950 --> 00:06:30.520 
the black line the conversion rate as
a function of the of the data volume.

97
00:06:31.370 --> 00:06:34.940 
Note that in this figure the
quantities are scaled to the

98
00:06:35.940 --> 00:06:37.450 
range from zero to one.

99
00:06:38.740 --> 00:06:41.530 
As expected, the computing
time increases

100
00:06:42.110 --> 00:06:44.540 
with an increasing
amount of data.

101
00:06:45.520 --> 00:06:48.680 
The conversion rate however
shows a different trend.

102
00:06:49.400 --> 00:06:52.300 
In the region of low values
for the amount of data,

103
00:06:53.240 --> 00:06:54.920 
the conversion
rate increases

104
00:06:55.810 --> 00:06:58.490 
peeking further
comparatively data lean

105
00:06:59.220 --> 00:07:01.940 
recommender system that only uses
twenty percent of the data.

106
00:07:02.880 --> 00:07:06.630 
This twenty percent corresponds
to sixty days of user sessions.

107
00:07:07.660 --> 00:07:10.950 
However, beyond this
twenty percent point

108
00:07:11.470 --> 00:07:13.860 
the conversion rate on
steadily falls.

109
00:07:15.560 --> 00:07:20.620 
While using as much care as possible
is often said to optimize

110
00:07:21.140 --> 00:07:24.700 
the performance of an AI
application until its performance

111
00:07:24.710 --> 00:07:26.960 
measure converges
to maximum,

112
00:07:27.590 --> 00:07:29.080 
our system
performs best

113
00:07:29.840 --> 00:07:31.790 
when using only twenty
percent of the data.

114
00:07:33.240 --> 00:07:36.260 
At this point when using only
twenty percent of the data, the

115
00:07:36.670 --> 00:07:42.450 
computing time also only amounts about
twenty percent of that computing time

116
00:07:42.960 --> 00:07:45.020 
that is needed to train
the model that uses

117
00:07:45.450 --> 00:07:46.570 
the maximum of data.

118
00:07:48.140 --> 00:07:52.660 
To conclude, by reducing the amount
of training data to twenty percent,

119
00:07:53.210 --> 00:07:58.180 
we can decrease the computing
time by eighty percent

120
00:07:58.620 --> 00:08:01.880 
while not only maintaining the
performance of the system but

121
00:08:02.850 --> 00:08:04.170 
even improving it.

122
00:08:05.550 --> 00:08:09.080 
We reduce the amount of data, by
this we reduce the computing time

123
00:08:09.540 --> 00:08:11.890 
and improve the
system's performance.

124
00:08:16.390 --> 00:08:18.980 
This reduced computing
time corresponds to a

125
00:08:20.180 --> 00:08:25.680 
reduced power usage which then can
be related to a reduction of

126
00:08:26.630 --> 00:08:29.160 
the CO2 footprint
off this application.

127
00:08:29.990 --> 00:08:33.360 
The calculation of the latter however
is not topic of this project.

128
00:08:35.740 --> 00:08:39.100 
Now with an indicator on
how much data we need.

129
00:08:40.090 --> 00:08:43.130 
However so far we didn't care
about the quiz question

130
00:08:43.540 --> 00:08:44.860 
which data we
should use.

131
00:08:45.640 --> 00:08:49.210 
We simply increased the man of
thereby adding more and more

132
00:08:49.210 --> 00:08:50.480 
days of user sessions

133
00:08:51.180 --> 00:08:55.430 
to our training dataset but did not
purposefully select any specific

134
00:08:56.210 --> 00:08:57.650 
user sessions
among these.

135
00:08:59.370 --> 00:09:02.720 
Additional results of this
data volume experiment

136
00:09:03.320 --> 00:09:07.560 
indicate that among these user
sessions there are valuable

137
00:09:07.650 --> 00:09:12.240 
user sessions but also sessions that
carry redundant information or

138
00:09:12.410 --> 00:09:14.030 
are even toxic in
the sense that

139
00:09:15.020 --> 00:09:18.560 
they confused the machine learning system
and learning product similarities.

140
00:09:19.260 --> 00:09:23.490 
So if we could only select
the valuable user sessions

141
00:09:24.190 --> 00:09:29.010 
from our twenty percent data set
we most likely can further

142
00:09:29.680 --> 00:09:33.510 
decrease the computing time and
improve the system's performance.

143
00:09:35.310 --> 00:09:38.240 
To get an idea on how
we might distinguish

144
00:09:39.650 --> 00:09:41.110 
valuable user sessions

145
00:09:41.730 --> 00:09:46.610 
from redundant and toxic ones, we
set up a second experiment.

146
00:09:47.440 --> 00:09:51.140 
In this experiment we fixed
the training data size

147
00:09:51.720 --> 00:09:55.940 
and for this fixed
training data set we

148
00:09:57.000 --> 00:10:00.010 
calculated the
contribution

149
00:10:00.990 --> 00:10:03.870 
of individual user sessions
to the system's performance.

150
00:10:05.280 --> 00:10:08.120 
This experiment is described
in detail in our paper.

151
00:10:08.550 --> 00:10:12.470 
Here in this video I only would like to
present you one of the key results.

152
00:10:14.410 --> 00:10:17.430 
For this fixed training
data set, we measured

153
00:10:18.390 --> 00:10:18.830 


154
00:10:21.190 --> 00:10:25.290 
seventy one percent of the data
points to have a positive value,

155
00:10:26.070 --> 00:10:29.190 
meaning that if they're included
in the training data set

156
00:10:29.650 --> 00:10:31.760 
they increase the
conversion rate.

157
00:10:32.360 --> 00:10:35.590 
We observed that twelve point five
percent of the user sessions

158
00:10:36.060 --> 00:10:39.840 
virtually do not have an impact
on the system's performance,

159
00:10:40.450 --> 00:10:44.760 
while sixteen point five percent
of the user sessions are toxic

160
00:10:45.150 --> 00:10:48.730 
in the sense that if they're
included in the training data set,

161
00:10:49.650 --> 00:10:55.140 
and they drastically  or
partly drastically decrease

162
00:10:55.570 --> 00:11:01.680 
the conversion rate. Now if you were
able to not only to measure but to

163
00:11:02.020 --> 00:11:07.620 
predict the contribution of
individual user sessions to the

164
00:11:08.220 --> 00:11:12.490 
system's performance and before
training the system we could

165
00:11:12.820 --> 00:11:16.410 
purposefully select only the
valuable user sessions,

166
00:11:16.990 --> 00:11:20.840 
in order to further decrease the
computing time and improve

167
00:11:21.250 --> 00:11:22.510 
the system's
performance.

168
00:11:23.940 --> 00:11:29.550 
However this last step, this
predicting the value of individual

169
00:11:29.970 --> 00:11:31.740 
data points is
not taking it.

170
00:11:32.390 --> 00:11:34.460 
Which brings me to a short
summary and an outlook.

171
00:11:37.380 --> 00:11:40.710 
I hope that I could convince you that
data minimalism has many ecological,

172
00:11:41.270 --> 00:11:43.270 
economical and
societal benefits.

173
00:11:44.430 --> 00:11:45.490 
Applied to our

174
00:11:47.170 --> 00:11:49.620 
example application of recommender
system, we learned that

175
00:11:50.030 --> 00:11:54.090 
data minimalism can significantly
decrease the computing time

176
00:11:54.090 --> 00:11:58.110 
of large-scale AI applications while
even improving the performance.

177
00:11:59.420 --> 00:12:02.380 
Our application performs best using
only twenty percent of the data.

178
00:12:03.920 --> 00:12:06.770 
Measuring and predicting the
value of individual data points

179
00:12:07.280 --> 00:12:10.540 
will allow to further reduce the amount
of data and to further improve

180
00:12:11.210 --> 00:12:13.480 
the performance of
such AI applications.

181
00:12:14.710 --> 00:12:17.490 
As part of the OTTO
group family OSP,

182
00:12:18.110 --> 00:12:21.980 
we will continue to promote
responsibility and sustainability

183
00:12:22.370 --> 00:12:24.500 
in IT and artificial
intelligence.

184
00:12:26.190 --> 00:12:27.130 
Thanks for your
interest.
