WEBVTT

1
00:00:05.160 --> 00:00:10.020 
Now we want a closer look to how the data are processed, how ranking is prepared

2
00:00:10.470 --> 00:00:13.360 
in the index based search machines and

3
00:00:14.080 --> 00:00:19.210 
for that we have to see how the collected documents, documents

4
00:00:19.210 --> 00:00:25.400 
that are collected by the crawlers, how they are transformed into an

5
00:00:25.580 --> 00:00:27.610 
efficiently searchable dataset.

6
00:00:29.300 --> 00:00:34.960 
This is important and the way how this is done describes the power of

7
00:00:35.130 --> 00:00:36.290 
the search machine.

8
00:00:37.570 --> 00:00:41.430 
So what needs to be done is that the content of the web page of the,

9
00:00:42.370 --> 00:00:46.270 
we consider text documents need to be indexed.

10
00:00:48.380 --> 00:00:52.750 
And the first step here is that the various documents coming

11
00:00:52.750 --> 00:00:57.360 
in different formats like HTML, Postscript, PDF, Doc or whatever, that

12
00:00:58.080 --> 00:01:03.410 
they need to be converted into a uniform internal document type.

13
00:01:03.990 --> 00:01:07.870 
An internal document type that is the starting point for the

14
00:01:08.980 --> 00:01:10.880 
analysis of the content.

15
00:01:12.410 --> 00:01:15.860 
Because then, by means of semantic analysis

16
00:01:16.530 --> 00:01:23.530 
a search machine or the service needs to investigate the content, the

17
00:01:24.290 --> 00:01:30.160 
content of a web page. And the question is how a machine can understand the content,

18
00:01:30.570 --> 00:01:34.130 
what a machine can do is the machine can see as a characters

19
00:01:34.630 --> 00:01:39.790 
the text is written by, but what the words are meaning,

20
00:01:40.910 --> 00:01:43.410 
it's difficult to understand, or impossible

21
00:01:44.140 --> 00:01:46.890 
to understand for a machine. So now

22
00:01:47.530 --> 00:01:52.850 
methods are needed to reveal the content of a web document

23
00:01:52.870 --> 00:01:57.290 
and to describe this document by means of descriptors.

24
00:01:58.150 --> 00:02:04.540 
The basic idea for the semantic analysis is to find relevant character strings.

25
00:02:05.480 --> 00:02:08.090 
Character strings- this is a syntactical feature.

26
00:02:11.020 --> 00:02:16.380 
These are a word. So the idea of the semantic analysis is to start with collecting

27
00:02:16.380 --> 00:02:21.280 
and characterizing an indemnifying strings words.

28
00:02:22.200 --> 00:02:25.830 
Without that the machine would be able to understand the content.

29
00:02:27.990 --> 00:02:33.730 
How is this done? So for example a machine looks for keywords.

30
00:02:34.060 --> 00:02:37.780 
Machine looks in an HTML document,in a word

31
00:02:38.760 --> 00:02:41.840 
document, one can differ between the headline and

32
00:02:42.410 --> 00:02:43.840 
text and the paragraph,

33
00:02:44.500 --> 00:02:50.270 
and it is assumable that words that occur in the headline are

34
00:02:50.270 --> 00:02:53.910 
more important for text are more text

35
00:02:54.590 --> 00:03:00.780 
or more have more description power for the content of the document.

36
00:03:01.040 --> 00:03:04.920 
But at point, many other things so it's a very complicated

37
00:03:06.060 --> 00:03:10.910 
approach. And then, out of this semantic analysis

38
00:03:11.340 --> 00:03:17.760 
descriptors are identified by which the document can be described.

39
00:03:18.470 --> 00:03:23.300 
We will see that descriptors are in most cases

40
00:03:25.310 --> 00:03:28.820 
words. Words that often occur in a text.

41
00:03:30.880 --> 00:03:36.410 
And then a final step in this data processing analysis is that

42
00:03:37.350 --> 00:03:41.100 
ranking information needs to be collected, ranking informations

43
00:03:41.100 --> 00:03:46.860 
that make them possible that later on the document can shown,

44
00:03:46.910 --> 00:03:53.450 
can presented to the user in the right order. Right means following the relevance

45
00:03:53.760 --> 00:03:55.390 
of the documents for this part.

46
00:03:56.050 --> 00:03:59.930 
So here the web crawler has done its work. He has provided the

47
00:03:59.930 --> 00:04:01.930 
documents, he has provided the

48
00:04:02.560 --> 00:04:08.380 
huge data collection and then each of the data, each of the document

49
00:04:08.560 --> 00:04:12.170 
needs to be analyzed. And here is a short overview

50
00:04:13.050 --> 00:04:15.090 
about the different steps of this

51
00:04:16.550 --> 00:04:21.100 
analysis. It starts with data normalization that was

52
00:04:22.210 --> 00:04:28.220 
to convert the document from the different formats into a uniform internal

53
00:04:28.510 --> 00:04:32.670 
format. Then the

54
00:04:33.860 --> 00:04:36.780 
character strings need to be identified. This

55
00:04:37.320 --> 00:04:40.100 
is the step of word identification.

56
00:04:40.760 --> 00:04:44.690 
Then the language needs to be identified.

57
00:04:45.150 --> 00:04:48.820 
All needs to be done automatically. All needs to be done by software.

58
00:04:49.300 --> 00:04:53.780 
Language is therefore important to be identified

59
00:04:54.250 --> 00:05:02.400 
because in a typical text, a word with the same meaning arrives in different forms.

60
00:05:02.650 --> 00:05:09.820 
It depends from the grammatical position and other things. Sometimes the temporal information.

61
00:05:10.170 --> 00:05:15.880 
So what we do in this step is to identify and to

62
00:05:16.530 --> 00:05:21.080 
characterize, to reduce, to reduce different words

63
00:05:21.480 --> 00:05:23.570 
to their word stem.

64
00:05:25.380 --> 00:05:28.080 
This word stemming

65
00:05:29.660 --> 00:05:32.330 
approach helps to see that

66
00:05:33.610 --> 00:05:38.060 
to count at the end how many, how often a word occurs in a text.

67
00:05:39.100 --> 00:05:43.350 
In the original text, a word occurs in many different variation-

68
00:05:43.690 --> 00:05:47.950 
depending from grammar, depending from the time, information, from others.

69
00:05:48.440 --> 00:05:56.260 
So it is necessary to reduce in this analysis to the word stem

70
00:05:56.520 --> 00:06:01.940 
and to produce out of this word stem later the descriptors.

71
00:06:02.380 --> 00:06:07.980 
The descriptors which are important, which are able to characterize

72
00:06:07.980 --> 00:06:12.780 
the content of the text. Then by means of a blacklist some of the words

73
00:06:13.400 --> 00:06:19.680 
for example search machine should not show physical power or

74
00:06:19.680 --> 00:06:23.160 
other things. Some of the descriptors can be

75
00:06:23.590 --> 00:06:28.500 
checked against such a blacklist. And this descriptors

76
00:06:29.030 --> 00:06:31.200 
mainly, this is what then

77
00:06:32.750 --> 00:06:37.340 
helps to characterize a document is a result

78
00:06:37.350 --> 00:06:40.100 
of this so called information retrieval.

79
00:06:42.050 --> 00:06:47.180 
Now let's have a look how these descriptors computed.

80
00:06:47.700 --> 00:06:52.210 
Descriptors are the describing keyboards. The goal is,

81
00:06:53.490 --> 00:06:58.110 
the document should be represented as completely as possible.

82
00:06:58.360 --> 00:07:02.900 
The content of the document should be represented as completely as possible

83
00:07:03.410 --> 00:07:09.040 
by means of these descriptors. By means of these keyboards.

84
00:07:10.710 --> 00:07:14.690 
For that reason a key word relevance filtering

85
00:07:15.670 --> 00:07:20.520 
is important for the context analysis of a text.

86
00:07:20.970 --> 00:07:25.740 
So for example here with such information in HTML documents,

87
00:07:26.030 --> 00:07:29.830 
which shows that there is a headline information

88
00:07:30.650 --> 00:07:34.100 
or there is a bold information. The

89
00:07:34.790 --> 00:07:38.990 
relevance of a describing keyword of a descriptor

90
00:07:39.410 --> 00:07:43.200 
can be evaluated. Or some of the

91
00:07:46.110 --> 00:07:52.770 
identified words have only filling character, are only connecting words, are pronouns or others.

92
00:07:53.420 --> 00:07:56.350 
Although these words can be omitted.

93
00:07:57.970 --> 00:08:01.860 
And then if a document is described by this

94
00:08:02.390 --> 00:08:04.530 
keywords, by this descriptors, one

95
00:08:05.690 --> 00:08:11.190 
can start to compute the relevance. And there is an interesting observation.

96
00:08:12.320 --> 00:08:18.340 
The more often a word occurs in a text, the more relevant,

97
00:08:18.890 --> 00:08:22.770 
the higher is the relevance of this word for the text.

98
00:08:23.240 --> 00:08:27.090 
And this goes back on a result of the

99
00:08:27.920 --> 00:08:29.630 
George Kingsley Zipf,

100
00:08:31.020 --> 00:08:34.340 
linguist who found out that

101
00:08:35.390 --> 00:08:40.700 
when texts are created for the author, it's much more simple

102
00:08:41.120 --> 00:08:48.950 
to use the same word to describe the same topic, instead of changing the words.

103
00:08:49.280 --> 00:08:52.990 
You remember to your school time when teacher said that you should

104
00:08:53.630 --> 00:08:58.410 
use synonyms, you should not every time in a text use the same word.

105
00:08:59.250 --> 00:09:02.410 
But Zipf found out that typically text are

106
00:09:03.290 --> 00:09:08.790 
character in the way that author use the same word and repeats a certain word

107
00:09:09.050 --> 00:09:10.300 
in describing the topic,

108
00:09:10.970 --> 00:09:14.650 
and from this observation it results that the more often a

109
00:09:14.650 --> 00:09:19.710 
word occurs in a text, the more relevant is the word

110
00:09:21.470 --> 00:09:24.300 
for describing the content of this document.

111
00:09:26.090 --> 00:09:30.640 
Now we have to do with algorithms. Algorithms

112
00:09:31.090 --> 00:09:34.380 
are based on mathematics. So what is done

113
00:09:34.990 --> 00:09:39.410 
the documents are described by mathematical features.

114
00:09:39.880 --> 00:09:46.210 
They are described by vectors, and these vectors are describing

115
00:09:46.210 --> 00:09:49.100 
a document are called 'document vectors'.

116
00:09:50.840 --> 00:09:56.530 
Vector space- here to use this helps to

117
00:09:57.180 --> 00:10:00.270 
apply proven mathematical methods

118
00:10:00.940 --> 00:10:06.410 
for the information retrieval task. And this is an old

119
00:10:07.630 --> 00:10:11.780 
result of scientific investigation by means of

120
00:10:12.420 --> 00:10:17.920 
this mathematical model of a vector space is information retrieval

121
00:10:18.330 --> 00:10:21.020 
can be done, can be performed very efficient.

122
00:10:21.730 --> 00:10:26.940 
So the idea is, a document is considered as a vector.

123
00:10:29.400 --> 00:10:33.960 
As a vector in an n-dimensional vector space,

124
00:10:34.240 --> 00:10:38.050 
and the number 'n' this is the number of descriptors.

125
00:10:39.280 --> 00:10:42.490 
So a document is described by its descriptors

126
00:10:42.950 --> 00:10:46.630 
but it's not simply described by a list of

127
00:10:47.150 --> 00:10:50.640 
descriptors. It is described by a vector. So if a

128
00:10:51.970 --> 00:11:00.060 
descriptor comes seven times or seven times occurs in text then we can

129
00:11:00.440 --> 00:11:02.060 
remember, we can

130
00:11:03.310 --> 00:11:08.090 
store this in the vector in the document vector in the following way.

131
00:11:09.460 --> 00:11:13.500 
The vector spaces are spanned by its so-called

132
00:11:14.490 --> 00:11:22.280 
basic vectors and the basic vectors are those vectors that represent exactly one

133
00:11:22.380 --> 00:11:26.310 
descriptor. So for each descriptor there is a new basic vector

134
00:11:26.750 --> 00:11:30.900 
and then we can compute the document vector

135
00:11:31.490 --> 00:11:34.980 
as a linear combination of all the basic vectors.

136
00:11:36.030 --> 00:11:42.150 
Of the basic vectors of belonging to descriptors that occur in the text.

137
00:11:44.710 --> 00:11:52.010 
Each of the basic vectors is multiplied by the number of occurrence of set word.

138
00:11:52.860 --> 00:11:58.310 
So in this way the vector description of a document- it's much more

139
00:11:58.430 --> 00:12:03.330 
characteristic than only to describe the document by the list of

140
00:12:04.490 --> 00:12:07.090 
descriptors.

141
00:12:08.340 --> 00:12:15.840 
Now the document analysis can be done by performing the mathematical operations and computations

142
00:12:16.080 --> 00:12:21.120 
from linear algebra manipulating the different document vectors,

143
00:12:21.280 --> 00:12:27.480 
for example in this way one can compute by means of mathematical methods the similarity

144
00:12:27.790 --> 00:12:30.920 
of different documents. One can

145
00:12:31.610 --> 00:12:34.330 
also compute relevance in others.

146
00:12:36.520 --> 00:12:42.340 
Let's have a look to an example. Here we have the document one.

147
00:12:43.710 --> 00:12:47.720 
Document one consists of a sentence which says that pineapple

148
00:12:47.720 --> 00:12:50.420 
is a citrus fruit and not a vegetable.

149
00:12:51.060 --> 00:12:53.970 
And here is another document- document two

150
00:12:54.430 --> 00:13:00.170 
and the document consists of the text that says the potato is

151
00:13:00.390 --> 00:13:04.260 
neither fruit nor vegetable. Onions are vegetables.

152
00:13:04.910 --> 00:13:10.390 
So in a first step the words are analyzed and the descriptors are found

153
00:13:10.590 --> 00:13:16.950 
and here for example there is an article so it helps nothing to in describing

154
00:13:17.060 --> 00:13:22.040 
the text. So the important descriptors of this document one is

155
00:13:22.710 --> 00:13:27.980 
pineapple is citrus, is fruit is vegetable and here in the document

156
00:13:27.980 --> 00:13:34.140 
two it's potato, fruit, vegetable, onions are vegetable. I underlined these keywords.

157
00:13:34.950 --> 00:13:38.400 
Now we create an index.

158
00:13:40.710 --> 00:13:44.490 
By the way this is the inverted index because this is not the index

159
00:13:44.980 --> 00:13:48.670 
each document has. Its here are the documents.

160
00:13:49.240 --> 00:13:54.820 
This typical index is to tell a document has this and this characteristic.

161
00:13:54.950 --> 00:13:58.760 
This is this characterizing vectors. Inverted means now we

162
00:13:59.440 --> 00:14:03.120 
do not start with the document, we start with the content of the document.

163
00:14:03.480 --> 00:14:06.570 
So here for as for example the pineapple.

164
00:14:08.200 --> 00:14:13.780 
The basic vector is that pineapple are put on place 1, vegetable

165
00:14:13.780 --> 00:14:19.040 
place 2, potato 3, fruit 4, citrus 5, onion 6. So

166
00:14:19.040 --> 00:14:24.020 
in this very simple example we have six dimensional vector space,

167
00:14:24.350 --> 00:14:27.560 
and these are the basic vectors for the different

168
00:14:28.060 --> 00:14:29.740 
descriptors.

169
00:14:30.520 --> 00:14:32.980 
And now we start in this inverted index

170
00:14:33.400 --> 00:14:39.960 
to collect what we know about our document one and our document two.

171
00:14:40.160 --> 00:14:43.530 
So for example in document one we have

172
00:14:44.210 --> 00:14:49.650 
one times the word pineapple. So the basic vector 1 later on has

173
00:14:49.650 --> 00:14:54.700 
to be multiplied with 1, then we have vegetable 1 times

174
00:14:55.850 --> 00:15:01.280 
and then we have citrus 1 times and we have fruit 1 times.

175
00:15:02.040 --> 00:15:06.190 
The potato it does not occur. The word potato is a descriptor

176
00:15:06.190 --> 00:15:09.740 
potato does not occur in document one and the word

177
00:15:10.210 --> 00:15:16.610 
onion does not occur. So I could also write zero but zero times basic vector

178
00:15:17.330 --> 00:15:19.260 
is zero. So we can also omit.

179
00:15:20.100 --> 00:15:24.660 
If we analyze document two, then we see here the word vegetable

180
00:15:24.960 --> 00:15:27.720 
comes two times, occurs two times in document two.

181
00:15:28.150 --> 00:15:33.540 
So for that reason we have to multiply our basic vector,

182
00:15:34.180 --> 00:15:39.430 
for basic vector for vegetables by means of the factor two.

183
00:15:39.880 --> 00:15:44.970 
One times we have potato, one times fruit and one times onion.

184
00:15:45.960 --> 00:15:52.500 
So if we now look to the description of the documents, then we can see

185
00:15:53.020 --> 00:15:56.330 
here is document one and document one is

186
00:15:57.580 --> 00:16:02.530 
the document vector of the document. One is the basic one

187
00:16:02.530 --> 00:16:07.610 
times the basic vector e1, one times the basic vector e2,

188
00:16:08.030 --> 00:16:14.720 
zero times e3, four one times e4, one times e5. So

189
00:16:14.770 --> 00:16:18.680 
this is a document vector of document one and

190
00:16:19.390 --> 00:16:22.210 
this is a document vector of document two.

191
00:16:22.930 --> 00:16:27.040 
So we have described the document by a mathematical

192
00:16:27.700 --> 00:16:32.920 
metric object by a vector. Of course this are very simple

193
00:16:32.920 --> 00:16:37.670 
documents so if you imagine more complex documents you see

194
00:16:37.890 --> 00:16:44.720 
that then the documents vectors are huge, but for a mathematical computer

195
00:16:45.330 --> 00:16:46.850 
analysis that's no problem.

196
00:16:48.690 --> 00:16:55.470 
Here in this way you already get an get a clue how later on

197
00:16:55.620 --> 00:17:00.090 
the search machine is working. So for example if the user types in pineapple

198
00:17:00.520 --> 00:17:05.340 
then in such an inverted index the search machine looks through

199
00:17:05.340 --> 00:17:10.980 
its collection of data, of its documents and finds all the documents the

200
00:17:12.250 --> 00:17:14.690 
the descriptor pineapple plays a role.

201
00:17:15.430 --> 00:17:17.770 
The higher the number

202
00:17:18.880 --> 00:17:21.840 
here as the word occurs in a text,

203
00:17:22.440 --> 00:17:25.270 
the more relevant is this document for

204
00:17:26.390 --> 00:17:27.680 
answering the question of the user.

205
00:17:29.870 --> 00:17:36.660 
Finding Documents- so finding documents we have the Query Q is also the query

206
00:17:36.930 --> 00:17:42.350 
is possible to represent as a document vector. If it's only

207
00:17:42.350 --> 00:17:46.440 
one keywords and it's a basic vector of that keyword,

208
00:17:46.860 --> 00:17:50.380 
and so we can to find the

209
00:17:51.830 --> 00:17:57.790 
search result we can describe the query of the user as a document vector.

210
00:17:58.390 --> 00:18:02.140 
And we have a description of the document vectors of all the data

211
00:18:02.370 --> 00:18:04.580 
in the data collection of the search machine.

212
00:18:05.270 --> 00:18:13.950 
So what needs to be done is to compute the similarity of the different document

213
00:18:14.710 --> 00:18:16.910 
of the different document vectors

214
00:18:17.530 --> 00:18:23.390 
with the query as a document vector of the query.

215
00:18:24.220 --> 00:18:29.440 
So here simply mathematically all the documents are compared to the

216
00:18:30.110 --> 00:18:34.580 
query and this comparison is done on the basis that as well

217
00:18:34.580 --> 00:18:38.090 
as the documents as well as the queries are described by document vectors,

218
00:18:38.870 --> 00:18:45.000 
and then one can compute the similarity, and it's very interesting that

219
00:18:45.110 --> 00:18:51.440 
this can be done by computing as a cosine of the angle between the two

220
00:18:51.880 --> 00:18:57.590 
document vectors Q belonging to the query and D to the document D.

221
00:18:58.310 --> 00:19:03.900 
So you see with mathematical methods, we can analyze the

222
00:19:04.680 --> 00:19:07.170 
content, the semantic of documents.

223
00:19:07.860 --> 00:19:12.860 
This is very powerful and this is explanation that search machines like Google

224
00:19:13.470 --> 00:19:18.070 
are so powerful on are able to provide so accurate data.

225
00:19:19.540 --> 00:19:23.200 
Now let's have a look to the ranking. So now we understand which

226
00:19:23.200 --> 00:19:26.930 
are the documents that are more similar to the queries

227
00:19:26.930 --> 00:19:29.790 
and the more similar the documents are to the query.

228
00:19:32.370 --> 00:19:36.610 
The more it provides an answer to the query of the user.

229
00:19:37.280 --> 00:19:44.260 
So the ranking is now to compute the relevance of the documents

230
00:19:45.240 --> 00:19:51.440 
to the query. So to achieve higher quality for the search result

231
00:19:51.770 --> 00:19:55.960 
the documents obtained from the document index must be weighted

232
00:19:56.160 --> 00:20:00.030 
according to the relevance. And so the question is what is important,

233
00:20:00.030 --> 00:20:02.150 
what is more important and

234
00:20:02.760 --> 00:20:05.020 
an answer is given by this

235
00:20:05.630 --> 00:20:09.630 
algorithm, the Term Frequency Algorithm

236
00:20:10.330 --> 00:20:15.700 
and this is using the Zipf's Law, the

237
00:20:18.860 --> 00:20:25.440 
result of Zipf's Law says the more often a keyword appears in a text

238
00:20:26.060 --> 00:20:31.030 
the more important must it be. The more important must it be

239
00:20:31.030 --> 00:20:35.960 
to describe the content of the text. So the Term Frequency Algorithms

240
00:20:37.120 --> 00:20:42.650 
simply counts the frequency, the number of occurrence of

241
00:20:43.520 --> 00:20:45.820 
the different keywords of the different

242
00:20:46.340 --> 00:20:51.870 
descriptors. So the simple way is this absolute word count.

243
00:20:52.890 --> 00:20:57.110 
But if you imagine that we have short text and long text,

244
00:20:57.830 --> 00:21:00.100 
then the absolute word frequents

245
00:21:01.070 --> 00:21:05.060 
often is not the right answer, because although a word more

246
00:21:05.060 --> 00:21:09.310 
often occurs in a long text it could be more important in a

247
00:21:09.310 --> 00:21:16.340 
short text. So what makes more sense is to compute the relative word frequency.

248
00:21:17.080 --> 00:21:20.320 
So to add to the norm,

249
00:21:20.990 --> 00:21:25.270 
to set the number of occurrence of a word into relation to

250
00:21:25.270 --> 00:21:26.430 
the length of the text.

251
00:21:27.860 --> 00:21:32.350 
He has a more of course this is only very simple ideas. This

252
00:21:32.430 --> 00:21:36.370 
algorithms, this ranking algorithms are much more complex and take

253
00:21:37.250 --> 00:21:40.930 
much more criteria to

254
00:21:41.620 --> 00:21:47.690 
compute the relevance. So a famous ranking algorithm

255
00:21:49.160 --> 00:21:52.670 
is a famous Google PageRank algorithm

256
00:21:54.100 --> 00:21:59.640 
which is used by a Google search machine to compute the ranking

257
00:22:00.110 --> 00:22:04.520 
of an answer. Of course this is the business secret of Google,

258
00:22:05.600 --> 00:22:11.080 
because the high popularity of the Google search machine depends

259
00:22:11.080 --> 00:22:14.710 
from the high accuracy of the results. A lot of results are

260
00:22:14.710 --> 00:22:16.770 
found and the results are found

261
00:22:17.470 --> 00:22:23.790 
in the good way. So in order to achieve high quality

262
00:22:24.410 --> 00:22:28.720 
in the search results, the documents obtained from the inverted index

263
00:22:28.860 --> 00:22:33.070 
now must be weighted according to their relevance.

264
00:22:34.590 --> 00:22:39.720 
And this relevance weighting so it is preparing though that when a user

265
00:22:39.880 --> 00:22:45.500 
types in a query, that documents that are provide answers to this query

266
00:22:46.010 --> 00:22:52.690 
offered in a way that the most relevant document is offered first.

267
00:22:53.570 --> 00:23:00.240 
To get such a relevance weighting, Google has a lot of different ideas.

268
00:23:00.390 --> 00:23:06.360 
Ideas that of course are implemented in the Google PageRank algorithm.

269
00:23:06.960 --> 00:23:13.410 
And one idea is to consider document as important or as unimportant.

270
00:23:14.860 --> 00:23:19.260 
This importance does not depend from the semantic analysis

271
00:23:19.430 --> 00:23:22.960 
of the document. This importance depends from how

272
00:23:23.680 --> 00:23:29.560 
the authors in the web considers the document. So to make it clear a document

273
00:23:29.780 --> 00:23:32.800 
is considered of higher importance

274
00:23:33.540 --> 00:23:37.920 
the more other documents refer to the document via a link. Many,

275
00:23:37.920 --> 00:23:42.370 
many outdoors also think this is an important document. And of course

276
00:23:42.620 --> 00:23:45.390 
this tells something about its importance.

277
00:23:46.580 --> 00:23:50.890 
Without that the machine, the algorithm is able to

278
00:23:51.610 --> 00:23:54.190 
to see this importance. But

279
00:23:55.370 --> 00:23:58.290 
algorithm can easily count the number of links

280
00:23:58.710 --> 00:24:00.630 
pointing to this document.

281
00:24:01.780 --> 00:24:06.180 
Another idea is that a document that refers to an important

282
00:24:06.180 --> 00:24:09.750 
document is also considered important. Or that

283
00:24:11.290 --> 00:24:16.320 
document the more links a document contains referring to other documents

284
00:24:16.760 --> 00:24:23.550 
the less important is each single link. So in this way syntactical

285
00:24:24.230 --> 00:24:26.550 
features with that properties

286
00:24:27.190 --> 00:24:31.050 
links, number of links, weight of links.

287
00:24:32.010 --> 00:24:35.920 
Google is able in this Google PageRank

288
00:24:36.950 --> 00:24:39.920 
to weight the relevance of documents.

289
00:24:41.310 --> 00:24:45.580 
Of course the algorithm itself is famous Google's PageRank algorithm

290
00:24:46.010 --> 00:24:51.010 
is one of the central cooperate assets of Google and therefore

291
00:24:51.130 --> 00:24:52.530 
it's not disclosed.
