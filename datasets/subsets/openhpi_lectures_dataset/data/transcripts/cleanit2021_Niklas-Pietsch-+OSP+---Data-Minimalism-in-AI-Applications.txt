WEBVTT

1
00:00:03.470 --> 00:00:06.560 
Welcome everyone to the third clean-IT openXchange live

2
00:00:06.560 --> 00:00:09.930 
talk. My name is Nils KÃ¶nig and I'm here today with

3
00:00:09.940 --> 00:00:13.040 
Sophie and we're going to moderate this live talk today.

4
00:00:13.560 --> 00:00:16.690 
With us today as our third guest is Niklas Pietsch.

5
00:00:17.180 --> 00:00:21.740 
Doctor Niklas Pietsch studied physics and philosophy at the University of Hamburg.

6
00:00:22.220 --> 00:00:26.920 
For the past five years he has been developing and implementing machine learning applications

7
00:00:27.270 --> 00:00:31.100 
for clients from the retail e-commerce and logistics industry,

8
00:00:32.300 --> 00:00:37.150 
and today he will be talking about how he managed to not only reduce energy consumption,

9
00:00:37.590 --> 00:00:41.310 
but also improve the results of their recommendation system

10
00:00:41.500 --> 00:00:44.010 
by using not more but less data.

11
00:00:44.910 --> 00:00:45.960 
Welcome Doctor Pietsch.

12
00:00:51.110 --> 00:00:53.610 
Hi, can you see my screen?

13
00:00:55.960 --> 00:00:59.030 
Thanks! Thanks for the kind introduction. Well,

14
00:01:00.000 --> 00:01:01.950 
the results present here are not

15
00:01:03.730 --> 00:01:10.070 
done by me only. So it's a collaborative work and

16
00:01:11.030 --> 00:01:14.460 
I did this together with Michaela Regneri, which also joined

17
00:01:16.060 --> 00:01:20.250 
this meeting, Julia Georgi, Jurij Kost and Sabine Stamm,

18
00:01:20.810 --> 00:01:22.830 
who most likely also will

19
00:01:23.590 --> 00:01:29.130 
join the meeting. So yeah, welcome. My name is Niklas Pietsch. I'm working

20
00:01:29.130 --> 00:01:32.580 
as data scientist at the Otto Group Solution Provider (OSP)

21
00:01:33.710 --> 00:01:36.550 
and I present you the results of a research project about

22
00:01:37.410 --> 00:01:39.600 
Data Minimalism in AI Applications.

23
00:01:40.440 --> 00:01:45.060 
This project is a joint research project of

24
00:01:46.020 --> 00:01:51.190 
OSP and Otto which are companies of the Otto Group- one of the biggest e-commerce

25
00:01:51.690 --> 00:01:52.630 
companies in the world.

26
00:01:55.930 --> 00:01:59.380 
In recognition of our responsibility and commitment to

27
00:02:00.300 --> 00:02:02.640 
people and the planet, the Otto Group is a business and

28
00:02:03.040 --> 00:02:04.960 
responsible commerce that inspires.

29
00:02:05.600 --> 00:02:08.180 
And this vision is some kind of common thread that

30
00:02:09.030 --> 00:02:12.530 
has run through the Otto group for decades or generations.

31
00:02:14.370 --> 00:02:19.230 
Already in nineteen eighty six Michael Otto had put the topic of environmental protection and

32
00:02:19.660 --> 00:02:24.180 
working in a sustainable way to the heart of the car business and since then

33
00:02:25.340 --> 00:02:28.590 
its anchor in the goals and the strategy of to see an Otto Group.

34
00:02:31.910 --> 00:02:34.130 
OSP is the IT service provider of

35
00:02:35.480 --> 00:02:37.800 
the Otto group and founded in nineteen ninety one, we have

36
00:02:39.270 --> 00:02:42.810 
thirty years of experience in IT for retail and logistics and

37
00:02:43.290 --> 00:02:45.930 
with about four hundred colleagues at five

38
00:02:47.490 --> 00:02:49.010 
locations worldwide, we

39
00:02:50.820 --> 00:02:54.510 
focus on software development and big data analysis for

40
00:02:55.390 --> 00:02:56.280 
clients from the retail and

41
00:02:57.290 --> 00:02:58.300 
logistics industry.

42
00:03:00.880 --> 00:03:04.190 
As member of the Otto group family, we seek opportunities to

43
00:03:05.130 --> 00:03:09.880 
reduce CO2 footprint for our clients worldwide and

44
00:03:09.890 --> 00:03:11.350 
among these efforts, we

45
00:03:12.280 --> 00:03:15.670 
also explore and implement the concept of

46
00:03:16.650 --> 00:03:19.440 
data management in two large-scale AI applications.

47
00:03:26.050 --> 00:03:30.570 
While most companies try to collect as much data as possible whether they

48
00:03:31.990 --> 00:03:35.850 
need it or not, the basic idea of data minimalism is to only use

49
00:03:36.990 --> 00:03:40.070 
that data that really has some value for business

50
00:03:40.520 --> 00:03:46.640 
decisions. And this is beneficial in different ways and just

51
00:03:47.090 --> 00:03:50.560 
using less data means that you need less office to store the data and less

52
00:03:51.360 --> 00:03:54.310 
computing time to process the data and

53
00:03:55.480 --> 00:03:57.240 
therefore minimalism allows for a

54
00:03:58.090 --> 00:03:58.920 
reduction of

55
00:04:01.350 --> 00:04:02.690 
of power usage and a

56
00:04:03.350 --> 00:04:05.170 
significant reduction of carbon emissions.

57
00:04:06.140 --> 00:04:07.760 
Nowadays, data traffic actually causes

58
00:04:08.970 --> 00:04:09.730 
about as much

59
00:04:11.030 --> 00:04:12.730 
carbon emissions as air traffic.

60
00:04:14.410 --> 00:04:17.350 
Definitely, yet it has the various

61
00:04:17.990 --> 00:04:21.580 
economic and societal benefits that reduces monetary cost

62
00:04:22.040 --> 00:04:25.860 
and also applications to run faster and to scale better and also as

63
00:04:26.610 --> 00:04:27.770 
privacy and security risk.

64
00:04:29.090 --> 00:04:33.500 
And last but not least as I would like to show you in this talk-

65
00:04:34.950 --> 00:04:38.630 
it even can improve the performance of large scale AI applications.

66
00:04:42.100 --> 00:04:43.460 
In our research, we

67
00:04:47.870 --> 00:04:50.490 
apply the concept of data minimalism to a

68
00:04:51.700 --> 00:04:55.170 
machine learning system that computes product recommendations.

69
00:04:56.140 --> 00:05:00.510 
Given the product a user of an online shop is currently viewing such

70
00:05:01.700 --> 00:05:06.710 
a system and computes positive possible alternatives for that products

71
00:05:07.170 --> 00:05:10.850 
that are offered to the user in order to help them make decisions.

72
00:05:11.950 --> 00:05:14.930 
I guess all of you know such recommendations

73
00:05:15.770 --> 00:05:17.370 
from online shopping and

74
00:05:18.130 --> 00:05:20.320 
typically a display at the bottom of a product

75
00:05:21.180 --> 00:05:24.510 
within such a recommendation cinema that states things like- you also might like,

76
00:05:25.340 --> 00:05:30.770 
or a customer support that item also bought that

77
00:05:32.200 --> 00:05:35.720 
and yeah an input of our

78
00:05:36.650 --> 00:05:42.120 
system that we call user sessions. These are the chronologically sorted series of

79
00:05:43.040 --> 00:05:45.610 
clicks on product pages by one user.

80
00:05:46.310 --> 00:05:50.740 
So every time you visit Otto.de or a different online shop, you

81
00:05:50.740 --> 00:05:53.190 
are creating such a user session.

82
00:05:53.820 --> 00:05:56.520 
And these are then processed by

83
00:05:57.780 --> 00:06:00.950 
the algorithm which is called a word2vec.

84
00:06:01.560 --> 00:06:03.930 
This is a machine learning algorithm that uses

85
00:06:03.930 --> 00:06:05.200 
a new network, and was

86
00:06:06.230 --> 00:06:11.030 
originally designed to compute so-called word embeddings from

87
00:06:11.880 --> 00:06:13.310 
the corpus of natural language.

88
00:06:14.580 --> 00:06:18.410 
Meanwhile it's also commonly used for recommender systems and

89
00:06:18.410 --> 00:06:21.850 
in our case this algorithm

90
00:06:22.780 --> 00:06:27.080 
represents the products that occur in our

91
00:06:27.080 --> 00:06:29.200 
sessions as vectors in vector space.

92
00:06:30.240 --> 00:06:33.940 
As illustrated in the figure in the middle of the slide here, this

93
00:06:34.570 --> 00:06:37.980 
vector space has actually much more than two dimensions.

94
00:06:38.720 --> 00:06:43.450 
I think it's about hundred dimensions, and of course illustration for this purposes,

95
00:06:43.830 --> 00:06:46.760 
we had to restrict this to

96
00:06:47.360 --> 00:06:49.550 
two dimensions here. And

97
00:06:50.510 --> 00:06:53.970 
the key to this algorithm is that products that occur in a

98
00:06:54.410 --> 00:06:57.760 
similar user journey context and may therefore serve as a

99
00:06:58.590 --> 00:07:02.870 
good recommendation for each other are represented by vectors

100
00:07:02.870 --> 00:07:05.750 
that are close to each other in the vector space.

101
00:07:06.440 --> 00:07:10.710 
And, so based on the distances of the vectors in the vector

102
00:07:10.710 --> 00:07:12.370 
space, we then can compute

103
00:07:13.160 --> 00:07:16.320 
a list of good recommendations for each of our programs,

104
00:07:18.180 --> 00:07:20.890 
which in the end can be displayed to the user.

105
00:07:21.590 --> 00:07:27.260 
And yeah, we quantify the performance of such a recommender system using

106
00:07:28.150 --> 00:07:31.440 
different key performance indicators. One of these is

107
00:07:32.390 --> 00:07:37.080 
the conversion rate,  which is a widely used quantity in e-commerce and

108
00:07:37.870 --> 00:07:40.730 
defined as a percentage of customers who bought a recommendation.

109
00:07:44.750 --> 00:07:49.960 
So, the first goal of our data

110
00:07:49.960 --> 00:07:52.080 
minimalistic approach is to

111
00:07:52.830 --> 00:07:55.480 
reduce the amount of data that we use for the training of

112
00:07:55.910 --> 00:08:00.060 
the recommender system to that minimum that is needed

113
00:08:00.810 --> 00:08:04.110 
in order to maintain the performance of the system.

114
00:08:04.810 --> 00:08:10.530 
And to get an indicator on how much data we need for this purpose, we

115
00:08:11.050 --> 00:08:12.810 
performed an experiment where we

116
00:08:13.860 --> 00:08:16.980 
step by step increased the amount of

117
00:08:17.920 --> 00:08:20.430 
data. And at each of these steps, we

118
00:08:21.600 --> 00:08:25.460 
calculated the top five recommendations for a fixed set of

119
00:08:26.650 --> 00:08:30.460 
seat products and based on these recommendations we

120
00:08:31.620 --> 00:08:33.730 
quantified the performance of the systems

121
00:08:34.440 --> 00:08:35.830 
calculating the conversion rate.

122
00:08:37.120 --> 00:08:42.390 
So that we can then map the volume

123
00:08:43.690 --> 00:08:48.890 
to the performance of the system as quantified by the conversion rate.

124
00:08:51.720 --> 00:08:55.790 
Which brings me to a few results which I would like to show you on the next slide. So

125
00:08:56.340 --> 00:08:57.750 
in this figure here -

126
00:08:58.540 --> 00:09:02.140 
the x axis represents the amount of data in

127
00:09:02.710 --> 00:09:05.060 
percentage of the total amount of data that we

128
00:09:05.540 --> 00:09:06.810 
used for this experiment.

129
00:09:07.460 --> 00:09:11.000 
This total amount comprises the user sessions are

130
00:09:11.410 --> 00:09:15.240 
three hundred days which occupy about nine terabyte of disk space.

131
00:09:17.720 --> 00:09:20.990 
The red line here presents the computing time. The

132
00:09:21.470 --> 00:09:24.920 
black line- the conversion rate as function of the amount of data.

133
00:09:25.870 --> 00:09:30.090 
And those quantities are scaled to the range from zero to one.

134
00:09:31.970 --> 00:09:33.970 
As expected, the computing time

135
00:09:35.330 --> 00:09:39.050 
increases with the increasing amount of data.

136
00:09:39.820 --> 00:09:43.960 
The conversion rate in contrast shows a different trend.

137
00:09:44.880 --> 00:09:49.160 
In the lower regions of the amount of data, the conversion rate

138
00:09:50.010 --> 00:09:53.380 
increases speaking for the comparatively data lean

139
00:09:54.520 --> 00:09:57.310 
recommender system that uses only twenty percent of the data.

140
00:09:59.430 --> 00:10:03.480 
However, beyond this point it almost daily falls which was

141
00:10:05.240 --> 00:10:07.440 
in contrast to what you might expect.

142
00:10:08.770 --> 00:10:15.380 
While using as much care as possible is often said to optimize the performance of

143
00:10:16.540 --> 00:10:20.650 
the machine learning system under the performance measure and converges to a

144
00:10:21.430 --> 00:10:26.230 
maximum our system performs best using only twenty percent of the data.

145
00:10:26.940 --> 00:10:31.140 
And at this point when when we use only twenty percent of the

146
00:10:31.620 --> 00:10:35.790 
the data.The computing time also

147
00:10:36.310 --> 00:10:39.450 
only amounts to twenty percent of that computing time that

148
00:10:39.640 --> 00:10:43.660 
is needed to train the model that uses all data.

149
00:10:44.710 --> 00:10:49.010 
So, to conclude we only use twenty percent of the data but

150
00:10:50.140 --> 00:10:54.210 
this decreased the computing time by eighty percent while the

151
00:10:54.900 --> 00:10:58.210 
performance of the system is not only maintained but even improved.

152
00:11:00.030 --> 00:11:04.810 
So yeah, we reduce the amount of data and this reduced the computing

153
00:11:04.810 --> 00:11:06.840 
time but improved the performance of the system.

154
00:11:07.740 --> 00:11:09.830 
And this reduced computing time

155
00:11:10.640 --> 00:11:13.140 
corresponds to a reduced power usage which

156
00:11:13.960 --> 00:11:16.960 
then related to a reduction of the

157
00:11:17.630 --> 00:11:20.930 
CO2 footprint for this application.

158
00:11:23.420 --> 00:11:26.190 
In this research project, we

159
00:11:26.830 --> 00:11:31.490 
didn't calculate the reduction of the CO2 footprint. This

160
00:11:31.490 --> 00:11:33.390 
was not the topic of this project here.

161
00:11:34.770 --> 00:11:40.630 
So now we have a first indicator on how much data we need

162
00:11:41.440 --> 00:11:44.870 
to train our system, but so far we didn't care about the question and

163
00:11:46.140 --> 00:11:47.940 
which data we should use. So we

164
00:11:48.970 --> 00:11:52.560 
simply increase the amount of data by adding more and more

165
00:11:52.560 --> 00:11:55.480 
days of user sessions to our training dataset but

166
00:11:56.320 --> 00:11:57.720 
did not purposefully select any

167
00:11:58.470 --> 00:11:59.980 
specific user sessions

168
00:12:01.110 --> 00:12:02.760 
from our training dataset. And

169
00:12:03.550 --> 00:12:05.560 
additional results of this

170
00:12:06.570 --> 00:12:08.010 
data volume experiment

171
00:12:09.140 --> 00:12:11.260 
indicate that among the

172
00:12:12.420 --> 00:12:16.020 
user sessions there are valuable user sessions but also

173
00:12:16.940 --> 00:12:19.850 
sessions that carry an redundant information or are

174
00:12:20.840 --> 00:12:22.580 
toxic in the sense that they

175
00:12:23.580 --> 00:12:26.140 
confuse the machine-learning system in learning and

176
00:12:27.710 --> 00:12:29.860 
product similarities and good

177
00:12:30.650 --> 00:12:31.950 
product recommendations.

178
00:12:32.620 --> 00:12:34.840 
And now if we

179
00:12:36.560 --> 00:12:39.100 
could somehow distinguish

180
00:12:39.530 --> 00:12:42.660 
the valuable user sessions from the redundant and the toxic

181
00:12:42.660 --> 00:12:45.140 
ones we most likely can further decrease

182
00:12:46.440 --> 00:12:51.170 
the computing time and improve the performance of the system.

183
00:12:52.820 --> 00:12:57.360 
And to get an idea on on how one might distinguish

184
00:12:58.790 --> 00:13:00.770 
these classes of user sessions

185
00:13:01.870 --> 00:13:03.440 
from each other, we performed a

186
00:13:04.230 --> 00:13:08.460 
second experiment which we called an n minus one experiment.

187
00:13:09.250 --> 00:13:10.980 
And in this experiment, we

188
00:13:11.990 --> 00:13:15.920 
first fixed the size of our training dataset to

189
00:13:16.380 --> 00:13:17.930 
two days of user sessions.

190
00:13:18.620 --> 00:13:21.490 
It was a fixed size and we

191
00:13:23.010 --> 00:13:27.540 
trained the recommender system which I will refer to as the

192
00:13:27.700 --> 00:13:31.850 
reference system in the following. And for this reference system, we

193
00:13:33.590 --> 00:13:38.190 
again calculated our kbis. I must assess the conversion rate.

194
00:13:39.400 --> 00:13:44.370 
And then we randomly to the single user session

195
00:13:44.880 --> 00:13:48.710 
removed it from the training dataset and again trained the

196
00:13:48.710 --> 00:13:50.870 
model and calculated the conversion rate.

197
00:13:52.210 --> 00:13:58.590 
And so by this, we now can assign a value value to the single data point,

198
00:13:59.740 --> 00:14:03.810 
which simply is the difference in conversion rate. So we

199
00:14:04.390 --> 00:14:07.540 
have a reference system and a conversion rate this corresponds

200
00:14:07.540 --> 00:14:09.150 
to this system in our

201
00:14:10.290 --> 00:14:13.760 
which we call n minus one system where we just left out one

202
00:14:13.760 --> 00:14:14.990 
single user session and

203
00:14:15.610 --> 00:14:18.830 
a conversion rate this corresponds to this system. And now if

204
00:14:18.830 --> 00:14:21.870 
you calculate the difference in the conversion rate. we

205
00:14:22.450 --> 00:14:27.230 
know if this user session has a positive contribution to the

206
00:14:27.650 --> 00:14:29.470 
performance of the system or a negative contribution.

207
00:14:30.630 --> 00:14:36.270 
So, if we leave out a single session and afterwards the conversion rate is

208
00:14:37.270 --> 00:14:40.810 
better and this means that the user session has a

209
00:14:41.890 --> 00:14:44.050 
negative contribution to the system's performance.

210
00:14:44.680 --> 00:14:48.760 
Vice versa I feel and router session and the

211
00:14:50.370 --> 00:14:52.980 
conversion rate is worse afterwards. And this means that

212
00:14:53.760 --> 00:14:55.110 
the single session has a

213
00:14:57.570 --> 00:14:58.380 
positive value

214
00:14:59.930 --> 00:15:01.820 
in this context. And

215
00:15:03.880 --> 00:15:08.000 
that this we can also use other kbis

216
00:15:09.050 --> 00:15:10.570 
to quantify the value of

217
00:15:11.820 --> 00:15:16.100 
the data point. So we also

218
00:15:17.200 --> 00:15:19.970 
use the difference in revenue to

219
00:15:20.750 --> 00:15:22.590 
assign a monetary value to a

220
00:15:23.240 --> 00:15:24.460 
single user sessions.

221
00:15:25.790 --> 00:15:27.400 
And now in order to

222
00:15:28.190 --> 00:15:29.520 
learn which

223
00:15:30.890 --> 00:15:33.960 
types of user sessions are valuable or

224
00:15:34.780 --> 00:15:38.500 
redundant, or toxic, we ideally then

225
00:15:39.250 --> 00:15:41.670 
map from certain properties to

226
00:15:42.960 --> 00:15:47.210 
the value of the user sessions we performed this experiment some

227
00:15:47.950 --> 00:15:51.980 
five hundred times. And the next slide I'd like to show you

228
00:15:51.980 --> 00:15:54.250 
the results of this experiment and

229
00:15:55.410 --> 00:15:57.970 
I would like to mention that of course

230
00:15:59.960 --> 00:16:05.970 
we measured the value of these user sessions in a

231
00:16:06.620 --> 00:16:09.890 
very specific context which is the context of our

232
00:16:11.220 --> 00:16:13.710 
deck based commander system.

233
00:16:14.360 --> 00:16:18.180 
Using two days to specific case of user sessions and

234
00:16:18.630 --> 00:16:19.480 
given a different

235
00:16:21.640 --> 00:16:24.060 
machine learning application and a different training data set,

236
00:16:24.640 --> 00:16:27.700 
the value of a single data point might differ.

237
00:16:29.060 --> 00:16:31.830 
And also we didn't

238
00:16:33.620 --> 00:16:34.550 
pay attention to any

239
00:16:35.650 --> 00:16:37.010 
interaction effects which

240
00:16:37.930 --> 00:16:39.480 
might obscure if you not only

241
00:16:40.560 --> 00:16:46.760 
remove one data point at once but two or three or more. So

242
00:16:48.920 --> 00:16:52.990 
nevertheless, the results have a business impact in that they

243
00:16:52.990 --> 00:16:53.880 
allowed us to

244
00:16:56.030 --> 00:16:59.810 
identify a few classes of data point that are really toxic and should

245
00:17:00.360 --> 00:17:05.470 
not be used for the training of such a recommender system. And

246
00:17:07.070 --> 00:17:10.710 
here you see the results of this experiment. So from these

247
00:17:11.640 --> 00:17:16.080 
five hundred data points for which we measure the

248
00:17:17.470 --> 00:17:19.950 
contribution to the system's performance

249
00:17:20.770 --> 00:17:23.100 
seventy one percent have

250
00:17:23.860 --> 00:17:27.460 
turned out to have a positive value in the sense that if he

251
00:17:28.040 --> 00:17:30.350 
leave them out, the conversion rate

252
00:17:31.280 --> 00:17:35.320 
gets worse while twelve and five percent

253
00:17:35.820 --> 00:17:41.240 
to virtually not change the system's performance. So these are redundant

254
00:17:41.810 --> 00:17:46.000 
user sessions that carry redundant information and can simply

255
00:17:48.010 --> 00:17:49.890 
be left out from the training dataset.

256
00:17:51.010 --> 00:17:53.880 
Sixteen point five percent of the data points

257
00:17:55.540 --> 00:17:57.810 
turned out to be toxic.

258
00:17:58.770 --> 00:18:02.920 
In the sense that if we leave them out from the training dataset,

259
00:18:03.350 --> 00:18:05.030 
the conversion rates

260
00:18:06.780 --> 00:18:09.900 
get significantly better.

261
00:18:10.830 --> 00:18:17.460 
And well, so far we have measured the contribution of user sessions.

262
00:18:18.950 --> 00:18:22.850 
If we now were able not only to measure

263
00:18:23.440 --> 00:18:26.690 
the value afterwards, but

264
00:18:28.400 --> 00:18:32.770 
predict the value- in the middle: of the middle user sessions.

265
00:18:33.470 --> 00:18:36.440 
From session properties before we train the system,

266
00:18:36.920 --> 00:18:41.180 
we could before training our recommender system and only select

267
00:18:41.180 --> 00:18:43.080 
the valuable user sessions. That is,

268
00:18:44.370 --> 00:18:47.390 
reduce the data, reduce the computing time and

269
00:18:48.170 --> 00:18:50.300 
further improve the performance of the system.

270
00:18:51.210 --> 00:18:53.860 
However this last step is not only measuring but

271
00:18:55.230 --> 00:18:56.700 
predicting the value of

272
00:18:58.020 --> 00:18:59.020 
data points

273
00:19:00.230 --> 00:19:07.280 
is not taken yet. And trust me to do a summary and then a short outlook.

274
00:19:09.790 --> 00:19:11.990 
So I hope that I could convince you that data

275
00:19:13.460 --> 00:19:18.090 
minimalism has many ecological, economical, and societal benefits.

276
00:19:19.790 --> 00:19:22.030 
Plateaux example of the
recommender system

277
00:19:22.850 --> 00:19:26.420 
we learned that data minimalism can significantly

278
00:19:26.420 --> 00:19:28.190 
decrease the computing time of

279
00:19:28.980 --> 00:19:30.830 
large scale AI applications by

280
00:19:31.620 --> 00:19:32.830 
improving the performance.

281
00:19:34.130 --> 00:19:38.980 
In our case, the system performs best using only twenty

282
00:19:38.980 --> 00:19:45.050 
percent of the data and measuring and then predicting the value of individual

283
00:19:45.630 --> 00:19:48.900 
data points will allow to further reduce the amount of data

284
00:19:49.100 --> 00:19:51.940 
and to improve the performance of such applications.

285
00:19:55.350 --> 00:20:02.380 
For the outlook at OSP, we will continue to promote responsibility and sustainability in

286
00:20:02.740 --> 00:20:06.300 
IT and AI for retail and logistics.

287
00:20:07.060 --> 00:20:11.870 
And just to give you an example of our activities I would like

288
00:20:11.870 --> 00:20:16.750 
to mention that we recently carried out a sustainable programming challenge where we

289
00:20:17.850 --> 00:20:21.750 
he was pm police were called to brainstorm

290
00:20:23.040 --> 00:20:29.940 
ideas; how we further could contribute to the reduction of CO2 footprints

291
00:20:29.940 --> 00:20:31.310 


292
00:20:31.940 --> 00:20:35.550 
of our applications within our daily work and

293
00:20:36.680 --> 00:20:41.890 
then also implemented prototypes of such applications,

294
00:20:42.470 --> 00:20:45.120 
which might be a topic of

295
00:20:47.330 --> 00:20:48.000 
future presentations.

296
00:20:50.710 --> 00:20:51.710 
Thank you for your interest.

297
00:20:56.400 --> 00:20:59.320 
Thank you very much mister Pietsch for your presentation.

298
00:20:59.770 --> 00:21:04.110 
So after that presentation we will now proceed to the

299
00:21:04.110 --> 00:21:08.750 
Q and A session. If you have any questions- you can either

300
00:21:09.510 --> 00:21:13.110 
put them in the chat or maybe raise your hand. Because we're

301
00:21:13.110 --> 00:21:16.420 
not so many today, you can maybe just also unmute yourself

302
00:21:16.430 --> 00:21:18.420 
and ask the question directly.

303
00:21:19.830 --> 00:21:22.470 
We are still recording as a quick note so if you don't want

304
00:21:22.470 --> 00:21:26.270 
to be seen, you can of course let your camera be turned off.

305
00:21:27.640 --> 00:21:31.740 
And so I prepared a few quick questions, so everyone else

306
00:21:31.740 --> 00:21:37.040 
can think a moment. My first question would be how did you start

307
00:21:37.040 --> 00:21:40.460 
this project. So has there been previous work in this field

308
00:21:40.460 --> 00:21:45.710 
to which you could relate with similar experiments or similar goals or

309
00:21:45.950 --> 00:21:48.310 
did you start your project from scratch?

310
00:21:52.870 --> 00:21:55.260 
No, we started the project from scratch.

311
00:21:57.010 --> 00:22:01.170 
Maybe Michaela, would you like to

312
00:22:02.160 --> 00:22:05.450 
say a few words to this?

313
00:22:07.740 --> 00:22:13.950 
She was the product owner of this project. Welcome.

314
00:22:18.330 --> 00:22:23.970 
Ok so I'm not allowed to put on my video but that doesn't matter. So a short

315
00:22:25.730 --> 00:22:29.240 
insight into history of data minimalism. So there is none.

316
00:22:31.080 --> 00:22:33.720 
The project actually was initially put up as a

317
00:22:34.300 --> 00:22:38.250 
applied research project and the question we were initially after was-

318
00:22:38.850 --> 00:22:40.530 
what is actually the

319
00:22:41.190 --> 00:22:46.470 
value of our data. So we wanted to know can we put a euro mark on our data.

320
00:22:46.700 --> 00:22:50.610 
Which was the initial question. And it turned out that the method

321
00:22:50.630 --> 00:22:55.450 
we found to do so, that is, was you saw exactly in the pictures for

322
00:22:55.910 --> 00:22:59.540 
looking at which data points change conversion rate for the

323
00:22:59.540 --> 00:23:01.170 
better or worse or not at all.

324
00:23:01.880 --> 00:23:04.980 
This you can also translate to euros of course.

325
00:23:05.880 --> 00:23:09.950 
And this question led to the question- so if you have so much data that

326
00:23:10.150 --> 00:23:15.390 
actually has no effect or even damages our algorithms results

327
00:23:15.620 --> 00:23:19.290 
couldn't we say we minimize all of this so that we

328
00:23:19.290 --> 00:23:22.160 
have the maximum effect out of little data.

329
00:23:22.760 --> 00:23:25.740 
So and that was how to turn data minimalism was actually born.

330
00:23:25.740 --> 00:23:28.010 
Because we thought that is exactly what we should do. Because

331
00:23:28.010 --> 00:23:32.580 
this has also environmental impact. It's an important part of digital responsibility

332
00:23:32.980 --> 00:23:37.040 
and also of accounting. So we also do money minimalism, right?

333
00:23:37.040 --> 00:23:39.100 
We don't want to spend more money than we have to.

334
00:23:39.540 --> 00:23:42.460 
And we thought the same way about data, but there was no previous

335
00:23:42.460 --> 00:23:45.930 
work. It was just an idea, but born out of a question that was

336
00:23:45.930 --> 00:23:49.680 
only present at the time because many people still are looking for way

337
00:23:49.930 --> 00:23:53.640 
to actually price in their data and their countings.

338
00:23:56.650 --> 00:24:00.420 
I'm honestly quite surprised I would have thought that at least

339
00:24:00.420 --> 00:24:03.570 
some work in this field would have been done already, but

340
00:24:03.580 --> 00:24:06.060 
very interesting and I think it's very cool that you

341
00:24:06.590 --> 00:24:10.970 
have had very striking results. So I was very surprised by the chart with the

342
00:24:11.410 --> 00:24:15.770 
twenty percent where the best results were achieved when you only used

343
00:24:16.140 --> 00:24:17.970 
twenty percent of the data.

344
00:24:19.060 --> 00:24:23.990 
So have you also experimented with any other systems or applications

345
00:24:24.010 --> 00:24:28.380 
and then the recommendation system or maybe as you've already said-

346
00:24:28.960 --> 00:24:32.870 
could you imagine that these results or the system could be used

347
00:24:33.020 --> 00:24:33.950 
at other places?

348
00:24:38.680 --> 00:24:39.230 
So I think that

349
00:24:40.460 --> 00:24:42.770 
this experimental setup could

350
00:24:43.580 --> 00:24:51.940 
also apply to any other machine learning system and however

351
00:24:53.200 --> 00:24:55.570 
how this curve looks like

352
00:24:56.760 --> 00:24:59.530 
for different application, different training data set, it's

353
00:25:00.780 --> 00:25:02.480 
hard to say before. And I guess

354
00:25:04.320 --> 00:25:06.330 
this also strongly depends on the

355
00:25:07.340 --> 00:25:09.250 
machine learning algorithm you use.

356
00:25:10.930 --> 00:25:13.910 
As I said in the talk,

357
00:25:15.550 --> 00:25:17.570 
now you expect that your

358
00:25:19.090 --> 00:25:22.330 
performance measure converges to maximum at that sometimes and then

359
00:25:22.530 --> 00:25:26.890 
does not fall up again. How I am

360
00:25:27.910 --> 00:25:31.210 
well, I think in principle you can apply this

361
00:25:32.090 --> 00:25:35.130 
experiment to any application.

362
00:25:36.770 --> 00:25:41.020 
What the results were will depend on the algorithm.

363
00:25:41.990 --> 00:25:44.110 
I think this is also a

364
00:25:51.020 --> 00:25:55.790 
Thank you. So a Maxim Asjoma has raised his hand. So, you

365
00:25:55.790 --> 00:25:58.450 
can unmute yourself and just ask your question if you want to.

366
00:25:59.960 --> 00:26:03.990 
Yes thank you thank you Niklas for this amazing research.

367
00:26:04.190 --> 00:26:07.190 
And my line of work, I've been going around for years telling

368
00:26:07.190 --> 00:26:11.450 
everybody we need a lot of data to make AI work and now to

369
00:26:11.450 --> 00:26:12.960 
see this, this is just amazing.

370
00:26:13.810 --> 00:26:19.290 
I got two questions which you could tell maybe a little bit about.

371
00:26:19.590 --> 00:26:24.080 
So when looking at this slide you just show, you

372
00:26:24.080 --> 00:26:28.210 
say that at twenty percent of the nine terabyte data you achieve

373
00:26:28.790 --> 00:26:29.820 
the best conversion rate.

374
00:26:31.070 --> 00:26:32.380 
What I'm wondering is,

375
00:26:33.450 --> 00:26:38.480 
is this always a relative amount or is there a fixed absolute

376
00:26:38.620 --> 00:26:41.350 
data size which is optimal for,

377
00:26:42.510 --> 00:26:47.130 
what I'd say if you get more data on this specific algorithm for example,

378
00:26:47.670 --> 00:26:51.090 
would it still be twenty percent or is this really

379
00:26:51.610 --> 00:26:55.440 
the twenty percent of nine terabyte which is the best.

380
00:26:56.210 --> 00:27:02.270 
And the second would be, do you have a nice example of a

381
00:27:03.100 --> 00:27:06.590 
toxic data set. Is there something you can tell about this or

382
00:27:06.590 --> 00:27:08.370 
is this just like some number stuff.

383
00:27:10.750 --> 00:27:12.260 
So the first question I'm

384
00:27:13.110 --> 00:27:18.020 
yeah so this is a thirty percent of three hundred days

385
00:27:18.520 --> 00:27:22.410 
of user sessions. So in this is a

386
00:27:23.220 --> 00:27:27.250 
an absolute value actually. So

387
00:27:27.690 --> 00:27:29.390 
of course using a different data set

388
00:27:30.460 --> 00:27:33.510 
this might not be twenty percent but fifty percent of ten percent

389
00:27:33.580 --> 00:27:35.920 
depending on the amount of user sessions.

390
00:27:36.420 --> 00:27:38.920 
So at Otto, we have about

391
00:27:41.400 --> 00:27:46.970 
a couple of million visits each day. So if you have a different

392
00:27:46.970 --> 00:27:49.770 
online shop, then you most likely have less

393
00:27:51.310 --> 00:27:57.070 
user sessions a day or more. And so I guess

394
00:27:57.850 --> 00:28:04.190 
this would, then you would have a different results. So that

395
00:28:04.430 --> 00:28:05.360 
might not be twenty percent

396
00:28:06.000 --> 00:28:06.640 
but fifty or

397
00:28:08.100 --> 00:28:11.940 
honestly, does it answer your question? So I'm

398
00:28:16.560 --> 00:28:21.160 
ok

399
00:28:23.810 --> 00:28:24.920 
Maxim, are you still there?

400
00:28:26.650 --> 00:28:32.620 
Well yeah I seem to have some internet problem stability.

401
00:28:33.990 --> 00:28:37.870 
I couldn't hear most of the answer unfortunately but if

402
00:28:37.870 --> 00:28:40.040 
you gotta recorded I will look it up.

403
00:28:42.800 --> 00:28:48.220 
Ok then the second question was if we have a nice example for

404
00:28:48.230 --> 00:28:49.640 
toxic data points and

405
00:28:51.560 --> 00:28:56.120 
yeah there are a couple of nice examples. One is-

406
00:28:57.050 --> 00:28:58.340 
occurs when you have

407
00:29:01.440 --> 00:29:06.010 
flash sales or deals of the day. I don't know if you'll know

408
00:29:06.010 --> 00:29:09.450 
this from Otto. I guess Amazon has some other

409
00:29:12.110 --> 00:29:16.630 
types of such sales where you get some

410
00:29:18.000 --> 00:29:20.010 
I don't know playstation five for

411
00:29:22.410 --> 00:29:26.410 
really cheap. So and then all your customers

412
00:29:27.230 --> 00:29:28.930 
click on this playstation and

413
00:29:30.470 --> 00:29:33.180 
maybe also the recommendations of the playstation and vice

414
00:29:33.180 --> 00:29:34.960 
versa. Also the playstation might be a

415
00:29:35.720 --> 00:29:38.260 
recommendation for other products and since it

416
00:29:39.590 --> 00:29:40.580 
is cheap then

417
00:29:42.480 --> 00:29:45.100 
there much more customers click on this product. Than

418
00:29:47.530 --> 00:29:50.970 
if you don't have this, this flash sale or whatever,

419
00:29:52.110 --> 00:29:53.020 
it's kind of an

420
00:29:55.480 --> 00:30:01.780 
artificial product similarity which was caused by the sale. And

421
00:30:02.730 --> 00:30:05.550 
so these are really toxic and what we observed is that

422
00:30:06.490 --> 00:30:11.130 
after these sales when you have such articles the

423
00:30:13.520 --> 00:30:14.460 
recommendations

424
00:30:16.630 --> 00:30:19.640 
drop off, as the quality of the recommendations

425
00:30:21.220 --> 00:30:24.520 
fell off after the after days of these flash sale.

426
00:30:25.520 --> 00:30:27.560 
So by this you

427
00:30:29.850 --> 00:30:30.380 


428
00:30:35.120 --> 00:30:36.950 
you decrease your

429
00:30:37.630 --> 00:30:40.450 
conversion rate afterwards, and so these are data points

430
00:30:41.170 --> 00:30:44.490 
to remove from your time graph.

431
00:30:46.850 --> 00:30:47.970 
Very interesting, thank you.

432
00:30:51.190 --> 00:30:54.230 
um I just had another question as well.

433
00:30:54.880 --> 00:30:58.390 
So you have brought two charts with you. This one- where

434
00:30:58.390 --> 00:31:02.070 
we have these two lines where it basically says at twenty percent

435
00:31:02.120 --> 00:31:06.700 
we had the highest conversion rate and then later you said that

436
00:31:06.910 --> 00:31:11.860 
with seventy percent of the data was useful for your AI.

437
00:31:12.800 --> 00:31:17.410 
Is there any combination or correlation between those two graphs. So

438
00:31:18.030 --> 00:31:21.610 
twenty percent of the amount of data is the best but seventy

439
00:31:21.610 --> 00:31:26.610 
percent of the data is at least useful or are these two completely different findings?

440
00:31:27.320 --> 00:31:30.570 
Two completely different finding, so

441
00:31:31.100 --> 00:31:32.650 
in this first experiment we

442
00:31:33.830 --> 00:31:37.100 
increase the amount of data up to three hundred days of user sessions and

443
00:31:37.450 --> 00:31:40.430 
for the second experiment we chose this point here,

444
00:31:41.170 --> 00:31:43.980 
where we only use zero point seven percent of the data.

445
00:31:44.510 --> 00:31:47.650 
These correspond to the two days of user sessions and we did this because

446
00:31:48.340 --> 00:31:50.810 
we couldn't do this

447
00:31:52.070 --> 00:31:53.950 
in minus one experiment using

448
00:31:55.190 --> 00:31:56.680 
all the data because

449
00:31:57.680 --> 00:31:58.510 
it quite

450
00:32:00.670 --> 00:32:03.420 
needs some time to train all the models and then

451
00:32:04.760 --> 00:32:08.870 
calculate the recommendations and do the analysis. So

452
00:32:11.100 --> 00:32:14.780 
if we use twenty percent of the data and then perform this

453
00:32:15.790 --> 00:32:18.690 
experiments, the results might be different.

454
00:32:22.250 --> 00:32:24.230 
This is what I try to

455
00:32:25.270 --> 00:32:26.990 
address in these talks, that

456
00:32:27.880 --> 00:32:30.420 
there's still some work to do and so far we didn't

457
00:32:32.220 --> 00:32:35.190 
care about the combination of such effects.

458
00:32:38.070 --> 00:32:40.540 
It's rather to get a first indicator on

459
00:32:40.950 --> 00:32:45.140 
how far we can reduce the amount of data and then in the next step

460
00:32:47.120 --> 00:32:48.330 
if we have fixed the

461
00:32:49.320 --> 00:32:53.730 
size of the dataset and how many data points among

462
00:32:55.500 --> 00:32:57.900 
the sixth training dataset

463
00:32:59.070 --> 00:33:00.990 
are valuable or toxic.

464
00:33:04.380 --> 00:33:07.170 
Yeah I think that answers my question. Thank you.

465
00:33:08.690 --> 00:33:14.020 
So i would look at the participation list but I don't see anyone having

466
00:33:14.140 --> 00:33:18.970 
raised their hand, so I will maybe just put one last quick question.

467
00:33:20.080 --> 00:33:23.580 
Is the system or the results of your experiments-

468
00:33:24.540 --> 00:33:27.560 
did you find a way to put this at work already so

469
00:33:28.360 --> 00:33:31.940 
were you able to identify some data that are not being used?

470
00:33:31.950 --> 00:33:35.090 
Have you changed anything in the currently working

471
00:33:35.930 --> 00:33:39.210 
recommendation system or does it still need a lot more work

472
00:33:39.210 --> 00:33:41.560 
before you can actually use these results?

473
00:33:43.450 --> 00:33:46.950 
That still needs at least some more work.

474
00:33:47.690 --> 00:33:48.960 
So this is not

475
00:33:50.890 --> 00:33:53.990 
Up until now, research it's not

476
00:33:55.470 --> 00:33:57.550 
applied to any application yet.

477
00:34:01.720 --> 00:34:04.810 
Okay, thank you. So

478
00:34:05.710 --> 00:34:09.160 
I think that's it for this live talk then. Today

479
00:34:10.050 --> 00:34:12.550 
we are perfectly at around forty minutes. Again

480
00:34:13.070 --> 00:34:16.280 
thank you very much for your very interesting talk and for

481
00:34:16.760 --> 00:34:19.410 
the answers you gave on our questions.

482
00:34:20.310 --> 00:34:24.640 
I'm really surprised that there was no prior work into this field but

483
00:34:24.760 --> 00:34:27.900 
I think that's a very interesting pioneer work then.

484
00:34:29.410 --> 00:34:31.630 
I will quickly share my screen to the end

485
00:34:32.630 --> 00:34:37.220 
so I can give you an outlook. Can you see my screen everyone?

486
00:34:39.620 --> 00:34:43.970 
Ok so yes the next live talk will be on the twenty seventh

487
00:34:43.970 --> 00:34:48.580 
of September. So in around four weeks again. And with us will be professor

488
00:34:48.780 --> 00:34:53.250 
Jaafar Elmirghani and he will be talking about Energy-efficient

489
00:34:53.250 --> 00:34:58.520 
Cloud and fog computing. he also did a video in the open HPI- MOOC

490
00:34:58.850 --> 00:35:03.460 
on clean IT. So you can watch this video first

491
00:35:03.520 --> 00:35:05.440 
if you want to prepare some questions.

492
00:35:06.570 --> 00:35:11.710 
Yes as always we will also put announcements in our clean IT forum.

493
00:35:11.950 --> 00:35:15.950 
You can join it on the Open HPI platform or you can contact

494
00:35:15.950 --> 00:35:18.920 
us directly at clean-IT@hpi.de.

495
00:35:20.390 --> 00:35:24.280 
Without being said, thank everybody for joining here today. Thank you again

496
00:35:24.710 --> 00:35:26.980 
Niklas Pietsch and see you next day.
