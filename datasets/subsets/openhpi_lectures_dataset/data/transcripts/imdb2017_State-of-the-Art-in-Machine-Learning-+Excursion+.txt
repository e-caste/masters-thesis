WEBVTT

1
00:00:00.000 --> 00:00:04.400 
I want to share with you a few
of the trends that are happening

2
00:00:04.400 --> 00:00:08.800 
across these spaces. I've
had people attending CVPR

3
00:00:08.800 --> 00:00:12.120 
this week, NIPS all the
major conferences we

4
00:00:12.120 --> 00:00:14.140 
go to because
we believe in

5
00:00:14.140 --> 00:00:18.180 
applied technology, our mission
is to take the latest and greatest

6
00:00:18.180 --> 00:00:21.210 
from research and to turn it
into product that customers want

7
00:00:21.210 --> 00:00:24.240 
to buy. We don't have to be
at the forefront of research,

8
00:00:25.250 --> 00:00:27.270 
we have to be at the forefront
of technology transfer

9
00:00:27.270 --> 00:00:31.310 
and applied engineering. Lets
look at the few of the things

10
00:00:31.310 --> 00:00:34.340 
that are trends
in 2017 or

11
00:00:35.000 --> 00:00:37.370 
reasonably hot to be still
considered a trend although they

12
00:00:37.370 --> 00:00:41.000 
happened in 2016. I
want to share a few

13
00:00:41.410 --> 00:00:44.440 
random examples of this that
mainly have one characteristic

14
00:00:44.440 --> 00:00:47.470 
in common, I find them
quite cool to look at.

15
00:00:49.490 --> 00:00:52.520 
This is one that
came out of A 2016

16
00:00:52.520 --> 00:00:55.550 
where folks
try to generate

17
00:00:55.550 --> 00:00:59.590 
image descriptions by saying
I am giving a bunch of pixels,

18
00:00:59.590 --> 00:01:02.620 
here is five mega
pixels of RGB values.

19
00:01:03.630 --> 00:01:07.670 
Can you generate a description
of what is in that scene?

20
00:01:07.670 --> 00:01:10.700 
People have tried that
a lot with sort of

21
00:01:10.700 --> 00:01:13.730 
the next step from classifying
and labeling images

22
00:01:13.730 --> 00:01:17.770 
is to actually create a synthesis
of what is in the image.

23
00:01:17.770 --> 00:01:21.810 
The heart thing about this is
that there's a whole bunch of

24
00:01:21.810 --> 00:01:23.830 
things that could be
said about the image.

25
00:01:23.830 --> 00:01:28.880 
You could for example say that
this is a woman with a daughter

26
00:01:28.880 --> 00:01:32.920 
standing on green grass
with trees in the background

27
00:01:32.920 --> 00:01:36.960 
and that's a valid
description. You could also say

28
00:01:36.960 --> 00:01:39.990 
that this is
a mowed lawn

29
00:01:39.990 --> 00:01:43.103 
in front of the forest
with people standing on at,

30
00:01:43.103 --> 00:01:46.106 
one of whom happens to be a
woman and wearing a green shirt.

31
00:01:47.107 --> 00:01:50.110 
This is sort of at the
root of things when we say

32
00:01:50.110 --> 00:01:53.113 
an image says more
than a thousand words,

33
00:01:53.113 --> 00:01:57.117 
the problem is you never quite
know which thousand words because

34
00:01:57.117 --> 00:02:00.120 
there is so many different
interpretations open to a single

35
00:02:00.120 --> 00:02:03.123 
image which is why they are
great for communication. They

36
00:02:03.123 --> 00:02:06.126 
create these associations in
everybody's head but you never

37
00:02:06.126 --> 00:02:09.129 
quite know what they
are. The recent advance

38
00:02:09.129 --> 00:02:12.132 
here in generating relevant
descriptions like. "Hey, the woman

39
00:02:12.132 --> 00:02:16.136 
is throwing a frisbee in the
park" is trying to capture where

40
00:02:16.136 --> 00:02:20.140 
humans would direct their
attention when looking at an image

41
00:02:20.140 --> 00:02:22.142 
and to sort of wait
this attention function.

42
00:02:22.142 --> 00:02:25.145 
What you can see in the image
is rendered on the right

43
00:02:25.145 --> 00:02:29.149 
is that the neural network
model actually has a fairly good

44
00:02:29.149 --> 00:02:32.152 
idea of what is the center
of attention in that image

45
00:02:32.152 --> 00:02:35.155 
because it looks at foreground
background contrasts, it looks

46
00:02:35.155 --> 00:02:37.157 
at motion, it looks
at all these elements.

47
00:02:38.158 --> 00:02:41.161 
But really all it does is calculate
a hundred fifty layers of matrix

48
00:02:41.161 --> 00:02:45.165 
multiplications to mimic what
the human does in this kind

49
00:02:45.165 --> 00:02:49.169 
and that enables us to sort
of direct the generation

50
00:02:49.169 --> 00:02:53.173 
of descriptions in a way that
makes them humanly relevant. Now

51
00:02:53.173 --> 00:02:56.176 
you probably want to say
that the right thing is a

52
00:02:57.177 --> 00:03:00.180 
is about a stop sign,
it's not about clouds,

53
00:03:00.180 --> 00:03:03.183 
it's not about sort of meadows
or something like that, the

54
00:03:03.183 --> 00:03:06.186 
key element is the stop sign.
The key person in the picture

55
00:03:06.186 --> 00:03:09.189 
on the lower left is not
the teddy bear because

56
00:03:09.189 --> 00:03:11.191 
the teddy bear there is
not looking at us, the key

57
00:03:12.192 --> 00:03:15.195 
player here is that little girl
with the face turned towards

58
00:03:15.195 --> 00:03:19.199 
us and attention mechanisms are
all over deep learning right

59
00:03:19.199 --> 00:03:22.202 
now because we're trying to direct
the attention of the networks

60
00:03:22.202 --> 00:03:25.205 
going somewhere. I think
the analogy is that

61
00:03:26.206 --> 00:03:29.209 
humans continuously direct their
attention, we have this illusion

62
00:03:29.209 --> 00:03:33.213 
of a complete room in front
of us, actually it's not

63
00:03:33.213 --> 00:03:36.216 
there, the room is not
there. What our brain sees is

64
00:03:36.216 --> 00:03:40.220 
little saccades of eye movements
with a sharp zone where we are

65
00:03:40.220 --> 00:03:43.223 
looking right now and with
the blurry zone surrounding it

66
00:03:43.223 --> 00:03:47.227 
in the attention mechanism is
built in to how we look at the

67
00:03:47.227 --> 00:03:50.230 
scenes, how we look at documents,
and how we interact with

68
00:03:50.230 --> 00:03:54.234 
our environment.
They are all over

69
00:03:54.234 --> 00:03:55.235 
AI at the moment.

70
00:03:57.237 --> 00:04:00.240 
Here is another thing, this
is from cvpr this week, a

71
00:04:00.240 --> 00:04:03.243 
paper called simGAN. We
never have enough data

72
00:04:04.244 --> 00:04:08.248 
to really deal with complex
challenges in the enterprise world,

73
00:04:08.248 --> 00:04:12.252 
machine learning and deep learning
is not a big data problem.

74
00:04:12.252 --> 00:04:15.255 
I would love to have big data
problems because it means all

75
00:04:15.255 --> 00:04:16.256 
my machine learning
problems are solved,

76
00:04:17.257 --> 00:04:19.259 
unfortunately there
is never enough data.

77
00:04:19.259 --> 00:04:23.263 
What people are doing
now is they are using

78
00:04:23.263 --> 00:04:27.267 
a combination of synthetic
and real images in order to

79
00:04:27.267 --> 00:04:31.271 
train and in order to build ever
more sophisticated and better

80
00:04:31.271 --> 00:04:34.274 
models. We are doing that
too, we've patented that

81
00:04:34.274 --> 00:04:38.278 
technique, for example
logo detection and

82
00:04:38.278 --> 00:04:42.282 
product object detection.
The idea behind

83
00:04:42.282 --> 00:04:46.286 
this is that you can render
synthetic scenes, in this example

84
00:04:46.286 --> 00:04:49.289 
it's an eye of a
person and you can use

85
00:04:49.289 --> 00:04:54.294 
style transfer to bring in
information from real images

86
00:04:54.294 --> 00:04:58.298 
to make the rendered image more
realistic and training successful

87
00:04:58.298 --> 00:05:02.302 
with this. We are using this,
for example, in our own work to

88
00:05:02.302 --> 00:05:05.305 
render scenes of soccer
stadiums and they

89
00:05:05.305 --> 00:05:08.308 
look like soccer stadiums in a
computer game, they are pretty

90
00:05:08.308 --> 00:05:14.314 
sort of cold and and life-less.
But when you do style transfer

91
00:05:14.314 --> 00:05:17.317 
from unlabelled
real images,

92
00:05:17.317 --> 00:05:20.320 
what you can actually do is
you can feel the ranks with an

93
00:05:20.320 --> 00:05:24.324 
audience, you can make
the players have emotions,

94
00:05:24.324 --> 00:05:28.328 
you can sort of bring real
world grit into the scene and

95
00:05:28.328 --> 00:05:32.332 
that means the learning becomes
so much more successful because

96
00:05:32.332 --> 00:05:36.336 
you are basing it of a combination
of an infinite amount of

97
00:05:37.337 --> 00:05:40.340 
rendered information
and some real world.

98
00:05:40.340 --> 00:05:44.344 
The beauty of the rendering is
I do not need to pay people to

99
00:05:44.344 --> 00:05:48.348 
label my data for me because
I've rendered the image, I know

100
00:05:48.348 --> 00:05:51.351 
where the objects are, I placed
them, I placed the camera, I placed

101
00:05:51.351 --> 00:05:54.354 
the lights or at least some
randomizer in the computer did.

102
00:05:55.355 --> 00:05:57.357 
Thus we know what to look
for and we know how to train.

103
00:05:59.359 --> 00:06:02.362 
That's the beauty of any generation
based environment for ML.

104
00:06:05.365 --> 00:06:08.368 
Here is another one, this
is called deeply connected

105
00:06:08.368 --> 00:06:11.371 
convolutional neural
networks. This is the best

106
00:06:11.371 --> 00:06:16.376 
paper at CVPR 2017 this
week and it's currently

107
00:06:16.376 --> 00:06:19.379 
the world record-holder for super
human performance in computer

108
00:06:19.379 --> 00:06:24.384 
vision. The previous world
record holder was called resnet,

109
00:06:24.384 --> 00:06:28.388 
that was a paper by microsoft
and they were the first

110
00:06:28.388 --> 00:06:30.390 
to build networks
that were really deep,

111
00:06:30.390 --> 00:06:33.393 
not sort of ten or twenty
layers but actually 150

112
00:06:33.393 --> 00:06:36.396 
layers or more. The

113
00:06:36.396 --> 00:06:39.399 
densely connected
convolutional networks

114
00:06:39.399 --> 00:06:43.403 
have found a new way
to exploit locality

115
00:06:44.000 --> 00:06:47.407 
because they are not just
exploiting locality in the

116
00:06:47.407 --> 00:06:52.412 
image, they are also exploiting
locality in the neural network

117
00:06:52.412 --> 00:06:55.415 
that's processing
this information.

118
00:06:55.415 --> 00:06:58.418 
The way to visualize this
is the chart at the bottom

119
00:06:58.418 --> 00:07:00.420 
that basically shows

120
00:07:00.420 --> 00:07:04.424 
the transformation from an input
image to a lable or prediction

121
00:07:04.424 --> 00:07:07.427 
that says horse, in this
case. What we're doing

122
00:07:07.427 --> 00:07:10.430 
with densely
connected networks is

123
00:07:10.430 --> 00:07:14.434 
we're connecting each layer to the
successive layers and we're creating

124
00:07:14.434 --> 00:07:18.438 
bypasses so that information
at multiple resolutions

125
00:07:18.438 --> 00:07:21.441 
can be propagated
forward and exploited.

126
00:07:21.441 --> 00:07:25.445 
This is in a way inspired from
how the cortical columns in

127
00:07:25.445 --> 00:07:29.449 
the visual cortex in the brain
work. They have a very repetitive

128
00:07:29.449 --> 00:07:33.453 
structure, they are these
little columns of a couple

129
00:07:33.453 --> 00:07:37.457 
of dozens of layer that don't
just have feed forward connections

130
00:07:37.457 --> 00:07:40.460 
like we do in computer science
with matrix multipliers going

131
00:07:40.460 --> 00:07:44.464 
from one stage to the next.
But you have these little axons

132
00:07:44.464 --> 00:07:47.467 
that are going up and connecting
with successive layers and

133
00:07:47.467 --> 00:07:49.469 
layers that are
behind them.

134
00:07:50.470 --> 00:07:54.474 
This simple idea, hey I have
a neural network that works,

135
00:07:55.475 --> 00:07:58.478 
can I just add a few more
connections that also go deeper

136
00:07:59.479 --> 00:08:04.484 
in the network geometry itself.
This simple single thought

137
00:08:04.484 --> 00:08:08.488 
was the most, was
the best paper

138
00:08:08.488 --> 00:08:12.492 
at the world's most renowned
computer vision conference

139
00:08:12.492 --> 00:08:18.498 
this week. Doing this is within
the reach of any group with

140
00:08:18.498 --> 00:08:21.501 
a few people who can do deep
learning, we are in a golden age

141
00:08:21.501 --> 00:08:24.504 
where simple tricks
and changes like this

142
00:08:25.505 --> 00:08:28.508 
create a step change in performance
and create a step change

143
00:08:28.508 --> 00:08:31.511 
in the capabilities of
the networks. This is

144
00:08:31.511 --> 00:08:35.515 
sort of, this is the one
thought that I want to inspire

145
00:08:35.515 --> 00:08:39.519 
with you, doing this is super
easy, it's five hundred or thousand

146
00:08:39.519 --> 00:08:42.522 
lines of code. There is no way
you can achieve as much with

147
00:08:42.522 --> 00:08:45.525 
five hundred or thousand lines
of code in any other discipline

148
00:08:45.525 --> 00:08:48.528 
of computer science
and drive as much value

149
00:08:48.528 --> 00:08:52.532 
for businesses and for customers,
it's a golden age of tinkerers

150
00:08:52.532 --> 00:08:54.534 
right now,you should
be part of it.

151
00:08:57.537 --> 00:09:01.541 
Of course every rose
has it's thorns,

152
00:09:01.541 --> 00:09:05.545 
this is another
one from CVPR 2017,

153
00:09:06.000 --> 00:09:09.549 
there were papers out 2-3
years ago saying deep learning

154
00:09:09.549 --> 00:09:14.554 
is easily fooled because they
were able to generate images that

155
00:09:15.555 --> 00:09:18.558 
were systematically being miss
classified by the networks.

156
00:09:19.559 --> 00:09:23.563 
A litle game of war between
sort of the good guys trying to

157
00:09:23.563 --> 00:09:26.566 
find the truth and the bad
guys trying to spoof it

158
00:09:27.567 --> 00:09:30.570 
is bringing up. We have
the computer security

159
00:09:30.570 --> 00:09:33.573 
and we have the cyber security
things bringing up and forth

160
00:09:33.573 --> 00:09:36.576 
also for ML and deep learning
because it's getting real world

161
00:09:36.576 --> 00:09:41.581 
relevant. Somebody came out
with something extremely clever

162
00:09:41.581 --> 00:09:44.584 
it's called a
universal perturbator

163
00:09:45.585 --> 00:09:48.588 
and it turns out that for
every network geometry

164
00:09:48.588 --> 00:09:53.593 
that you can create, there is
a kind of noise image that you

165
00:09:53.593 --> 00:09:56.596 
can create that's only linked
to the network geometry.

166
00:09:57.597 --> 00:10:01.601 
Because it represents the higher
dimensional decision surfaces

167
00:10:01.601 --> 00:10:04.604 
off the thing and how the
100+ layers of thin network

168
00:10:04.604 --> 00:10:08.608 
interact with each other. The
exciting thing is if you take

169
00:10:09.609 --> 00:10:12.612 
an existing image in
any category and you add

170
00:10:12.612 --> 00:10:15.615 
your bit of carefully
crafted noise to it

171
00:10:15.615 --> 00:10:18.618 
the network will get it
wrong almost every time.

172
00:10:18.618 --> 00:10:22.622 
Yyou can see it's sort of
stable, it only depends

173
00:10:22.622 --> 00:10:24.624 
on the network geometry
and it's a bit frightening

174
00:10:24.624 --> 00:10:27.627 
actually because it's
like whispering these

175
00:10:27.627 --> 00:10:30.630 
hypnotic words of suggestion
to a human that will make

176
00:10:30.630 --> 00:10:33.633 
them turn around and sort
of kill the president,

177
00:10:34.000 --> 00:10:36.636 
at least in movies
and science fiction.

178
00:10:36.636 --> 00:10:39.639 
You can do that with
these networks right now,

179
00:10:39.639 --> 00:10:42.642 
whispering a universal
adversarial perturbation

180
00:10:42.642 --> 00:10:47.647 
in and the face powder box
will become a chihuahua, the

181
00:10:47.647 --> 00:10:51.651 
joystick will also become a
chihuahua, the car grill will become

182
00:10:51.651 --> 00:10:55.655 
a j which is a kind of board
and the thresher which is

183
00:10:55.655 --> 00:10:58.658 
farm equipment will be
classified as a labrador,

184
00:10:58.658 --> 00:11:00.660 
the flagpole will also
become a labrador.

185
00:11:01.661 --> 00:11:04.664 
There's something deep at work
here because these perturbations

186
00:11:04.664 --> 00:11:08.668 
are universal, it doesn't
necessarily limit the

187
00:11:08.668 --> 00:11:12.672 
real world applications
of what we can do with ML

188
00:11:12.672 --> 00:11:16.676 
but it creates an
interesting field of study

189
00:11:16.676 --> 00:11:19.679 
and another arms race between
the good guys and the bad guys

190
00:11:19.679 --> 00:11:22.682 
trying to build better systems
and trying to fool the systems

191
00:11:22.682 --> 00:11:23.683 
into something else.

192
00:11:25.685 --> 00:11:29.689 
Here is another thing
that's fairly recent,

193
00:11:29.689 --> 00:11:32.692 
this is the face to face demo
which teaches us to be very

194
00:11:32.692 --> 00:11:36.696 
wary of any image that we
see in the outside world.

195
00:11:37.697 --> 00:11:39.699 
What it
basically does is

196
00:11:39.699 --> 00:11:44.704 
do a style transfer of facial
expressions from one image to

197
00:11:44.704 --> 00:11:48.708 
another and in this case style
transfer means if we can change

198
00:11:48.708 --> 00:11:52.712 
the person that is doing the talking
or that is doing the emotions

199
00:11:52.712 --> 00:11:56.716 
and the rendered image
will completely follow

200
00:11:56.716 --> 00:12:00.720 
that. We can have dead
actors coming alive and

201
00:12:00.720 --> 00:12:04.724 
real world living actors sort
of transferring what they

202
00:12:04.724 --> 00:12:07.727 
do want to onto these actors
or you could have completely

203
00:12:07.727 --> 00:12:11.731 
fabricated scenes that are
one hundred percent synthetic

204
00:12:11.731 --> 00:12:14.734 
because we able to
do style transfer

205
00:12:14.734 --> 00:12:19.739 
on to canvases and on
to pixel collections

206
00:12:19.739 --> 00:12:22.742 
in a way that resembles
real world videos.

207
00:12:23.743 --> 00:12:25.745 
Here is a bit of what's
behind a lot of these,

208
00:12:26.746 --> 00:12:29.749 
the technique is called
adversarial networks

209
00:12:29.749 --> 00:12:34.754 
and it's basically something
straight from game theory and

210
00:12:34.754 --> 00:12:37.757 
sort of, you have probably
heard mini max and these

211
00:12:37.757 --> 00:12:41.761 
AP games in many of your
lectures. What we're doing here is

212
00:12:41.761 --> 00:12:44.764 
we're pitting to networks
against each other

213
00:12:44.764 --> 00:12:47.767 
and we having them play out
a zero sum game in training.

214
00:12:47.767 --> 00:12:51.771 
One network has to look at an
image and tell us whether it's

215
00:12:51.771 --> 00:12:56.776 
real or fake and the other
network takes a bit of noise

216
00:12:57.000 --> 00:13:00.780 
and sort of expands that
noise until it fills an image

217
00:13:01.781 --> 00:13:05.785 
and it tries to create the best
and most convincing fakes that

218
00:13:05.785 --> 00:13:08.788 
are possible. Just train the
two things against each other

219
00:13:09.789 --> 00:13:12.792 
and what you will have in
the end is a connoisseur of

220
00:13:12.792 --> 00:13:16.796 
fine art who can tell apart a
van gogh from a work of van gogh

221
00:13:16.796 --> 00:13:20.800 
students and you will
have a master forger

222
00:13:20.800 --> 00:13:22.802 
who is able to paint
van goghs even better

223
00:13:23.803 --> 00:13:26.806 
than van goghs own
students. This arms race,

224
00:13:26.806 --> 00:13:30.810 
it's what creates these
possibilities and these

225
00:13:30.810 --> 00:13:33.813 
techniques. Because
what it gains you is

226
00:13:34.814 --> 00:13:37.000 
your little generator that sitting
there, that's the upper one,

227
00:13:38.000 --> 00:13:41.821 
can take a hundred floating
point noise numbers

228
00:13:42.822 --> 00:13:46.826 
and it can turn that into, for
example, a real world interior

229
00:13:46.826 --> 00:13:50.830 
scene. That thing has so
many incredible properties

230
00:13:50.830 --> 00:13:53.833 
because the characteristics of
the rendered scene are linear

231
00:13:54.834 --> 00:13:57.837 
in that space of a hundred
words. You take an image with the

232
00:13:57.837 --> 00:14:01.841 
room in an image without the
room, oh, interpellation between

233
00:14:01.841 --> 00:14:05.845 
them becomes linear. If you've trained
on people you have a person looking

234
00:14:05.845 --> 00:14:08.848 
to the left, you have a
person looking to the right,

235
00:14:09.849 --> 00:14:12.852 
oh. the interpellation in that
feature space becomes linear.

236
00:14:13.853 --> 00:14:15.855 
You can do all kinds
of things that were

237
00:14:15.855 --> 00:14:19.859 
completely impossible before
because suddenly you are given

238
00:14:19.859 --> 00:14:22.862 
a generator with a
linear set of parameters

239
00:14:22.862 --> 00:14:26.866 
that can put glasses on
people's faces or take them

240
00:14:26.866 --> 00:14:29.869 
off, turns them from
the left to the right,

241
00:14:29.869 --> 00:14:33.873 
changes the illumination, or even
takes the patterns that we're

242
00:14:33.873 --> 00:14:37.877 
seeing on one and transfers
them to another, pretty

243
00:14:37.877 --> 00:14:40.880 
much immediately.
Yann LeCun who is

244
00:14:40.880 --> 00:14:43.883 
one of the godfathers of deep
learning and artificial intelligence

245
00:14:44.884 --> 00:14:47.887 
has called these generative
adversarial networks,

246
00:14:48.888 --> 00:14:50.890 
the hottest thing
since sliced bread

247
00:14:51.891 --> 00:14:54.894 
and I'm with Jan on this
one, they are actually

248
00:14:54.894 --> 00:14:58.898 
fairly hot. There is
much interesting stuff,

249
00:14:58.898 --> 00:15:01.901 
we're using that for training
some of our computer vision

250
00:15:01.901 --> 00:15:02.902 
solutions as well.

251
00:15:05.905 --> 00:15:08.908 
Here is more coming to
text which is sort of

252
00:15:08.908 --> 00:15:11.911 
sexy to show but for an enterprise
context actually much more

253
00:15:11.911 --> 00:15:14.914 
interesting because most of
the documents we see tend to be

254
00:15:14.914 --> 00:15:18.918 
text documents, the attention
mechanisms that we talked

255
00:15:18.918 --> 00:15:22.922 
about for computer vision
hold just the same.

256
00:15:23.923 --> 00:15:28.928 
All state of the art text models
now incorporate an attention

257
00:15:28.928 --> 00:15:31.931 
mechanism that will highlight
the words to look for and

258
00:15:31.931 --> 00:15:34.934 
assign them special importance
based on the way humans assign

259
00:15:34.934 --> 00:15:37.937 
that as importance when
they scan through documents.

260
00:15:37.937 --> 00:15:40.940 
you probably do that yourself
when you have your course

261
00:15:40.940 --> 00:15:42.942 
notes or lecture notes
and read through something

262
00:15:42.942 --> 00:15:44.944 
there are the key
words and concepts that

263
00:15:44.944 --> 00:15:47.947 
leap out to you and they're all
the fillers and the introductory

264
00:15:47.947 --> 00:15:51.951 
stuff that you skim over, this
attention mechanism is the same

265
00:15:51.951 --> 00:15:56.956 
thing. The other being element
in text processing right

266
00:15:56.956 --> 00:16:01.961 
now is un-supervised
pre training

267
00:16:01.961 --> 00:16:06.966 
because it turns out that
text has such a rich structure

268
00:16:07.967 --> 00:16:10.970 
that you actually don't need
any labels and you don't need

269
00:16:10.970 --> 00:16:14.974 
any descriptions or anything,
text is so rich in itself

270
00:16:14.974 --> 00:16:18.978 
that the computer will learn by
itself. What you are seeing on the

271
00:16:18.978 --> 00:16:22.982 
right here is a recent,
I think spring 2017

272
00:16:22.982 --> 00:16:26.986 
open AI paper and what
they've done is they've

273
00:16:26.986 --> 00:16:29.989 
taken a super simple network,
it's a recurrent network with

274
00:16:29.989 --> 00:16:34.994 
just 4,000 nodes, they
trained it for a month on

275
00:16:34.994 --> 00:16:38.998 
very simple task. Can you please
predict the next character

276
00:16:39.999 --> 00:16:43.100 
in amazon product reviews? Now
the amazon product reviews is

277
00:16:43.100 --> 00:16:47.100 
like a 50 gigabyte open source
data set that any one of

278
00:16:47.100 --> 00:16:52.101 
you can download, creating a
4,000 node recurrent network

279
00:16:52.101 --> 00:16:55.101 
is something that is probably
500 lines of code in TensorFlow

280
00:16:55.101 --> 00:16:58.101 
if you want to connect it
to the loading and to the

281
00:16:58.101 --> 00:16:59.101 
unloading of this.

282
00:17:02.102 --> 00:17:04.102 
Every single person or every two
single people in this audience

283
00:17:04.102 --> 00:17:06.102 
can probably do this
if they put their mind

284
00:17:07.102 --> 00:17:11.103 
to it for bit. Open AI did
it, they ran it for a month on

285
00:17:11.103 --> 00:17:14.103 
forr state of the
art cards from Nvidia

286
00:17:15.103 --> 00:17:18.103 
and the result is, they have
learned a sentiment function

287
00:17:19.103 --> 00:17:22.104 
because what happens is it tried
to predict the next character

288
00:17:22.104 --> 00:17:24.104 
and the next character
and the next character.

289
00:17:25.104 --> 00:17:26.104 
They are building
two things,

290
00:17:26.104 --> 00:17:29.104 
they are building a

291
00:17:29.104 --> 00:17:34.105 
static view of that space of
valid documents which is encoded

292
00:17:34.105 --> 00:17:38.105 
in the weights of the neuron
model and they are creating a

293
00:17:38.105 --> 00:17:41.106 
dynamic summary of what the
document that they have read

294
00:17:42.106 --> 00:17:45.106 
is about in those
4,000 activations of

295
00:17:45.106 --> 00:17:49.106 
the cells that change over
time. It turned out that one

296
00:17:49.106 --> 00:17:51.107 
of the cells has learned
the sentiment function

297
00:17:52.107 --> 00:17:55.107 
and learned that sentiment
function better than any prior

298
00:17:55.107 --> 00:17:59.107 
existing approach in state of
the art literature and they

299
00:17:59.107 --> 00:18:02.108 
out of the box, beat sentiment
classification on the

300
00:18:02.108 --> 00:18:05.108 
stanford treebank, which is
one of the well recognized

301
00:18:05.108 --> 00:18:09.108 
benchmarks for this, by a
couple percentage points.

302
00:18:10.109 --> 00:18:15.109 
Not 50 PhDs slaving for
ten years to incrementally

303
00:18:15.109 --> 00:18:19.109 
improve stuff here
and there. Two guys,

304
00:18:20.110 --> 00:18:24.110 
five hundred lines of
code, a big data set, and

305
00:18:24.110 --> 00:18:26.110 
they have pulverised the
state of the art with it.

306
00:18:26.110 --> 00:18:31.111 
This is the nature of what ML
research and ML applications

307
00:18:31.111 --> 00:18:35.111 
is like right now and I've
always loved the stuff that

308
00:18:35.111 --> 00:18:37.111 
you can do in500 lines of
code best because that's

309
00:18:37.111 --> 00:18:41.112 
what I can reasonably aspire
to without having to assemble

310
00:18:41.112 --> 00:18:45.112 
large teams around it.
The fact that we can bring

311
00:18:45.112 --> 00:18:48.112 
that into products
is just amazing.
