WEBVTT

1
00:00:00.640 --> 00:00:07.059 
Hello and welcome, In the last video, we briefly introduced the motivation of Edge AI.

2
00:00:08.140 --> 00:00:09.080 
In this video,

3
00:00:09.089 --> 00:00:14.560 
I will talk about how we could do training and influence in the edge AI scenarios.

4
00:00:17.239 --> 00:00:27.050 
Training in the cloud and influence at the edge is a commonly used solution and it can retain the usability and flexibility

5
00:00:27.059 --> 00:00:30.160 
of the cloud centralized AI paradigm.

6
00:00:31.140 --> 00:00:38.549 
The influence is performed on the local devices so there's no need to transfer user data to the cloud.

7
00:00:39.039 --> 00:00:42.960 
That's better data privacy than cloud centralized solution.

8
00:00:43.939 --> 00:00:49.960 
This is the easiest way to adapt the current models and resources to edge AI scenarios.

9
00:00:50.439 --> 00:00:55.759 
And it can also task advantage of the cloud resources.

10
00:00:55.770 --> 00:01:03.960 
For example, when data scientist needs to train a new machine learning model or use new algorithms, the cloud mainly solve

11
00:01:03.960 --> 00:01:06.239 
the problem,

12
00:01:06.239 --> 00:01:09.150 
but the shortcomings are also apparent.

13
00:01:09.640 --> 00:01:19.840 
There is no general model that can serve all the edge devices at the same time and we have to perform specific model optimization

14
00:01:19.849 --> 00:01:24.159 
for each different devices in particular.

15
00:01:24.540 --> 00:01:33.750 
Edge devices often require the model to be optimized for resource constraints due to the capacity limitations.

16
00:01:33.760 --> 00:01:43.379 
The device usually does not retain historical data locally and it is difficult to share with other devices, which is the

17
00:01:43.379 --> 00:01:45.159 
data silos problem.

18
00:01:45.840 --> 00:01:53.359 
On the other hand, we cannot improve the cloud side model training by fully utilizing the client side data.

19
00:01:54.640 --> 00:02:04.560 
A possible solution for this is to automatically discover the complex examples and unknown tasks and classes and forward

20
00:02:04.560 --> 00:02:12.439 
this data to the cloud for incremental processing or incremental training.

21
00:02:12.439 --> 00:02:19.360 
This category refers to training and influence on the corresponding edge device based on the local data.

22
00:02:20.039 --> 00:02:29.849 
This method uses an embedded or specific operating system for edge device such as a development environment

23
00:02:31.439 --> 00:02:42.439 
to fully achieve this goal, developer typically rely on the low code platforms, digital trains viral models to update the

24
00:02:42.439 --> 00:02:52.090 
generated data regularly and local development platform can quickly create applications without coding or just a tiny amount

25
00:02:52.090 --> 00:02:52.780 
of code.

26
00:02:53.840 --> 00:03:00.280 
The advantage is that this sort of method can better utilize the local data for model training.

27
00:03:00.289 --> 00:03:09.860 
The privacy issue can be solved since the training and inference use the same data resource, it can also solve the non IID

28
00:03:09.860 --> 00:03:09.969 


29
00:03:09.969 --> 00:03:10.180 


30
00:03:10.180 --> 00:03:12.460 
problem to some extent.

31
00:03:13.340 --> 00:03:21.650 
However, training AI models on the edge devices are rarely excluded due to the limited capacity at the edge side.

32
00:03:22.139 --> 00:03:32.969 
Although the current intelligent edge software ecosystem has begun to develop, only very limited support can be found in this

33
00:03:32.969 --> 00:03:43.759 
case and maturity or really or related services require more attention, investment and support for the age computing ecosystem.

34
00:03:45.039 --> 00:03:53.879 
In addition, machine learning performed entirely on edge cannot support long term knowledge, persistence and cross device

35
00:03:53.879 --> 00:03:55.259 
knowledge integration.

36
00:03:55.840 --> 00:04:04.330 
In other words, the method cannot use the history and knowledge of other devices as part of the training and incremental

37
00:04:04.330 --> 00:04:05.460 
updating the model.

38
00:04:06.439 --> 00:04:16.550 
This shortcoming is especially significant when an edge node is newly created and doesn't have enough training examples and

39
00:04:16.550 --> 00:04:24.050 
doesn't have a history of knowledge leading to low accuracy or even failure to converge.

40
00:04:27.240 --> 00:04:28.509 
The third method,

41
00:04:28.519 --> 00:04:33.420 
the most promising paradigm in the future is collaborative training and inference.

42
00:04:34.029 --> 00:04:42.339 
It means that the training or inference of machine learning task is completed through a synergy of cloud and edge

43
00:04:42.339 --> 00:04:52.629 
nodes, the edge clouds energy method is still relatively unfamiliar to most people, but it can balance the latency and modeling

44
00:04:52.629 --> 00:04:55.560 
accuracy in the fine granite manner.

45
00:04:56.339 --> 00:05:05.089 
In this method especially they compose the process of machine learning training or influence into multiple modules so that the

46
00:05:05.089 --> 00:05:09.589 
computing tasks of each module can be scheduled to the edge.

47
00:05:09.589 --> 00:05:12.660 
Notes all the clouds for execution.

48
00:05:13.540 --> 00:05:22.860 
This is no need to transfer all the original data from the edges to the cloud and it can achieve avoid data transfer between

49
00:05:22.860 --> 00:05:24.060 
different devices.

50
00:05:24.740 --> 00:05:33.500 
A commonly used method is to transfer the features or training models from the edge device to the cloud and perform feature

51
00:05:33.500 --> 00:05:42.730 
model aggregation in the cloud or large scale training at the cloud node since no original data is directly transmitted.

52
00:05:42.740 --> 00:05:47.250 
This method also more friendly to the privacy protection.

53
00:05:47.839 --> 00:05:57.240 
However, the collaborative training and influence methods involve many aspects of AI systems and the technical paths and

54
00:05:57.240 --> 00:06:00.050 
the development circle are complicated.

55
00:06:01.639 --> 00:06:07.060 
There is still a gap between current progress and technology maturity.

56
00:06:07.839 --> 00:06:13.800 
In the meantime, a the scope of applications lacks a precise definition.

57
00:06:13.810 --> 00:06:18.160 
In addition, there is still no game changer applications.

58
00:06:20.740 --> 00:06:24.899 
I want to explain our three methods further using an example.

59
00:06:26.040 --> 00:06:31.649 
Let's take the radiographic diagnosis example.

60
00:06:32.139 --> 00:06:43.160 
And so the large specialized hospital may have a relatively massive amount of diagnostic diagnostic data and powerful computing

61
00:06:43.160 --> 00:06:46.560 
resources such as the private cloud.

62
00:06:47.139 --> 00:06:57.139 
In addition, we also have many small clinics where the radiologists work for their daily diagnosis and there they all have

63
00:06:57.139 --> 00:07:05.459 
relatively small amount of patient data, although the database are the same, the data can be completely different.

64
00:07:06.040 --> 00:07:13.060 
Therefore we use different depths of color and size to represent the heterogeneous city of the data.

65
00:07:14.829 --> 00:07:23.509 
Under the paradigm of training at the cloud and influence at the edge, the AI model is only traded on the private cloud of

66
00:07:23.670 --> 00:07:28.160 
the large hospital and only access database there.

67
00:07:28.839 --> 00:07:32.079 
The training model is sent to the small clinic.

68
00:07:32.089 --> 00:07:41.350 
The edge note in our case where the local computing resources are used to run the model to assist the diagnosis here, I use

69
00:07:41.350 --> 00:07:48.259 
the arrows of different colors to indicate the transmission of data or models.

70
00:07:49.740 --> 00:07:53.100 
We see that there is no data flow between various nodes.

71
00:07:53.110 --> 00:08:00.360 
We regularly do not allow the diagnosis data to flow out of the clinic where it is located.

72
00:08:01.040 --> 00:08:08.060 
We can see that in this way we cannot use the local data of the edge node to optimize the AI model.

73
00:08:08.639 --> 00:08:16.709 
There is likely an out of distribution problem with the training data on the cloud and the local data on the edge.

74
00:08:16.709 --> 00:08:20.860 
Notes, the accuracy of model may not be guaranteed.

75
00:08:22.740 --> 00:08:31.649 
We know that the field of healthcare and medical imaging is very sensitive to privacy protection and diagnostic position,

76
00:08:32.740 --> 00:08:35.299 
the requirements are very high.

77
00:08:35.309 --> 00:08:44.350 
But as we analyze before under this situation, this traditional paradigm may not be robust enough for this use case.

78
00:08:47.240 --> 00:08:50.200 
How about training and influence happening on the edge?

79
00:08:50.559 --> 00:08:54.539 
Let's take a look at the same example at this time.

80
00:08:54.700 --> 00:09:01.450 
Each node including private cloud and edge Notes only use local data to train the model.

81
00:09:02.139 --> 00:09:04.240 
This can indeed solve the non IID

82
00:09:04.240 --> 00:09:04.419 


83
00:09:04.419 --> 00:09:05.049 
problem.

84
00:09:05.539 --> 00:09:07.899 
You can also protect the data privacy.

85
00:09:07.909 --> 00:09:13.960 
However, a large hospital can enjoy the cloud computing power and software support.

86
00:09:14.340 --> 00:09:20.179 
In addition, it has the largest amount of data to have the relatively most security model.

87
00:09:20.840 --> 00:09:23.490 
As for the small clinics on the edge

88
00:09:23.490 --> 00:09:31.950 
node because the quality and quantity of the data are uneven, the training models also differ quite a lot.

89
00:09:32.340 --> 00:09:41.980 
In addition, the hardware facilities of the small clinics are relatively weak and it is challenging to have the ability to

90
00:09:41.980 --> 00:09:43.559 
train the complex models.

91
00:09:44.340 --> 00:09:53.330 
Therefore, we can see that the difference between edge nodes and ground nodes are enlarged and the distribution imbalances

92
00:09:53.340 --> 00:09:55.740 
get magnified.

93
00:09:55.750 --> 00:09:58.970 
It is difficult for small organizations to enjoy

94
00:09:58.970 --> 00:10:00.750 
technical fairness.

95
00:10:01.639 --> 00:10:10.820 
On the other hand, because there is no data flowing, no model transfer, The data resources of each knows entirely isolated,

96
00:10:10.830 --> 00:10:14.909 
therefore there is still the serious data silos problem.

97
00:10:14.919 --> 00:10:19.000 
We know that machine learning is a data driven technology.

98
00:10:19.009 --> 00:10:28.990 
If we cannot continuously process and optimize our data set, we will miss the best opportunity to improve the model performance

99
00:10:31.840 --> 00:10:41.190 
Now let's look at how the collaborative training method is applied to this example and effectively improve the models capacity

100
00:10:41.200 --> 00:10:42.460 
on all nodes.

101
00:10:44.629 --> 00:10:53.830 
First we will train the model on the local data on all know similar to the previous method due to the uneven data distribution

102
00:10:54.120 --> 00:11:01.659 
and the different computing power on each node the accuracy and the complexity of the model are also different.

103
00:11:02.340 --> 00:11:12.129 
The statistical information that a machine learning model learns from the data is encoded as it weighs so and such rates

104
00:11:12.139 --> 00:11:15.460 
can also be considered as the kind of knowledge.

105
00:11:16.240 --> 00:11:19.399 
Then after each note model is trained.

106
00:11:19.960 --> 00:11:23.759 
We can further integrate their knowledge into a certain way.

107
00:11:24.940 --> 00:11:34.710 
One possible method is to transmit the trained ways of each nodes to the cloud node where they are fused with the ways of

108
00:11:34.710 --> 00:11:44.580 
the clouds and other nodes in some intelligent way we can call it knowledge aggregation, the aggregated ways and then return

109
00:11:44.580 --> 00:11:48.649 
to each edge node and use for influence on the local data.

110
00:11:49.139 --> 00:11:53.200 
In this way we consider the local data of all nodes.

111
00:11:53.210 --> 00:12:02.590 
When we train the model, we can generate a better model through knowledge aggregation because it aggregators more knowledge

112
00:12:02.600 --> 00:12:04.039 
from all the nodes.

113
00:12:04.059 --> 00:12:09.960 
This better model is fairly distributed to each node in the network for influence.

114
00:12:10.840 --> 00:12:18.919 
Therefore we see that with such a collaborative training method, the data on all nodes can be fully utilized.

115
00:12:18.929 --> 00:12:24.960 
It also allow the traded model to be used by all the members more fairly.

116
00:12:25.929 --> 00:12:33.460 
It is worth noting that we do not need to transmit any original user data in this example.

117
00:12:33.840 --> 00:12:41.149 
We believe the models ways is not easy to reverse engineering to expose the original data.

118
00:12:41.159 --> 00:12:50.659 
Of course we can use encryption technology before weight transmission if we want stricter privacy protection measures

119
00:12:50.940 --> 00:12:55.049 
and many encryption technologies can be applied at present.

120
00:12:55.139 --> 00:13:00.059 
It is out of scope of this video that I won't go into more detail.

121
00:13:03.340 --> 00:13:08.259 
In addition to collaborative training, you can also perform collaborative influence.

122
00:13:09.440 --> 00:13:19.970 
For example, the result confidence is very low with the model on our edge node is processing some local data and at this time

123
00:13:19.980 --> 00:13:28.759 
we can send the intermediate future output of the edge side model to the high precision cloud model to make further predictions

124
00:13:29.139 --> 00:13:32.259 
and we may obtain more accurate results.

125
00:13:33.139 --> 00:13:37.179 
Then the cloud model sends the results back to the edge nodes.

126
00:13:37.190 --> 00:13:47.169 
In this way it can be ensured that the edge node can also obtain good recognition accuracy in case when the agent side model

127
00:13:47.169 --> 00:13:55.250 
is confident enough for its prediction results, there's no need to access the cloud which ensures a high processing speed.

128
00:13:55.259 --> 00:14:05.639 
On the other hand here, we can also use encryption technology to protect the transformation process.

129
00:14:05.639 --> 00:14:14.059 
To summarize, this video introduced our training and inference methods in the edge AI scenarios.

130
00:14:14.539 --> 00:14:21.759 
After that we combined an example of clinical data processing to analyze these three methods.

131
00:14:23.740 --> 00:14:25.259 
Thank you for watching the video.
