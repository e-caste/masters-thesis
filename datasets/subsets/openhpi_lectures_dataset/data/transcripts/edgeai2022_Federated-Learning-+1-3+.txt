WEBVTT

1
00:00:01.080 --> 00:00:05.969 
In the previous video, we introduced the method of collaborative training.

2
00:00:05.980 --> 00:00:13.859 
In fact this is a method of Federated Learning in this video, we will further learn the details of this method.

3
00:00:16.039 --> 00:00:24.350 
First of all, Federated Learning has been a popular topic recently, but why do we need this new concept?

4
00:00:25.070 --> 00:00:29.760 
How is it different from the traditional distributed machine learning?

5
00:00:29.769 --> 00:00:32.750 
After several years of development?

6
00:00:32.770 --> 00:00:35.369 
Many people are doing jobs in this field.

7
00:00:35.380 --> 00:00:40.210 
What are the main basic paradigm of Federated Learning at present?

8
00:00:40.270 --> 00:00:43.039 
What is the biggest bottleneck currently?

9
00:00:43.799 --> 00:00:50.740 
What are the key directions that academic community is paying attention to?

10
00:00:50.750 --> 00:00:54.649 
And in this part we will mainly answer those questions.

11
00:00:57.240 --> 00:01:07.739 
Mobile phones, variable devices and autonomous vehicles are just a few of the modern distributed network generating a means

12
00:01:07.739 --> 00:01:18.829 
of data each day. Due to the growing computational power of these devices, couple of the concern about transmitting private

13
00:01:18.829 --> 00:01:19.769 
information

14
00:01:19.780 --> 00:01:27.489 
it is increasingly attractive to store data locally and push network computation to the edge devices.

15
00:01:27.500 --> 00:01:33.109 
Federated Learning has emerged as a training paradigm in such settings.

16
00:01:34.060 --> 00:01:43.930 
Except for a few giant companies, most companies have the problem of small amount of data and poor data quality, which is

17
00:01:43.930 --> 00:01:48.569 
not enough to support the realization of AI technology.

18
00:01:48.579 --> 00:01:59.920 
On the other hand, the regular environment is gradually strengthening data protection and the relevant policies have

19
00:01:59.920 --> 00:02:08.639 
been introduced one after another, for example, the EU introduced the general data protection regulation GDPR,

20
00:02:08.639 --> 00:02:08.819 


21
00:02:08.819 --> 00:02:09.000 


22
00:02:09.000 --> 00:02:09.490 


23
00:02:09.500 --> 00:02:19.629 
and the three flow of data and there is a promise of security and agreement has become a general trend. As mentioned before

24
00:02:19.639 --> 00:02:23.740 
there is also the problem of data silos. Together

25
00:02:23.740 --> 00:02:28.550 
these factors have produced a need for the new machine learning paradigm.

26
00:02:28.560 --> 00:02:35.479 
Therefore shared learning of Federated Learning has been proposed

27
00:02:35.479 --> 00:02:40.229 
an example that we have already learned in the previous video here.

28
00:02:40.229 --> 00:02:49.780 
The problem is that several hospitals and clinics wants to jointly build a trust robust machine learning model for existing

29
00:02:49.780 --> 00:02:59.509 
the diagnosis, a possible solution would be to use centralized learning just to aggregate the data into a central node and

30
00:02:59.509 --> 00:03:00.659 
train the model there.

31
00:03:01.409 --> 00:03:13.129 
This kind of approach has been developed very well but in practice big challenges that our policies forbid transferring

32
00:03:13.139 --> 00:03:20.219 
patients data to others so we cannot simply collect the data and do the training in the central place.

33
00:03:20.849 --> 00:03:30.199 
The same situation also appears in other business areas such as health care, insurance, financial finance, banking and so

34
00:03:30.199 --> 00:03:30.710 
on.

35
00:03:32.419 --> 00:03:36.080 
So the company does not allow the transfer of user data.

36
00:03:37.680 --> 00:03:44.539 
Let me show you a motivating example from the Google.

37
00:03:44.550 --> 00:03:54.120 
In 2016, Google first proposed the Federated learning concept to solve the local model updating problem on Android phones.

38
00:03:54.789 --> 00:03:59.020 
Normally google changed their machine learning model on google cloud.

39
00:03:59.330 --> 00:04:03.129 
The power of the mobile devices increased a lot.

40
00:04:04.169 --> 00:04:10.139 
It allows us to run machine learning directly on the smartphone,

41
00:04:10.139 --> 00:04:19.519 
through this, the phone will feel more personal and we could have a better data plan and a battery life. Google introduced

42
00:04:19.519 --> 00:04:28.589 
federated Learning to allow a single phone to learn from other phones while keeping data private, which means that the data

43
00:04:28.600 --> 00:04:31.680 
on your phone will not be transferred anywhere.

44
00:04:31.689 --> 00:04:37.060 
First the phone will download a general machine learning model

45
00:04:37.069 --> 00:04:46.149 
and if you use the data on the phone to train the model. Training only happens when charging dry at night and after it

46
00:04:46.149 --> 00:04:55.240 
computes a summary of the changes, thousands of summary from different phones will be encrypted and sent to the cloud to

47
00:04:55.240 --> 00:04:58.620 
create a global improvement of the model.

48
00:04:59.540 --> 00:05:06.019 
The improved model will finally return to the android phones that make it work better for everybody.

49
00:05:06.550 --> 00:05:15.759 
This work of google caused a great reputation at that time and opened up a concept of Federated Learning.

50
00:05:18.399 --> 00:05:27.410 
It sounds like Federated Learning is very useful, but but those familiar with the traditional distributed machine learning

51
00:05:27.410 --> 00:05:30.910 
may ask, what is the difference between them?

52
00:05:30.920 --> 00:05:40.750 
A common method of distributed machine learning is using a central server to aggregate and update model ways and multiple

53
00:05:40.750 --> 00:05:49.829 
computing nodes to train the model in parallel. There are three layers in this architecture, a central server for wage saving

54
00:05:49.829 --> 00:05:56.930 
and update, a middle layer for message passing and multiple age nodes for gradient computation.

55
00:05:58.029 --> 00:06:05.980 
Each edge node can communicate with the central server and we can use this system to train the neural network model using

56
00:06:05.990 --> 00:06:09.980 
stochastic gradient descent. During the training,

57
00:06:09.990 --> 00:06:11.300 
the central server view

58
00:06:11.300 --> 00:06:20.480 
first send the initial ways to the workers for this step, communication complexity is the number of the weights parameters

59
00:06:21.259 --> 00:06:27.680 
then the work review compute the gradients using obtained parameters and the local data.

60
00:06:27.689 --> 00:06:30.350 
The step doesn't need message passing.

61
00:06:31.060 --> 00:06:39.600 
Subsequently the workers will send the gradients to the server which needs the communication since the dimension of gradients

62
00:06:39.600 --> 00:06:43.680 
tensors equals the dimension of weight parameters,

63
00:06:43.689 --> 00:06:53.279 
the communication complexity is also the same. Once the server receives the workers gradients and if you update the ways using

64
00:06:53.279 --> 00:06:53.459 
SGD

65
00:06:53.459 --> 00:06:53.639 


66
00:06:53.639 --> 00:07:05.629 
this completes the iteration. We can figure out that we need to make a message passing

67
00:07:05.629 --> 00:07:11.209 
times for each iteration and the main computation occurs on the edge

68
00:07:11.209 --> 00:07:16.100 
workers, the central server only has more amount of calculations.

69
00:07:16.110 --> 00:07:24.230 
It only calculates the difference between ways and gradients, but to completely train the machine learning model, we need

70
00:07:24.230 --> 00:07:25.769 
a lot of iterations.

71
00:07:25.779 --> 00:07:31.379 
It means we need a lot of communications which creates a huge overhead.

72
00:07:31.860 --> 00:07:39.269 
In our previous example, we can find that the biggest challenge is the privacy protection of local user data.

73
00:07:39.670 --> 00:07:44.720 
We really can now gather all of the data in the central place for training.

74
00:07:44.730 --> 00:07:52.699 
However, through understanding distributed machine learning, we found that there is no need to transmit any local data to

75
00:07:52.699 --> 00:07:53.740 
the central node.

76
00:07:53.750 --> 00:07:58.060 
It seems that privacy problem has been while solved.

77
00:07:58.069 --> 00:07:58.680 
Right?

78
00:07:58.689 --> 00:08:05.220 
So why do we still need the Federated Learning, Why do we need this new concept?

79
00:08:07.170 --> 00:08:09.709 
Thus what is Federated learning?

80
00:08:09.720 --> 00:08:13.910 
Federated learning is a kind of distributed machine learning.

81
00:08:13.920 --> 00:08:15.199 
That's indeed.

82
00:08:15.209 --> 00:08:18.839 
And there's no essential difference between the two.

83
00:08:19.480 --> 00:08:23.259 
But federal learning has some more challenging definitions.

84
00:08:23.889 --> 00:08:32.429 
So now let's look at the special points of Federated Learning, first through the term Federated in the in its name.

85
00:08:32.440 --> 00:08:42.100 
We can imagine that each worker node is equivalent to the federation members in this scenario, the definition of federation

86
00:08:42.110 --> 00:08:46.059 
gives each member a high degree of autonomy.

87
00:08:46.070 --> 00:08:52.559 
Therefore the independence of each node is an essential feature in Federated Learning.

88
00:08:53.419 --> 00:09:00.659 
Each node can independently decide when to join or withdraw from the Federated Learning process.

89
00:09:00.669 --> 00:09:09.519 
This is quite different from the traditional distributed learning, formal distributed learning is mostly a top down paradigm

90
00:09:09.529 --> 00:09:14.289 
and each child node has no autonomy.

91
00:09:14.289 --> 00:09:23.519 
Secondly, the scenario of Federated Learning to solve the problem, put more emphasize on the privacy protection and security

92
00:09:23.519 --> 00:09:27.210 
of user data. In the Federated Learning scenario,

93
00:09:27.220 --> 00:09:33.879 
each worker node comes from various types of users such as different types of mobile phones.

94
00:09:33.889 --> 00:09:41.879 
Therefore each node is relatively unstable in terms of connectivity band width capacity etcetera.

95
00:09:42.409 --> 00:09:45.490 
In addition the data has the non IID

96
00:09:45.490 --> 00:09:45.679 


97
00:09:45.679 --> 00:09:51.370 
problem because the mobile phone data of different users is usually personalized.

98
00:09:51.379 --> 00:09:54.370 
We don't have the same data distribution.

99
00:09:54.379 --> 00:10:03.490 
Federated Learning has higher communication cost because of the heterogeneity of the worker devices such as mobile phone,

100
00:10:03.500 --> 00:10:04.950 
edge cameras,

101
00:10:04.960 --> 00:10:12.009 
IOT devices usually the communication cost is much higher than the computational cost.

102
00:10:12.269 --> 00:10:18.769 
The highly complex communication situation significantly increases the training difficulty.

103
00:10:19.500 --> 00:10:29.360 
You can imagine that an iPhone 13 device can do 100 training iterations and they're the same duration, while an iPhone 6

104
00:10:29.370 --> 00:10:31.710 
can only perform five iterations.

105
00:10:32.750 --> 00:10:38.389 
Synchronizing the training among those heterogeneous devices is very challenging.

106
00:10:39.320 --> 00:10:42.399 
The federal learning laws is unbalanced.

107
00:10:42.409 --> 00:10:50.669 
Traditional distributed machine learning requires load balancing but Federated Learning cannot do that.

108
00:10:50.679 --> 00:10:58.070 
For example, the amount of and distribution of data for each mobile phone users are different.

109
00:10:58.080 --> 00:11:07.080 
Users who love to watch video will have more video data and users who love to take a picture will have more picture data

110
00:11:07.080 --> 00:11:08.309 
on their cell phone.

111
00:11:08.320 --> 00:11:14.629 
The difference in their amount and type may make our model training difficult.

112
00:11:14.830 --> 00:11:26.360 
If we adopt a user average approach, the model will be more biased towards users with high with more data and if the sample

113
00:11:26.360 --> 00:11:30.820 
averaging method is used, the model's performance may work worsen.

114
00:11:32.539 --> 00:11:36.200 
We see in this numerous challenges.

115
00:11:36.210 --> 00:11:44.159 
One of the current research direction is how to reduce the number of communications between nodes in the following video

116
00:11:44.169 --> 00:11:52.980 
I will continue to introduce Google's example to see how it can do Federated Learning on android phones and make the communication

117
00:11:52.990 --> 00:11:54.169 
more efficient.

118
00:11:56.909 --> 00:11:58.230 
Thank you for your watching.
