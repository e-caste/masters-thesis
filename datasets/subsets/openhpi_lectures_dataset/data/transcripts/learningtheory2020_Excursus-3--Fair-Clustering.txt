WEBVTT

1
00:00:00.650 --> 00:00:04.540 
Hello. Welcome to this excursion on
Fair Clustering. You've probably

2
00:00:04.540 --> 00:00:08.100 
chosen to watch this because of
the funny title. So let's dive

3
00:00:08.100 --> 00:00:10.080 
right in and see what
this is about.

4
00:00:10.870 --> 00:00:14.390 
I hope you are not disappointed
to see that the fair doesn't

5
00:00:14.390 --> 00:00:19.020 
stand for carnival or anything but for
fairness. So rather serious topic.

6
00:00:19.360 --> 00:00:23.430 
So what is fairness? Well this is
in general a very tough question

7
00:00:23.430 --> 00:00:27.890 
and we don't want to answer this
here but rather look at why

8
00:00:27.890 --> 00:00:30.950 
does this come up when we
discuss learning from data.

9
00:00:31.690 --> 00:00:36.030 
Well over the past couple of years we
have gathered so much personal data

10
00:00:36.730 --> 00:00:42.050 
and it is very tempting to now base decisions
on it, especially difficult decisions

11
00:00:42.410 --> 00:00:46.520 
like who gets a bank loan, who gets
a job offer, medical treatment

12
00:00:46.520 --> 00:00:47.990 
or insurance and so on.

13
00:00:49.300 --> 00:00:53.440 
And we would like to have or
make these decisions without

14
00:00:53.440 --> 00:00:57.090 
taking into account attributes
like race, gender or religion

15
00:00:57.250 --> 00:00:59.350 
to say that our decision
is a fair one.

16
00:01:00.460 --> 00:01:05.870 
And well our computer is not
biased so it's tempting to think

17
00:01:05.870 --> 00:01:09.930 
that if we feed the data to the computer
it will make a fair decision.

18
00:01:11.070 --> 00:01:14.750 
But simply deleting the attributes
to say ok the computer doesn't

19
00:01:14.750 --> 00:01:18.970 
know race, gender or religion so it
cannot base its decisions on it.

20
00:01:19.300 --> 00:01:25.510 
This is not enough because if our
algorithm learns from our data

21
00:01:25.870 --> 00:01:29.940 
then it also learns our bias even if
we don't include this attributes.

22
00:01:30.300 --> 00:01:34.010 
So just like when you delete a
name from personal data this

23
00:01:34.010 --> 00:01:38.220 
doesn't make your data set
anonymous you can still find out

24
00:01:38.300 --> 00:01:42.670 
which record belongs to
which person and just like

25
00:01:43.180 --> 00:01:47.120 
that is not enough to preserve
privacy, you cannot simply delete

26
00:01:47.320 --> 00:01:51.340 
these sensitive attributes like
race, gender or religion to

27
00:01:51.350 --> 00:01:53.670 
force an algorithm to
make fair decisions.

28
00:01:54.540 --> 00:01:58.850 
Let's look at the second part of our title
clustering. So what is clustering?

29
00:01:59.550 --> 00:02:03.640 
Very vaguely speaking you have
given a base set of objects

30
00:02:03.860 --> 00:02:06.600 
and you want to partition
them into sets that

31
00:02:07.070 --> 00:02:09.480 
represent their
similarity,

32
00:02:10.260 --> 00:02:13.880 
meaning that similar objects land
in the same set and dissimilar

33
00:02:13.880 --> 00:02:15.540 
objects lie in
different sets.

34
00:02:16.760 --> 00:02:20.980 
Looking at such an example here,
these points now are our objects

35
00:02:20.980 --> 00:02:22.280 
that we want to cluster

36
00:02:22.970 --> 00:02:27.520 
and say the distance that you
put read of a rule or a few

37
00:02:27.520 --> 00:02:32.350 
places between these points mimics
the dissimilarity of two points.

38
00:02:32.350 --> 00:02:36.180 
So let's say here's a distance of three
and here is a distance of five.

39
00:02:36.850 --> 00:02:41.900 
Then we would if we had to cluster
these points into two clusters

40
00:02:41.900 --> 00:02:43.960 
you would get a picture
that looks like this.

41
00:02:45.270 --> 00:02:49.520 
So more formally speaking there
are many types of objectives

42
00:02:49.520 --> 00:02:53.250 
that you would want from such a clustering,
many types of clustering algorithms

43
00:02:53.650 --> 00:02:56.400 
and I want to give you here
a couple of examples

44
00:02:56.820 --> 00:03:00.800 
like centroid based clusterings
where you say I have a given

45
00:03:00.800 --> 00:03:03.400 
distance just like we have
in our picture here

46
00:03:03.850 --> 00:03:08.740 
between the points and we
want to find a partitioning

47
00:03:09.560 --> 00:03:15.800 
and we choose for every cluster one
representative to minimize functions like this.

48
00:03:16.840 --> 00:03:19.730 
Looking at this ugly expression
this really just means that

49
00:03:19.730 --> 00:03:24.200 
I choose one representative for each
cluster and then I sum up the distances

50
00:03:24.480 --> 00:03:27.830 
of each point to its
cluster representative.

51
00:03:29.080 --> 00:03:34.430 
And this is the famous k-median
problem. Another option is to say

52
00:03:34.630 --> 00:03:38.740 
I want to minimize this weird
function which really just says

53
00:03:38.980 --> 00:03:40.730 
I mean he makes
the radius of

54
00:03:41.380 --> 00:03:45.210 
the maximum radius or clusters.
So this thick line there

55
00:03:45.210 --> 00:03:50.500 
in the cluster with center c2 is the
maximum radius of this clustering.

56
00:03:51.500 --> 00:03:56.130 
There are also other
clustering algorithms

57
00:03:57.140 --> 00:04:02.200 
and correlation clustering is a very different
type to model the quality of clusters,

58
00:04:02.610 --> 00:04:06.710 
so there we have given not just the
distance function but we have

59
00:04:07.060 --> 00:04:12.090 
a function that says for a pair of points
if they are similar or dissimilar

60
00:04:12.280 --> 00:04:15.540 
or if we have no
information about them.

61
00:04:16.280 --> 00:04:19.410 
If we have such information we
want a clustering that best

62
00:04:19.410 --> 00:04:23.570 
represents similarity here, what does
this mean? I want to partition

63
00:04:23.800 --> 00:04:25.960 
such that the following
thing is minimized.

64
00:04:26.390 --> 00:04:31.680 
So if I place two objects in the
same set although they are

65
00:04:31.680 --> 00:04:36.840 
dissimilar I don't want that, so this would
be if I place back my clustering into

66
00:04:37.060 --> 00:04:41.480 
the two sets blue and red here,
this would be the two edges

67
00:04:41.480 --> 00:04:46.660 
in red that are thick now. So I don't
want this and also I don't want

68
00:04:46.870 --> 00:04:53.020 
to cut similarities, so I also add the
number of green edges that I cut

69
00:04:53.220 --> 00:04:59.170 
so these two. So this would be a good still
a good solution for correlation clustering

70
00:04:59.300 --> 00:05:00.880 
and there are
many many more.

71
00:05:01.880 --> 00:05:06.290 
But regardless of how you model
your specific clustering

72
00:05:06.800 --> 00:05:12.060 
fairness does look the same essentially
for clustering. So now let's

73
00:05:12.300 --> 00:05:16.350 
put our two words here together and
see what is fairness for clustering.

74
00:05:17.950 --> 00:05:21.600 
Well we would like to have if we
have a sensitive attribute like

75
00:05:21.710 --> 00:05:25.580 
race or gender in our data we don't
want to cluster with respect to that

76
00:05:25.790 --> 00:05:30.740 
but we rather would like to have
a solution such that in each

77
00:05:30.740 --> 00:05:34.230 
cluster the distribution of the
sensitive attribute is the same.

78
00:05:34.530 --> 00:05:37.100 
So it did not play a
role in our decision

79
00:05:37.880 --> 00:05:42.450 
and this means that essentially every
cluster has the same distribution of

80
00:05:42.550 --> 00:05:47.610 
sensitive attribute as the input set
does, if you do the math and our

81
00:05:47.790 --> 00:05:50.960 
clustering has to be a partition
this is really your only choice.

82
00:05:51.370 --> 00:05:55.390 
But let's not do the math but
look at another example.

83
00:05:56.590 --> 00:06:00.880 
Say these points are again the
points that we want to cluster

84
00:06:00.880 --> 00:06:05.080 
and they now represent persons
and the colour of these dots

85
00:06:05.080 --> 00:06:08.040 
represents the sensitive
attribute and

86
00:06:08.480 --> 00:06:11.870 
it can be yellow or blue and
you can have even more colors

87
00:06:11.870 --> 00:06:15.880 
to represent something like race
that has more than two values but

88
00:06:16.090 --> 00:06:19.240 
for the sake of simplicity of
the example bear with me here

89
00:06:19.240 --> 00:06:20.280 
with the two colours.

90
00:06:21.030 --> 00:06:22.730 
So if we were to
cluster these

91
00:06:23.360 --> 00:06:29.260 
people into two sets then this would
certainly not be a fair clustering.

92
00:06:29.410 --> 00:06:35.020 
Because here we have essentially
separated yellow from blue so this is

93
00:06:35.160 --> 00:06:37.240 
basically
clustering that

94
00:06:37.940 --> 00:06:43.630 
is actually based on the sensitive
attribute that we are we should exclude.

95
00:06:45.030 --> 00:06:48.410 
So the better choice for clustering
with respect to fairness

96
00:06:48.410 --> 00:06:53.620 
would be to cluster like this. So
here we have on both in both

97
00:06:53.620 --> 00:06:57.360 
clusters the same
ratio of blue

98
00:06:57.920 --> 00:07:00.490 
people as we have of yellow
people if you will.

99
00:07:00.930 --> 00:07:03.990 
So formally we would like the
following for fairness.

100
00:07:04.960 --> 00:07:10.740 
This refined the partition such that the
ratio of yellow points in our cluster

101
00:07:10.950 --> 00:07:15.920 
is the same as the ratio in the original
set. So this is if you will the

102
00:07:16.040 --> 00:07:21.920 
percentage of the attribute
is the same for all

103
00:07:22.800 --> 00:07:25.650 
for all clusters. So we want this
not just for the cluster as one

104
00:07:25.820 --> 00:07:29.160 
but we want this for all clusters
and also for all colors.

105
00:07:29.790 --> 00:07:34.650 
So this is one option to model fairness
that has become quite popular.

106
00:07:35.180 --> 00:07:39.180 
Fairness for clustering that is
and now this means well this

107
00:07:39.180 --> 00:07:44.560 
is a new condition we want
on our clustering, do

108
00:07:44.560 --> 00:07:49.360 
we now have to start over with all the
decades of research for clustering?

109
00:07:49.920 --> 00:07:55.160 
Not really. Let's look at how
can we find a fair clustering.

110
00:07:55.590 --> 00:08:00.030 
So there are essentially three options
to tackle now this new problem.

111
00:08:00.620 --> 00:08:05.300 
The first one would be post-processing.
So in the first step

112
00:08:05.300 --> 00:08:08.890 
we cluster like we usually do
without fairness constraints and

113
00:08:09.560 --> 00:08:13.520 
then look at what we got here.
So this clustering that

114
00:08:13.520 --> 00:08:16.760 
you see here now that's not fair
because again where there's

115
00:08:16.760 --> 00:08:21.340 
one class that's only yellow and the blue
guys are distributed in the others

116
00:08:22.470 --> 00:08:27.460 
and so afterwards we do post-processing
and try to make this clustering fair

117
00:08:27.600 --> 00:08:29.810 
by doing as little
swaps as possible.

118
00:08:30.660 --> 00:08:35.960 
So this is one option to recycle
basically our good clustering

119
00:08:35.960 --> 00:08:37.290 
algorithms that we head

120
00:08:37.950 --> 00:08:41.150 
and have developed
over centuries and

121
00:08:41.740 --> 00:08:45.910 
we just make small changes to
make the clustering fair.

122
00:08:46.540 --> 00:08:51.000 
Another option would be preprocessing.
This means we have our

123
00:08:51.000 --> 00:08:55.260 
set of points with our sensitive
attribute and we first build

124
00:08:55.260 --> 00:08:59.440 
very small clusters that are
fair. So we build the smallest

125
00:08:59.440 --> 00:09:04.130 
type of unit that has the correct
distribution that we want in every cluster

126
00:09:04.830 --> 00:09:12.010 
and then we treat these small clusters as input
for our traditional unfair clustering algorithm

127
00:09:12.240 --> 00:09:16.040 
and this one then groups these
meta clusters that we build

128
00:09:16.040 --> 00:09:20.730 
in the preprocessing step and this way
automatically produces a fair clustering.

129
00:09:21.050 --> 00:09:24.870 
So there we can also recycle our
known clustering algorithms

130
00:09:25.620 --> 00:09:29.800 
and if we dare we could also do in
processing look at our clustering

131
00:09:29.800 --> 00:09:33.240 
algorithms and try to tweak
them to make them fair.

132
00:09:33.700 --> 00:09:38.900 
This is a nice algorithmic challenge,
so why not look at this?

133
00:09:39.480 --> 00:09:44.290 
So with some effort but not too
much you can see that you can

134
00:09:44.290 --> 00:09:49.820 
make clustering algorithms fair
and as a last note on fairness

135
00:09:50.200 --> 00:09:53.920 
I would like to say that this
one notion of fairness that I

136
00:09:53.920 --> 00:09:57.140 
have presented to you now is only
one choice that you can make

137
00:09:57.470 --> 00:10:02.500 
and it's not just an issue for
clustering but for any learning

138
00:10:02.510 --> 00:10:06.040 
that you do from personal data.
So I'd like to point you to

139
00:10:06.040 --> 00:10:11.000 
this paper with the very scary
long list of authors if you

140
00:10:11.000 --> 00:10:15.690 
only search for the title "Bias in data
driven artificial intelligence systems"

141
00:10:15.970 --> 00:10:19.970 
you will find this and this is a nice
survey that gives you pointers

142
00:10:20.230 --> 00:10:25.870 
to bias in various areas
and how u can mend this.

143
00:10:26.690 --> 00:10:29.770 
So this is all that I wanted to
say. Thank you very much for

144
00:10:29.770 --> 00:10:30.310 
your attention.
