WEBVTT

1
00:00:00.000 --> 00:00:03.300 
So yeah, the next part
would be joins, which is

2
00:00:03.300 --> 00:00:06.600 
just an overview. We have
usually inner and outter joins

3
00:00:06.600 --> 00:00:09.900 
the two main classes of
joints and how to combine

4
00:00:09.900 --> 00:00:12.120 
the results and then we
have specializations on the

5
00:00:12.120 --> 00:00:15.150 
predicates. So we
have equi joins

6
00:00:16.160 --> 00:00:20.200 
which have an equi predicate
and inequality joins.

7
00:00:20.200 --> 00:00:24.240 
Key thing that we will look
into now we will be inner

8
00:00:24.240 --> 00:00:27.270 
joins and combined
with equi joins

9
00:00:27.270 --> 00:00:29.290 
because these
are, this is

10
00:00:29.290 --> 00:00:32.320 
the make a majority of
joints you see in a system.

11
00:00:33.330 --> 00:00:36.360 
The most free, the
three most basic join

12
00:00:36.360 --> 00:00:40.400 
algorithms that every
database has are listed

13
00:00:40.400 --> 00:00:44.440 
here. We have a nested loop
join,sort-merge join, hash based

14
00:00:44.440 --> 00:00:48.480 
join and the interesting
part is usually what

15
00:00:48.480 --> 00:00:50.500 
you learned is that
the nested loop join

16
00:00:51.510 --> 00:00:54.540 
is the slowest join you
have in the database.

17
00:00:54.540 --> 00:00:57.570 
Still pretty much every
database has a limitation of

18
00:00:57.570 --> 00:01:00.600 
a nested loop join and
the reason for that is

19
00:01:00.600 --> 00:01:03.630 
that in a lot
cases you have

20
00:01:03.630 --> 00:01:06.660 
no, the implementation
has a very low

21
00:01:06.660 --> 00:01:09.690 
memory consumption so
you don't have a lot of

22
00:01:09.690 --> 00:01:12.720 
additional data structures
you need to allocate

23
00:01:12.720 --> 00:01:17.770 
before executing the join and
in real time, real world systems

24
00:01:17.770 --> 00:01:21.810 
you often have joins where some
predicates are highly restrictive.

25
00:01:21.810 --> 00:01:24.840 
So the actual data
that you will join

26
00:01:24.840 --> 00:01:27.870 
in the end might just be,
maybe, 100 columns. In that

27
00:01:27.870 --> 00:01:31.910 
case the the complexity
hit that you take within

28
00:01:31.910 --> 00:01:33.930 
a nested-loop join doesn't really
matter because it just it's

29
00:01:33.930 --> 00:01:36.960 
fast enough, you won't
see the difference,

30
00:01:36.960 --> 00:01:39.990 
and constructing all the additional
data structures for other

31
00:01:39.990 --> 00:01:42.102 
join implementations might
not be worth the overhead

32
00:01:42.102 --> 00:01:45.105 
in that case.
Nonetheless if you have

33
00:01:45.105 --> 00:01:48.108 
analytical joins over
millions of lines

34
00:01:48.108 --> 00:01:52.112 
you will never probably
use a nested loop join.

35
00:01:53.113 --> 00:01:56.116 
The question, also, when we
talk about query optimization

36
00:01:56.116 --> 00:02:00.120 
is which query, which joined
to use in case we have to join

37
00:02:00.120 --> 00:02:04.124 
two relations. As I've said,
a nested loop join might

38
00:02:04.124 --> 00:02:08.128 
make sense for really small
data sets or when you're

39
00:02:08.128 --> 00:02:11.131 
traversing of the second
relation, when for this column

40
00:02:11.131 --> 00:02:14.134 
there might been index.
So in this case you don't

41
00:02:14.134 --> 00:02:16.136 
have, you have a better

42
00:02:17.137 --> 00:02:20.140 
run time complexity there
so in this case a nested

43
00:02:20.140 --> 00:02:24.144 
loop join might make sense.
Then, there are cases where

44
00:02:24.144 --> 00:02:28.148 
you data is already sorted, in
this case obviously a sort-merge

45
00:02:28.148 --> 00:02:31.151 
join might be the
best case for the

46
00:02:31.151 --> 00:02:32.152 
issues and then we
have to hash join

47
00:02:33.153 --> 00:02:37.000 
which is the way to
go most in most cases.

48
00:02:37.157 --> 00:02:39.159 
So the most real world joins
you will see, that one relation

49
00:02:39.159 --> 00:02:43.163 
is smaller than the other and
this is usually a good case for

50
00:02:43.163 --> 00:02:47.167 
a hash join. What is a problem
with two standard joins

51
00:02:47.167 --> 00:02:49.169 
for large data sets? If
you look at the hash join

52
00:02:50.170 --> 00:02:53.173 
we have random accessess
to large hash maps

53
00:02:53.173 --> 00:02:57.177 
and the hash maps might be
large, can be larger, usually are

54
00:02:57.177 --> 00:03:01.181 
larger when your cache. So if
you join now, for example, with a

55
00:03:01.181 --> 00:03:02.182 
relation that is
a on distant node

56
00:03:03.183 --> 00:03:06.186 
you have random accessess to
distant node, to distant memory.

57
00:03:06.186 --> 00:03:09.189 
For the sort-based join,

58
00:03:10.190 --> 00:03:13.193 
sort-based join is pretty much the
same. So sorting data, there is

59
00:03:13.193 --> 00:03:16.196 
much larger than your cache and
we shoving data out of the cache.

60
00:03:16.196 --> 00:03:20.200 
That's a relatively
expensive operation so

61
00:03:20.200 --> 00:03:25.205 
these two mutations are
usually adapted heavily

62
00:03:25.205 --> 00:03:28.208 
for a large joins
and for large

63
00:03:28.208 --> 00:03:31.211 
polar systems and what
we will talk about is

64
00:03:31.211 --> 00:03:35.215 
the radix-partitioned hash
join. So we can see the problem

65
00:03:35.215 --> 00:03:38.218 
here, we have a large
relation on the right side

66
00:03:38.218 --> 00:03:41.221 
which we scan sequentially
in the probe phase.

67
00:03:41.221 --> 00:03:45.225 
This is fine, this is fast, this
is sequential data, sequential

68
00:03:45.225 --> 00:03:49.229 
access even though this relation
might sit on another note,

69
00:03:49.229 --> 00:03:52.232 
this is still fast. But the
problem is with a smaller hash,

70
00:03:52.232 --> 00:03:55.235 
smaller relation we have
now built the hash table

71
00:03:55.235 --> 00:03:59.239 
and all the sockets here,
we will randomly access this

72
00:03:59.239 --> 00:04:02.000 
hash table and this hash table
is larger than our cache.

73
00:04:02.242 --> 00:04:05.245 
So especially if the
table sits on another node

74
00:04:06.246 --> 00:04:08.248 
every time you
access to hash map

75
00:04:08.248 --> 00:04:11.251 
this will be a full cache
miss to a distant node,

76
00:04:11.251 --> 00:04:15.255 
so it's relatively
expensive. The idea of radix

77
00:04:15.255 --> 00:04:18.258 
partitioning now is that
we split up the table

78
00:04:19.259 --> 00:04:23.263 
into several partitions

79
00:04:23.263 --> 00:04:28.268 
and to have much smaller partitions
to join, so we have an additional

80
00:04:28.268 --> 00:04:32.272 
step. But now, when we we
look into the data, resulting

81
00:04:32.272 --> 00:04:35.275 
data we have still sequential
scans on the right side so nothing

82
00:04:35.275 --> 00:04:39.279 
changed a lot there but we
have now a couple of small hash

83
00:04:39.279 --> 00:04:43.283 
tables. The goal is when we
choose the partition, the

84
00:04:43.283 --> 00:04:47.287 
size of the number of
partitions, we want to find a

85
00:04:47.287 --> 00:04:51.291 
a number such that
the hash tables

86
00:04:51.291 --> 00:04:54.294 
can be a cache resistant so
they might fit into the L3

87
00:04:54.294 --> 00:04:58.298 
or even in the L2 cache.
The goal is basically to do

88
00:04:58.298 --> 00:05:02.302 
now that execute,
the join is better.

89
00:05:02.302 --> 00:05:06.306 
After a couple of accesses the
complete hash table will be cache local

90
00:05:07.307 --> 00:05:10.310 
so even though the the smaller
relation might have been sitting

91
00:05:10.310 --> 00:05:13.313 
on another distant core, now
that we do all the probing into

92
00:05:13.313 --> 00:05:17.317 
the hash table at some point
will be completely cache resident

93
00:05:17.317 --> 00:05:21.321 
on a local node and it will
be much faster to execute

94
00:05:21.321 --> 00:05:25.325 
the join right now,
with this consolation.

95
00:05:34.334 --> 00:05:38.338 
This would

96
00:05:38.338 --> 00:05:41.341 
what it will still help if you
reallyÂ consider large ones that might

97
00:05:41.341 --> 00:05:44.344 
be parallelized
over several sockets

98
00:05:44.344 --> 00:05:47.347 
but if they would be larger
than the L2, yeah, there will be

99
00:05:47.347 --> 00:05:49.349 
all the workers on
a different node

100
00:05:49.349 --> 00:05:51.351 
would basically pollute
each others caches

101
00:05:52.352 --> 00:05:57.357 
or would pollute their chat
cache. Usually what you try to do

102
00:05:57.357 --> 00:05:59.359 
there is to aim
for the L2 cache.

103
00:05:59.359 --> 00:06:01.361 
So try to find a side
that might be, to find,

104
00:06:02.362 --> 00:06:05.365 
try to find a partition number
where estimate your hash table

105
00:06:05.365 --> 00:06:07.367 
to be like half
the size of L2.

106
00:06:11.371 --> 00:06:13.373 
Ok so how is that done?

107
00:06:14.374 --> 00:06:17.377 
Yeah, again the idea is to
partition both relations and then

108
00:06:17.377 --> 00:06:20.380 
just to merge the the
fitting partitions.

109
00:06:20.380 --> 00:06:22.382 
We come to that how we
could do that partitioning

110
00:06:23.383 --> 00:06:26.386 
but the really
interesting part here is

111
00:06:27.387 --> 00:06:29.389 
when we talk about NUMA
systems and in memory systems.

112
00:06:30.390 --> 00:06:34.394 
As you see we partition
the data before we join

113
00:06:34.394 --> 00:06:37.397 
so we scan over
all that data

114
00:06:37.397 --> 00:06:40.400 
sequentially into some
form of partitioning

115
00:06:40.400 --> 00:06:43.403 
which is work that you actually
don't have to do. So we could

116
00:06:43.403 --> 00:06:46.406 
do this hash
join immediately

117
00:06:46.406 --> 00:06:50.410 
without all this partitioning
but the interesting part is that

118
00:06:50.410 --> 00:06:54.414 
because our systems or
our scanning is so fast

119
00:06:54.414 --> 00:06:58.418 
that it works out. So, we are, in
the end we are faster even though

120
00:06:58.418 --> 00:07:01.421 
we have multiple scans of all our
data because of the partitioning

121
00:07:01.421 --> 00:07:03.423 
is still faster than
doing the pure hash join.

122
00:07:04.424 --> 00:07:08.428 
This is because all the
data is in columnar format

123
00:07:08.428 --> 00:07:11.431 
in the memory database, that
wouldn't work are usually for, if

124
00:07:11.431 --> 00:07:14.434 
you have an row
store for example.

125
00:07:16.436 --> 00:07:20.440 
So this is the idea of
radix partitioning, we

126
00:07:20.440 --> 00:07:22.442 
have here six example
values on the left

127
00:07:22.442 --> 00:07:24.444 
and now we
look into their

128
00:07:24.444 --> 00:07:28.448 
binary representations
and the idea is to

129
00:07:29.449 --> 00:07:33.453 
to cluster all the
elements that have

130
00:07:33.453 --> 00:07:37.457 
the same radix bits in the end.
So for example here we have

131
00:07:37.457 --> 00:07:42.462 
two bits, so we might have four
partitions, up to four partitions

132
00:07:42.462 --> 00:07:46.466 
here and now we go over data
we can also parallelize that

133
00:07:46.466 --> 00:07:50.470 
and all the items
with a 1 0 here

134
00:07:50.470 --> 00:07:53.473 
go to the last partition,
you can see here.

135
00:07:53.473 --> 00:07:55.475 
Depending on the
last two bits

136
00:07:55.475 --> 00:07:58.478 
we assign them to
to the partitions.

137
00:08:01.481 --> 00:08:05.485 
Process, so we do that is,
first, we need to determine

138
00:08:05.485 --> 00:08:09.489 
n, so the number
of radix bits

139
00:08:09.489 --> 00:08:11.491 
and then we prepare
a counter. So we

140
00:08:12.492 --> 00:08:16.496 
have first step, where we go over all
the data of our smaller relation and

141
00:08:16.496 --> 00:08:19.499 
we just count how many
hits, how many do we

142
00:08:19.499 --> 00:08:23.503 
have for the 0, for 0 1, the
1 0 and the 1 1, for example,

143
00:08:23.503 --> 00:08:27.507 
if we have two bits.
Afterwards we have calculated

144
00:08:27.507 --> 00:08:29.509 
this, we are going to
calculate a prefix sum

145
00:08:30.510 --> 00:08:32.512 
and allocate a
distant vector. So we,

146
00:08:33.513 --> 00:08:36.516 
as you prepare the
vector, their locations

147
00:08:36.516 --> 00:08:42.522 
for the partitions and
you can see that here.

148
00:08:42.522 --> 00:08:45.525 
So basically if we have
calculated our counters here,

149
00:08:45.525 --> 00:08:47.527 
in this case we have
two for each partition

150
00:08:48.528 --> 00:08:50.530 
except for the last one.
So we are going to prepare

151
00:08:50.530 --> 00:08:54.534 
three partitions here and as soon
as we have this data structure

152
00:08:54.534 --> 00:08:58.538 
we can fully, sequentially, without
any blocking or latching and

153
00:08:58.538 --> 00:09:02.542 
so on can have multiple
queries write data directly

154
00:09:02.542 --> 00:09:07.547 
into these packets. This
can be fully parallelized,

155
00:09:07.547 --> 00:09:11.551 
there's no blocking here so
the first step, so we have two

156
00:09:11.551 --> 00:09:14.554 
added scans on the data,
first to build up the

157
00:09:15.555 --> 00:09:18.558 
sums here and the second to
write the data into buckets

158
00:09:18.558 --> 00:09:21.561 
but still again, this is
for the architecture that we

159
00:09:21.561 --> 00:09:25.565 
talk about here, really fast
operations, so that doesn't really

160
00:09:25.565 --> 00:09:27.567 
hurt us. All that the
preparations do not hurt us.

161
00:09:28.568 --> 00:09:30.570 
They hurt us,
but in the end

162
00:09:30.570 --> 00:09:32.572 
it turned out to
be a really good

163
00:09:33.573 --> 00:09:37.577 
alternative. This
kind of radix

164
00:09:37.577 --> 00:09:41.581 
partitioning is the fastest way
that is currently in research,

165
00:09:41.581 --> 00:09:45.585 
at least that we know
about, to range partition

166
00:09:45.585 --> 00:09:48.588 
data. The obvious shortcoming
here is we only talk about

167
00:09:48.588 --> 00:09:51.591 
equi. So the cool thing
is now if you have,

168
00:09:51.591 --> 00:09:54.594 
on the left side, a
partition for the radix bit

169
00:09:54.594 --> 00:09:57.597 
0 0 and on the right side,
the same, you just have to

170
00:09:57.597 --> 00:10:01.601 
merge or join these two
partitions. Because they can not be

171
00:10:01.601 --> 00:10:05.605 
a hit with the 0 0 partition
in the 0 1 partition

172
00:10:05.605 --> 00:10:09.609 
on the right relation,
but this obviously doesn't

173
00:10:09.609 --> 00:10:13.613 
work if you have a less
equals or a non equi

174
00:10:13.613 --> 00:10:16.616 
predicate. So this is an
optimization only for equi joins.

175
00:10:17.617 --> 00:10:21.621 
So what we learned
now for the join?

176
00:10:21.621 --> 00:10:23.623 
Join is one of the most,

177
00:10:23.623 --> 00:10:27.627 
yeah it's probably the
most expensive operation in

178
00:10:27.627 --> 00:10:30.630 
database that you see in or
at least in the databases that

179
00:10:30.630 --> 00:10:34.634 
we look at, so mixed local
databases for ERP systems

180
00:10:35.635 --> 00:10:37.637 
as such we need to optimize
it as much as possible

181
00:10:38.638 --> 00:10:41.641 
or we need to optimize
it relatively good,

182
00:10:41.641 --> 00:10:44.644 
at least. So what we have
talked about a little

183
00:10:44.644 --> 00:10:46.646 
bit more in detail is the
radix partition hash join.

184
00:10:47.647 --> 00:10:50.000 
There are also implementations
for radix parition sort joins and

185
00:10:50.000 --> 00:10:53.653 
other joins, so this was just
one example for equi joins.

186
00:10:53.653 --> 00:10:58.658 
Equi inner joins, please
keep that in mind and

187
00:10:59.659 --> 00:11:02.662 
what the goal was of
this radix join was

188
00:11:02.662 --> 00:11:05.665 
that we leverage the scan
performance of a database,

189
00:11:05.665 --> 00:11:08.668 
that we can sequentially
run really fast over data,

190
00:11:09.669 --> 00:11:13.673 
and then optimize through this
partitioning, optimise our caching

191
00:11:13.673 --> 00:11:16.676 
locality when we do the
actual joining of the tables.

192
00:11:17.677 --> 00:11:20.680 
So we had a lot of work
before we do the joining

193
00:11:20.680 --> 00:11:23.683 
but it turns out to be
faster than not doing that.
