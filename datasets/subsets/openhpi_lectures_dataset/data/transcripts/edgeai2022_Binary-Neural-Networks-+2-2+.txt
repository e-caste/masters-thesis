WEBVTT

1
00:00:00.640 --> 00:00:03.270 
Hello and welcome. In this video,

2
00:00:03.279 --> 00:00:06.610 
We will continue our topic on binary neural networks.

3
00:00:07.240 --> 00:00:12.250 
We will see the challenges and some of the current progress in this research direction.

4
00:00:14.740 --> 00:00:22.960 
If we are discussing the challenges of current binary neural networks, the loss of precision is an unavoidable topic.

5
00:00:23.739 --> 00:00:30.320 
It suffers from substantial accuracy degradation compared to its real valued counterparts.

6
00:00:30.329 --> 00:00:40.520 
For example, directly binarising a neural network training on image net is to an accuracy loss of almost 10% which is

7
00:00:40.520 --> 00:00:41.359 
huge.

8
00:00:41.369 --> 00:00:49.649 
So how to solve this problem? It is probably the most concerned topic in the research community for now.

9
00:00:50.840 --> 00:00:52.920 
How to build a binary neural network

10
00:00:52.929 --> 00:00:54.649 
tailor-made optimizers.

11
00:00:55.340 --> 00:01:02.329 
So the optimization method of current BNN is a potential optimization method.

12
00:01:02.340 --> 00:01:12.180 
So it's partially works that borrows the full position network. Therefore an optimizer tailor rate for a binary neural

13
00:01:12.180 --> 00:01:15.060 
network is the next important research topic.

14
00:01:15.840 --> 00:01:22.159 
And the third issue is balancing AI accelerators, accuracy and energy consumption.

15
00:01:22.640 --> 00:01:31.409 
Since there are still a lot of full precision operations in the existing BNNs, this dramatically increases the difficulty of

16
00:01:31.420 --> 00:01:41.159 
BNN accelerator design. Last but not least, lack of support for solid inference acceleration heterogeneous hardware.

17
00:01:41.739 --> 00:01:51.099 
Most of the research works in this domain rely on theoretical complexity reduction and they don't report real speed up because

18
00:01:51.099 --> 00:01:54.250 
there's no solid implementation for this target.

19
00:01:55.140 --> 00:02:01.120 
So now let's recap several classical work in their

20
00:02:01.129 --> 00:02:04.659 
not so long history of binary neural network development.

21
00:02:05.640 --> 00:02:08.919 
The first work I want to introduce is XNOR-Net.

22
00:02:08.949 --> 00:02:18.379 
This work brings BNN into the public vision for the first time. Because it's author also demonstrated the real time version

23
00:02:18.379 --> 00:02:25.460 
of the YOLO object detector based on the being an implementation running on the CPU of iPhone 4.

24
00:02:25.939 --> 00:02:28.849 
This is a brilliant achievement in performance,

25
00:02:28.860 --> 00:02:32.960 
back to the year 2017 or 2016.

26
00:02:33.939 --> 00:02:43.120 
The algorithm size contribution is first introduced the scaling factor for the activation and weights. It is the first binary

27
00:02:43.120 --> 00:02:51.319 
neural network paper reporting results on image net dataset and achieved 51.2%

28
00:02:51.319 --> 00:02:55.360 
top one accuracy using the binary ResNet 18 model.

29
00:02:56.139 --> 00:03:01.840 
I think this paper has a lot of ground breaking benefits.

30
00:03:01.849 --> 00:03:11.139 
However, it also has some misleading and over claims such as resulting in 58 times faster convolutional operations.

31
00:03:11.150 --> 00:03:20.069 
This conclusion is made based on the comparison to vary naive implementation of GEMM kernel on the CPU, which is rarely

32
00:03:20.069 --> 00:03:21.349 
used in practice.

33
00:03:21.360 --> 00:03:26.750 
Still, the author didn't notice it in the paper.

34
00:03:26.750 --> 00:03:32.650 
XNOR-Net offers the possibility of running state of the art network on CPUS rather than GPUs

35
00:03:32.650 --> 00:03:33.550 
in real time.

36
00:03:34.139 --> 00:03:42.530 
So this is author's claim in the paper running on CPU in real time is still a challenging task for most of the deep learning

37
00:03:42.530 --> 00:03:43.259 
models

38
00:03:43.270 --> 00:03:55.139 
even now. This paper didn't mention a few essential details such as using it to use this full precision 1x1 down

39
00:03:55.139 --> 00:04:00.659 
sampling convolutions in the network which dramatically impacts the accuracy.

40
00:04:02.939 --> 00:04:05.090 
The ABC-net and GroupNet

41
00:04:05.099 --> 00:04:14.120 
The authors proposed to approximate full preciison parameters using multiple binary ways based and groups in a binary activation

42
00:04:14.120 --> 00:04:14.849 
basis.

43
00:04:15.389 --> 00:04:24.709 
The idea is quite straightforward. Using the linear combination of several weighted a one bit basis, we can effectively

44
00:04:24.709 --> 00:04:35.430 
increase the expressiveness of the model of the layer and information loss can be recovered very well. To achieve the performance

45
00:04:35.430 --> 00:04:37.360 
again is also apparent.

46
00:04:37.939 --> 00:04:47.259 
But a possible shortcoming of this sort of approach is that the corresponding computational complexity has also been increased

47
00:04:47.740 --> 00:04:56.860 
whether this kind of method can bring speed up improvement and has not been verified in the paper.

48
00:04:57.439 --> 00:05:05.540 
Nevertheless, it offers successful experience in the accuracy improvement.

49
00:05:05.540 --> 00:05:11.959 
BiRealNet made several important contributions which are used by many follow up studies.

50
00:05:12.540 --> 00:05:17.250 
First, it introduced a binary to real value information flow.

51
00:05:17.930 --> 00:05:26.680 
In short, it only makes the convolutional layer and a dense layer in the binary computation to gain speed up the activation

52
00:05:26.680 --> 00:05:30.250 
flow during the whole network keeps in full precision.

53
00:05:31.040 --> 00:05:38.060 
This can be effectively implemented by adding a shortcut connection for every binary convolutional layer.

54
00:05:38.639 --> 00:05:47.779 
So we can always forward the full precision information by a shortcut and fuse with convolutional output and it always puts

55
00:05:47.779 --> 00:05:54.639 
the batch norm layer of the binary com layer, so it will convert the output to full precision value

56
00:05:54.639 --> 00:06:02.750 
again. Other techniques are proposed in this paper, like better approximating the sign function in the backward pause.

57
00:06:02.779 --> 00:06:11.560 
It also proposed to apply a two stage training strategy. In the first stage we will train the model using binary activations

58
00:06:11.569 --> 00:06:13.660 
but keep the weight in full precision.

59
00:06:14.040 --> 00:06:21.660 
Then in the second stage we will fine tune the model from the first stage and binarise the weight.

60
00:06:22.329 --> 00:06:33.139 
Such a training strategy offers a smoother binarization process which is helpful for accuracy improvement.

61
00:06:33.139 --> 00:06:34.000 
After that,

62
00:06:34.009 --> 00:06:44.339 
a lot of work was devoted to continuing to improve accuracy and has achieved very great results.

63
00:06:44.350 --> 00:06:53.769 
The gap between classification accuracy of binary network and the full precision model has been continuously reduced. Among

64
00:06:53.769 --> 00:06:55.040 
them, MeliusNet

65
00:06:55.040 --> 00:07:05.090 
first achieved the mobileNet level accuracy with aligned computational complexity and the model size. From

66
00:07:05.139 --> 00:07:05.939 
XNOR-Net

67
00:07:06.439 --> 00:07:15.560 
it's result 51.2% on ImageNet we can see that the accuracy has increased by more than 12%.

68
00:07:18.439 --> 00:07:25.459 
Okay, I want to offer more details about the current solution of binary neural network optimization.

69
00:07:26.939 --> 00:07:34.660 
Binary weights can only flip the sign and therefore they cannot train it using standard SGD method.

70
00:07:35.040 --> 00:07:46.540 
You cannot minus a sign right? Now according to the figure I do the forward pass and have the prediction and calculate the

71
00:07:46.540 --> 00:07:47.050 
loss.

72
00:07:47.740 --> 00:07:49.630 
How do we use SGD

73
00:07:49.631 --> 00:07:50.860 
for BNN training?

74
00:07:51.439 --> 00:07:54.319 
The current BNN training relies on full precision

75
00:07:54.319 --> 00:07:58.250 
latent weights at the epoxy of the binary weights.

76
00:07:58.639 --> 00:08:06.970 
Latent weights means that we will always keep a full precision version of the weights during the training. In the back propagation

77
00:08:06.980 --> 00:08:10.160 
we will use them for the SGD computation.

78
00:08:11.339 --> 00:08:20.459 
After the weight update, we will create the binarirised version for the forward computation again. In practice, this method

79
00:08:20.459 --> 00:08:30.110 
works quite well. But as you can see the loss is computed based on the binary weights but we have to use the full precision

80
00:08:30.110 --> 00:08:34.659 
proxy for the backward pass for the gradient descent.

81
00:08:35.539 --> 00:08:43.659 
There is an obvious mismatch of optimization objectives and it also brings unnecessary computation.

82
00:08:45.539 --> 00:08:49.029 
Thus how to build a BNN tailor-made

83
00:08:49.029 --> 00:08:51.960 
optimizer is still an open research question.

84
00:08:54.840 --> 00:09:03.149 
We already know that the two stage training can significantly benefit accuracy because it creates a smoother way for parameter

85
00:09:03.149 --> 00:09:04.159 
binarization.

86
00:09:05.039 --> 00:09:11.549 
This paper proposed a progressive binarization method showing similar or even better effects.

87
00:09:11.940 --> 00:09:20.620 
It progressively transfer neural network parameters from 32 bit to 1 bit. The benefits first.

88
00:09:20.629 --> 00:09:23.500 
We can better utilize SGD

89
00:09:23.509 --> 00:09:31.190 
since the model is not a binary in the training time and the gradient mismatching problem has been somehow better

90
00:09:31.190 --> 00:09:43.139 
treated. The magnitude of weights and activation progressively converge to 1. And the optimization objective has gradually shifted

91
00:09:43.139 --> 00:09:50.840 
from magnitude and sign to only consider sign flipping.

92
00:09:50.840 --> 00:10:00.049 
A recent work BoolNet tries to address the trade off between accuracy and energy consumption of binary network for AI accelerators.

93
00:10:00.740 --> 00:10:08.059 
This paper studied the energy consumption of 32 bit layers used in the existing BNN model for the first time.

94
00:10:08.840 --> 00:10:17.620 
It points out that previous work like by BiReal net and MinusNet using fool precision activation flow through the whole

95
00:10:17.620 --> 00:10:25.059 
network is not energy efficient for AI accelerator, since the memory consumption is still very high.

96
00:10:25.740 --> 00:10:33.809 
Someone says in the field of hardware accelerators that computation is cheap but the memory is expensive.

97
00:10:33.820 --> 00:10:40.350 
It is a reasonable interpretation of existing problems in the current BNN model design.

98
00:10:40.740 --> 00:10:44.750 
Although binary operations are used in the binary convolution layer.

99
00:10:44.759 --> 00:10:54.299 
However, the full precision information flow brings more significant memory overhead resulting in higher power consumption

100
00:10:54.309 --> 00:11:00.360 
on the accelerator such as ASIC or FPGA based accelerator.

101
00:11:01.039 --> 00:11:13.299 
Usually the on-chip memory of the accelerator is tiny. For example, sometimes only 192 kB and the full precision components

102
00:11:13.299 --> 00:11:22.950 
in BNN models such as the batchnorm operation or full precision activations result in a large number of alt shift memory access

103
00:11:23.639 --> 00:11:26.129 
which should significantly increase power

104
00:11:26.129 --> 00:11:36.759 
consumption. The most contribution of BoolNet is to propose a fully binarised information flow that is almost all worthwhile

105
00:11:36.769 --> 00:11:44.450 
which dramatically reduce the memory consumption, makes the power consumption about three times more than three times lower

106
00:11:44.460 --> 00:11:50.250 
than the current state of the art BNN model while only a small accuracy loss.

107
00:11:53.539 --> 00:12:03.450 
Okay, this slide shows the most existing BNN frameworks. We commented on their support for different bitwidth, API interface

108
00:12:03.450 --> 00:12:03.960 
types,

109
00:12:04.250 --> 00:12:08.139 
demo support and support for the state of the art

110
00:12:08.149 --> 00:12:14.830 
BNN Models where bolt is an inference framework precisely optimized for the arm

111
00:12:14.830 --> 00:12:21.970 
CPU using assembly. Larq supports the training and inference based on tensorflow and

112
00:12:21.970 --> 00:12:23.519 
TF tensorflow lite.

113
00:12:23.529 --> 00:12:26.720 
Um okay Riptide's

114
00:12:26.730 --> 00:12:34.259 
training code is also based on tensorflow, the inference engine relies on the Apache TVM engine.

115
00:12:34.840 --> 00:12:44.080 
Our group develops BMXNet and pytorch or BITorch. Specifically in  BITorch with our collaborators,

116
00:12:44.090 --> 00:12:48.960 
we develop efficient cuda kernels for bit wise matrix operations.

117
00:12:49.340 --> 00:12:57.429 
You can also gain the real acceleration of the binary model on GPU hardware. In addition to the traditional computer vision

118
00:12:57.429 --> 00:13:04.779 
models, BITorch also plans to support the low bit version of recommendation model like the DLRM

119
00:13:04.779 --> 00:13:05.169 


120
00:13:05.179 --> 00:13:11.740 
and the large language model BERT.

121
00:13:11.740 --> 00:13:15.860 
For those who are more interested in exploring binary neural network,

122
00:13:15.870 --> 00:13:21.549 
I have prepared some more papers as advanced reading materials for you.

123
00:13:25.240 --> 00:13:33.289 
Okay, in the practical sessions this week we will try to implement a binary neural network model for image classification

124
00:13:33.289 --> 00:13:42.409 
using CIFAR dataset and using the BITorch framework developed by our group. This time required for the task

125
00:13:42.419 --> 00:13:52.769 
it's about 3-6 hours and BITorch is a very new open source framework and I hope to get your kind support and

126
00:13:52.889 --> 00:13:54.759 
any comments are welcome.

127
00:13:55.139 --> 00:13:56.250 
Thank you very much.
