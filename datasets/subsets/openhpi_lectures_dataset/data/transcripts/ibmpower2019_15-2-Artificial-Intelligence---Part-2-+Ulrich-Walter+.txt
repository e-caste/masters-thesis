WEBVTT

1
00:00:00.490 --> 00:00:05.049 
Hello, I'm Ulrich Walter, again, thanks for having you with me today,

2
00:00:05.050 --> 00:00:10.179 
and I'm very happy to give you the second part of our speech about Power9 and

3
00:00:10.180 --> 00:00:13.149 
beyond for artificial intelligence.

4
00:00:13.150 --> 00:00:17.739 
This is a follow on of our part-1. We have discussed already about the economy and

5
00:00:17.740 --> 00:00:22.359 
possibilities of artificial intelligence and the systems

6
00:00:22.360 --> 00:00:26.739 
we are going to see the future in our industries.

7
00:00:26.740 --> 00:00:31.389 
In this part we will discuss more in detail about the technology,

8
00:00:31.390 --> 00:00:35.919 
from the hardware perspective, but also from the data line perspective and last

9
00:00:35.920 --> 00:00:40.599 
not least also of the orchestration and the software stack, what is necessary to build

10
00:00:40.600 --> 00:00:42.880 
an AI system and the efficiency of that.

11
00:00:44.470 --> 00:00:49.119 
So, when we talk about A.I. in general, we have the three very important parts

12
00:00:49.120 --> 00:00:53.769 
to consider, which is scale, how does a system scale,

13
00:00:53.770 --> 00:00:58.269 
the timeline to actually get something finishe, and last not least, and this is

14
00:00:58.270 --> 00:01:02.979 
I think most important is the accuracy, what you want to achieve with your AI system

15
00:01:02.980 --> 00:01:08.169 
in general, because an AI system in comparison to traditional data systems

16
00:01:08.170 --> 00:01:12.609 
has never, ever a 100 percent accuracy, because everything is built

17
00:01:12.610 --> 00:01:17.139 
on dependencies, on evaluation of

18
00:01:17.140 --> 00:01:22.059 
the data, in combination of correlations of, let's say, mathematical functions

19
00:01:22.060 --> 00:01:25.929 
in here and weights and states within the data and the algorithms.

20
00:01:25.930 --> 00:01:30.699 
So, therefore, accuracy is actually very much important

21
00:01:30.700 --> 00:01:34.629 
item for getting things done to a very high level.

22
00:01:34.630 --> 00:01:39.159 
Especially in automotive driving, autonomous driving you probably won't have a car

23
00:01:39.160 --> 00:01:43.729 
with an accuracy ratio of less than 90 percent

24
00:01:43.730 --> 00:01:45.819 
otherwise or even a higher rate.

25
00:01:45.820 --> 00:01:50.499 
Otherwise of course, you are actually heading to to an accident or whatever

26
00:01:50.500 --> 00:01:55.359 
else. So, therefore, accuracy is a very, very important part

27
00:01:55.360 --> 00:01:57.369 
of our endeavor here.

28
00:01:57.370 --> 00:01:59.649 
And how is it actually established?

29
00:01:59.650 --> 00:02:03.429 
So, the scale of our system is driven by data and storage.

30
00:02:03.430 --> 00:02:08.109 
So, data is one of the most important things what we have here to consider

31
00:02:08.110 --> 00:02:12.639 
in our AI system, without data, we can't do anything and the more data

32
00:02:12.640 --> 00:02:14.740 
we have, the better the system might be.

33
00:02:16.390 --> 00:02:18.939 
But the timeline also is important.

34
00:02:18.940 --> 00:02:23.439 
So, the timeline we are talking about here is a compute power to achieve a

35
00:02:23.440 --> 00:02:28.269 
certain job and the bandwidth for transporting that data to the computer itself.

36
00:02:28.270 --> 00:02:32.829 
So, this can take a long, long time on traditional systems but here,

37
00:02:32.830 --> 00:02:37.449 
of course, we actually have new systems to come and new technologies that

38
00:02:37.450 --> 00:02:42.189 
save a lot of time, especially for long lasting training runs.

39
00:02:42.190 --> 00:02:46.629 
And lastly, accuracy as I said before, it's very much depending

40
00:02:46.630 --> 00:02:51.099 
on the quality of your data and the model of your implementation you are going

41
00:02:51.100 --> 00:02:55.719 
to use. These as a three important factors we have always to consider

42
00:02:55.720 --> 00:03:00.039 
when we talk about an AI system and the dependencies and the benefits of it.

43
00:03:02.230 --> 00:03:04.449 
What do we need for building up in the system?

44
00:03:04.450 --> 00:03:09.069 
As I said before, we need a large set of data and of course, the more data

45
00:03:09.070 --> 00:03:13.689 
we have, so better it is about the quality of the data needs to match also

46
00:03:13.690 --> 00:03:18.399 
our requirements. Then we need to have a neural network, there are a couple

47
00:03:18.400 --> 00:03:22.899 
of them, I don't go in that session in detail, the neural networks

48
00:03:22.900 --> 00:03:27.369 
will definitely become a separate session here but therefore, of course, it's a very

49
00:03:27.370 --> 00:03:31.869 
important to understand the concept of neural networks, how they act, how they

50
00:03:31.870 --> 00:03:36.609 
can be implemented. And of course, there's a foundation of all the inference

51
00:03:36.610 --> 00:03:41.739 
models, what we see in the artificial intelligence in production.

52
00:03:41.740 --> 00:03:46.599 
And last not least, we need a system that is very much

53
00:03:46.600 --> 00:03:51.069 
designed for the usage of such models and also also compute

54
00:03:51.070 --> 00:03:56.499 
power to get the data being transported to an inference

55
00:03:56.500 --> 00:03:57.939 
model quite shortly.

56
00:04:00.070 --> 00:04:03.099 
So, how do humans learn how the systems learn?

57
00:04:03.100 --> 00:04:07.719 
It's actually very similar. Humans learn by teachers and teachers

58
00:04:07.720 --> 00:04:12.399 
learn by books and of course, at the end of the day, a student or a pupil can

59
00:04:12.400 --> 00:04:16.898 
write this exam or can write this task by the learning he has done, the more

60
00:04:16.899 --> 00:04:20.259 
he learned, of course, the better maybe his results will be.

61
00:04:20.260 --> 00:04:24.909 
And this is very true also for his system and as I said before, we will have

62
00:04:24.910 --> 00:04:29.919 
in the system we have our data, which is our library, to be honest here,

63
00:04:29.920 --> 00:04:34.869 
and we have our neural network for recognition, learning and education

64
00:04:34.870 --> 00:04:39.339 
and last not least we have our inference model who actually will action on that

65
00:04:39.340 --> 00:04:41.979 
inference model, what he has learned of.

66
00:04:41.980 --> 00:04:46.419 
So these are the three important parts, what we have in our data pipeline down

67
00:04:46.420 --> 00:04:51.489 
to the insurance model but this is not just only that, we need a lot of technology

68
00:04:51.490 --> 00:04:55.929 
actually to comply on that. And the most important part, as I said before,

69
00:04:55.930 --> 00:05:00.449 
is data. Data is one of the most critical parts of our

70
00:05:00.450 --> 00:05:05.219 
road map here. Imagine that in 2019,

71
00:05:05.220 --> 00:05:10.019 
on a single day, the whole mankind creates more data than the whole mankind

72
00:05:10.020 --> 00:05:14.489 
in the last five thousand years before until the year two thousand

73
00:05:14.490 --> 00:05:18.989 
and three. This is just happening because of all of our mobile phones,

74
00:05:18.990 --> 00:05:23.669 
of all the sensors, of all the cameras we have deployed, we expect

75
00:05:23.670 --> 00:05:28.229 
to see the year 2020, about 50 billion of sensors and

76
00:05:28.230 --> 00:05:32.369 
devices which are attached to the Internet and transporting data.

77
00:05:32.370 --> 00:05:37.479 
Every one of us is moving data around from the TV at home, coffee makers,

78
00:05:37.480 --> 00:05:41.829 
sensors at doors, whatever it takes out of autonomous systems, trash bins with

79
00:05:43.380 --> 00:05:48.419 
intelligence. So, there's a lot of data flowing around everywhere you go,

80
00:05:48.420 --> 00:05:53.099 
across all fields actually, this is definitely something we will see the future

81
00:05:53.100 --> 00:05:57.958 
much more to come and this is also a source then for building up AI

82
00:05:57.959 --> 00:06:00.539 
Systems. So this is a very important thing.

83
00:06:00.540 --> 00:06:04.979 
On the other hand, that data needs to be stored in the data storage for that

84
00:06:04.980 --> 00:06:09.539 
is a highly critical thing. Also in our data pipeline, how to

85
00:06:09.540 --> 00:06:14.909 
take the data, how to proceed to data and how to store the data wisely

86
00:06:14.910 --> 00:06:17.734 
in usage of the AI system.

87
00:06:18.960 --> 00:06:23.459 
In AI systems, you have usually also two phases, which is one is a learning

88
00:06:23.460 --> 00:06:27.929 
phase, what we discussed before and you have the inference phase, which is actually the

89
00:06:27.930 --> 00:06:32.159 
action phase when the system is taking action on the and learning.

90
00:06:32.160 --> 00:06:36.779 
The learning phase, of course, is a most essential phase

91
00:06:36.780 --> 00:06:41.219 
where it takes most compute power and also, of course, a lot of storage and

92
00:06:41.220 --> 00:06:43.949 
a lot of technology to be completed.

93
00:06:43.950 --> 00:06:48.599 
We have in the learning face a collection phase for the data that we need to clean

94
00:06:48.600 --> 00:06:52.979 
and analyze the data, to prepare the data, analyze it again.

95
00:06:52.980 --> 00:06:56.879 
Then we have to learn from the data and last at least, we have to bring it to an

96
00:06:56.880 --> 00:07:01.499 
inference model and the inference models, they feedback the data to the system.

97
00:07:01.500 --> 00:07:06.209 
So we have a continuous learning here in our training cycle.

98
00:07:06.210 --> 00:07:10.709 
On the inference part, you have a sensor or a camera or some detector

99
00:07:10.710 --> 00:07:16.169 
that actually understands an image or can read something from it from a sign

100
00:07:16.170 --> 00:07:20.729 
he is doing the perception, cognition and last not least, also the

101
00:07:20.730 --> 00:07:25.439 
action, because he has to do something, either sending a signal

102
00:07:25.440 --> 00:07:30.349 
or closing the door or doing something, of course but this is on the inference

103
00:07:30.350 --> 00:07:34.199 
side and this is actually what our system has been trained for.

104
00:07:34.200 --> 00:07:38.639 
So we always have to separate the learning phase from the inference phase in here.

105
00:07:38.640 --> 00:07:42.389 
Both phases are very relevant in questions of compute power.

106
00:07:42.390 --> 00:07:46.829 
But of course, a most important part for storage is happening always in the

107
00:07:46.830 --> 00:07:47.830 
learning phase.

108
00:07:50.280 --> 00:07:54.719 
Coming to the point of our learning phase here in questions

109
00:07:54.720 --> 00:07:59.699 
of storage and storage, we have to understand there is a need for data.

110
00:07:59.700 --> 00:08:04.229 
Yes, that's right. But of course, we always have to understand also the volume

111
00:08:04.230 --> 00:08:08.849 
of data we are creating. Are we talking about gigabytes, terabytes, petabytes

112
00:08:08.850 --> 00:08:12.539 
or even exabytes of data that needs to be stored?

113
00:08:12.540 --> 00:08:16.799 
What is the velocity of that data to be flowing into such a system?

114
00:08:16.800 --> 00:08:21.569 
Are we talking about millions of IO's per second or

115
00:08:21.570 --> 00:08:26.219 
trillions of IO's per second? This is really a very important question, how

116
00:08:26.220 --> 00:08:30.019 
to actually understand the performance needs for that data.

117
00:08:30.020 --> 00:08:34.439 
The most important part for our training itself is to trust the veracity

118
00:08:34.440 --> 00:08:39.089 
of that data, because if I cannot trust my data, then my inference model

119
00:08:39.090 --> 00:08:43.709 
and my trading model will probably not actually work as I was expecting it to be,

120
00:08:43.710 --> 00:08:47.429 
because trust is the ground truth of the data, these are

121
00:08:48.600 --> 00:08:52.162 
some of the most fundamental requirements what we have for AI.

122
00:08:53.190 --> 00:08:55.919 
Then we have to understand the variability of data.

123
00:08:55.920 --> 00:08:59.759 
Not every record actually is actually the same format.

124
00:08:59.760 --> 00:09:04.529 
We have data coming from sensors, from databases, from social networks, from wherever,

125
00:09:04.530 --> 00:09:09.389 
from mobile phones, that type of variability, we need to understand

126
00:09:09.390 --> 00:09:14.039 
and then bring in context of our learning model what we want to achieve.

127
00:09:14.040 --> 00:09:19.169 
And if we actually have created such a record as such a data system

128
00:09:19.170 --> 00:09:23.739 
where we have set ground rules and we have a proven record here, then the data

129
00:09:23.740 --> 00:09:28.409 
has also a value for us, because this is something of course, this is also, in many

130
00:09:28.410 --> 00:09:32.969 
cases, an intellectual property of many companies, which is not giving

131
00:09:32.970 --> 00:09:37.919 
away quite easily. So, you have to pay a lot of money for some ground proven

132
00:09:37.920 --> 00:09:42.389 
certified data sets to actually for doing your training rather than

133
00:09:42.390 --> 00:09:45.869 
doing just, let's say, online images with cats and dogs.

134
00:09:45.870 --> 00:09:50.789 
This is a very important point here to get the value out of the data.

135
00:09:50.790 --> 00:09:55.439 
Last not least, we have to understand how frequently is a training

136
00:09:55.440 --> 00:09:59.882 
cycle running through that means are we taking this only one time,

137
00:09:59.883 --> 00:10:04.319 
are we taking it on a daily basis or even maybe on an hourly basis.

138
00:10:04.320 --> 00:10:07.559 
So, what is actually the performance requirements on that?

139
00:10:07.560 --> 00:10:12.119 
And then all the states and the special requirements for storage, but also for our

140
00:10:12.120 --> 00:10:14.429 
technology and the infrastructure bill?

141
00:10:16.260 --> 00:10:22.019 
Look at that, that's our CORAL infrastructure in Oak Ridge at Lawrence Livermore.

142
00:10:22.020 --> 00:10:26.729 
Last year actually, IBM has deployed the largest and the fastest computer

143
00:10:26.730 --> 00:10:31.829 
here on the planet today and this is really an outstanding

144
00:10:31.830 --> 00:10:36.479 
machine and this is actually designed especially for the performance needs

145
00:10:36.480 --> 00:10:41.039 
they have in questions of their data flow, but also, of course,

146
00:10:41.040 --> 00:10:44.999 
with their compute needs for performance in the CPU at the CPUs.

147
00:10:45.000 --> 00:10:49.589 
I come to that later. And the important point that this was a very first time

148
00:10:49.590 --> 00:10:54.329 
and that's very astonishing, was not the compute power

149
00:10:54.330 --> 00:10:59.069 
alone was a driver for that infrastructure, it was a bandwidth

150
00:10:59.070 --> 00:11:03.869 
because as a research, as an all people who are actually driving a project,

151
00:11:03.870 --> 00:11:08.369 
they say we need to have a system that is able to transport the

152
00:11:08.370 --> 00:11:11.369 
data also at a very, very high speed.

153
00:11:11.370 --> 00:11:16.059 
So, as I said before, data is a source, but it needs

154
00:11:16.060 --> 00:11:20.519 
to actually be transported very, very fast to the CPU or the compute

155
00:11:20.520 --> 00:11:25.439 
time in order to not to waste actually IO cycles here or CPU

156
00:11:25.440 --> 00:11:27.329 
cycles and waiting for data.

157
00:11:27.330 --> 00:11:32.519 
So bandwidth is one of the most critical parts here in our technology.

158
00:11:32.520 --> 00:11:35.640 
Yeah, and just to give you another example,

159
00:11:37.170 --> 00:11:41.919 
and this is, I think, a very important one, if you want to create an accuracy

160
00:11:41.920 --> 00:11:46.679 
in an autonomous car driving about 20 percent higher accuracy

161
00:11:46.680 --> 00:11:50.604 
like an average human, it would take a fleet of one hundred cars,7x24

162
00:11:52.260 --> 00:11:56.940 
hours a day and at a speed of 50 kilometers per hour would require

163
00:11:58.860 --> 00:12:03.359 
388 years to to complete a data set that's necessary to

164
00:12:03.360 --> 00:12:05.399 
train such a system today.

165
00:12:05.400 --> 00:12:09.839 
That means there's a whole lot of things to be done here and considered to

166
00:12:09.840 --> 00:12:13.379 
to get the right data in time and in numbers.

167
00:12:14.730 --> 00:12:19.199 
So, big data we talked about, we have also the requirements of the

168
00:12:19.200 --> 00:12:23.699 
neural networks, the CPU's but now, of course, a new thing comes to the block.

169
00:12:23.700 --> 00:12:28.259 
It's called the accelerators and accelerators have a special function

170
00:12:28.260 --> 00:12:33.239 
because they are the drivers for getting these neural networks performing really,

171
00:12:33.240 --> 00:12:38.039 
really fast and this is something, of course, we could not see in the past because

172
00:12:38.040 --> 00:12:42.119 
actually technology needs did not allow that before.

173
00:12:42.120 --> 00:12:46.589 
One of the reasons is the so-called von-Neumann bottleneck and if every one of you

174
00:12:46.590 --> 00:12:51.119 
who knows about the design of a CPU knows that we actually have the problem

175
00:12:51.120 --> 00:12:55.559 
always between the CPU and the memory and the RAM, how

176
00:12:55.560 --> 00:13:00.030 
to access that memory quite fast and also the number of, let's say,

177
00:13:02.070 --> 00:13:06.599 
the AlU's in our CPU and the component we have

178
00:13:06.600 --> 00:13:11.579 
in here to have parallelization are, for example, matrix,

179
00:13:11.580 --> 00:13:14.849 
matrix, multiplication and things like that.

180
00:13:14.850 --> 00:13:19.379 
This is not something of traditional CPU was designed for, or therefore

181
00:13:19.380 --> 00:13:24.779 
new kids are now coming on the block and some of them, like GPU's or

182
00:13:24.780 --> 00:13:29.549 
IPU's, are specially designed for exactly for running such

183
00:13:29.550 --> 00:13:34.229 
kind of technologies and such kind of algorithms because

184
00:13:34.230 --> 00:13:39.029 
they have a much, much higher ratio, of course, and are really driven to

185
00:13:39.030 --> 00:13:44.009 
the right to the range where these operations can be done much more efficiently.

186
00:13:44.010 --> 00:13:48.449 
On the other side we see a lot of new technologies rising up, like quantum

187
00:13:48.450 --> 00:13:53.309 
computing. IBM also has a lot of knowledge in the quantum computing here,

188
00:13:53.310 --> 00:13:57.029 
which can be also utilized for AI in certain cases.

189
00:13:57.030 --> 00:14:01.259 
IBM TrueNorth chip, which has a very low footprint on energy.

190
00:14:01.260 --> 00:14:03.809 
We also have, let's say, impact here on the AI.

191
00:14:03.810 --> 00:14:08.399 
So there's a lot of development going on around the globe and you can

192
00:14:08.400 --> 00:14:13.169 
rest assured that in the future this is definitely something what we definitely see

193
00:14:13.170 --> 00:14:17.759 
to come with new technologies and very, very fast technologies that we haven't

194
00:14:17.760 --> 00:14:22.469 
seen before. Just another number here at that point, about six years ago,

195
00:14:22.470 --> 00:14:27.149 
the Google Brain consisted of approximately sixteen thousand systems running

196
00:14:27.150 --> 00:14:31.829 
at 50 teraflops. A single GPU today has about 120

197
00:14:31.830 --> 00:14:36.509 
teraflops. That means you have actually the power of three times Google

198
00:14:36.510 --> 00:14:41.219 
brain, of course, six years ago in the size of, let's say, a

199
00:14:41.220 --> 00:14:43.109 
stamp in your pocket.

200
00:14:43.110 --> 00:14:47.099 
This is really some some very astonishing technology shift.

201
00:14:47.100 --> 00:14:49.859 
And therefore, of course, that means A.I.

202
00:14:49.860 --> 00:14:54.959 
is becoming also a commodity and commoditized for everyone and can be integrated

203
00:14:54.960 --> 00:14:58.722 
and can be utilized quite easily rather than running in large data

204
00:14:59.770 --> 00:15:04.899 
farms. So what are the differences between the CPU's and the GPU'S,

205
00:15:04.900 --> 00:15:09.849 
so on the left hand side of the CPU, as I said, we have a limited number of cores,

206
00:15:09.850 --> 00:15:13.989 
we are also limited in threads and we are limited in throughput and calculations per

207
00:15:13.990 --> 00:15:18.189 
second. And this is, of course, a major difference what we see in the GPU'S, where we

208
00:15:18.190 --> 00:15:22.059 
have thousands of cores and thousands of threads going in parallel.

209
00:15:22.060 --> 00:15:26.529 
That means everything that can be parallelized, a GPU might be a very well

210
00:15:26.530 --> 00:15:28.809 
fit for doing that.

211
00:15:28.810 --> 00:15:33.399 
On the other side we also have to see how Power9 technology

212
00:15:33.400 --> 00:15:35.799 
drives exactly the bandwidth demand.

213
00:15:35.800 --> 00:15:40.449 
What I have seen before in our CORAL set up here in Oak Ridge, but

214
00:15:40.450 --> 00:15:44.949 
also of course, we see a lot of other things between the bandwidth between

215
00:15:44.950 --> 00:15:47.199 
the CPU and the GPU.

216
00:15:47.200 --> 00:15:51.639 
In a traditional PCI system the bandwidth between

217
00:15:51.640 --> 00:15:56.859 
the CPU and the GPU, is becoming more and more a bottleneck because

218
00:15:56.860 --> 00:16:01.269 
the data has always to bypass through that small bandwidth from our

219
00:16:02.830 --> 00:16:07.599 
PCI box and this is, of course, not sufficient to satisfy the

220
00:16:07.600 --> 00:16:12.399 
data hungry GPU'S with sufficient the number of data.

221
00:16:12.400 --> 00:16:16.869 
That means our systems here are really designed, especially

222
00:16:16.870 --> 00:16:21.759 
for this AI era. And with that system, of course, we can bring up quite

223
00:16:21.760 --> 00:16:26.379 
nicely a building block where you can actually build up on your own, let's

224
00:16:26.380 --> 00:16:30.939 
say summit cluster if you wish so but on a very much smaller footprint

225
00:16:30.940 --> 00:16:35.529 
as well. As I said, the GPU is

226
00:16:35.530 --> 00:16:39.669 
one of the most important components in such an AI endeavor.

227
00:16:39.670 --> 00:16:44.229 
That means in a traditional system we have, let's say our PCI

228
00:16:44.230 --> 00:16:48.669 
bus and the PCI bus is our bottleneck in our power9 system.

229
00:16:48.670 --> 00:16:53.319 
On the other side, you have a technology called NVLink 2.0 where we have

230
00:16:53.320 --> 00:16:58.239 
a 150 gigabytes in the four GPU systems

231
00:16:58.240 --> 00:17:02.799 
set up that gives you the flexibility and the performance and the throughput to

232
00:17:02.800 --> 00:17:04.689 
operate much, much better.

233
00:17:04.690 --> 00:17:09.189 
That gives you a throughput of more than five times against the traditional PCI

234
00:17:09.190 --> 00:17:13.689 
based machine. Only five times as much means also five times

235
00:17:13.690 --> 00:17:18.368 
time saving for your training, runs for your actually development, runs

236
00:17:18.369 --> 00:17:20.318 
for your AI Systems in here.

237
00:17:20.319 --> 00:17:24.789 
And you can do much more on that because the memory, as you can see here,

238
00:17:24.790 --> 00:17:27.489 
becomes also common, shared memory because of

239
00:17:29.350 --> 00:17:33.879 
NVLink 2.0. That means GPU RAM and CPU RAM can actually be in

240
00:17:33.880 --> 00:17:36.099 
one ratio, in one range.

241
00:17:36.100 --> 00:17:40.299 
And therefore, of course, you have a global area of memory on that as well.

242
00:17:42.070 --> 00:17:46.719 
On the data pipeline, as I said, we have the five faces in here.

243
00:17:46.720 --> 00:17:51.249 
And the most important face actually in that is not just only the learning

244
00:17:51.250 --> 00:17:56.019 
path, it's important. But the most time consuming path is actually

245
00:17:56.020 --> 00:17:58.959 
the collection and the preparation of the data.

246
00:17:58.960 --> 00:18:03.429 
For that we have to consider that takes about 80 percent

247
00:18:03.430 --> 00:18:07.959 
of our development efforts today with IBM technology

248
00:18:07.960 --> 00:18:10.629 
and what we have actually on the software part as well.

249
00:18:10.630 --> 00:18:15.309 
We have a lot of things that can help you to get that

250
00:18:15.310 --> 00:18:20.949 
data phases much more quickly defined and quickly developed.

251
00:18:20.950 --> 00:18:25.749 
That means actually when we start from the data aggregation through the data storage

252
00:18:25.750 --> 00:18:30.579 
to the transformation and validation paths, there are a lot of possibilities

253
00:18:30.580 --> 00:18:35.559 
what IBM can bring in on technology here with IBM AI

254
00:18:35.560 --> 00:18:40.059 
vision, where we, for example, have automated labeling systems that can help

255
00:18:40.060 --> 00:18:44.589 
you to label such data automatically, get you a better accuracy, and

256
00:18:44.590 --> 00:18:48.129 
therefore, of course, a higher throughput in your training runs.

257
00:18:48.130 --> 00:18:52.779 
In those phases you always have to understand also about what other requirements

258
00:18:52.780 --> 00:18:57.699 
in each individual step interface about the data validation, for example,

259
00:18:57.700 --> 00:19:00.489 
or the volume of the data that needs to be stored.

260
00:19:00.490 --> 00:19:04.479 
What is actually my storage, my underlying storage.

261
00:19:04.480 --> 00:19:09.519 
IBM has a system called Spectrum Scale, which can give you the flexibility

262
00:19:09.520 --> 00:19:12.720 
of a tiered storage concept that you can use NVME,

263
00:19:14.050 --> 00:19:18.639 
but also SAS disks, for example, or SATAs or even cloud storage

264
00:19:18.640 --> 00:19:23.409 
if you wish so and can run it as one single instance and one single few

265
00:19:23.410 --> 00:19:26.109 
here for your data management.

266
00:19:26.110 --> 00:19:31.419 
Copies and other feature what we build in our system that helps you eliminate,

267
00:19:31.420 --> 00:19:35.993 
eliminate the necessity of having data access to

268
00:19:37.030 --> 00:19:41.724 
the CPU by reducing CPU cycles from 20

269
00:19:41.725 --> 00:19:45.819 
thousand to about 300 for a single IO.

270
00:19:45.820 --> 00:19:49.296 
But IBM is also working closely with a lot of partners in the openPower world,

271
00:19:50.320 --> 00:19:54.939 
also, as you see, we have open power partners here like Mellanox and Nvidia, but also

272
00:19:54.940 --> 00:19:56.608 
Xilinx and others.

273
00:19:57.670 --> 00:20:02.679 
And on the software side we see a lot of things to rise here, like

274
00:20:02.680 --> 00:20:07.269 
Spark, MongoDB, EDB, but also here in Germany, we have a lot

275
00:20:07.270 --> 00:20:11.739 
of partners here building up new workloads based on A.I., based on

276
00:20:11.740 --> 00:20:16.749 
training systems here, where you can use also on IBM power.

277
00:20:16.750 --> 00:20:20.209 
Open frameworks we have a lot of them today.

278
00:20:20.210 --> 00:20:24.639 
And there is still continuing, of course, a new rise

279
00:20:24.640 --> 00:20:29.109 
of of of of frameworks like Caffe2, Pytorch and others

280
00:20:29.110 --> 00:20:33.789 
to come tensorflow, Keras, you all know them, but of course you

281
00:20:33.790 --> 00:20:38.649 
can rest assured, assured that those frameworks are always running with performance

282
00:20:38.650 --> 00:20:41.439 
also on the IBM Power9 Systems.

283
00:20:41.440 --> 00:20:46.119 
OpenCAPI also, as you see, our competitors are taking pace on here.

284
00:20:46.120 --> 00:20:50.619 
They actually want to implement also openCAPI in their own system, that means

285
00:20:50.620 --> 00:20:55.149 
this is an open world in IBM actually opens the technology

286
00:20:55.150 --> 00:20:56.529 
possibilities on here.

287
00:20:57.880 --> 00:21:02.409 
On the software stack side, we actually have, as I said before,

288
00:21:02.410 --> 00:21:07.059 
a whole bundle of software that can help to drive, especially

289
00:21:07.060 --> 00:21:11.049 
the demand here for building up such a system in a more robust way.

290
00:21:11.050 --> 00:21:15.459 
You can do it in two flavors. You can either build up your own stack, build your own,

291
00:21:15.460 --> 00:21:20.169 
bring your own with open source libraries, with open source components,

292
00:21:20.170 --> 00:21:24.639 
that's fine. But in many cases, actually, customers have the need to

293
00:21:24.640 --> 00:21:29.379 
have, let's say, a more robust development environment and a trusted environment

294
00:21:29.380 --> 00:21:31.779 
where they can build on their experience.

295
00:21:31.780 --> 00:21:36.519 
And IBM Watson ML accelerator and the with spectrum

296
00:21:36.520 --> 00:21:41.199 
conductor integrated is actually one of those tools where you have,

297
00:21:41.200 --> 00:21:45.639 
let's say, a building block in your life cycle pipeline to build

298
00:21:45.640 --> 00:21:50.139 
up an AI system. In combination with Watson studio, you actually

299
00:21:50.140 --> 00:21:54.939 
can really reach out to your end users, to developers.

300
00:21:54.940 --> 00:21:59.469 
That means you have this whole process under control and not the hassle and

301
00:21:59.470 --> 00:22:01.509 
doing all your things on your own.

302
00:22:03.310 --> 00:22:07.749 
And to finalize a presentation here, just give you some examples of

303
00:22:07.750 --> 00:22:12.519 
what is possible to do today with tools like AI

304
00:22:12.520 --> 00:22:17.649 
or Watson ML accelerator here in the first

305
00:22:17.650 --> 00:22:22.539 
image we see, for example, some very simple thing, but it's called mask detection

306
00:22:22.540 --> 00:22:26.079 
here. For example, if you have a bank robber somewhere sitting there in the bank.

307
00:22:26.080 --> 00:22:30.609 
So it could actually see there is a guy coming in with a mask on its face

308
00:22:30.610 --> 00:22:35.049 
and maybe then automatic alerts would start automatically or somebody would

309
00:22:35.050 --> 00:22:37.479 
actually get notice about that.

310
00:22:37.480 --> 00:22:42.099 
This is, of course, something which can be done quite quickly and easily and this

311
00:22:42.100 --> 00:22:46.539 
can be done also implemented on cameras or on sensors, however you

312
00:22:46.540 --> 00:22:47.540 
would like to do it.

313
00:22:48.790 --> 00:22:53.319 
On the other hand, we have also possibilities for drone surveillance, not just only for

314
00:22:53.320 --> 00:22:57.969 
parking lots or people movement in crowds but you mentioned everything where

315
00:22:57.970 --> 00:23:02.499 
a drone can be of help in controlling, let's say, trash on the

316
00:23:02.500 --> 00:23:07.329 
beaches of the oceans, or, for example, what we have seen getting

317
00:23:07.330 --> 00:23:11.949 
rid of poachers in Africa who are running for rhinos or something

318
00:23:11.950 --> 00:23:15.549 
like that, wherever drones might actually be of help.

319
00:23:15.550 --> 00:23:20.409 
So as an AI system definitely can be of help in here and the inference

320
00:23:20.410 --> 00:23:24.999 
model can be transported then from our systems, from our power line system

321
00:23:25.000 --> 00:23:26.000 
or the drone.

322
00:23:27.090 --> 00:23:31.979 
A car, a smarter and safer cities, for example, traffic

323
00:23:31.980 --> 00:23:37.439 
control in the cities, how people are behaving, travel control

324
00:23:37.440 --> 00:23:41.939 
in any dimension, that's actually something we can what we can see here.

325
00:23:41.940 --> 00:23:44.429 
For example, parking regulations.

326
00:23:44.430 --> 00:23:49.019 
So this is all very much depending on the use case, but it just should show

327
00:23:49.020 --> 00:23:52.770 
you about the possibilities, what can be established today.

328
00:23:53.820 --> 00:23:57.899 
A very important part here and this is one of my, let's say, hobbies here in Germany is

329
00:23:57.900 --> 00:24:02.549 
actually document and archive digitalization and probably,

330
00:24:02.550 --> 00:24:06.779 
you know, Germany is not one of the countries with the best, let's say, digitalization

331
00:24:06.780 --> 00:24:09.149 
ratio here, especially in public services.

332
00:24:09.150 --> 00:24:13.859 
So therefore, of course, we have a high demand here for getting that old archives

333
00:24:13.860 --> 00:24:18.359 
and files away on paper to, let's say, automated

334
00:24:18.360 --> 00:24:22.799 
systems. So that means there are a lot of data sitting there

335
00:24:22.800 --> 00:24:27.749 
and that costs millions of working hours spent on search before actionable results

336
00:24:27.750 --> 00:24:32.459 
every year. Just sees this is a software we have built

337
00:24:32.460 --> 00:24:37.169 
together with a software company here in the next in Germany.

338
00:24:37.170 --> 00:24:42.059 
And this software company, actually, they have created a system that is able

339
00:24:42.060 --> 00:24:46.799 
to read and understand and transcript handwritten forms in any

340
00:24:46.800 --> 00:24:50.839 
kind of language, in any kind of writing.

341
00:24:50.840 --> 00:24:55.859 
That means that's a good thing, actually, if you want to take that part of a software

342
00:24:55.860 --> 00:24:58.469 
automatically in your business process.

343
00:24:58.470 --> 00:25:01.529 
In our little example here, we are just looking for words.

344
00:25:01.530 --> 00:25:06.209 
But imagine if you can really take out such information from forms

345
00:25:06.210 --> 00:25:10.769 
and can process it in an SAP process, by the way, or something like that, where

346
00:25:10.770 --> 00:25:15.269 
you actually have a description of a doctor or something, you can

347
00:25:15.270 --> 00:25:17.339 
put it to your insurance company.

348
00:25:17.340 --> 00:25:22.019 
That's all can be done automatically with the accuracy of, let's say, higher

349
00:25:22.020 --> 00:25:26.699 
than 90 percent. With that particular software, we can run about sixty

350
00:25:26.700 --> 00:25:31.349 
thousand pages of handwriting per hour on a single system, on a single

351
00:25:31.350 --> 00:25:35.909 
power line system that gives you the flexibility and the performance indication

352
00:25:35.910 --> 00:25:40.259 
where we are heading to. And this is, of course, approximately the size.

353
00:25:40.260 --> 00:25:44.759 
If the same thing would be done by humans, you would take about one thousand

354
00:25:44.760 --> 00:25:49.259 
four hundred people eight hours working a day for completing that single

355
00:25:49.260 --> 00:25:51.480 
task. That's really astonishing, isn't it?

356
00:25:53.040 --> 00:25:57.629 
But I think before we conclude on that session,

357
00:25:57.630 --> 00:26:02.159 
I think the future and people usually say, why don't we do it on

358
00:26:02.160 --> 00:26:06.479 
cloud? And I think I'm pretty sure that the future is a very hybrid future.

359
00:26:06.480 --> 00:26:09.989 
We will have data resting on side in data centers.

360
00:26:09.990 --> 00:26:13.919 
We will see a lot of data is flowing from the cloud or in the cloud.

361
00:26:13.920 --> 00:26:18.389 
And you can see that there is definitely a demand to do some AI systems within the

362
00:26:18.390 --> 00:26:23.099 
cloud because the data should stay there where the actually the data

363
00:26:23.100 --> 00:26:25.109 
storage to be stored to.

364
00:26:25.110 --> 00:26:27.419 
The cloud might be an option on that.

365
00:26:27.420 --> 00:26:32.189 
And the question is how to be integrated with our on premise system and the answer

366
00:26:32.190 --> 00:26:35.460 
for that is we call it a hybrid system in the hybrid world.

367
00:26:36.840 --> 00:26:41.309 
That will actually finally bring me to that charge where we can

368
00:26:41.310 --> 00:26:46.049 
see there's definitely a combination of multiple data sources.

369
00:26:46.050 --> 00:26:50.759 
And this closes our little circle here also where we see that data

370
00:26:50.760 --> 00:26:53.489 
is flowing from any kind of data source.

371
00:26:53.490 --> 00:26:57.389 
And at the end of the day, this is really something what we can see in the future of

372
00:26:57.390 --> 00:27:02.009 
General AI, where systems actually can reason and conclude on

373
00:27:02.010 --> 00:27:06.839 
various inputs, on various parameters from around the globe, from around

374
00:27:06.840 --> 00:27:09.989 
other systems here. And therefore, of course, that's a

375
00:27:11.550 --> 00:27:16.079 
universe what we call the digital universe by the way,

376
00:27:16.080 --> 00:27:18.210 
this is going to happen very soon.

377
00:27:19.830 --> 00:27:24.359 
Last not least, there are some considerations always you have to consider

378
00:27:24.360 --> 00:27:29.249 
because AI is not a self sufficient implementation,

379
00:27:29.250 --> 00:27:33.754 
you always have to consider how do I orchestrate my AI

380
00:27:33.755 --> 00:27:36.029 
system in my organization?

381
00:27:36.030 --> 00:27:40.469 
Because presently today, just only five percent of all developments are really

382
00:27:40.470 --> 00:27:42.729 
ranging up to the production level.

383
00:27:42.730 --> 00:27:47.789 
That means a lot of time is really in research and development.

384
00:27:47.790 --> 00:27:52.019 
But of course, we have to focus also on the orchestration level in the production.

385
00:27:52.020 --> 00:27:55.619 
Ethics a very important point here we didn't touch today.

386
00:27:55.620 --> 00:27:58.319 
Data governance, who's accessing the data?

387
00:27:58.320 --> 00:28:00.749 
How long does it take time to market?

388
00:28:00.750 --> 00:28:03.569 
How do I integrate legacy systems here?

389
00:28:03.570 --> 00:28:06.749 
Do I have to consider some legal requirements?

390
00:28:06.750 --> 00:28:10.079 
In many countries we have different legal requirements.

391
00:28:10.080 --> 00:28:14.729 
So ground truth of the data, as I said before, do we need actually to consider

392
00:28:14.730 --> 00:28:19.229 
security or even patterns? What we have achieved is our development

393
00:28:19.230 --> 00:28:21.389 
compatibility with other systems.

394
00:28:21.390 --> 00:28:26.009 
How do we integrate and interact availability concepts of our high

395
00:28:26.010 --> 00:28:30.479 
concept? Because if you have to trust such a system, you have to also trust the

396
00:28:30.480 --> 00:28:34.209 
technology to make that system happen all the time.

397
00:28:34.210 --> 00:28:38.969 
Compliance issues, if you actually have some systems that make decisions

398
00:28:38.970 --> 00:28:41.909 
on their own. Do we have to consider compliance?

399
00:28:41.910 --> 00:28:45.749 
And a very important point, as last not least, is also privacy.

400
00:28:45.750 --> 00:28:50.219 
So, this is a couple of considerations and each of those would fill a full session

401
00:28:50.220 --> 00:28:54.719 
here, probably but of course, at the end of my presentation, this is just one

402
00:28:54.720 --> 00:28:59.169 
thing I say that we always should keep in mind that these are something

403
00:28:59.170 --> 00:29:04.229 
we need to consider when building up the infrastructure and AI architecture

404
00:29:04.230 --> 00:29:08.910 
and remember, there's no AI without IA, thank you very much.
