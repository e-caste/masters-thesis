WEBVTT

1
00:00:01.040 --> 00:00:04.040 
Hello and welcome!

2
00:00:04.040 --> 00:00:05.080 
In this video

3
00:00:05.089 --> 00:00:14.740 
we will discuss a prevalent method for deep model compression, the
Compact network design,

4
00:00:14.740 --> 00:00:24.469 
Image classification models serve as cornerstones in the current
AI revolution. The various network architectures have evolved

5
00:00:24.469 --> 00:00:26.339 
over the past few years.

6
00:00:26.350 --> 00:00:31.359 
Just like Tony Stark's mobile suit, where to the best of our knowledge,

7
00:00:31.370 --> 00:00:40.850 
the MobileNet v3 model so far is probably always always the
most efficient and popular backbone model for mobile applications.

8
00:00:41.939 --> 00:00:49.350 
We can see that the most popular backbone model in the
downstream applications is ResNet and its variants.

9
00:00:50.039 --> 00:00:54.630 
The compact network design evolving from the right to the center

10
00:00:54.640 --> 00:01:04.870 
in the model architecture designed by experts and the evolution from
the left to center is to conduct a more efficient model

11
00:01:04.870 --> 00:01:08.010 
architecture through the neural architecture search,

12
00:01:08.019 --> 00:01:17.810 
so, the automatic search method. In the end we may gather in the
compact network model of MobileNet v3 which is

13
00:01:17.810 --> 00:01:25.459 
a combination of handcrafted basic block design and neural
architect search (NAS) for the whole structure.

14
00:01:31.640 --> 00:01:38.359 
This figure demonstrates the trend of deep learning models designed
in the recent years.

15
00:01:38.739 --> 00:01:42.000 
The histogram shows the computation complexity,

16
00:01:42.010 --> 00:01:55.060 
for example, for one simple inference of the resNet 152 model
needs 3870 million multiply-addition operations.

17
00:01:56.040 --> 00:02:01.750 
The line chart shows the top1 classification accuracy on imageNet
dataset.

18
00:02:02.640 --> 00:02:13.370 
We can see that the complexity of the model becomes smaller,
and its accuracy gradually rises from the initial decline.

19
00:02:13.370 --> 00:02:15.650 
From 2015 to 2019

20
00:02:15.659 --> 00:02:26.340 
the decrease in model complexity is already an order of magnitude.

21
00:02:26.340 --> 00:02:36.189 
And we predict that this trend will continue, and the
complexity will further decrease while maintaining roughly
the same

22
00:02:36.189 --> 00:02:46.710 
accuracy level, because we believe that this level of accuracy has
been accepted by many applications. Keep the accuracy the same,

23
00:02:46.719 --> 00:02:53.460 
and further improve the efficiency will enable more and
more applications on the edge and client.

24
00:02:58.639 --> 00:03:09.340 
Okay, SueezeNet introduces a new building block "Fire moduleâ€,
aimed at reducing the model size while maintaining the AlexNet

25
00:03:09.340 --> 00:03:10.860 
level of accuracy.

26
00:03:12.240 --> 00:03:20.000 
Given a budget of a certain number of convolution filters,
squeezeNet chooses to make the majority of these 1x1

27
00:03:20.000 --> 00:03:20.810 
filters.

28
00:03:20.819 --> 00:03:27.259 
Since then a 1x1 filter has 9X fewer
parameters than a 3x3 filter.

29
00:03:28.340 --> 00:03:37.509 
Second, to maintain a small number of parameters in a ConvNet,
it is essential to decrease the number of 3x3 filters,

30
00:03:37.520 --> 00:03:41.150 
and the number of input channels to those filters.

31
00:03:41.840 --> 00:03:49.539 
Thus, in its so-called squeeze layers, it
decreases the number of input channels to all 3x3 filters.

32
00:03:49.550 --> 00:03:57.229 
The third design is Downsample late in the network.

33
00:03:57.240 --> 00:04:05.759 
So, maintaining the large activation maps. The intuition is that
large activation maps due to the delayed

34
00:04:05.759 --> 00:04:12.560 
downsampling can lead to higher classification accuracy because
you're actually maintaining rich information.

35
00:04:13.139 --> 00:04:22.980 
So, previous works also confirm this assumption, they applied
delayed downsampling to four different CNN architectures,

36
00:04:22.990 --> 00:04:28.860 
and in each case delayed downsampling led to higher
classification accuracy.

37
00:04:29.339 --> 00:04:38.350 
But this decision will also increase the memory computation cost and
the memory consumption. For instance, the Flops, the

38
00:04:38.350 --> 00:04:46.350 
computation costs Flops, which have been ignored in this work, they
didn't measure the Flops in their evaluation.

39
00:04:46.370 --> 00:04:49.959 
They only focused on the reduction of parameters.

40
00:04:52.240 --> 00:04:56.160 
The final SqueezeNet is built on the stack of fire module.

41
00:04:56.170 --> 00:05:00.110 
This paper introduces two slightly different architectural designs.

42
00:05:00.120 --> 00:05:06.759 
You can see that the network on the right has added a
shortcut connection to the fire module.

43
00:05:07.939 --> 00:05:12.459 
The corresponding accuracy has also been significantly improved.

44
00:05:13.939 --> 00:05:19.279 
However, we have a clear understanding of the performance of
shortcut connections in resnet.

45
00:05:19.279 --> 00:05:23.339 
Through the table on the right,

46
00:05:23.339 --> 00:05:34.810 
we can see that based on the existing squeeze net,
the author can continue to compress the size of the
model to only 0.47MB by using

47
00:05:34.810 --> 00:05:44.310 
the compression technology in the deep compression paper and
maintaining the accuracy alexNet level. It achieves 510 times

48
00:05:44.310 --> 00:05:45.660 
of model size reduction.

49
00:05:46.240 --> 00:05:54.860 
Deep compression is a method that uses Hoffmann coding and lookup
table technique to quantize the parameters to lower

50
00:05:54.860 --> 00:06:05.370 
bit-width. SqueezeNet can compress the model size extremely and
keep the accuracy not dropping. However, I think that

51
00:06:05.379 --> 00:06:09.050 
its design ignores two crucial issues.

52
00:06:09.639 --> 00:06:19.620 
First of all, when ConvNet inference, the memory required by
the weights of the model is small compared with the memory footprint

53
00:06:19.620 --> 00:06:29.519 
of its feature map. Therefore, only the model size is compressed,
but the late downsampling is used, which does not effectively reduce

54
00:06:29.519 --> 00:06:37.660 
the memory and energy consumption. Because they decide to use the
late downsampling strategy for their activation map.

55
00:06:38.439 --> 00:06:48.410 
Second, it completely ignores the factor of computational
complexity. Although the model size is small, the
computational complexity

56
00:06:48.439 --> 00:06:51.949 
may be higher due to late downsampling as well.

57
00:06:52.540 --> 00:07:01.839 
These two critical issues are unfortunately not discussed in
depth in the paper.

58
00:07:01.839 --> 00:07:09.829 
Before we move to the next network architecture, lets
first recap the group convolution. In the standard convolution layer, If

59
00:07:09.829 --> 00:07:18.129 
you have an input feature tensor with width, height, and channel number.
We have a convolution filter with the kernel size K

60
00:07:18.129 --> 00:07:20.250 
times K times channel number.

61
00:07:20.939 --> 00:07:24.860 
So, basically both channel filters are the same.

62
00:07:25.839 --> 00:07:32.639 
And after the convolution operation we have an output feature
tensor with the same size H-

63
00:07:32.639 --> 00:07:33.920 
prime and W-

64
00:07:33.920 --> 00:07:34.459 
Prime.

65
00:07:35.040 --> 00:07:45.839 
So, if we have another filter, here indicated by
another color, then we have another output feature map.

66
00:07:45.839 --> 00:07:49.439 
Group convolution introduces a new hyperparameter:

67
00:07:49.740 --> 00:07:51.060 
the group number g.

68
00:07:51.740 --> 00:07:56.360 
The number of channels C will be evenly divided by g.

69
00:07:57.139 --> 00:08:00.319 
And we will do the same to the convolution filter.

70
00:08:00.560 --> 00:08:03.649 
Now, we can see that have a group number equals 2,

71
00:08:04.040 --> 00:08:13.810 
the input tensor will be divided into two groups, and assume
that we have two conv filters with the channel number equals

72
00:08:13.810 --> 00:08:14.860 
the half of C.

73
00:08:16.339 --> 00:08:19.829 
So, each convolution kernel will only be applied to

74
00:08:19.829 --> 00:08:25.290 
the corresponding group, and it will not be involved with
another groups.

75
00:08:25.519 --> 00:08:35.149 
In other words, the groups are relatively independent. So,
the receptive field of each convolution kernel has been
reduced

76
00:08:35.159 --> 00:08:40.860 
in this case and the computation overhead has also been largely reduced.

77
00:08:44.440 --> 00:08:52.450 
Xception Network first time introduced depthwise
separable convolutio. What is depthwise separable?

78
00:08:52.940 --> 00:09:01.960 
In short, we can simply consider this kind of convolution maximizes
the decoupling of spatial correlation and channel correlation.

79
00:09:02.740 --> 00:09:09.659 
It shows an extreme case of group convolution that
the group number g equals the input channel number.

80
00:09:10.440 --> 00:09:16.710 
So each input feature map will be independently
scanned by a conv filter

81
00:09:16.720 --> 00:09:18.960 
with the channel equals 1.

82
00:09:20.740 --> 00:09:25.149 
In this case the receptive field equals K times K.

83
00:09:25.639 --> 00:09:27.100 
It becomes smaller.

84
00:09:27.110 --> 00:09:35.019 
The output feature maps only learn the spatial correlations.
The channel correlations will be aggregated later by applying

85
00:09:35.029 --> 00:09:40.539 
another 1x1 convolution.

86
00:09:40.539 --> 00:09:45.360 
Between standard convolution and depthwise
separable convolution,

87
00:09:45.940 --> 00:09:50.259 
it is considered that there is a discrete spectrum,

88
00:09:50.840 --> 00:10:00.889 
then each input channel corresponds to a frequency, and the number of
spatial convolutions parameterizes this spectrum. And standard

89
00:10:00.889 --> 00:10:06.159 
convolution  integrates various frequencies
in the spectrum into one,

90
00:10:07.539 --> 00:10:18.789 
the inception module integrates them into 3-4 groups, and the
depthwise separable convolution performs convolution

91
00:10:18.789 --> 00:10:21.059 
operations on each frequency.

92
00:10:22.139 --> 00:10:30.149 
There is an intermediate state between the inception module
structure and the depth wise separable convolution.

93
00:10:30.740 --> 00:10:40.009 
This paper has a milestone significance and the depth wise
separable convolution has become the cornerstone of a series

94
00:10:40.009 --> 00:10:42.860 
of subsequent compact network designs.

95
00:10:45.440 --> 00:10:46.460 
Thank you for watching.
