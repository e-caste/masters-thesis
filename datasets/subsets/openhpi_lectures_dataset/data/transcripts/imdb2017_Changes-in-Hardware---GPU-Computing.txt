WEBVTT

1
00:00:01.100 --> 00:00:04.400 
Let's spend the next
ten minutes or so and

2
00:00:04.400 --> 00:00:06.600 
take a look into the
side of accelerators.

3
00:00:06.600 --> 00:00:10.100 
As Markus already mentioned,
CPUs are generally

4
00:00:10.100 --> 00:00:13.130 
great at executing all different
types of tasks. So you could

5
00:00:13.130 --> 00:00:16.160 
use them for, of course
enterprise workloads,

6
00:00:16.160 --> 00:00:18.180 
you could have your weather
predictions down there,

7
00:00:19.190 --> 00:00:22.220 
you can even play games or watch
videos but for some of the tasks,

8
00:00:22.220 --> 00:00:26.260 
especially last ones, there
exists specialized hardware

9
00:00:26.260 --> 00:00:29.290 
that performs way
better. So GPUs have been

10
00:00:29.290 --> 00:00:33.330 
around for years
and they have been

11
00:00:34.340 --> 00:00:38.380 
built on purpose for 3D
rendering with quite a lot

12
00:00:38.380 --> 00:00:41.410 
of parallel floating
point computations.

13
00:00:41.410 --> 00:00:43.430 
So they are basically
built for the 3D

14
00:00:43.430 --> 00:00:46.460 
processing but it
turns out that those

15
00:00:47.470 --> 00:00:51.510 
GPUs or generally some other
accelerators as well, also can

16
00:00:51.510 --> 00:00:54.540 
be used in other workloads
and quite recently

17
00:00:55.550 --> 00:00:58.580 
as Markus mentioned,
deep learning

18
00:00:58.580 --> 00:01:01.610 
for example or if you look
at a other ones besides

19
00:01:01.610 --> 00:01:05.650 
a GPU like in FPGA down
there on the bottom

20
00:01:06.660 --> 00:01:09.690 
or the Intel Xeon
Phi co-processor.

21
00:01:10.700 --> 00:01:12.720 
They have being
used for other

22
00:01:12.720 --> 00:01:16.760 
applications as well,
but we want to focus on

23
00:01:16.760 --> 00:01:21.810 
GPUs in the next couple of
minutes. To give some numbers here

24
00:01:21.810 --> 00:01:24.840 
that is, I think it's not
the top notch Intel Xeon

25
00:01:24.840 --> 00:01:26.860 
processor any longer because
I just learned that there's

26
00:01:27.870 --> 00:01:28.880 
some out there
with the 28 cores

27
00:01:29.890 --> 00:01:33.930 
and compare that
with the recent

28
00:01:33.930 --> 00:01:37.970 
NVIDIA Tesla V100
which I learned about

29
00:01:37.970 --> 00:01:43.103 
today has been given
to some of the first

30
00:01:43.103 --> 00:01:47.107 
A.I. researchers. You can
actually see that there is quite a

31
00:01:47.107 --> 00:01:52.112 
big difference in peak
performance for single precision

32
00:01:52.112 --> 00:01:53.113 
floating point
computations

33
00:01:54.114 --> 00:01:59.119 
also GPUs benefit from the
memory bandwidth because

34
00:01:59.119 --> 00:02:01.121 
of high bandwidth
memory that is on chip,

35
00:02:01.121 --> 00:02:04.124 
on the graphics card, so it
can actually go up to 900

36
00:02:04.124 --> 00:02:07.127 
gigabytes per second
there and it's got

37
00:02:08.128 --> 00:02:11.131 
80 streaming multiprocessors
which are comparable

38
00:02:11.131 --> 00:02:15.135 
to the CPU cores, with
some limitations of course,

39
00:02:15.135 --> 00:02:18.138 
and purchasing
price, it's not been

40
00:02:18.138 --> 00:02:21.141 
not been announced yet
but it will probably be

41
00:02:21.141 --> 00:02:24.144 
even a little bit more expensive,
what I would think, than

42
00:02:24.144 --> 00:02:27.147 
the into Intel Xeon
processor in comparison here

43
00:02:28.148 --> 00:02:32.152 
and there's a huge but, it has
drawbacks as well if you use

44
00:02:32.152 --> 00:02:36.156 
a GPU, because the high bandwidth
memory that brings us the

45
00:02:36.156 --> 00:02:40.160 
great memory bandwidth
is quite limited. So for

46
00:02:40.160 --> 00:02:44.164 
for this specific graphics card
we're down to 16 gigabytes.

47
00:02:44.164 --> 00:02:49.169 
You would also always have
to point in additional time

48
00:02:49.169 --> 00:02:53.173 
to transfer memory from
DRAM to the device.

49
00:02:54.174 --> 00:02:56.176 
There is some
some recent

50
00:02:56.176 --> 00:03:00.180 
trends going from
PCI express towards,

51
00:03:00.180 --> 00:03:03.183 
especially with NVIDIA, towards
NV Link which is a proprietary

52
00:03:03.183 --> 00:03:06.186 
interconnect and they
provide better numbers there

53
00:03:06.186 --> 00:03:09.189 
and also to really utilize
hardware such as a GPU

54
00:03:10.190 --> 00:03:12.192 
you need to have
highly parallel tasks.

55
00:03:12.192 --> 00:03:16.196 
Generally, just going over the
characteristics as mentioned

56
00:03:16.196 --> 00:03:18.198 
before, so you
need to have

57
00:03:18.198 --> 00:03:22.202 
a high degree of data
parallelism which is true for

58
00:03:22.202 --> 00:03:26.206 
3D rendering for example which
is also true for deep learning

59
00:03:26.206 --> 00:03:29.209 
when you do the whole matrix
computation so there's

60
00:03:29.209 --> 00:03:33.213 
quite a lot of machine
learning algorithms that

61
00:03:33.213 --> 00:03:37.217 
can benefit here, also you want
to have vector-based processing

62
00:03:37.217 --> 00:03:40.220 
and as mentioned before,
computational-intensive because you don't

63
00:03:40.220 --> 00:03:43.223 
want to just transfer data
once and never re-use it again,

64
00:03:44.224 --> 00:03:46.226 
you have the transfer time,
that's always an overhead.

65
00:03:47.227 --> 00:03:50.230 
Example use cases as
mentioned, deep learning,

66
00:03:51.231 --> 00:03:54.234 
there's also, for example,
enterprise simulations

67
00:03:54.234 --> 00:03:57.237 
using linear equation solving
because that also boils down

68
00:03:57.237 --> 00:04:00.240 
to matrix operations at the
end and they are quite powerful

69
00:04:00.240 --> 00:04:04.244 
there, and also database
management systems and that

70
00:04:04.244 --> 00:04:08.248 
is what I want to point
out here a little bit more.

71
00:04:09.249 --> 00:04:12.252 
So, what would be the
opportunities here?

72
00:04:12.252 --> 00:04:15.255 
First of all we have the
higher bandwidth memory

73
00:04:16.256 --> 00:04:19.259 
and that gives us, especially
when we are memory bound,

74
00:04:19.259 --> 00:04:23.000 
that gives us quite an opportunity.
Also, we have the large number

75
00:04:23.000 --> 00:04:25.265 
of computational cores,
if we do for example

76
00:04:26.266 --> 00:04:29.269 
the operations quite often
we can benefit there,

77
00:04:29.269 --> 00:04:33.273 
again limited available memory and
there are strategies as mentioned

78
00:04:34.274 --> 00:04:38.278 
by professor Plattner, to avoid
this also the time to transfer

79
00:04:39.279 --> 00:04:41.281 
of course you want to
reduce that as well

80
00:04:41.281 --> 00:04:45.285 
and there are also, if you
want to consider the GPU as

81
00:04:45.285 --> 00:04:49.289 
a main storage or as a main
processing unit, you would also have

82
00:04:49.289 --> 00:04:53.293 
to cope with other problems
like under-utilization

83
00:04:53.293 --> 00:04:56.296 
when you do have your
OLTP transactions

84
00:04:56.296 --> 00:05:00.300 
that's why you would try
to stick them together

85
00:05:00.300 --> 00:05:03.303 
also you would try to avoid
a complex control flow.

86
00:05:04.304 --> 00:05:07.307 
There is also other
accelerators, i just wanted to

87
00:05:08.308 --> 00:05:11.311 
briefly skip over them just to
not leave them out, so there is

88
00:05:12.312 --> 00:05:15.315 
the Xeon Phi
co-processor from Intel

89
00:05:16.316 --> 00:05:19.319 
also with high bandwidth memory
and quite a lot of cores.

90
00:05:19.319 --> 00:05:23.323 
There is, FPGA is quite
interesting because you can

91
00:05:23.323 --> 00:05:27.327 
program them and change them
according to your workloads

92
00:05:28.328 --> 00:05:32.332 
and something interesting
from the machine learning

93
00:05:32.332 --> 00:05:36.336 
side, the Tensor processing unit
that had been developed at Google so

94
00:05:36.336 --> 00:05:39.339 
they built their own
processes especially for their

95
00:05:39.339 --> 00:05:42.342 
machine learning
workload demands

96
00:05:42.342 --> 00:05:46.346 
and it's not open to public,
they use it themselves

97
00:05:46.346 --> 00:05:49.349 
that's just to some of
the accelerator part.

98
00:05:51.351 --> 00:05:53.353 
Markus: Ok, to quickly
wrap that up...

99
00:05:53.353 --> 00:05:57.357 
We have been talking about accelerators
here - as you can see in the slide

100
00:05:57.357 --> 00:06:00.360 
accelerators are still
connected to the CPU

101
00:06:00.360 --> 00:06:04.364 
so whenever we access data,
we first have to ask the CPU

102
00:06:04.364 --> 00:06:08.368 
and we get it from the memory through
the memory controller of the CPU.

103
00:06:08.368 --> 00:06:13.373 
Another, some
initiatives that

104
00:06:13.373 --> 00:06:16.376 
try to find a way to move
the accelerator closer

105
00:06:16.376 --> 00:06:20.380 
to the memory, so you have
three consortiums right now

106
00:06:20.380 --> 00:06:24.384 
that are working together to
actually get those accelerators

107
00:06:24.384 --> 00:06:27.387 
closer to memory and
finally to increase

108
00:06:27.387 --> 00:06:32.392 
the bandwidth and
decrease the latency.

109
00:06:32.392 --> 00:06:36.396 
So, at some point we might
overcome that problem

110
00:06:36.396 --> 00:06:40.400 
that the accelerators are
limited by PCI express.

111
00:06:41.401 --> 00:06:44.404 
Now one thing that we
haven't talked about yet

112
00:06:44.404 --> 00:06:48.408 
is the network interface
here and I am just going

113
00:06:48.408 --> 00:06:51.411 
to skip over that really
quickly, one thing that we would

114
00:06:51.411 --> 00:06:54.414 
want you to be aware of is that
the network interface, as a

115
00:06:54.414 --> 00:06:58.418 
separate entity, is slowly
disappearing. There are

116
00:06:58.418 --> 00:07:02.000 
some CPUs that actually
bring the network interface

117
00:07:02.000 --> 00:07:04.424 
into the CPU and of
course that means that

118
00:07:04.424 --> 00:07:07.427 
by decreasing the distance
between network and CPU

119
00:07:08.428 --> 00:07:11.431 
you can do a lot of
network tasks much faster

120
00:07:11.431 --> 00:07:16.436 
than before. So when you put the
network interface in here, into

121
00:07:16.436 --> 00:07:19.439 
the CPU, you get
something like this

122
00:07:19.439 --> 00:07:21.441 
which is something that Intel
announced this month for their

123
00:07:21.441 --> 00:07:25.445 
new processor line. We actually
have this small thing down

124
00:07:25.445 --> 00:07:28.448 
here which is your network
adapter so instead of going all

125
00:07:28.448 --> 00:07:31.451 
the way through the main board
to that Cat 5 plug that you

126
00:07:31.451 --> 00:07:35.455 
know you have memory here
that is so much closer

127
00:07:35.455 --> 00:07:37.457 
to the processor and
obviously much faster.

128
00:07:39.459 --> 00:07:43.463 
So, finally, when we look
into this where we started

129
00:07:44.464 --> 00:07:45.465 
with the CPU in the
middle and all the

130
00:07:45.465 --> 00:07:49.469 
components around it, we
realized that everything

131
00:07:49.469 --> 00:07:52.472 
is shifting more and
more to words the data

132
00:07:52.472 --> 00:07:55.475 
that is towards the memory and
especially when we think about

133
00:07:55.475 --> 00:07:58.478 
something like universal
memory you can think

134
00:07:58.478 --> 00:08:02.482 
about a system set up where the
CPU is not at the center anymore

135
00:08:02.482 --> 00:08:05.485 
and CPU has to delegate
to the GPU, the CPU has

136
00:08:05.485 --> 00:08:08.488 
to delegate to the network where
actually, the memory is the

137
00:08:08.488 --> 00:08:11.491 
most important part and everyone
groups around the memory.

138
00:08:12.492 --> 00:08:15.495 
One way we can envision
it for the future is

139
00:08:15.495 --> 00:08:19.499 
that we're shifting from
something that is CPU centric

140
00:08:19.499 --> 00:08:21.501 
to something that is
more memory centric

141
00:08:21.501 --> 00:08:24.504 
where different components
attach and everyone does whatever

142
00:08:24.504 --> 00:08:25.505 
they can do best.
