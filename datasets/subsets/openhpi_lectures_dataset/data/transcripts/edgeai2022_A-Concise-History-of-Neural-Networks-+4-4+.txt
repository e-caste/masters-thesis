WEBVTT

1
00:00:00.640 --> 00:00:11.019 
Hello and welcome. In the last session we have talked about the Alex net which kicks off the recent AI revolution.

2
00:00:11.019 --> 00:00:12.759 
In the year 2014,

3
00:00:13.339 --> 00:00:17.059 
the ImageNet winners are GoogleNet from Google and

4
00:00:17.059 --> 00:00:19.460 
VGG net from Oxford University.

5
00:00:20.140 --> 00:00:32.590 
Both networks have around 20 layers and in 2015 Microsoft Research Asia introduced a model with 152 layers is called deep

6
00:00:32.590 --> 00:00:33.759 
residual network.

7
00:00:34.240 --> 00:00:39.909 
In short, RestNet. It achieves significant improvement and RestNet

8
00:00:39.909 --> 00:00:44.560 
is regarded as one of the most commonly used models today.

9
00:00:46.840 --> 00:00:52.039 
So let's take a look the main idea of RestNet.

10
00:00:52.039 --> 00:01:01.049 
When talking about deep learning many of us may ask, hey you actually try to make the network deep but why sometimes

11
00:01:01.060 --> 00:01:07.340 
a model with more layers will perform worse? Back to the year 2014.

12
00:01:07.349 --> 00:01:10.980 
This is an embarrassing question.

13
00:01:10.989 --> 00:01:18.439 
Imagine that if we have wild training the neural network model and then we want to further create a deeper one.

14
00:01:18.450 --> 00:01:27.500 
We just copy the learnt layers from the trained model, adding some new layers on top of it and setting the additional layers

15
00:01:27.500 --> 00:01:30.480 
to identity mapping.

16
00:01:30.489 --> 00:01:37.519 
And this means that we just let the information get through the additional layers without changing anything.

17
00:01:37.530 --> 00:01:42.480 
And the deeper model should be able to perform at least as well as the shallow one.

18
00:01:42.489 --> 00:01:43.060 
Right?

19
00:01:43.640 --> 00:01:46.180 
Unfortunately things work

20
00:01:46.180 --> 00:01:51.239 
not as we expected, let's look at the Diagram.

21
00:01:51.250 --> 00:01:59.459 
A network with 56 layers has higher training and the testing error compared to the model with only 20 layers.

22
00:01:59.939 --> 00:02:01.950 
Is that overfitting problem?

23
00:02:01.959 --> 00:02:09.569 
No, this is not overfitting but the network degradation problem announced in the RestNet paper.

24
00:02:09.580 --> 00:02:16.050 
And another fact is that multi layer feed forward network is hard to learn identity mapping.

25
00:02:18.740 --> 00:02:25.759 
Let's take a look at the intuition of the residual blog of RestNet. Here we have an input X.

26
00:02:26.539 --> 00:02:34.080 
And two conv layers and the output H(x). According to the example from the last last slides.

27
00:02:34.639 --> 00:02:38.759 
And we figure out that it is hard to learn H(X) directly.

28
00:02:38.770 --> 00:02:39.379 
Right.

29
00:02:39.389 --> 00:02:50.580 
And that's the author proposed to add a shortcut connections keeping over two conv layers and directly integrated

30
00:02:50.580 --> 00:02:53.659 
with the layer output using element wise addition.

31
00:02:54.340 --> 00:03:02.569 
So please ignore this add operator and we have seen it before in the RSTM network. Still remember that.

32
00:03:02.580 --> 00:03:12.310 
It is used to store the long term memory in RSTM. Again here we can also take the advantage of it in the back

33
00:03:12.310 --> 00:03:13.150 
propagation.

34
00:03:14.039 --> 00:03:19.050 
If F(X) equals zero then H(X) is an identity mapping.

35
00:03:19.539 --> 00:03:26.699 
And also assume that learning the residual function F(X) should be easier for the network.

36
00:03:26.710 --> 00:03:34.039 
And moreover, we can have a better gradient flow in the backward path.

37
00:03:34.039 --> 00:03:34.590 
A RestNet.

38
00:03:34.590 --> 00:03:37.780 
The model consists of a stark to residual blocks.

39
00:03:38.240 --> 00:03:45.310 
So blogs contain a contained special dimension will be grouped into specific stages.

40
00:03:45.319 --> 00:03:56.539 
We will go to detail in the upcoming videos for RestNet and it periodically double the number of futures and down

41
00:03:56.539 --> 00:03:58.520 
the feature map dimensions.

42
00:03:58.530 --> 00:04:06.460 
And this way we can roughly maintain the similar representation capacity across the different stages of the network.

43
00:04:09.139 --> 00:04:21.240 
As I mentioned before, the author traded 152 layer RestNet on imaging image net and 1202 layers model on Cifa dataset without

44
00:04:21.240 --> 00:04:22.850 
gradient vanishing problem.

45
00:04:23.639 --> 00:04:33.540 
RestNet became the winning architecture in numbers of computations, for example, image classification, object detection and

46
00:04:33.540 --> 00:04:35.069 
object segmentation.

47
00:04:35.079 --> 00:04:45.790 
Specifically RestNet 152 model first time surpassed human performance in image classification. With RestNet we are able

48
00:04:45.790 --> 00:04:53.350 
to train extreme deeper networks and it indicators the importance of information flow for the network accuracy.

49
00:05:01.139 --> 00:05:07.259 
As you can see, the recent efforts are more based on the information and gradient flows.

50
00:05:07.939 --> 00:05:16.240 
So, the sigmoid function has gradient vanishing problem then ReLU has been proposed. To deal with the death value problem,

51
00:05:16.250 --> 00:05:23.160 
there are several variations variants of rally proposed such as leakyReLU, and PReLU and so on.

52
00:05:23.839 --> 00:05:28.759 
If the network is too deep, we still suffer from the gradient vanishing problem.

53
00:05:29.139 --> 00:05:39.009 
The highway network with parameter rise the shortcut can help, but simplified highway networks such as RestNet works even

54
00:05:39.009 --> 00:05:39.519 
better.

55
00:05:39.560 --> 00:05:47.459 
So we have RestNet by forcing stability of the mean and variance of activations at each layer.

56
00:05:47.470 --> 00:05:56.420 
We use the batch normalization to ease the training and speed up the convergence. With the dropout technique

57
00:05:56.430 --> 00:06:04.639 
we can add notes in the gradient flow at the noise in the gradient flow to improve the generalization ability. For solving

58
00:06:04.639 --> 00:06:08.300 
the gradient problem of recurrent neural network,

59
00:06:08.310 --> 00:06:09.209 
We got LSTM.

60
00:06:09.209 --> 00:06:09.410 


61
00:06:09.410 --> 00:06:09.860 


62
00:06:10.339 --> 00:06:18.959 
At a long term state and straight gate the control to make the information flow um better under control and make the network

63
00:06:18.959 --> 00:06:20.110 
more intelligent.

64
00:06:21.040 --> 00:06:26.220 
But training LSTM is somehow too slow and too simplified, so

65
00:06:26.230 --> 00:06:27.139 
we have the GRU.

66
00:06:27.139 --> 00:06:27.360 


67
00:06:27.360 --> 00:06:27.759 


68
00:06:30.439 --> 00:06:40.029 
Of course there are there are many other innovations and some of recent innovations will also be present in this in the upcoming

69
00:06:40.029 --> 00:06:44.889 
videos. I cannot describe, summarize them all of them here

70
00:06:44.899 --> 00:06:45.649 
one by one.

71
00:06:46.139 --> 00:06:55.560 
After more than 70 years development we finally ushured in another revolution of artificial intelligence. We should think

72
00:06:55.560 --> 00:07:05.500 
about the mistake we have made in the past and avoid it happening again. You know in the first and the second AI

73
00:07:05.500 --> 00:07:14.810 
winter neural network has become a subject that everyone hates, and wants to stay away from almost our research findings

74
00:07:14.810 --> 00:07:16.980 
for neural network have dried up.

75
00:07:17.019 --> 00:07:20.959 
Sorry to say, we did through the baby out of the bathwater.

76
00:07:22.240 --> 00:07:28.990 
Therefore we should learn from history and avoid such mistakes happen again. At present,

77
00:07:29.000 --> 00:07:36.839 
the biggest challenge fronting deep learning is to solve the dependence on fully supervised learning and the large scale

78
00:07:36.839 --> 00:07:46.040 
label datasets, as well as the reliance on powerful computing hardware. Thankfully there are still a large number of developers

79
00:07:46.040 --> 00:07:51.560 
in our community who will keep the field moving forward at a rapid pace.

80
00:07:54.439 --> 00:07:55.759 
Thank you for watching.
