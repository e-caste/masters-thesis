WEBVTT

1
00:00:01.500 --> 00:00:04.800 
One thing you have to
learn is, we have extra,

2
00:00:04.800 --> 00:00:06.100 
actually extra curriculum
for that in the

3
00:00:06.100 --> 00:00:13.170 
design school, design thinking.
If something is therefore nearly

4
00:00:13.170 --> 00:00:16.200 
50 years on those of
nearly, for over 40 years

5
00:00:17.000 --> 00:00:18.220 
you have to question
that whether it's true.

6
00:00:25.290 --> 00:00:32.360 
We carry forward the same idea
the same opinion which becomes

7
00:00:32.360 --> 00:00:36.400 
actually close to
something like truth

8
00:00:37.410 --> 00:00:43.470 
something we take as basic knowledge.
And actually a few students

9
00:00:43.470 --> 00:00:46.500 
can take it apart in
a few experiments.

10
00:00:48.520 --> 00:00:50.540 
So how did we
take this apart?

11
00:00:53.570 --> 00:00:57.610 
I didn't know whether it works when
we started, it was not complete

12
00:00:57.610 --> 00:01:00.640 
foresight and
wisdom, it was more,

13
00:01:02.660 --> 00:01:07.710 
I was intruiged
with the prospect of

14
00:01:08.720 --> 00:01:12.760 
the computers growing bigger
and bigger and bigger and when

15
00:01:12.760 --> 00:01:15.790 
we started seven years ago,
we didn't get your four

16
00:01:15.790 --> 00:01:21.850 
terabyte as the main memory, so
now the transitions from disks

17
00:01:21.850 --> 00:01:28.920 
to main memory. We got
probably 32 gigabytes.

18
00:01:30.940 --> 00:01:34.980 
You laugh about the sizes so it's
only seven and a half years ago

19
00:01:34.980 --> 00:01:37.101 
and we had a machine
with 16 cores.

20
00:01:39.103 --> 00:01:43.107 
A machine which can
work 16 times parallel

21
00:01:43.107 --> 00:01:48.112 
on a memory of 32
gigabytes. There was a

22
00:01:49.113 --> 00:01:53.117 
little box you could buy in
those days around the corner at

23
00:01:53.117 --> 00:02:00.124 
any electronic shop. But you
could google "Intel"

24
00:02:00.124 --> 00:02:05.129 
where is Intel going and
even in 2006 you could hear

25
00:02:05.129 --> 00:02:12.136 
already or you could read already
that probably in a few years

26
00:02:12.136 --> 00:02:16.140 
to come, 1000 cores
will be possible

27
00:02:17.141 --> 00:02:23.147 
and several terabyte of DRAM will come,
this is where technology is going.

28
00:02:25.149 --> 00:02:29.153 
And probably my biggest
contribution was to say ok if this

29
00:02:29.153 --> 00:02:32.156 
is happening then let's
start now while we continue

30
00:02:32.156 --> 00:02:36.160 
to work on, the hardware
situation will get

31
00:02:36.160 --> 00:02:38.162 
better and better.
The second thing is

32
00:02:38.162 --> 00:02:41.165 
great advantage to
be at the university

33
00:02:42.166 --> 00:02:46.170 
is you don't have to deliver
something within twelve months

34
00:02:46.170 --> 00:02:50.174 
and make money with it, you
just have to deliver papers.

35
00:02:53.177 --> 00:02:55.179 
And if you deliver good papers
then you have done already a good

36
00:02:55.179 --> 00:02:58.182 
job, for some time.

37
00:03:03.187 --> 00:03:04.188 
So that's where
we started,

38
00:03:06.190 --> 00:03:10.194 
So we had motivation that the
computer development is going

39
00:03:10.194 --> 00:03:13.197 
in the direction more
cores, more computing power,

40
00:03:14.198 --> 00:03:19.203 
and more memory. The disk
has gotten only probably,

41
00:03:20.204 --> 00:03:26.210 
over the last 20 years, maximum
10 times faster probably

42
00:03:26.210 --> 00:03:30.214 
not even 10 times faster.
So there is an and

43
00:03:30.214 --> 00:03:35.219 
since it's still a mechanical
device there's physical limitations

44
00:03:36.220 --> 00:03:38.222 
you can predict, you could
predict there's another

45
00:03:39.223 --> 00:03:42.226 
another 10 times is not coming
it's very easy to predict.

46
00:03:43.227 --> 00:03:48.232 
So the switch like from
tape to disc from disk to

47
00:03:50.234 --> 00:03:54.238 
direct, non
mechanical storage

48
00:03:55.239 --> 00:03:58.242 
and there are different types
of, we concentrated on DRAM

49
00:03:58.242 --> 00:04:03.247 
for read write
capabilities will happen

50
00:04:03.247 --> 00:04:06.250 
sooner or later so
let's start working now.

51
00:04:10.254 --> 00:04:12.256 
The sentence
which confused me

52
00:04:14.258 --> 00:04:19.263 
for quite some time was this
write optimization and the

53
00:04:19.263 --> 00:04:23.267 
read optimization. The
students looked at the

54
00:04:24.268 --> 00:04:25.269 
at the numbers

55
00:04:30.274 --> 00:04:35.279 
and we found out, you
can find out, if both

56
00:04:35.279 --> 00:04:40.284 
both systems should have
the same, keep the same data

57
00:04:41.285 --> 00:04:44.288 
we go to this in a
second, the same data

58
00:04:44.288 --> 00:04:47.291 
then we have to have the
same number of inserts.

59
00:04:50.294 --> 00:04:53.297 
Data goes in here as line
item and data goes in here as

60
00:04:53.297 --> 00:04:57.301 
line item so first of
all we have inserts

61
00:05:01.305 --> 00:05:06.310 
OLTP and OLAP, we
have number inserts.

62
00:05:10.314 --> 00:05:11.315 
So we have here

63
00:05:14.318 --> 00:05:16.320 
I start with n, and then we
have the same number here.

64
00:05:19.323 --> 00:05:24.328 
So the inserts can not be really,
the difference in behavior

65
00:05:24.328 --> 00:05:28.332 
and we will soon see
there is, then we do the

66
00:05:29.333 --> 00:05:36.340 
number of reads and then
we do the direct reads

67
00:05:37.341 --> 00:05:41.345 
and we have the
number of reads as a

68
00:05:42.346 --> 00:05:45.349 
range, so we
read multiple.

69
00:05:50.354 --> 00:05:55.359 
We see a huge difference in
the number of direct reads

70
00:05:56.360 --> 00:05:59.363 
and we probably see,

71
00:06:04.368 --> 00:06:08.372 
how do I do this, you are a
mathematician professor Meinel

72
00:06:09.373 --> 00:06:11.375 
I want to show that this
is more than this here

73
00:06:23.387 --> 00:06:30.394 
Not one more? Ok,
we do obviously more

74
00:06:30.394 --> 00:06:35.399 
range reads in a system which
is primarily doing reporting.

75
00:06:35.399 --> 00:06:41.405 
We don't do that
many direct reads.

76
00:06:42.406 --> 00:06:46.410 
I will explain this in a second,
we have a huge thing which

77
00:06:46.410 --> 00:06:50.414 
is different. This
is number of updates

78
00:06:52.416 --> 00:06:53.417 
and

79
00:06:55.419 --> 00:06:59.423 
number of updates
because of aggregates.

80
00:07:04.428 --> 00:07:08.432 
So we built huge aggregates
here but we built them

81
00:07:14.438 --> 00:07:18.442 
because I said we did several
already, but we do even more here.

82
00:07:22.446 --> 00:07:26.450 
So it's not even true,
so even if this is

83
00:07:27.451 --> 00:07:31.455 
if this is equal or it is
slightly favored all up

84
00:07:34.458 --> 00:07:38.462 
we still haven't found what the
sentence means, is it write oriented

85
00:07:38.462 --> 00:07:43.467 
it is read oriented. We have
updates which are changes,

86
00:07:44.468 --> 00:07:48.472 
it's different then
aggregates and this is

87
00:07:49.473 --> 00:07:51.475 
very much here,
sorry, very much on

88
00:07:56.480 --> 00:08:01.485 
the transactional side because
in this statistics system nothing

89
00:08:01.485 --> 00:08:05.489 
is happening anymore. There are
no updates anymore, only we carry

90
00:08:05.489 --> 00:08:06.490 
only the data over.

91
00:08:09.493 --> 00:08:15.499 
Now it's not totally true,
sometimes we move these changes also

92
00:08:15.499 --> 00:08:19.503 
but only if you do
the OLAP system.

93
00:08:21.505 --> 00:08:25.509 
So when you look at this,
just by thinking about it

94
00:08:26.510 --> 00:08:29.513 
you can not keep the
sentence the one system is

95
00:08:31.515 --> 00:08:35.519 
heavily focused on
writes and the other

96
00:08:35.519 --> 00:08:39.523 
one is heavily focused on reads.
Yes, we have more reads in the

97
00:08:39.523 --> 00:08:43.527 
system but when you
look at the real reason,

98
00:08:44.528 --> 00:08:50.534 
it is because we migrated anything
which looked like reporting, analytics,

99
00:08:53.537 --> 00:08:54.538 
statistics.

100
00:08:57.541 --> 00:09:00.544 
All the same, we migrated
in this direction

101
00:09:01.545 --> 00:09:04.548 
because the load on the
transactional system became higher

102
00:09:04.548 --> 00:09:08.552 
and higher and higher. It is
the absolute must you have to do

103
00:09:08.552 --> 00:09:13.557 
in a company. This is a little
bit like you can do it you can

104
00:09:13.557 --> 00:09:16.560 
do it at any time, you have more
time, you can do it over night,

105
00:09:16.560 --> 00:09:20.564 
you can do it next day so we
move more and more stuff of

106
00:09:20.564 --> 00:09:25.569 
the reporting analytics,
of the read only stuff

107
00:09:27.571 --> 00:09:33.577 
to this direction here. That
changed the the ratio and what

108
00:09:33.577 --> 00:09:38.582 
did we find? So
look up all these

109
00:09:38.582 --> 00:09:43.587 
direct reads. Why do we have
more in the OLTP system, every

110
00:09:43.587 --> 00:09:46.590 
single line item which
goes into the system,

111
00:09:46.590 --> 00:09:50.594 
all the attributes have to be
checked whether they are valid,

112
00:09:51.595 --> 00:09:54.598 
we have to check whether a product
number is in a product catalog

113
00:09:54.598 --> 00:09:57.601 
whether customer number is in
the customer database and so

114
00:09:57.601 --> 00:10:01.605 
on. So we have to check all these,
this checking is taking place

115
00:10:01.605 --> 00:10:05.609 
in the OLTP system. We do not
re-check the data when we move it

116
00:10:05.609 --> 00:10:11.615 
over into the online
analytical processing system.

117
00:10:14.618 --> 00:10:18.622 
The number of range selects,
larger on the right-hand side

118
00:10:18.622 --> 00:10:24.628 
yes, because we do
more reporting there

119
00:10:24.628 --> 00:10:28.632 
that the inserts are
proportionally different

120
00:10:28.632 --> 00:10:32.636 
this this has to do,
because we normalised

121
00:10:34.638 --> 00:10:37.641 
by, to a 100%. When we look
at the absolute numbers,

122
00:10:37.641 --> 00:10:39.643 
the inserts are equal.

123
00:10:41.645 --> 00:10:43.647 
So just from
looking at this,

124
00:10:46.650 --> 00:10:50.654 
why should we have different
databases for that?

125
00:10:57.661 --> 00:11:05.669 
The OLAP guys invented databases
which are extremely fast,

126
00:11:06.670 --> 00:11:08.672 
much faster than the
transactional databases,

127
00:11:08.672 --> 00:11:09.673 
in range reach.

128
00:11:12.676 --> 00:11:16.680 
The transaction or the
database we started with,

129
00:11:16.680 --> 00:11:19.683 
we took three databases from SAP
and looked at them and played

130
00:11:19.683 --> 00:11:24.688 
with them and we got one call
t-rex, was an in memory database

131
00:11:25.689 --> 00:11:27.691 
was actually built as
a text search engine

132
00:11:28.692 --> 00:11:31.695 
to do something like google,
was a research project

133
00:11:32.696 --> 00:11:36.700 
which did then
some jobs in

134
00:11:36.700 --> 00:11:42.706 
in read, in memory
storage of data

135
00:11:42.706 --> 00:11:47.711 
and read intensive applications
here and there and SAP as an

136
00:11:47.711 --> 00:11:49.713 
optimizer.

137
00:11:52.716 --> 00:11:57.721 
And I had this organization
column store organization versus

138
00:11:57.721 --> 00:11:59.000 
row store organization.

139
00:12:01.725 --> 00:12:09.733 
This is faster for range selects.
We will look at this then

140
00:12:09.733 --> 00:12:11.735 
in detail, but it's
faster in range selects

141
00:12:13.737 --> 00:12:17.741 
times faster, easily
ten times faster.

142
00:12:20.744 --> 00:12:25.749 
The nature of
the database, the

143
00:12:26.750 --> 00:12:32.756 
column organization allows for
easier parallelisation on multicore.

144
00:12:34.758 --> 00:12:38.762 
So we were actually able, in the
very early stage of the research

145
00:12:38.762 --> 00:12:43.767 
project, to ask SAP to modify
their database a little bit so

146
00:12:43.767 --> 00:12:46.770 
that we can have
parallel processing for

147
00:12:47.771 --> 00:12:48.772 
sequential sequences.

148
00:12:51.775 --> 00:12:54.778 
It's interesting when you
look at the literature

149
00:12:55.779 --> 00:12:58.782 
Patterson and

150
00:13:01.785 --> 00:13:06.790 
Hennesy, the standard book every
computer engineer in America

151
00:13:06.790 --> 00:13:10.794 
has to read, right on
I don't know page 256,

152
00:13:10.794 --> 00:13:13.797 
might be another page, so
it's only in this range

153
00:13:14.798 --> 00:13:18.802 
parallelism is a very
interesting concept.

154
00:13:19.803 --> 00:13:21.805 
Very difficult
to achieve,

155
00:13:23.807 --> 00:13:26.810 
difficult style of writing
of americans, note:

156
00:13:26.810 --> 00:13:33.817 
If you have a longer
sequence, sequence of

157
00:13:33.817 --> 00:13:38.822 
a processing of the same type,
there's a good chance that you

158
00:13:38.822 --> 00:13:39.122 
can split it up.

159
00:13:42.826 --> 00:13:44.828 
If the type of
processing, the algorithm

160
00:13:45.829 --> 00:13:48.832 
can actually be chopped
up into pieces and

161
00:13:49.833 --> 00:13:53.837 
we can have a union in the end
and there are a lot of processes

162
00:13:54.838 --> 00:13:57.841 
which are of that type, find is
something like that, google is

163
00:13:57.841 --> 00:14:02.846 
doing this. Many other companies
are doing this for many years

164
00:14:02.846 --> 00:14:05.849 
already so it was clear in the
air, parallelism is the way to

165
00:14:05.849 --> 00:14:10.854 
go. Hard to achieve but note
here is a tip how to do it,

166
00:14:12.000 --> 00:14:14.858 
we gave this tip to
SAP and six weeks later

167
00:14:15.859 --> 00:14:19.863 
we got a prototype
of a columnar

168
00:14:19.863 --> 00:14:24.868 
store database which can
parallelise. So if we have ten cores

169
00:14:25.869 --> 00:14:28.872 
we are ten times faster because
of the data organization,

170
00:14:28.872 --> 00:14:32.876 
we are ten times faster because of
parallelism, we are already hundred

171
00:14:32.876 --> 00:14:35.879 
times faster than a
traditional database.

172
00:14:36.880 --> 00:14:40.884 
That was motivation to dig
deeper and to go ahead.

173
00:14:43.887 --> 00:14:44.888 
So now

174
00:14:47.891 --> 00:14:52.896 
we have already figured out
why, what the bottleneck of

175
00:14:52.896 --> 00:14:58.902 
transactional systems is. This
is maintaining the aggregates

176
00:15:02.906 --> 00:15:04.908 
and maintaining a certain
number of aggregates

177
00:15:05.909 --> 00:15:08.912 
and we have already seen that
the people running a company

178
00:15:09.913 --> 00:15:13.917 
want to have many more aggregates
of different flavors than the

179
00:15:13.917 --> 00:15:17.921 
ones we provide in the
transactional system.

180
00:15:21.925 --> 00:15:26.930 
Why did we do these aggregates?
You have to go back to

181
00:15:26.930 --> 00:15:30.934 
1966 with the punch card, you
can not read the punch cards

182
00:15:30.934 --> 00:15:34.938 
of a whole year of a larger
company, it's impossible.

183
00:15:35.939 --> 00:15:39.943 
You can not do processing even
if you put all the punch cards

184
00:15:39.943 --> 00:15:42.946 
on tape and you read them only
from tape which is probably

185
00:15:42.946 --> 00:15:46.950 
already hundred times faster,
it is still not fast enough.

186
00:15:47.951 --> 00:15:51.955 
So therefore ever since we
did enterprise computing

187
00:15:52.956 --> 00:15:57.961 
we had the idea that we can
foresee what the users want to

188
00:15:57.961 --> 00:16:03.967 
ask and for the main questions they
ask we pre-build the aggregates.

189
00:16:05.969 --> 00:16:10.974 
So instead of running through,
SAP has today in the accounting,

190
00:16:10.974 --> 00:16:14.978 
no this is the whole ERP
database, probably in accounting

191
00:16:14.978 --> 00:16:18.982 
we have, let's say 20
million line items.

192
00:16:19.983 --> 00:16:20.984 
This is covering

193
00:16:23.987 --> 00:16:28.992 
the last, at least the last
five years probably more,

194
00:16:29.993 --> 00:16:30.994 
20 million line items

195
00:16:33.997 --> 00:16:36.100 
and we have in SAP, five
thousand general ledger accounts.

196
00:16:36.100 --> 00:16:40.100 
If we maintain these
aggregates for these accounts

197
00:16:40.100 --> 00:16:44.000 
and all reporting plus minus
in financials is built then

198
00:16:44.000 --> 00:16:47.101 
on top of these aggregates,
then we only have to deal with

199
00:16:47.101 --> 00:16:51.101 
five thousand records in
one company, now we have a

200
00:16:51.101 --> 00:16:54.101 
hundred fifty companies
so it's a little bit more.

201
00:16:55.101 --> 00:16:59.102 
But it is obviously
much faster than to go

202
00:17:00.102 --> 00:17:04.102 
through the line items, through
the 16 million again and again.

203
00:17:07.103 --> 00:17:11.103 
It was, that flipped,
this is not true anymore

204
00:17:13.103 --> 00:17:17.104 
the aggregates were only
built for performance reasons.

205
00:17:19.104 --> 00:17:21.104 
Any aggregate
you ever build

206
00:17:23.104 --> 00:17:28.105 
leads to reduction of information,
you reduce the information

207
00:17:28.105 --> 00:17:33.105 
you have in the line items by
a formula, by an algorithm.

208
00:17:34.105 --> 00:17:39.106 
So we want to add up
everything per product number

209
00:17:41.106 --> 00:17:47.107 
then obviously we lose the
individual information when

210
00:17:47.107 --> 00:17:51.107 
was something sold, to whom it
was sold etc. etc. because we have

211
00:17:51.107 --> 00:17:53.107 
aggregated it
by product.

212
00:17:57.108 --> 00:18:01.108 
Every single aggregation
is all loss of

213
00:18:01.108 --> 00:18:06.109 
information for the
purpose of gaining speed

214
00:18:06.109 --> 00:18:08.109 
to answer certain
question faster.

215
00:18:14.109 --> 00:18:19.110 
We have done this for a
certain number of questions

216
00:18:20.110 --> 00:18:23.110 
and obviously we
struggle to do this with

217
00:18:25.110 --> 00:18:26.111 
any type of question.

218
00:18:30.111 --> 00:18:34.111 
Now we do the first
experiments, the objective

219
00:18:34.111 --> 00:18:38.112 
was we want to build a
database or we want to research

220
00:18:38.112 --> 00:18:42.112 
a database which combines
OLTP and OLAP again.

221
00:18:43.112 --> 00:18:48.113 
So we put this in
one system again,

222
00:18:51.113 --> 00:18:58.114 
and yes if we use, this works,
but it doesn't work great

223
00:19:00.114 --> 00:19:05.114 
because the actual transactional
part of the process

224
00:19:07.115 --> 00:19:10.115 
is not getting faster, despite
we have a database which can

225
00:19:10.115 --> 00:19:13.115 
do reporting at least
100 times faster.

226
00:19:14.115 --> 00:19:19.116 
We achieved up to 100,000
times faster, SAP achieved

227
00:19:19.116 --> 00:19:20.116 
not we.

228
00:19:25.116 --> 00:19:27.117 
We want to, we
only achieve

229
00:19:28.117 --> 00:19:33.117 
best case, a factor 2 in
transactional processing because

230
00:19:34.117 --> 00:19:38.118 
all data is in memory and anyone
who has ever played with a

231
00:19:38.118 --> 00:19:42.118 
decent database basically
any relational database,

232
00:19:45.118 --> 00:19:49.119 
there is anyway any data you
ever access is in memory.

233
00:19:50.119 --> 00:19:54.119 
So any well tuned database
in enterprise business

234
00:19:54.119 --> 00:19:57.120 
has a 98, 99 % hit rate.

235
00:19:57.120 --> 00:20:01.120 
So whatever data is being accessed
in the OLTP system, it's being

236
00:20:01.120 --> 00:20:05.120 
accessed, is already in
memory so since we have 100 %

237
00:20:05.120 --> 00:20:10.121 
in memory we're not that much
better. Actually we will see this

238
00:20:10.121 --> 00:20:14.121 
in columns store database with
compression has a few disadvantages

239
00:20:14.121 --> 00:20:16.122 
in performance because
of the compression.

240
00:20:17.122 --> 00:20:21.122 
That takes time, the compression
is not free of charge.

241
00:20:22.122 --> 00:20:27.123 
So transactional processing gets
a little bit, the processing

242
00:20:27.123 --> 00:20:32.123 
gets a little bit slower, the in
memory helps, it's a wash it's

243
00:20:32.123 --> 00:20:35.123 
either same speed or
maximum a factor two faster.

244
00:20:36.124 --> 00:20:41.124 
So we have not achieved
a great deal, probably

245
00:20:42.124 --> 00:20:44.124 
people from the

246
00:20:45.124 --> 00:20:50.125 
old world were arguing, all
that to achieve par is already,

247
00:20:51.125 --> 00:20:55.125 
the thread is already difficult,
it might be even worse, it

248
00:20:55.125 --> 00:21:01.126 
might be slower. This is true
if you continue with the concept

249
00:21:01.126 --> 00:21:06.127 
of enterprise programs
from '66, the 70's

250
00:21:07.127 --> 00:21:12.127 
or the 90s which is, I
repeat one last time.

251
00:21:14.127 --> 00:21:18.128 
Get in line items,
typically in an object

252
00:21:18.128 --> 00:21:24.128 
manner, a whole invoice,
a whole payment, a whole

253
00:21:24.128 --> 00:21:29.129 
customer order, check this and
update aggregates as many as

254
00:21:29.129 --> 00:21:35.000 
you can master to update
within the transaction.

255
00:21:38.130 --> 00:21:43.130 
If the aggregates or actually
a loss of information,

256
00:21:43.130 --> 00:21:47.131 
every single aggregate itself
has lost information for the

257
00:21:47.131 --> 00:21:52.131 
purpose of gaining performance.
If having multiple aggregates

258
00:21:52.131 --> 00:21:59.132 
and more and more aggregates
is a pain, then we have to ask

259
00:21:59.132 --> 00:22:01.132 
ourselves the question,
why do we have aggregate?

260
00:22:05.000 --> 00:22:07.133 
We have aggregates for
performance reasons.

261
00:22:08.133 --> 00:22:11.133 
Are these performance
reasons still valid

262
00:22:14.133 --> 00:22:19.134 
in 2014, actually
probably 2010

263
00:22:19.134 --> 00:22:24.134 
we ask the question officially
and I gave a speech at the

264
00:22:24.134 --> 00:22:29.135 
very large database conference
and said, the problem of having

265
00:22:29.135 --> 00:22:34.135 
different databases is
only because we still

266
00:22:35.135 --> 00:22:40.136 
want to have aggregates, we
maintain in transactional mode.

267
00:22:41.136 --> 00:22:47.000 
This is the problem, once you
have isolated this problem

268
00:22:48.137 --> 00:22:50.137 
at least at the
university you can think

269
00:22:50.137 --> 00:22:54.137 
let's get rid of the problem.
So we started to think about,

270
00:22:55.137 --> 00:22:59.138 
is young Schaffner here?
He is not here, here he is.

271
00:22:59.138 --> 00:23:03.138 
So I explained to him and his
colleagues for a whole day

272
00:23:04.138 --> 00:23:07.139 
how an accounting system works
and what the parts are and

273
00:23:07.139 --> 00:23:10.139 
so I want to reduce
the aggregation.

274
00:23:12.139 --> 00:23:16.140 
I want to reduce
the aggregations to,

275
00:23:17.140 --> 00:23:22.140 
to a level so that we lose
the minimum of information

276
00:23:23.140 --> 00:23:26.141 
and my proposal is, we
give up time of the day

277
00:23:28.141 --> 00:23:31.141 
if we have all these
attributes and we aggregate

278
00:23:32.141 --> 00:23:36.142 
over various
aggregations over

279
00:23:36.142 --> 00:23:41.142 
the different attributes. Then
one idea is if we take the

280
00:23:41.142 --> 00:23:46.143 
time of day out then we can
collapse data just based on time

281
00:23:46.143 --> 00:23:50.143 
of day so were everything
is the same except

282
00:23:50.143 --> 00:23:55.143 
the value, typically the amount
value and the time of day, we

283
00:23:55.143 --> 00:23:56.144 
can collapse on day.

284
00:24:02.144 --> 00:24:06.145 
And young Schaffner
asked me, after explained

285
00:24:06.145 --> 00:24:09.145 
everything to me so
what do you think is the

286
00:24:09.145 --> 00:24:15.145 
collapse-ization rate
so what is the reduction

287
00:24:15.145 --> 00:24:17.146 
in records?

288
00:24:19.146 --> 00:24:23.146 
Actually the calculation
is probably still

289
00:24:23.146 --> 00:24:27.147 
there, this is the photo we
took you can show you don't have

290
00:24:27.147 --> 00:24:31.147 
to really just can see
how intensively we worked.

291
00:24:32.147 --> 00:24:36.148 
So you see there are tons of
numbers, how large the companies,

292
00:24:36.148 --> 00:24:41.148 
how many transactions
etc. etc. etc. and then

293
00:24:42.148 --> 00:24:47.149 
I finally come to the conclusion,
we compress probably from

294
00:24:47.149 --> 00:24:52.149 
one million line
items to a 100,000

295
00:24:52.149 --> 00:24:56.150 
aggregates, if we take the time
out, time of day out, not the

296
00:24:56.150 --> 00:24:58.000 
day, time of
the day out.

297
00:25:00.150 --> 00:25:03.150 
And then I don't know whether
it's the style today anymore but

298
00:25:04.150 --> 00:25:08.151 
in those days as a young
PhD candidate, young set

299
00:25:10.151 --> 00:25:14.151 
I try to quote you correctly,
from an academic point of view

300
00:25:14.151 --> 00:25:19.152 
a reduction of factor
ten is not very exciting.

301
00:25:20.152 --> 00:25:25.152 
Is that close? Ok, I
memorized it and I said

302
00:25:27.153 --> 00:25:32.153 
I work a whole week on,
is the re compression

303
00:25:32.153 --> 00:25:36.154 
or reduction of all aggregates
just on the fact I take the

304
00:25:36.154 --> 00:25:41.154 
time away. Is
that good enough?

305
00:25:41.154 --> 00:25:45.154 
Because i still can do now any
kind of aggregation on top of

306
00:25:45.154 --> 00:25:50.155 
these aggregations, so I achieved
a factor ten in performance.

307
00:25:52.155 --> 00:25:54.155 
Was not good enough
to convince him

308
00:25:56.156 --> 00:25:56.556 
and I

309
00:25:58.156 --> 00:26:03.156 
know aggregates anymore so from
now on we don't even use the

310
00:26:04.156 --> 00:26:08.157 
aggregates, we maintain on the
transactional level we still

311
00:26:08.157 --> 00:26:12.157 
have aggregates but we do not do
aggregates on the transactional

312
00:26:12.157 --> 00:26:16.158 
level anymore. We do
not do what we did

313
00:26:18.158 --> 00:26:22.158 
from 1996 on
what we still did

314
00:26:23.158 --> 00:26:28.159 
in the 90's, we got
rid of the aggregates.

315
00:26:29.159 --> 00:26:33.159 
If you get rid of the aggregates,
funny things are happening,

316
00:26:34.159 --> 00:26:40.160 
just conceptually. We do
not lose any information

317
00:26:42.160 --> 00:26:45.160 
because this is the information
we recorded and it's still

318
00:26:45.160 --> 00:26:50.161 
there. Any application we built
on top of this information

319
00:26:51.161 --> 00:26:58.162 
whether it is a statistic of
the monthly sales, whether it's

320
00:26:58.162 --> 00:27:01.162 
a PNL profit and loss statement,
whether it's a balance sheet,

321
00:27:01.162 --> 00:27:06.163 
whether it's what we have on
stock. We create out of these line

322
00:27:06.163 --> 00:27:11.163 
items we do now what we said
in '66 is not possible we

323
00:27:11.163 --> 00:27:16.164 
run with all the punch cards for
every single application through

324
00:27:18.164 --> 00:27:22.164 
or the application runs through all
the punch cards every single time.

325
00:27:25.164 --> 00:27:31.000 
That means theoretically we
can have an unlimited number of

326
00:27:31.000 --> 00:27:36.166 
algorithms, if this program has
to run, a certain program has

327
00:27:36.166 --> 00:27:41.166 
to run then it runs on
demand. So it runs through

328
00:27:41.166 --> 00:27:47.167 
all line items. We have
no restriction anymore

329
00:27:48.167 --> 00:27:53.167 
with regards to the number of
reporting analytic statistical

330
00:27:53.167 --> 00:27:57.168 
programs. We can do
as many as we want, a

331
00:27:58.168 --> 00:27:59.168 
huge achievement.

332
00:28:03.168 --> 00:28:06.169 
If you go to a company, any
of the company here, we should

333
00:28:06.169 --> 00:28:09.169 
do this probably in
one of our seminars,

334
00:28:10.169 --> 00:28:14.169 
to ask the question if they want
to maintain in the transactional

335
00:28:14.169 --> 00:28:18.170 
system, a new aggregation
with a new hierarchy,

336
00:28:19.170 --> 00:28:24.170 
how much time this costs? If
somebody says they can do it in

337
00:28:24.170 --> 00:28:27.171 
less than three month, they are lying
because they can't do it without

338
00:28:27.171 --> 00:28:31.171 
SAP. They can do
it asynchronously

339
00:28:32.171 --> 00:28:35.171 
in these statistics system in
the OLAP system, they can not do

340
00:28:35.171 --> 00:28:36.172 
it in the
transactional system.

341
00:28:38.172 --> 00:28:44.172 
So now you can add any
kind of program on the fly,

342
00:28:45.172 --> 00:28:49.173 
any kind of algorithm on the
fly with any kind of hierarchy.

343
00:28:50.173 --> 00:28:55.173 
That means a new manager or
new situation in the company

344
00:28:55.173 --> 00:28:59.174 
and we want to aggregate
data in a different form

345
00:29:00.174 --> 00:29:06.175 
using different math, it's no
problem. There is no limitation

346
00:29:06.175 --> 00:29:10.175 
per definition whether this
all works we have to see.

347
00:29:12.175 --> 00:29:17.176 
This is the number one
achievement of using

348
00:29:17.176 --> 00:29:22.176 
a database which is
significantly faster

349
00:29:23.176 --> 00:29:29.177 
in range select, in range
read because we feel already

350
00:29:29.177 --> 00:29:35.177 
we might be able to afford this.
So we drop the most significant

351
00:29:35.177 --> 00:29:39.178 
part of all enterprise systems
over the last 40 plus years

352
00:29:39.178 --> 00:29:40.178 
and that was
aggregation.
