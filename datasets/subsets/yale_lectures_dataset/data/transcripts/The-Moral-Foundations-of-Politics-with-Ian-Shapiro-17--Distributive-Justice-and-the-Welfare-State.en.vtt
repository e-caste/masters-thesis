WEBVTT
Kind: captions
Language: en

00:00:01.600 --> 00:00:07.227
I want to pick up where I left
off on Monday speaking about

00:00:07.227 --> 00:00:10.720
Rawls's two principles of
justice.

00:00:10.720 --> 00:00:16.232
And as you will recall,
I mentioned that Rawls really

00:00:16.232 --> 00:00:22.698
changed the subject with respect
to what the metric of justice

00:00:22.698 --> 00:00:23.440
is.
 

00:00:23.440 --> 00:00:27.230
That rather than focus on
utility somehow measured,

00:00:27.230 --> 00:00:29.850
or welfare as it's sometimes
called,

00:00:29.850 --> 00:00:35.613
instead Rawls embraces the
resourcist idea of focusing on

00:00:35.613 --> 00:00:38.290
certain basic resources.
 

00:00:38.290 --> 00:00:41.925
The assumption being that no
matter what your particular

00:00:41.925 --> 00:00:46.222
goals in life turn out to be,
no matter what your particular

00:00:46.222 --> 00:00:50.114
life plan turns out to be,
and those are not things we

00:00:50.114 --> 00:00:53.550
know because we're behind the
veil of ignorance,

00:00:53.550 --> 00:00:57.100
you're going to want more
rather than less in the way of

00:00:57.100 --> 00:00:59.570
liberties,
more rather than less in the

00:00:59.570 --> 00:01:03.850
way of opportunities,
and more rather than less in

00:01:03.850 --> 00:01:07.000
the way of income and wealth.
 

00:01:07.000 --> 00:01:11.012
Something I didn't mention
that'll come into play in

00:01:11.012 --> 00:01:14.159
today's lecture is that Rawls
has to,

00:01:14.159 --> 00:01:17.873
of course, deal with the fact
that the moment you have a

00:01:17.873 --> 00:01:21.656
theory that affirms more than
one value you have to think

00:01:21.656 --> 00:01:23.927
about,
well, what happens when the

00:01:23.927 --> 00:01:24.890
values conflict?
 

00:01:24.890 --> 00:01:31.417
What if maximizing liberties
can only come at the expense of

00:01:31.417 --> 00:01:34.536
opportunities or,
if you like,

00:01:34.536 --> 00:01:39.221
distributing income and wealth
in a way that you regard as fair

00:01:39.221 --> 00:01:43.756
or just conflicts with what you
say about the distribution of

00:01:43.756 --> 00:01:44.890
liberties.
 

00:01:44.890 --> 00:01:48.566
Any time you have more than one
value you have to deal with the

00:01:48.566 --> 00:01:50.760
possibility of conflict among
them.

00:01:50.760 --> 00:01:53.140
And he does deal with that.
 

00:01:53.140 --> 00:01:58.061
He has an appeal to what he
calls a lexical ranking which is

00:01:58.061 --> 00:02:03.150
short for the more cumbersome
term lexicographical ranking.

00:02:03.150 --> 00:02:07.147
And what that means is that
anytime you want more rather

00:02:07.147 --> 00:02:09.908
than fewer in the way of
liberties,

00:02:09.908 --> 00:02:12.610
you want more than fewer in the
way of opportunities,

00:02:12.610 --> 00:02:15.680
and you want more rather than
less in the way of income and

00:02:15.680 --> 00:02:18.236
wealth,
but in anytime there's a

00:02:18.236 --> 00:02:23.389
conflict something higher in the
lexical ranking trumps what's

00:02:23.389 --> 00:02:24.150
lower.
 

00:02:24.150 --> 00:02:27.715
So that if the only way you
could get more rather than less

00:02:27.715 --> 00:02:31.034
income and wealth was to
compromise people's liberties,

00:02:31.034 --> 00:02:32.449
you wouldn't do it.
 

00:02:32.449 --> 00:02:35.620
So that's the notion of a
lexical ranking.

00:02:35.620 --> 00:02:40.708
You want to maximize each item
in the lexical ranking subject

00:02:40.708 --> 00:02:45.456
to the constraint that it does
not come at the expense of

00:02:45.456 --> 00:02:50.800
maximizing something that's
higher in that lexical ranking.

00:02:50.800 --> 00:02:54.288
Okay, and then we talked about
his first principle,

00:02:54.288 --> 00:02:58.949
and I gave you the illustration
of religious freedom as the kind

00:02:58.949 --> 00:03:03.385
of thing he's thinking about
when he talks about distributing

00:03:03.385 --> 00:03:07.675
all liberties in a way that
gives people the most extensive

00:03:07.675 --> 00:03:12.408
possible freedom compatible with
the like freedom for all.

00:03:12.408 --> 00:03:16.270
And this is not to be confused
with the idea of neutrality.

00:03:16.270 --> 00:03:18.110
We worked through that.
 

00:03:18.110 --> 00:03:22.663
And so we gave the example of
whether to have if you were

00:03:22.663 --> 00:03:25.578
comparing,
say, a fundamentalist regime

00:03:25.578 --> 00:03:28.830
with a regime that has a
disestablished church,

00:03:28.830 --> 00:03:33.138
the reason for preferring the
regime with the disestablished

00:03:33.138 --> 00:03:36.863
church is that the most
disadvantaged person in that

00:03:36.863 --> 00:03:40.407
regime,
namely say the fundamentalist,

00:03:40.407 --> 00:03:45.741
is less disadvantaged than the
person who does not affirm the

00:03:45.741 --> 00:03:50.453
established fundamentalist
beliefs of a fundamentalist

00:03:50.453 --> 00:03:51.520
regime.
 

00:03:51.520 --> 00:03:56.379
So you always compare the
least--and I know it's a

00:03:56.379 --> 00:04:02.431
cumbersome way of putting it but
there's not an elegant way of

00:04:02.431 --> 00:04:03.919
putting it.
 

00:04:03.919 --> 00:04:08.729
What you want to do is compare
the condition of the most

00:04:08.729 --> 00:04:13.538
adversely affected person in
each situation and say,

00:04:13.538 --> 00:04:17.310
"Which would you rather
be?"

00:04:17.310 --> 00:04:21.523
basically, and you're going to
always pick the one that

00:04:21.523 --> 00:04:25.579
minimizes the harm to the least
advantaged person.

00:04:25.579 --> 00:04:29.293
So the standpoint of justice is
the standpoint of the least

00:04:29.293 --> 00:04:32.492
advantaged person,
but this isn't a bleeding heart

00:04:32.492 --> 00:04:35.351
point,
but rather a self-interested

00:04:35.351 --> 00:04:40.283
point because you don't know who
you are behind the veil of

00:04:40.283 --> 00:04:41.389
ignorance.
 

00:04:41.389 --> 00:04:48.307
Okay, now, let's talk about his
second principle which is,

00:04:48.307 --> 00:04:54.740
in fact, divided into two
principles, so he really has

00:04:54.740 --> 00:04:57.290
three principles.
 

00:04:57.290 --> 00:05:02.358
The first part of it he says,
"Social and economic

00:05:02.358 --> 00:05:08.086
inequalities are to be arranged
so that they are both attached

00:05:08.086 --> 00:05:13.720
to offices and positions open to
all under conditions of fair

00:05:13.720 --> 00:05:17.100
equality of opportunity."
 

00:05:17.100 --> 00:05:18.939
That is, you'll see 2b.
 

00:05:18.939 --> 00:05:20.459
That's not a typo.
 

00:05:20.459 --> 00:05:22.740
I'll come to 2a in a minute.
 

00:05:22.740 --> 00:05:26.685
For some reason,
known only to John Rawls,

00:05:26.685 --> 00:05:31.399
he put 2a before 2b,
but he meant to put 2b before

00:05:31.399 --> 00:05:36.500
2a in the sense that 2b is
lexically prior to 2a.

00:05:36.500 --> 00:05:39.160
So that's why I'm doing 2b
first.

00:05:39.160 --> 00:05:43.329
And that 2b is what governs the
distribution of opportunities.

00:05:43.329 --> 00:05:47.463
And he's essentially saying
"fair equality of

00:05:47.463 --> 00:05:49.149
opportunity."
 

00:05:49.149 --> 00:05:51.089
What does that mean?
 

00:05:51.089 --> 00:05:53.230
It means no apartheid.
 

00:05:53.230 --> 00:05:57.521
It means, for instance,
we still today have a system in

00:05:57.521 --> 00:06:00.940
America where
occupation-by-occupation women

00:06:00.940 --> 00:06:05.711
earn about eighty-six percent of
what men earn in exactly the

00:06:05.711 --> 00:06:07.459
same occupation.
 

00:06:07.459 --> 00:06:12.144
So there's gender
discrimination in remuneration

00:06:12.144 --> 00:06:15.235
for employment,
so we would say,

00:06:15.235 --> 00:06:19.920
"Those systems are
illegitimate."

00:06:19.920 --> 00:06:25.074
Systems which reward women less
than men on a systematic basis

00:06:25.074 --> 00:06:26.850
wouldn't be chosen.
 

00:06:26.850 --> 00:06:30.261
You would never choose a system
that privileges one gender

00:06:30.261 --> 00:06:33.853
because you don't know whether
you're going to turn out to be

00:06:33.853 --> 00:06:35.350
the women or the men.
 

00:06:35.350 --> 00:06:38.718
You would never accept the
system of job reservation such

00:06:38.718 --> 00:06:42.387
as apartheid because you don't
know whether you're going to be

00:06:42.387 --> 00:06:45.990
black or white,
and not knowing you always look

00:06:45.990 --> 00:06:50.639
from the standpoint of the most
adversely affected person,

00:06:50.639 --> 00:06:54.610
and so you would say no to
apartheid.

00:06:54.610 --> 00:06:59.692
You would say no to a system
which privileges one gender over

00:06:59.692 --> 00:07:00.709
the other.
 

00:07:00.709 --> 00:07:04.718
And then you can see,
I think, how their lexical

00:07:04.718 --> 00:07:09.836
ranking would come into play
because let's suppose you have a

00:07:09.836 --> 00:07:13.689
status quo in which,
as I said, women on average

00:07:13.689 --> 00:07:16.939
earn eighty-six percent of what
men earn in the same

00:07:16.939 --> 00:07:19.442
professions,
and somebody comes along and

00:07:19.442 --> 00:07:22.300
says, "Well,
so we need an affirmative

00:07:22.300 --> 00:07:25.040
action program to remedy
that."

00:07:25.040 --> 00:07:30.129
Then the question would be,
but does the affirmative action

00:07:30.129 --> 00:07:34.690
program conflict with anything
protected by the first

00:07:34.690 --> 00:07:35.920
principle?
 

00:07:35.920 --> 00:07:37.920
And those opposed to it would
say,

00:07:37.920 --> 00:07:39.838
"Yes,"
and those in favor of it would

00:07:39.838 --> 00:07:40.956
say,
"No,"

00:07:40.956 --> 00:07:43.379
and that's what you would be
arguing about.

00:07:43.379 --> 00:07:48.331
Because it might be the case
that if the only way in which

00:07:48.331 --> 00:07:53.541
you could achieve affirmative
action actually interfered with

00:07:53.541 --> 00:07:58.754
the liberties protected by the
first principle then you would

00:07:58.754 --> 00:08:00.762
say,
"Even though it's

00:08:00.762 --> 00:08:03.439
necessary from the standpoint of
the second principle,

00:08:03.439 --> 00:08:05.189
we won't do it."
 

00:08:05.189 --> 00:08:09.687
And if we had more time we
could have gone into the New

00:08:09.687 --> 00:08:13.903
Haven firefighter's case,
and maybe we can do some of

00:08:13.903 --> 00:08:17.562
this in section,
that the Supreme Court dealt

00:08:17.562 --> 00:08:21.682
with last summer where
essentially they said some

00:08:21.682 --> 00:08:26.574
version of the fact that the
affirmative action program to

00:08:26.574 --> 00:08:31.809
achieve promotions in the New
Haven Fire Department interfered

00:08:31.809 --> 00:08:36.958
with basic freedoms that Rawls
would have put under the first

00:08:36.958 --> 00:08:38.590
principle.
 

00:08:38.590 --> 00:08:42.158
And of course the other side
made the opposite claim.

00:08:42.158 --> 00:08:47.629
But that is essentially how it
would be argued about.

00:08:47.629 --> 00:08:51.355
So I think that the principal
of fair equality of opportunity

00:08:51.355 --> 00:08:54.399
is relatively straightforward in
its own terms.

00:08:54.399 --> 00:09:02.836
The animating thought is that
not knowing who you're going to

00:09:02.836 --> 00:09:05.390
be,
you would never agree to a

00:09:05.390 --> 00:09:08.946
system that systematically
disprivileges some group for

00:09:08.946 --> 00:09:12.500
fear that you're going to turn
out to be in that group,

00:09:12.500 --> 00:09:13.158
okay?
 

00:09:13.158 --> 00:09:16.220
So it's relatively
straightforward.

00:09:16.220 --> 00:09:24.216
But now I want to come to 2a,
which is probably the argument

00:09:24.216 --> 00:09:31.668
in Rawls's book that's attracted
the most attention,

00:09:31.668 --> 00:09:37.288
and that is that--it's actually
third in his lexical ranking,

00:09:37.288 --> 00:09:41.938
and that is the claim that
income and wealth is to be

00:09:41.938 --> 00:09:46.678
distributed "to the
greatest benefit of the least

00:09:46.678 --> 00:09:49.538
advantaged individual."
 

00:09:49.538 --> 00:09:55.490
To the greatest benefit of the
least advantaged individual.

00:09:55.490 --> 00:09:59.720
This is not a principle that
Rawls invented.

00:09:59.720 --> 00:10:04.908
It's an old principle of
welfare economics which used to

00:10:04.908 --> 00:10:09.351
go under the label maximin,
not maximum,

00:10:09.351 --> 00:10:15.417
maximin, m-a-x-i-m-i-n,
short for maximize the minimum

00:10:15.417 --> 00:10:19.668
share,
maximin, maximize the minimum

00:10:19.668 --> 00:10:20.519
share.
 

00:10:20.519 --> 00:10:25.365
Rawls calls it the difference
principal, but it's the same

00:10:25.365 --> 00:10:28.678
idea as maximizing the minimum
share.

00:10:28.678 --> 00:10:32.971
And the intuition behind the
difference principle is exactly

00:10:32.971 --> 00:10:37.337
the same intuition that we've
been talking about by reference

00:10:37.337 --> 00:10:41.340
to the general conception of
distributive justice,

00:10:41.340 --> 00:10:45.345
which is, remember,
distribute all goods equally

00:10:45.345 --> 00:10:50.798
unless an unequal distribution
works to everybody's advantage.

00:10:50.798 --> 00:10:55.857
And you get from everybody's
advantage to focusing on the

00:10:55.857 --> 00:10:58.297
condition of the worst off.
 

00:10:58.297 --> 00:10:58.928
Why?
 

00:10:58.928 --> 00:11:03.527
Because of this argument that,
well, if you're the worst-off

00:11:03.527 --> 00:11:07.735
person and you can affirm
something then everybody else

00:11:07.735 --> 00:11:09.840
will affirm it as well.
 

00:11:09.840 --> 00:11:12.801
If you'll choose it when you're
the most adversely affected

00:11:12.801 --> 00:11:15.149
you'll also choose it if you're
the second,

00:11:15.149 --> 00:11:18.893
or third, or fourth,
or fifth most adversely

00:11:18.893 --> 00:11:20.460
affected person.
 

00:11:20.460 --> 00:11:25.003
Now, there's actually a
complexity when you start to

00:11:25.003 --> 00:11:30.437
think about the distribution of
income and wealth that has not

00:11:30.437 --> 00:11:35.960
come up in the consideration of
the other two principles,

00:11:35.960 --> 00:11:39.148
which I'll just mention,
and then say a couple of things

00:11:39.148 --> 00:11:41.447
about,
and then move on and we'll come

00:11:41.447 --> 00:11:42.480
back to it later.
 

00:11:42.480 --> 00:11:46.046
And that is,
well, but what if there was a

00:11:46.046 --> 00:11:51.265
principle that gave a very small
benefit to the person at the

00:11:51.265 --> 00:11:55.437
bottom,
but at a huge cost to the

00:11:55.437 --> 00:11:57.190
middle class?
 

00:11:57.190 --> 00:12:02.296
Would you choose it,
because what are the odds that

00:12:02.296 --> 00:12:08.730
you're going to turn out to be
the person at the very bottom?

00:12:08.730 --> 00:12:15.158
And we think about this for a
variety of reasons.

00:12:15.158 --> 00:12:17.988
It might be a trickle down
argument, or Bentham's claim

00:12:17.988 --> 00:12:21.130
that the rich will burn their
crops before giving them to the

00:12:21.130 --> 00:12:22.860
poor, or some other argument.
 

00:12:22.860 --> 00:12:26.166
But if you could achieve a very
minor increment to the condition

00:12:26.166 --> 00:12:29.629
of the person at the bottom at a
huge cost to the middle class,

00:12:29.629 --> 00:12:36.928
you wouldn't necessarily want
to do that.

00:12:36.928 --> 00:12:41.717
And so Rawls has two points to
make about that,

00:12:41.717 --> 00:12:46.090
neither of which is entirely
satisfying.

00:12:46.090 --> 00:12:51.480
The one is his argument about
grave risks, and it works like

00:12:51.480 --> 00:12:52.120
this.
 

00:12:52.120 --> 00:12:58.371
It's the claim that,
well, one of the things we

00:12:58.371 --> 00:13:01.573
know,
and this is a perfectly

00:13:01.573 --> 00:13:05.722
uncontroversial claim,
one of the things we know is

00:13:05.722 --> 00:13:10.096
that even when there's moderate
scarcity that doesn't mean there

00:13:10.096 --> 00:13:13.428
won't be some people who are in
grave danger.

00:13:13.428 --> 00:13:16.740
That is to say there's no
necessary relationship between

00:13:16.740 --> 00:13:19.932
the level of economic
development in a country and the

00:13:19.932 --> 00:13:22.220
distribution of income and
wealth.

00:13:22.220 --> 00:13:25.357
So you can have a wealthy
country, but there still can be

00:13:25.357 --> 00:13:27.039
extremely poor people in it.
 

00:13:27.039 --> 00:13:30.808
That's true.
 

00:13:30.808 --> 00:13:34.510
We can have bag ladies living
out of lockers in Grand Central

00:13:34.510 --> 00:13:36.702
Station,
at least when they used to have

00:13:36.702 --> 00:13:38.408
lockers in Grand Central
Station,

00:13:38.408 --> 00:13:43.378
which they don't anymore,
but let's not deal with that

00:13:43.378 --> 00:13:45.158
particular piece.
 

00:13:45.158 --> 00:13:48.272
So there's no necessary
relationship between the level

00:13:48.272 --> 00:13:51.620
of economic development and the
distribution of income and

00:13:51.620 --> 00:13:53.029
wealth in a society.
 

00:13:53.029 --> 00:14:00.168
Therefore, you have to assume
that even if there's relative

00:14:00.168 --> 00:14:07.798
scarcity you might turn out to
be the person who's starving.

00:14:07.798 --> 00:14:10.580
You might turn out to be that
bag lady.

00:14:10.580 --> 00:14:17.378
And even if the probability of
being that person is low,

00:14:17.378 --> 00:14:22.570
the costs of being that person
are high.

00:14:22.570 --> 00:14:30.251
I don't know if you remember
the argument Rumsfeld made in

00:14:30.251 --> 00:14:37.025
his counterterrorism strategy,
the so-called one percent

00:14:37.025 --> 00:14:39.981
solution,
and this was that even if

00:14:39.981 --> 00:14:44.003
there's a one percent
probability that we're going to

00:14:44.003 --> 00:14:48.565
be hit by a certain kind of
terrorist attack we should treat

00:14:48.565 --> 00:14:52.817
it as a hundred percent
probability because the costs of

00:14:52.817 --> 00:14:55.058
being hit are so high.
 

00:14:55.058 --> 00:14:59.580
So the probability of the event
may be low,

00:14:59.580 --> 00:15:03.623
but if you turn out to be that
person you're going to starve to

00:15:03.623 --> 00:15:07.599
death,
so this is Rawls's assumption

00:15:07.599 --> 00:15:09.820
about grave risks.
 

00:15:09.820 --> 00:15:12.908
So all of that's plausible
enough.

00:15:12.908 --> 00:15:16.333
The reason I say it's not
entirely satisfying is if you

00:15:16.333 --> 00:15:20.392
really took the grave risks idea
seriously why in the world would

00:15:20.392 --> 00:15:23.500
you make this third in your
lexical ranking,

00:15:23.500 --> 00:15:27.096
because after all what good is
freedom of speech or freedom of

00:15:27.096 --> 00:15:30.399
religion to somebody who's on
the verge of starvation.

00:15:30.399 --> 00:15:34.644
So it's not entirely satisfying
in that sense that if it

00:15:34.644 --> 00:15:37.676
justifies saying,
well, we will protect the

00:15:37.676 --> 00:15:41.363
person at the bottom even though
the probability of that person

00:15:41.363 --> 00:15:45.048
turns out to be low because of
the grave risks assumption,

00:15:45.048 --> 00:15:55.583
why then--this is very
annoying--why then would we make

00:15:55.583 --> 00:15:57.730
it third?
 

00:15:57.730 --> 00:16:01.014
But it's not really a deep
criticism of Rawls in that you

00:16:01.014 --> 00:16:03.649
could just say,
"Well, we should have

00:16:03.649 --> 00:16:07.538
reorder his lexical ranking and
put this higher up in it."

00:16:07.538 --> 00:16:11.528
But anyway, that's not entirely
satisfying.

00:16:11.528 --> 00:16:16.255
And then the second thing he
says that's not entirely

00:16:16.255 --> 00:16:21.619
satisfying is he says he's sort
of sensitive to this problem

00:16:21.619 --> 00:16:27.254
that you might get absurdities
out of this because if it's very

00:16:27.254 --> 00:16:32.798
costly to help the person at the
bottom in terms of what other

00:16:32.798 --> 00:16:37.434
people have to give up maybe
people wouldn't be that

00:16:37.434 --> 00:16:41.980
impressed by the grave risks
assumption.

00:16:41.980 --> 00:16:45.240
So he throws in this idea of
chain connection and he says,

00:16:45.240 --> 00:16:52.039
"Well, even though my
argument doesn't depend on this,

00:16:52.039 --> 00:16:55.700
I think it's true."
 

00:16:55.700 --> 00:16:58.928
When somebody does that you
know there's some slight-of-hand

00:16:58.928 --> 00:16:59.529
going on.
 

00:16:59.529 --> 00:17:03.679
And he basically says,
"Well, if you help the

00:17:03.679 --> 00:17:08.167
person at the bottom that will
have some kind of chain

00:17:08.167 --> 00:17:09.269
reaction.
 

00:17:09.269 --> 00:17:11.630
It'll help the person at the
next level,

00:17:11.630 --> 00:17:13.338
and that'll help the person at
the next level,

00:17:13.338 --> 00:17:16.269
and that'll help the person at
the next level,"

00:17:16.269 --> 00:17:19.545
so it's a kind of Keynesian
idea that if you stimulate the

00:17:19.545 --> 00:17:22.876
man at the bottom there'll be
multiplier effects throughout

00:17:22.876 --> 00:17:26.670
the whole system that'll make
everybody else better off too.

00:17:26.670 --> 00:17:28.868
Well, that may or may not be
true,

00:17:28.868 --> 00:17:31.833
and it also,
by the way, I think makes the

00:17:31.833 --> 00:17:36.169
disagreement between Rawlsianism
and utilitarianism much less

00:17:36.169 --> 00:17:40.215
interesting because then
anything that Rawls would choose

00:17:40.215 --> 00:17:43.180
a utilitarian would choose as
well.

00:17:43.180 --> 00:17:46.700
And we really want to look at
when they pull in opposite

00:17:46.700 --> 00:17:50.538
directions if we want to see
what's at stake between them.

00:17:50.538 --> 00:17:54.548
But so this chain connection
idea I think is just sort of--

00:17:54.548 --> 00:18:00.635
he throws it in there to make
his argument look more appealing

00:18:00.635 --> 00:18:05.804
on consequentialist grounds,
but actually there's (a) no

00:18:05.804 --> 00:18:09.419
reason to believe it's true,
and (b) if it were,

00:18:09.419 --> 00:18:13.487
then what's really at stake
between Rawls and utilitarianism

00:18:13.487 --> 00:18:17.346
becomes much less interesting
because by satisfying Rawls

00:18:17.346 --> 00:18:21.068
we're also going to be
satisfying utilitarianism.

00:18:21.068 --> 00:18:25.557
So I think the best thing about
chain connection is to ignore

00:18:25.557 --> 00:18:28.848
it, so I'm not going to say
anything more.

00:18:28.848 --> 00:18:35.910
But I will say this in Rawls's
defense on this point,

00:18:35.910 --> 00:18:41.200
which is, a lot of people who
criticize Rawls create--

00:18:41.200 --> 00:18:44.827
and I even did some of this
myself what I think is unfair in

00:18:44.827 --> 00:18:48.413
retrospect--
people create examples where

00:18:48.413 --> 00:18:54.444
helping the person at the bottom
comes at a huge cost to others

00:18:54.444 --> 00:18:57.848
and it looks rather implausible.
 

00:18:57.848 --> 00:19:02.065
But one thing we should say
about Rawls is,

00:19:02.065 --> 00:19:07.585
he's not trying to give policy
advice for every marginal

00:19:07.585 --> 00:19:08.690
choice.
 

00:19:08.690 --> 00:19:12.152
At one point he says in the
book, "I'm thinking about

00:19:12.152 --> 00:19:15.980
the basic structure of society,
the basic institutions."

00:19:15.980 --> 00:19:22.013
So he's not saying--I mean,
the example people sometimes

00:19:22.013 --> 00:19:28.707
give is the Reagan tax cuts in
the 1980s, or actually the Bush

00:19:28.707 --> 00:19:31.450
tax cut in the 2000s.
 

00:19:31.450 --> 00:19:34.896
But the Reagan one had this
structure more explicitly where

00:19:34.896 --> 00:19:37.750
there was a very big tax cut for
the wealthy,

00:19:37.750 --> 00:19:41.240
a tiny tax cut for the people
at the very bottom,

00:19:41.240 --> 00:19:45.490
and a huge increase in middle
class taxes.

00:19:45.490 --> 00:19:47.680
Basically that was the
structure of it.

00:19:47.680 --> 00:19:50.128
And people said,
"So Rawls would prefer

00:19:50.128 --> 00:19:50.868
this."
 

00:19:50.868 --> 00:19:57.128
And the answer is he's not
trying to make a recommendation

00:19:57.128 --> 00:20:03.058
at the level of the next
incremental policy choice,

00:20:03.058 --> 00:20:06.489
he's trying to say what the
underlying institution should be

00:20:06.489 --> 00:20:07.420
structured as.
 

00:20:07.420 --> 00:20:10.930
And so he would resist saying,
"Well,

00:20:10.930 --> 00:20:15.114
this shows my theory is silly,
or my theory doesn't generate

00:20:15.114 --> 00:20:17.809
conclusions that I want it
to."

00:20:17.809 --> 00:20:21.720
He's not a policy wonk.
 

00:20:21.720 --> 00:20:25.826
He's thinking about
constitutional principles,

00:20:25.826 --> 00:20:30.190
basic principles;
the basic structure of society.

00:20:30.190 --> 00:20:35.932
And indeed, I'll just make one
footnote to that footnote which

00:20:35.932 --> 00:20:41.676
is if you start at the front of
A Theory of Justice and

00:20:41.676 --> 00:20:45.348
you really plow through all of
it,

00:20:45.348 --> 00:20:50.717
you get to about page 300 and
something and he says words to

00:20:50.717 --> 00:20:56.175
the effect that his theory is
agnostic between capitalism and

00:20:56.175 --> 00:20:57.450
socialism.
 

00:20:57.450 --> 00:21:04.759
And he took a lot of abuse for
that in the 1970s and 1980s.

00:21:04.759 --> 00:21:07.022
People said,
"Wow, you mean I plowed

00:21:07.022 --> 00:21:10.192
through 300 pages of a book
about justice only to be told

00:21:10.192 --> 00:21:13.079
it's agnostic between capitalism
and socialism?

00:21:13.079 --> 00:21:14.548
Give me a break!"
 

00:21:14.548 --> 00:21:19.598
But in defense of Rawls on that
point he would say,

00:21:19.598 --> 00:21:25.267
"Look, what economic
system actually operates in the

00:21:25.267 --> 00:21:28.808
interest of the least
advantaged?

00:21:28.808 --> 00:21:32.144
That's an empirical question of
political economy,

00:21:32.144 --> 00:21:34.048
trial and error and so on.
 

00:21:34.048 --> 00:21:38.029
That is not a question for
political philosophy to settle.

00:21:38.029 --> 00:21:41.240
I don't know whether it's
capitalism,

00:21:41.240 --> 00:21:44.009
socialism, or some version of a
mixed economy,

00:21:44.009 --> 00:21:48.663
that works to the greatest
benefit of the least advantaged

00:21:48.663 --> 00:21:49.400
person.
 

00:21:49.400 --> 00:21:52.788
That's for the policy makers
and political economists to

00:21:52.788 --> 00:21:53.588
figure out.
 

00:21:53.588 --> 00:21:57.130
What I'm telling you is what
the standard should be."

00:21:57.130 --> 00:22:01.108
And that is a good argument on
Rawls' part.

00:22:01.108 --> 00:22:04.327
And he's saying,
"This is what the standard

00:22:04.327 --> 00:22:05.150
should be.
 

00:22:05.150 --> 00:22:08.740
The standard should be that
whatever system you have works

00:22:08.740 --> 00:22:12.394
to the greatest benefit of the
least advantaged player when

00:22:12.394 --> 00:22:14.788
compared with other
systems."

00:22:14.788 --> 00:22:20.444
And, yes, before the experience
of centrally planned economies

00:22:20.444 --> 00:22:26.007
people may have thought some
version of state socialism would

00:22:26.007 --> 00:22:27.118
do that.
 

00:22:27.118 --> 00:22:30.455
After a half a century of
experience with it,

00:22:30.455 --> 00:22:32.200
doesn't look so good.
 

00:22:32.200 --> 00:22:35.710
So we'll go back to some kind
of market system,

00:22:35.710 --> 00:22:39.840
and after decades of experience
with unregulated markets or

00:22:39.840 --> 00:22:43.717
minimally regulated markets,
and we discover the cost of

00:22:43.717 --> 00:22:47.238
those for the people at the
bottom maybe we'll end up with

00:22:47.238 --> 00:22:48.348
something else.
 

00:22:48.348 --> 00:22:51.896
"So it's not a failing of
my, John Rawls's,

00:22:51.896 --> 00:22:56.349
theory that I don't tell you
what kind of economic system to

00:22:56.349 --> 00:22:57.029
have.
 

00:22:57.029 --> 00:23:01.308
My aspiration is to tell you
what the normative criterion is

00:23:01.308 --> 00:23:03.338
that it should meet."
 

00:23:03.338 --> 00:23:11.170
So I think that's the most
important takeaway point.

00:23:11.170 --> 00:23:22.491
Now let's give you a picture
for those who like pictures,

00:23:22.491 --> 00:23:33.410
and for those who don't we will
explain it in words.

00:23:33.410 --> 00:23:37.548
This is going back to our
Pareto style of diagram.

00:23:37.548 --> 00:23:40.180
Let's suppose that's the status
quo.

00:23:40.180 --> 00:23:44.119
And now we've got primary
goods, in this case income and

00:23:44.119 --> 00:23:47.549
wealth for two people;
A up here and B along here.

00:23:47.549 --> 00:23:51.460
And that is the status quo.
 

00:23:51.460 --> 00:23:56.058
So A has more than B.
 

00:23:56.058 --> 00:23:59.536
Rawls' difference principle,
or the so-called maximin

00:23:59.536 --> 00:24:05.759
principle of welfare economics,
says, "Drop a line down to

00:24:05.759 --> 00:24:10.163
there,
(that point is perfect

00:24:10.163 --> 00:24:15.700
equality, right) and then go
east,

00:24:15.700 --> 00:24:23.623
and everything in this shaded
area is what we might call Rawls

00:24:23.623 --> 00:24:28.170
superior to the status
quo."

00:24:28.170 --> 00:24:31.730
So it's a kind of L-shaped
indifference curve.

00:24:31.730 --> 00:24:36.875
It goes down though the status
quo to equality and then it

00:24:36.875 --> 00:24:38.140
turns right.
 

00:24:38.140 --> 00:24:42.368
 
 

00:24:42.368 --> 00:24:50.118
Anyone want to take a stab at
telling us why?

00:24:50.118 --> 00:24:56.828
Why would you have these L
shaped indifference curves?

00:24:56.828 --> 00:24:57.943
Yeah?
 

00:24:57.943 --> 00:25:08.420
Why don't you get the mic,
or come to the mic?

00:25:08.420 --> 00:25:12.038
Student:  You can move
right as far as possible and

00:25:12.038 --> 00:25:14.769
that will be increasing the
goods for B,

00:25:14.769 --> 00:25:20.017
and then as you move down,
you have to stop at the

00:25:20.017 --> 00:25:21.410
quantity...
 

00:25:21.410 --> 00:25:26.200
Prof: Say A has this
much when we start out.

00:25:26.200 --> 00:25:29.673
Why isn't this point here that
I'm lining up,

00:25:29.673 --> 00:25:31.490
say, Rawls preferred?
 

00:25:31.490 --> 00:25:34.529
Student:  Because then A
becomes the least advantaged

00:25:34.529 --> 00:25:36.900
person and they have less than B
had before.

00:25:36.900 --> 00:25:37.674
Prof: Exactly right.
 

00:25:37.674 --> 00:25:38.019
You got it.
 

00:25:38.019 --> 00:25:42.740
So the reason we head east or
turn right at the point of

00:25:42.740 --> 00:25:46.088
equality is what are we trying
to do?

00:25:46.088 --> 00:25:49.038
We're maximizing the minimum
share.

00:25:49.038 --> 00:25:53.289
We're saying the person--all we
want to say is that whoever

00:25:53.289 --> 00:25:57.980
turns out to be at the bottom
has the highest possible share,

00:25:57.980 --> 00:26:02.569
so if we went from X to down
here then we would have changed

00:26:02.569 --> 00:26:06.338
who's at the bottom,
and that's not important.

00:26:06.338 --> 00:26:09.605
What would be important is that
this bottom share would be

00:26:09.605 --> 00:26:11.670
smaller and we wouldn't want
that.

00:26:11.670 --> 00:26:16.451
So we don't care who gets it
because we don't know whether

00:26:16.451 --> 00:26:17.710
we're A or B.
 

00:26:17.710 --> 00:26:21.200
That's not material.
 

00:26:21.200 --> 00:26:24.386
We don't know when the veil of
ignorance turns out to be

00:26:24.386 --> 00:26:27.690
lifted, we don't know whether
we're going to be A or B.

00:26:27.690 --> 00:26:31.980
So we're just going to assume
we're going to be whoever turns

00:26:31.980 --> 00:26:33.838
out to be the worst off.
 

00:26:33.838 --> 00:26:38.721
So this distance here
represents the minimum and you

00:26:38.721 --> 00:26:43.029
wouldn't want it to get smaller,
basically.

00:26:43.029 --> 00:26:48.393
So if we moved anywhere in this
area here the minimum share

00:26:48.393 --> 00:26:50.150
would get bigger.
 

00:26:50.150 --> 00:26:54.532
So if we went to Y then we
could do a new L-shaped

00:26:54.532 --> 00:26:58.380
indifference curve--why is it
doing this?

00:26:58.380 --> 00:27:02.743
Much later, why isn't there a
"restart much later"

00:27:02.743 --> 00:27:03.410
button?
 

00:27:03.410 --> 00:27:06.599
 
 

00:27:06.599 --> 00:27:10.220
Okay, so that's the basic idea.
 

00:27:10.220 --> 00:27:15.818
You just get keep getting these
L-shaped indifference curves.

00:27:15.818 --> 00:27:22.574
Now I want to say something
about what a radical idea this

00:27:22.574 --> 00:27:26.130
is in a philosophical sense.
 

00:27:26.130 --> 00:27:30.593
It's not necessarily that
radical in a distributive sense

00:27:30.593 --> 00:27:34.338
for the reason I've already
indicated to you.

00:27:34.338 --> 00:27:38.128
It could be compatible with
trickle-down if we took the view

00:27:38.128 --> 00:27:41.981
that trickle-down works better
than any other system from the

00:27:41.981 --> 00:27:44.680
point of view of the least
advantaged.

00:27:44.680 --> 00:27:58.250
Let's put this Rawls,
Bentham and Pareto compared.

00:27:58.250 --> 00:28:00.721
This is a little taking
something of a liberty because

00:28:00.721 --> 00:28:02.680
we've got different things on
the axes,

00:28:02.680 --> 00:28:06.338
so it's sort of a little
ultimately not coherent,

00:28:06.338 --> 00:28:09.720
but I think you can still get
an insight out of it.

00:28:09.720 --> 00:28:13.711
If we start with that status
quo we know what's

00:28:13.711 --> 00:28:16.923
Pareto-preferred,
so everything that's

00:28:16.923 --> 00:28:20.828
Pareto-preferred is also
Rawls-preferred.

00:28:20.828 --> 00:28:27.757
So if it turned out to be true
that the best way to help the

00:28:27.757 --> 00:28:34.683
person at the bottom is to have
only the market transactions

00:28:34.683 --> 00:28:40.079
then we would do it,
but if it turned out that there

00:28:40.079 --> 00:28:43.808
were other ways that were
Pareto-undecidable,

00:28:43.808 --> 00:28:49.538
like these, to help the person
at the bottom we would do that.

00:28:49.538 --> 00:28:53.660
So it's not necessarily radical
in a distributive sense.

00:28:53.660 --> 00:28:58.443
You could get very egalitarian
radical redistribution,

00:28:58.443 --> 00:29:03.680
but you could also get--you
could get no redistribution.

00:29:03.680 --> 00:29:08.976
You could get the Pareto system
if that turned out to be the way

00:29:08.976 --> 00:29:14.019
in which the most disadvantaged
person is helped the most.

00:29:14.019 --> 00:29:19.891
But it is radical in a
philosophical sense that I think

00:29:19.891 --> 00:29:26.307
is captured by the observation
that we don't care whether we

00:29:26.307 --> 00:29:32.069
turn out to be A or whether we
turn out to be B,

00:29:32.069 --> 00:29:37.450
and that is the following.
 

00:29:37.450 --> 00:29:46.470
 
 

00:29:46.470 --> 00:29:52.368
There's been a huge debate in
our lifetimes over whether the

00:29:52.368 --> 00:29:58.170
differences between us are the
result of nature or nurture,

00:29:58.170 --> 00:30:00.470
an enormous debate.
 

00:30:00.470 --> 00:30:06.098
You read a book like Charles
Murray, Losing Ground.

00:30:06.098 --> 00:30:08.630
How many people have heard of
that book?

00:30:08.630 --> 00:30:09.653
Nobody?
 

00:30:09.653 --> 00:30:14.778
Wow, so how quickly things
change.

00:30:14.778 --> 00:30:18.961
Well, it was a book that came
out about probably twenty years

00:30:18.961 --> 00:30:20.885
ago,
that's probably why you haven't

00:30:20.885 --> 00:30:23.662
read it,
basically saying that the

00:30:23.662 --> 00:30:28.380
differences between us are
genetically determined.

00:30:28.380 --> 00:30:33.492
There are genetic differences
in IQ that show up in various

00:30:33.492 --> 00:30:38.164
ways including racially,
and there was a huge storm of

00:30:38.164 --> 00:30:39.400
criticism.
 

00:30:39.400 --> 00:30:43.741
He was accused of being a
racist, and there were charges

00:30:43.741 --> 00:30:47.848
and countercharges,
and people said, "No,

00:30:47.848 --> 00:30:49.744
it's not genetics,
it's environment,"

00:30:49.744 --> 00:30:50.298
and so on.
 

00:30:50.298 --> 00:30:52.990
So one of the most important
things--

00:30:52.990 --> 00:30:55.798
and I'm going to focus on this
much more next Monday,

00:30:55.798 --> 00:30:59.390
I just want to mention it now
so you can think about it--

00:30:59.390 --> 00:31:03.240
one of the points Rawls makes
is, "Look,

00:31:03.240 --> 00:31:10.230
possibly the differences
between us are genetic.

00:31:10.230 --> 00:31:14.993
If the differences between us
are genetic it's just moral luck

00:31:14.993 --> 00:31:19.759
because you didn't choose to
have the genes that you have,

00:31:19.759 --> 00:31:23.604
and not only didn't you choose
it, you didn't do anything to

00:31:23.604 --> 00:31:25.690
get the genes that you've got.
 

00:31:25.690 --> 00:31:29.210
It's moral luck.
 

00:31:29.210 --> 00:31:32.922
On the other hand,
suppose differences between us

00:31:32.922 --> 00:31:34.470
are environmental?
 

00:31:34.470 --> 00:31:38.990
Well, it's moral luck.
 

00:31:38.990 --> 00:31:42.038
You didn't choose to be born in
the country and the family you

00:31:42.038 --> 00:31:42.890
were born into.
 

00:31:42.890 --> 00:31:46.029
You didn't make any choices in
that regard.

00:31:46.029 --> 00:31:50.240
Furthermore,
you didn't do any work to be in

00:31:50.240 --> 00:31:56.509
the country or the family you
happened to have been raised in.

00:31:56.509 --> 00:31:57.990
Again, it's just luck.
 

00:31:57.990 --> 00:32:01.150
From your point of view it's a
completely random thing.

00:32:01.150 --> 00:32:03.670
You could have been born
somewhere else to somebody else,

00:32:03.670 --> 00:32:08.241
or you could have been born to
parents who didn't have the

00:32:08.241 --> 00:32:11.048
resources that your parents
have.

00:32:11.048 --> 00:32:14.412
So this whole debate about
nature and nurture,"

00:32:14.412 --> 00:32:17.380
says John Rawls,
"is beside the point.

00:32:17.380 --> 00:32:23.480
From the standpoint of justice
we don't care."

00:32:23.480 --> 00:32:24.920
And that is his argument.
 

00:32:24.920 --> 00:32:30.031
I think, for what it's worth,
the most important argument in

00:32:30.031 --> 00:32:35.577
Rawls' book that the differences
between us are morally arbitrary

00:32:35.577 --> 00:32:38.609
whether it's nature or nurture.
 

00:32:38.609 --> 00:32:40.588
It doesn't matter.
 

00:32:40.588 --> 00:32:46.195
They're not the result of
choice, and they're not the

00:32:46.195 --> 00:32:48.029
result of work.
 

00:32:48.029 --> 00:32:56.037
They just fell out of the sky
as far as we're actually

00:32:56.037 --> 00:32:57.848
concerned.
 

00:32:57.848 --> 00:33:01.162
That being the case,
and I'm going to go into the

00:33:01.162 --> 00:33:04.750
assumptions behind that in more
detail on Monday,

00:33:04.750 --> 00:33:08.846
but from the point of view of
this discussion so we don't

00:33:08.846 --> 00:33:12.358
really care if A or B is the
worse-off person.

00:33:12.358 --> 00:33:15.904
We're just going to say from
the standpoint of justice we

00:33:15.904 --> 00:33:18.881
want to improve the lot of the
worst-off person,

00:33:18.881 --> 00:33:21.730
and even if the worst-off
person changes.

00:33:21.730 --> 00:33:25.430
So we go from X to G.
 

00:33:25.430 --> 00:33:28.660
 
 

00:33:28.660 --> 00:33:30.548
It's morally irrelevant.
 

00:33:30.548 --> 00:33:36.752
All we want to do is maximize
the share of the person at the

00:33:36.752 --> 00:33:37.700
bottom.
 

00:33:37.700 --> 00:33:44.108
 
 

00:33:44.108 --> 00:33:47.839
So that's the Rawlsian
difference principle.

00:33:47.839 --> 00:33:51.368
 
 

00:33:51.368 --> 00:33:59.865
And as I said,
you can see it overlaps with

00:33:59.865 --> 00:34:07.150
and contains the Pareto
Principle.

00:34:07.150 --> 00:34:11.349
And it has some overlap with
Bentham in that it would

00:34:11.349 --> 00:34:15.550
sanction moving into the
Pareto-undecidable zone here

00:34:15.550 --> 00:34:20.315
that would be Bentham preferred
if it works for the greatest

00:34:20.315 --> 00:34:23.949
benefit of the least advantaged
person.

00:34:23.949 --> 00:34:28.945
And Rawls' claim to you,
the reader, is that this is the

00:34:28.945 --> 00:34:31.579
principle you would choose.
 

00:34:31.579 --> 00:34:35.802
You would want the economic
system that works to the benefit

00:34:35.802 --> 00:34:37.949
of the person at the bottom.
 

00:34:37.949 --> 00:34:41.309
 
 

00:34:41.309 --> 00:34:43.670
What do you think?
 

00:34:43.670 --> 00:34:51.090
 
 

00:34:51.090 --> 00:34:53.650
Who likes this idea?
 

00:34:53.650 --> 00:34:56.300
Who doesn't like it?
 

00:34:56.300 --> 00:34:58.150
What don't you like about it?
 

00:34:58.150 --> 00:35:07.030
 
 

00:35:07.030 --> 00:35:11.539
Who was--it was here, yeah.
 

00:35:11.539 --> 00:35:14.197
Student:  It assumes
that once you're born you're

00:35:14.197 --> 00:35:17.050
going to stay in that position
for the rest of your life.

00:35:17.050 --> 00:35:20.349
There's nothing you can do
about it.

00:35:20.349 --> 00:35:22.579
Prof: Okay,
well that's a good observation.

00:35:22.579 --> 00:35:26.780
I'm not entirely sure what
you're saying.

00:35:26.780 --> 00:35:28.737
Just explain a little bit more
and I'll see if you're saying

00:35:28.737 --> 00:35:29.699
what I think you're saying.
 

00:35:29.699 --> 00:35:31.427
Student:  Well,
what about effort that people

00:35:31.427 --> 00:35:32.750
in to changing their social
position?

00:35:32.750 --> 00:35:34.340
Prof: Ah,
what about effort?

00:35:34.340 --> 00:35:36.010
Okay, what about effort?
 

00:35:36.010 --> 00:35:37.480
I thought you were making
another point,

00:35:37.480 --> 00:35:41.190
so let me just respond to the
point I thought you were making,

00:35:41.190 --> 00:35:44.248
which you weren't making,
but we should nonetheless

00:35:44.248 --> 00:35:46.940
address since people do
sometimes make it.

00:35:46.940 --> 00:35:48.855
But then I'll come to your
point which is,

00:35:48.855 --> 00:35:50.349
anyway, much more interesting.
 

00:35:50.349 --> 00:35:54.387
The point I thought you were
making is this has no dynamic

00:35:54.387 --> 00:35:55.309
side to it.
 

00:35:55.309 --> 00:35:59.871
That is to say it's static in
exactly the way the Pareto

00:35:59.871 --> 00:36:03.985
Principle is static,
but any economist would want a

00:36:03.985 --> 00:36:07.150
theory that has a dynamic
dimension to it.

00:36:07.150 --> 00:36:10.853
You would want to know over
time, what's the effect of a

00:36:10.853 --> 00:36:13.010
certain redistributive change?
 

00:36:13.010 --> 00:36:22.599
So we would want to say,
"Well,

00:36:22.599 --> 00:36:29.951
if benefiting the person at the
bottom slightly improves their

00:36:29.951 --> 00:36:34.289
welfare in the next three
months,

00:36:34.289 --> 00:36:38.050
but it comes at the cost of
lower economic growth over time,

00:36:38.050 --> 00:36:40.739
would we want to do that?"
 

00:36:40.739 --> 00:36:45.913
And it's fair to say Rawls
doesn't have an answer to that

00:36:45.913 --> 00:36:46.929
question.
 

00:36:46.929 --> 00:36:48.989
He doesn't have a dynamic
theory.

00:36:48.989 --> 00:36:51.050
On the other hand I think his
defense,

00:36:51.050 --> 00:36:54.510
this is why it's ultimately not
a very interesting criticism,

00:36:54.510 --> 00:36:56.639
I think his defense would kick
in, that,

00:36:56.639 --> 00:36:59.610
"Well, I'm telling you
what the criterion should be,

00:36:59.610 --> 00:37:01.800
not how to run the
economy."

00:37:01.800 --> 00:37:06.739
But let's come to the point
about effort.

00:37:06.739 --> 00:37:08.630
And this is going to,
to some degree,

00:37:08.630 --> 00:37:14.777
get us into next Monday's
lecture, but it's good to make a

00:37:14.777 --> 00:37:20.710
start at it because it's a very
deep point, actually.

00:37:20.710 --> 00:37:22.820
What about effort?
 

00:37:22.820 --> 00:37:30.108
So yes, the capacities we have
might be distributed in morally

00:37:30.108 --> 00:37:35.436
arbitrary ways,
but some people choose to work

00:37:35.436 --> 00:37:41.469
really hard and some people
choose to sit on the couch and

00:37:41.469 --> 00:37:43.800
watch ESPN all day.
 

00:37:43.800 --> 00:37:48.429
And let's suppose you have two
people with exactly the same IQ,

00:37:48.429 --> 00:37:54.030
but one watches ESPN all day
and one studies hard,

00:37:54.030 --> 00:37:58.760
so the one who studies hard
gets the A,

00:37:58.760 --> 00:38:01.940
and the one who watches ESPN
all day gets a C,

00:38:01.940 --> 00:38:14.079
and I take the import of what
you're saying,

00:38:14.079 --> 00:38:16.619
"Well, there's some
legitimate dessert there.

00:38:16.619 --> 00:38:21.670
The person who works should get
the A, yeah?"

00:38:21.670 --> 00:38:27.226
Now Rawls is sort of with you,
but in a way that I don't think

00:38:27.226 --> 00:38:32.329
works for him because if you
read Rawls carefully what he

00:38:32.329 --> 00:38:35.699
says is exactly what you've
said.

00:38:35.699 --> 00:38:41.161
He says, "Yes,
the differences between us are

00:38:41.161 --> 00:38:46.398
morally arbitrary,
but the use we choose to make

00:38:46.398 --> 00:38:50.300
of our capacities is not."
 

00:38:50.300 --> 00:38:53.429
Why doesn't it work for him?
 

00:38:53.429 --> 00:38:56.737
This is sort of like Bentham
being scared of the egalitarian

00:38:56.737 --> 00:39:00.157
implications of his theory and
so he wheels out the difference

00:39:00.157 --> 00:39:02.679
between absolute and practical
equality,

00:39:02.679 --> 00:39:04.940
but it doesn't really work for
him either for reasons we saw.

00:39:04.940 --> 00:39:08.880
Why doesn't this really work
for Rawls?

00:39:08.880 --> 00:39:10.710
Yeah?
 

00:39:10.710 --> 00:39:17.117
Student:  Couldn't you
say that someone's naturally,

00:39:17.117 --> 00:39:20.875
just by luck,
given a capacity or a

00:39:20.875 --> 00:39:24.190
predilection to work hard?
 

00:39:24.190 --> 00:39:30.190
Prof: So that's exactly
where I was hoping you would go.

00:39:30.190 --> 00:39:38.958
That, well, some people have a
supercharged work ethic and some

00:39:38.958 --> 00:39:41.079
people don't.
 

00:39:41.079 --> 00:39:44.139
And why do some people have a
supercharged work ethic?

00:39:44.139 --> 00:39:47.039
Because of the way they were
raised, perhaps?

00:39:47.039 --> 00:39:50.329
Maybe some of it's genetic,
perhaps?

00:39:50.329 --> 00:39:54.184
But why isn't that morally
arbitrary as well,

00:39:54.184 --> 00:39:58.480
if the differences in IQ are
morally arbitrary?

00:39:58.480 --> 00:40:05.233
So weakness of the will,
you know, morally arbitrary

00:40:05.233 --> 00:40:12.119
too, or strength of the will is
morally arbitrary.

00:40:12.119 --> 00:40:18.751
So the person who sits on the
couch watching ESPN all day just

00:40:18.751 --> 00:40:24.230
doesn't have the same--
he doesn't have the moral luck

00:40:24.230 --> 00:40:27.820
to have a lot of partisan work
ethic,

00:40:27.820 --> 00:40:34.380
so he shouldn't be penalized
for that.

00:40:34.380 --> 00:40:40.471
So now you can see why Rawls
doesn't want to go there because

00:40:40.471 --> 00:40:46.764
it has the effect of completely
obliterating the concept of any

00:40:46.764 --> 00:40:51.500
personal responsibility,
ultimately.

00:40:51.500 --> 00:40:55.876
Because once you make that
move, why should you

00:40:55.876 --> 00:41:00.824
differentiate between the
weakness of the will or the

00:41:00.824 --> 00:41:05.239
strength of the will and say,
"That's not morally

00:41:05.239 --> 00:41:08.085
arbitrary,
but differences in IQ are

00:41:08.085 --> 00:41:11.813
morally arbitrary,
or athletic ability are morally

00:41:11.813 --> 00:41:12.869
arbitrary"?
 

00:41:12.869 --> 00:41:15.800
It doesn't seem to work.
 

00:41:15.800 --> 00:41:19.918
So it's not a satisfying way
out for Rawls,

00:41:19.918 --> 00:41:24.626
and he does it because he's
afraid of the radical

00:41:24.626 --> 00:41:27.570
implications of this view.
 

00:41:27.570 --> 00:41:32.844
But what's interesting about
this, you know,

00:41:32.844 --> 00:41:39.224
Rawls' fix doesn't work,
but his underlying arguments

00:41:39.224 --> 00:41:43.150
are very powerful arguments.
 

00:41:43.150 --> 00:41:45.809
I mean, isn't it right?
 

00:41:45.809 --> 00:41:52.577
Isn't it just true that the
differences between us nature or

00:41:52.577 --> 00:41:56.250
nurture are morally arbitrary.
 

00:41:56.250 --> 00:42:01.190
It is moral luck whether it's
genetics or upbringing.

00:42:01.190 --> 00:42:05.047
Nothing you did,
nothing you chose,

00:42:05.047 --> 00:42:10.494
nothing you have,
therefore, any particular right

00:42:10.494 --> 00:42:11.289
to.
 

00:42:11.289 --> 00:42:14.443
So you guys think you all
worked so hard to get into Yale

00:42:14.443 --> 00:42:16.809
and all this and you deserve to
be here.

00:42:16.809 --> 00:42:19.369
It's a load of bunk.
 

00:42:19.369 --> 00:42:22.760
None of you deserve to be here
more than anybody else.

00:42:22.760 --> 00:42:25.639
That's what he's saying.
 

00:42:25.639 --> 00:42:29.900
It might be a nice fiction you
tell yourself.

00:42:29.900 --> 00:42:36.458
As this little exchange showed,
his attempt to put some limits

00:42:36.458 --> 00:42:39.900
on this idea is just pathetic.
 

00:42:39.900 --> 00:42:41.619
It doesn't work.
 

00:42:41.619 --> 00:42:48.432
But the basic argument about
moral arbitrariness it's totally

00:42:48.432 --> 00:42:49.909
compelling.
 

00:42:49.909 --> 00:42:52.369
Anyone here think it's not
compelling?

00:42:52.369 --> 00:42:54.018
And I don't think it's a good
argument.

00:42:54.018 --> 00:42:57.396
I don't think it's compelling
because it has implications I

00:42:57.396 --> 00:42:58.909
don't want to live with.
 

00:42:58.909 --> 00:43:02.670
 
 

00:43:02.670 --> 00:43:04.440
I mean, you have to have some
other reason.

00:43:04.440 --> 00:43:08.800
Let me say this,
I think it has implications

00:43:08.800 --> 00:43:13.159
that if you really drill down
into them,

00:43:13.159 --> 00:43:15.719
probably nobody in this room
wants to live with,

00:43:15.719 --> 00:43:18.969
just like John Rawls didn't
want to live with them.

00:43:18.969 --> 00:43:22.940
But what's a good way out?
 

00:43:22.940 --> 00:43:30.510
 
 

00:43:30.510 --> 00:43:32.898
Who thinks I'm wrong?
 

00:43:32.898 --> 00:43:36.654
Who thinks this is a bad
argument?

00:43:36.654 --> 00:43:40.409
It's just not a good argument.
 

00:43:40.409 --> 00:43:43.579
Nobody?
 

00:43:43.579 --> 00:43:48.309
Hmm.
 

00:43:48.309 --> 00:43:52.849
Yeah?
 

00:43:52.849 --> 00:43:56.099
Student:  Not that I
disagree,

00:43:56.099 --> 00:44:00.181
but it seems to strip down
human free will in that if the

00:44:00.181 --> 00:44:04.701
only condition which matters is
the circumstances of your birth

00:44:04.701 --> 00:44:09.148
then you don't really have any
choice as to the course of your

00:44:09.148 --> 00:44:09.949
life.
 

00:44:09.949 --> 00:44:13.871
So it seems completely
deterministic which might not

00:44:13.871 --> 00:44:14.409
be...
 

00:44:14.409 --> 00:44:16.579
Prof: Well, yes and no.
 

00:44:16.579 --> 00:44:20.449
I think it's agnostic on the
question of free will.

00:44:20.449 --> 00:44:22.539
He's not saying we don't have
free will.

00:44:22.539 --> 00:44:25.550
Maybe we do. Maybe we don't.
 

00:44:25.550 --> 00:44:31.181
I think what he's saying is,
if some of us have a greater

00:44:31.181 --> 00:44:36.007
capacity,
say, to work hard or to engage

00:44:36.007 --> 00:44:40.710
in delayed gratification than
others,

00:44:40.710 --> 00:44:43.679
that is a difference between us.
 

00:44:43.679 --> 00:44:46.974
Just as an empirical matter,
that is a difference between

00:44:46.974 --> 00:44:47.268
us.
 

00:44:47.268 --> 00:44:51.516
What he's saying is that the
person who has the greater

00:44:51.516 --> 00:44:55.369
capacity for deferred
gratification or the greater

00:44:55.369 --> 00:44:59.851
capacity to work hard isn't
entitled to more benefits than

00:44:59.851 --> 00:45:06.405
the person who doesn't have it,
just in virtue of that strength

00:45:06.405 --> 00:45:07.900
of the will.
 

00:45:07.900 --> 00:45:11.976
I mean it might also be true
that we don't have free will,

00:45:11.976 --> 00:45:15.340
that's another matter,
but I think he's just not

00:45:15.340 --> 00:45:18.130
taking a position on that
question.

00:45:18.130 --> 00:45:30.860
You could do it two by two and
fill in all the boxes.

00:45:30.860 --> 00:45:36.289
He's not saying we don't have
will, we can't make choices,

00:45:36.289 --> 00:45:41.813
he's just saying the choices
that we make don't give us any

00:45:41.813 --> 00:45:43.909
particular rights.
 

00:45:43.909 --> 00:45:46.835
Now, I mean,
I think what is,

00:45:46.835 --> 00:45:53.211
and maybe what you're getting
at that does pin the tail on the

00:45:53.211 --> 00:45:57.726
donkey is,
what he's saying is ultimately

00:45:57.726 --> 00:46:03.030
subversive of the idea of
individual responsibility,

00:46:03.030 --> 00:46:04.969
but that's not the same thing
as determinism.

00:46:04.969 --> 00:46:09.489
They come together in other
settings,

00:46:09.489 --> 00:46:13.268
so if somebody says,
"Well,

00:46:13.268 --> 00:46:17.693
I committed the murder,
but I was in the grip of a

00:46:17.693 --> 00:46:22.753
schizophrenic disorder,
and so I didn't have free will,

00:46:22.753 --> 00:46:28.007
so I'm not responsible,"
that's when determinism and the

00:46:28.007 --> 00:46:32.443
issue of the will come together,
but he's not making that kind

00:46:32.443 --> 00:46:33.018
of argument.
 

00:46:33.018 --> 00:46:36.496
He's conceding,
I think, for the purposes of

00:46:36.496 --> 00:46:39.570
discussion, that there is free
will.

00:46:39.570 --> 00:46:44.400
But I'm saying,
when you take away his fix,

00:46:44.400 --> 00:46:46.900
which really I don't think does
work,

00:46:46.900 --> 00:46:51.094
you're saying the differences
that flow from our strengths of

00:46:51.094 --> 00:46:54.869
will shouldn't entitle us to
anything in particular.

00:46:54.869 --> 00:46:59.695
So none of you deserve all the
good things you've gotten in

00:46:59.695 --> 00:47:02.690
life just because you worked
hard.

00:47:02.690 --> 00:47:05.000
So what if you worked hard?
 

00:47:05.000 --> 00:47:06.429
You had the capacity to work
hard.

00:47:06.429 --> 00:47:09.059
Other people didn't.
 

00:47:09.059 --> 00:47:16.280
So anyone think it's just not a
good argument?

00:47:16.280 --> 00:47:20.789
 
 

00:47:20.789 --> 00:47:26.737
Anyone think it seems like a
good argument but you really

00:47:26.737 --> 00:47:30.030
don't like it,
at least some?

00:47:30.030 --> 00:47:37.324
Who really likes it,
the people who want to go and

00:47:37.324 --> 00:47:40.449
watch ESPN all day?
 

00:47:40.449 --> 00:47:43.710
There are philosophers who
follow this intuition.

00:47:43.710 --> 00:47:46.864
There's a guy called Philippe
Van Parijs, a Belgian political

00:47:46.864 --> 00:47:47.389
thinker.
 

00:47:47.389 --> 00:47:49.420
Yeah, what were you going to
say?

00:47:49.420 --> 00:47:56.929
 
 

00:47:56.929 --> 00:48:00.255
Student:  Is this an
argument in favor of complete

00:48:00.255 --> 00:48:02.929
equality, then,
in terms of redistribution?

00:48:02.929 --> 00:48:04.304
Profession Ian Shapiro:
 Okay, so that's a good

00:48:04.304 --> 00:48:04.590
question.
 

00:48:04.590 --> 00:48:08.143
I'll leave Philippe out of it
because your question's more

00:48:08.143 --> 00:48:10.389
important than what he has to
say.

00:48:10.389 --> 00:48:14.809
Is this an argument for
equality?

00:48:14.809 --> 00:48:19.429
Rawls's answer is a qualified
yes.

00:48:19.429 --> 00:48:21.699
He's saying it's not an
argument for equality.

00:48:21.699 --> 00:48:25.420
It's an argument for the
difference principle.

00:48:25.420 --> 00:48:30.322
He's saying it's an argument
for distributing things in such

00:48:30.322 --> 00:48:34.559
a way that they benefit the
person at the bottom.

00:48:34.559 --> 00:48:38.664
Now you have to have a whole
theory of how the political

00:48:38.664 --> 00:48:42.695
economy works to say whether
redistribution to absolute

00:48:42.695 --> 00:48:47.101
equality would do that because
if redistribution to equality

00:48:47.101 --> 00:48:52.179
would destroy incentives,
let's say, and so that over

00:48:52.179 --> 00:48:58.719
time this would go this way,
then it wouldn't be.

00:48:58.719 --> 00:49:04.242
So he would say,
"What it's an argument for

00:49:04.242 --> 00:49:10.943
is detaching what we get from
any theory that ours is some

00:49:10.943 --> 00:49:17.760
kind of moral right and
connecting it to some theory,

00:49:17.760 --> 00:49:22.878
the best going theory of the
day about how you organize an

00:49:22.878 --> 00:49:27.010
economy to benefit the person at
the bottom.

00:49:27.010 --> 00:49:28.710
That's what you should do.
 

00:49:28.710 --> 00:49:32.409
If equality does that you have
equality.

00:49:32.409 --> 00:49:35.175
If the market does that you
have the market,

00:49:35.175 --> 00:49:37.039
but it's not anything else.
 

00:49:37.039 --> 00:49:40.099
It's just this pure
consequentialist claim.

00:49:40.099 --> 00:49:45.929
Do it in order to help the
person at the bottom."

00:49:45.929 --> 00:49:49.726
So from Parijs's point,
Philippe Van Parijs's point he

00:49:49.726 --> 00:49:51.641
says,
"Well,"-- he wrote a

00:49:51.641 --> 00:49:54.059
book called Real Freedom for
All and he said,

00:49:54.059 --> 00:49:58.630
"Yes, everybody should get
a minimum basic income,

00:49:58.630 --> 00:50:03.409
and it shouldn't have anything
to do with their work,

00:50:03.409 --> 00:50:06.409
their capacity to work."
 

00:50:06.409 --> 00:50:10.159
So in the famous one-liner he
says, "Even surfers should

00:50:10.159 --> 00:50:11.159
get pay."
 

00:50:11.159 --> 00:50:17.900
Even surfers should get pay,
as Van Parijs puts it.

00:50:17.900 --> 00:50:21.376
There should be the highest
sustainable universal basic

00:50:21.376 --> 00:50:23.630
income,
whatever that is,

00:50:23.630 --> 00:50:29.407
and it shouldn't be connected
to work because capacity to work

00:50:29.407 --> 00:50:31.679
is morally arbitrary.
 

00:50:31.679 --> 00:50:35.346
So what I'm going to talk about
on Monday is we're going to

00:50:35.346 --> 00:50:38.949
really dig into this question
because you can now see,

00:50:38.949 --> 00:50:42.635
I mean, one of the ironies I
want you to mull over between

00:50:42.635 --> 00:50:46.869
now and then,
one of the ironies is that this

00:50:46.869 --> 00:50:53.061
puts Rawls way to the left of
Marx in a certain sense because

00:50:53.061 --> 00:50:59.666
as we saw Marx was a straight-up
Enlightenment theorist wedded to

00:50:59.666 --> 00:51:02.969
the whole workmanship idea.
 

00:51:02.969 --> 00:51:04.288
Remember Locke,
workmanship,

00:51:04.288 --> 00:51:08.050
labor theory of value;
all that stuff?

00:51:08.050 --> 00:51:13.400
Marx's critique of capitalism
was the worker doesn't get what

00:51:13.400 --> 00:51:14.650
he produces.
 

00:51:14.650 --> 00:51:18.856
Rawls is saying,
"We don't care in any

00:51:18.856 --> 00:51:20.260
moral sense.
 

00:51:20.260 --> 00:51:27.880
We don't care who did the work
because the capacity for work

00:51:27.880 --> 00:51:35.112
isn't a capacity that brings
with it any particular moral

00:51:35.112 --> 00:51:43.250
valence because of this moral
arbitrariness argument."

00:51:43.250 --> 00:51:47.329
Okay, we will pick up from
there next week.

00:51:47.329 --> 00:51:52.000
 
 

