WEBVTT
Kind: captions
Language: es

00:00:01.100 --> 00:00:05.400
Primero quiero terminar el
debate de la clase pasada

00:00:05.400 --> 00:00:09.633
sobre las reacciones desconcertantes

00:00:09.633 --> 00:00:14.233
que generan los problemas del tranvía.

00:00:14.233 --> 00:00:17.867
Recordarán que el problema del
tranvía es desconcertante

00:00:17.867 --> 00:00:22.600
porque al parecer hay una
asimetría en nuestras respuestas

00:00:22.600 --> 00:00:26.000
al caso del transeúnte y al
caso del hombre gordo,

00:00:26.000 --> 00:00:34.400
aunque ambos parecen implicar matar a
1 persona para salvar a 5.

00:00:34.400 --> 00:00:40.033
La clase pasada vimos la
respuesta de Judy Thomson,

00:00:40.033 --> 00:00:42.800
que decía que no hay
asimetría en los 2 casos,

00:00:42.800 --> 00:00:46.733
porque cuando reflexionamos en el
tercer caso hipotético

00:00:46.733 --> 00:00:50.367
donde nosotros estábamos
parados en otra vía,

00:00:50.367 --> 00:00:55.233
entendimos que no es moralmente
aceptable desviar el tranvía

00:00:55.233 --> 00:00:57.167
en el caso del transeúnte

00:00:57.167 --> 00:00:59.167
ni tampoco aventar al hombre gordo.

00:00:59.167 --> 00:01:02.800
Por el contrario, vimos la respuesta
de Josh Greene, que decía

00:01:02.800 --> 00:01:05.567
que así como es moralmente
aceptable desviar el tranvía

00:01:05.567 --> 00:01:07.633
en el caso del transeúnte,

00:01:07.633 --> 00:01:11.500
también es moralmente aceptable
aventar al hombre gordo.

00:01:11.500 --> 00:01:15.033
Green dice que obtenemos respuestas
diferenciales en esos casos

00:01:15.033 --> 00:01:19.533
porque se activa el mecanismo
de respuesta emocional

00:01:19.533 --> 00:01:26.200
de nuestro cerebro, debido a la
naturaleza personal y cercana

00:01:26.200 --> 00:01:28.867
del caso del hombre gordo,
y como resultado,

00:01:28.867 --> 00:01:34.567
damos una respuesta que él
considera moralmente injustificada.

00:01:34.567 --> 00:01:40.067
Al final de la clase empezamos a
considerar una tercera posibilidad,

00:01:40.067 --> 00:01:43.300
que está entre las respuestas
de Thomson y de Greene,

00:01:43.300 --> 00:01:45.067
aunque más cerca de la de Greene:

00:01:45.067 --> 00:01:49.667
el argumento de Cass
Sunstein que dice que

00:01:49.667 --> 00:01:51.400
aunque nuestras
respuestas sean distintas

00:01:51.400 --> 00:01:54.300
y no las podamos cambiar,

00:01:54.300 --> 00:01:57.767
en el fondo los casos son iguales.

00:01:57.767 --> 00:02:02.600
Se inclina a pensar, aunque no
tanto como Josh Greene,

00:02:02.600 --> 00:02:05.667
que si queremos asimilar los casos,

00:02:05.667 --> 00:02:08.000
tenemos que aventar al hombre gordo.

00:02:08.000 --> 00:02:12.200
Recordarán que su argumento decía:

00:02:12.200 --> 00:02:14.433
En los casos no morales no resulta
polémico usar la heurística,

00:02:14.433 --> 00:02:17.467
pero aunque esto sea útil,

00:02:17.467 --> 00:02:22.533
con frecuencia nos lleva a errores.

00:02:22.533 --> 00:02:28.933
Después, sostenía que así como
ocurre en los casos no morales

00:02:28.933 --> 00:02:31.667
también ocurre en los casos morales.

00:02:31.667 --> 00:02:35.367
Nos quedamos pensando al
final de la clase

00:02:35.367 --> 00:02:40.500
en el argumento de Sunstein de
que en los casos morales

00:02:40.500 --> 00:02:42.933
la gente suele usar la heurística.

00:02:42.933 --> 00:02:46.033
Recordarán que Sunstein
dio algunos ejemplos

00:02:46.033 --> 00:02:51.567
propuestos por Jonathan Haidt
donde la gente expresaba

00:02:51.567 --> 00:02:53.967
su desaprobación moral
hacia ciertas acciones,

00:02:53.967 --> 00:02:56.467
aunque no pudieran
justificar sus motivos.

00:02:56.467 --> 00:02:59.233
Como el incesto
consentido entre hermanos

00:02:59.233 --> 00:03:02.000
o limpiar el baño con una
bandera de Estados Unidos.

00:03:02.000 --> 00:03:04.600
La gente tenía la inclinación a pensar

00:03:04.600 --> 00:03:08.200
que eran casos moralmente
problemáticos,

00:03:08.200 --> 00:03:09.900
aunque no eran capaces de explicar

00:03:09.900 --> 00:03:13.800
qué regla moral se
violaba en esos casos.

00:03:13.800 --> 00:03:19.400
Sunstein sugiere en su
texto que, en general,

00:03:19.400 --> 00:03:21.967
podemos ver la literatura de la
heurística y del sesgo,

00:03:21.967 --> 00:03:26.867
y descubrir que, en todos los
ejemplos, el marco del caso

00:03:26.867 --> 00:03:30.900
influye en nuestra respuesta,

00:03:30.900 --> 00:03:34.933
tanto en los casos no morales
como en los morales.

00:03:34.933 --> 00:03:37.967
Recordarán que en la tercera clase,

00:03:37.967 --> 00:03:41.200
cuando estábamos aprendiendo a
usar nuestros clickers,

00:03:41.200 --> 00:03:43.333
por cierto, los vamos a usar
un poco en esta clase,

00:03:43.333 --> 00:03:45.967
así que prepárenlos.

00:03:45.967 --> 00:03:48.267
Cuando estábamos aprendiendo a
usar nuestros clickers,

00:03:48.267 --> 00:03:51.167
vimos el famoso caso de la
enfermedad asiática,

00:03:51.167 --> 00:03:52.900
que dice así:

00:03:52.900 --> 00:03:56.700
Una terrible enfermedad ha atacado a
600 personas en su pueblo.

00:03:56.700 --> 00:03:59.967
Hay 600 personas en su pueblo
destinadas a morir.

00:03:59.967 --> 00:04:02.900
Ustedes son el alcalde y tienen
dos opciones disponibles:

00:04:02.900 --> 00:04:05.200
el plan A o el plan B.

00:04:05.200 --> 00:04:09.033
A la mitad del grupo le pedí que
mirara la descripción en verde:

00:04:09.033 --> 00:04:12.733
en el plan A sobreviven 200 personas,

00:04:12.733 --> 00:04:16.200
mientras que en el plan B hay
1/3 de probabilidad

00:04:16.200 --> 00:04:19.867
de que 600 personas vivan y 2/3 de
probabilidad de que nadie viva.

00:04:19.867 --> 00:04:24.633
La otra mitad del grupo miró
exactamente el mismo plan,

00:04:24.633 --> 00:04:27.133
pero descrito en términos de
cuántas personas morirían

00:04:27.133 --> 00:04:28.967
en vez de cuántas vivirían.

00:04:28.967 --> 00:04:34.167
El plan A dice: vivirán
200 personas de 600,

00:04:34.167 --> 00:04:36.733
es decir, morirán 400.

00:04:36.733 --> 00:04:41.533
El plan A significa que
morirán 400 de 600 personas,

00:04:41.533 --> 00:04:44.500
es decir, que vivirán 200 personas.

00:04:44.500 --> 00:04:48.033
Esta fue nuestra primera
encuesta con clickers

00:04:48.033 --> 00:04:52.267
y su actitud hacia los
casos fue distinta.

00:04:52.267 --> 00:04:56.133
Cuando se presentaba el número de
personas que vivirían,

00:04:56.133 --> 00:05:01.667
el 66% eligió el plan A y solo
el 34% eligió el plan B.

00:05:01.667 --> 00:05:04.133
Cuando se invirtió el marco,

00:05:04.133 --> 00:05:07.533
los resultados fueron
justamente al revés.

00:05:07.533 --> 00:05:11.000
El 66% prefería el plan A
en el caso verde

00:05:11.000 --> 00:05:13.467
y el 64% prefería el plan
A en el caso azul.

00:05:13.467 --> 00:05:17.833
Pero los planes A y B son
idénticos matemáticamente.

00:05:17.833 --> 00:05:24.800
Tal vez algo similar sucede en
los casos del tranvía.

00:05:24.800 --> 00:05:26.800
Este no es un ejemplo real
para votación,

00:05:26.800 --> 00:05:29.167
pero imagínense que se les
presenta el siguiente caso:

00:05:29.167 --> 00:05:32.133
Un terrible tranvía se
dirige a toda velocidad

00:05:32.133 --> 00:05:34.500
hacia 6 personas en su pueblo.

00:05:34.500 --> 00:05:38.367
Ustedes son el alcalde y tienen 2
opciones para el tranvía:

00:05:38.367 --> 00:05:39.600
el plan A y el plan B.

00:05:39.600 --> 00:05:44.600
Les presento el plan A: se
salvará 1 persona,

00:05:44.600 --> 00:05:47.267
es decir, que morirán 5 personas.

00:05:47.267 --> 00:05:49.800
O el plan B: morirá 1 persona,

00:05:49.800 --> 00:05:53.467
es decir, que se salvarán 5 personas.

00:05:53.467 --> 00:05:58.167
Creo que hay cierta tendencia a
elegir el plan B en el caso azul

00:05:58.167 --> 00:06:00.933
y el plan A en el caso verde.

00:06:00.933 --> 00:06:03.533
Esto sirve para generalizar.

00:06:03.533 --> 00:06:08.233
Tenemos distintas respuestas

00:06:08.233 --> 00:06:10.700
dependiendo de dónde nos enfoquemos
en estos dilemas morales.

00:06:10.700 --> 00:06:14.033
Veamos el caso de Josh Greene
del bebé que llora:

00:06:14.033 --> 00:06:17.533
están encerrados en un sótano
con otras 19 personas

00:06:17.533 --> 00:06:22.300
y su bebé llorando, están
rodeados por soldados enemigos

00:06:22.300 --> 00:06:24.733
que los matarán si los encuentran.

00:06:24.733 --> 00:06:30.233
El dilema de Greene es:
¿Asfixiarían a su bebé

00:06:30.233 --> 00:06:36.200
para evitar que su llanto
atraiga a los soldados,

00:06:36.200 --> 00:06:39.300
que sin duda matarían a los
20 si los encontraran?

00:06:39.300 --> 00:06:41.467
Es un caso muy parecido al
de Jim y los indios,

00:06:41.467 --> 00:06:44.900
pero con una premisa
incluso más dolorosa.

00:06:44.900 --> 00:06:49.100
Si centran su atención en la idea

00:06:49.100 --> 00:06:53.633
de poner su mano en la boca
del bebé que llora,

00:06:53.633 --> 00:06:57.667
es prácticamente imposible juzgar

00:06:57.667 --> 00:07:00.767
que esa acción sea
moralmente obligatoria.

00:07:00.767 --> 00:07:05.267
Pero si redirigen su
atención, incluso un poco,

00:07:05.267 --> 00:07:09.167
hacia la niña de 2 años que está a
su lado o al niño de 4 años

00:07:09.167 --> 00:07:11.700
que está al lado de ella, o al
anciano que está en el rincón,

00:07:11.700 --> 00:07:16.333
a todos los que morirán si
no asfixian al bebé,

00:07:16.333 --> 00:07:20.900
su respuesta al caso cambia.

00:07:20.900 --> 00:07:26.633
El cambio de sentido de nuestra
atención será algo endémico

00:07:26.633 --> 00:07:33.133
en todos los casos de este estilo.

00:07:33.133 --> 00:07:36.233
En cierta medida, solo somos
capaces de centrarnos

00:07:36.233 --> 00:07:39.567
en una parte del mundo a la vez.

00:07:39.567 --> 00:07:45.600
Esto ocasiona que sea
sumamente difícil centrarnos

00:07:45.600 --> 00:07:51.567
de tal forma que este tipo de dilemas
morales parezcan estables.

00:07:51.567 --> 00:07:55.533
Sunstein sugiere que debido
a este fenómeno

00:07:55.533 --> 00:07:59.400
los rasgos tendrían que ser
moralmente irrelevantes.

00:07:59.400 --> 00:08:01.433
No puede ser moralmente
relevante al decidir

00:08:01.433 --> 00:08:04.300
qué es lo correcto en el
caso del tranvía

00:08:04.300 --> 00:08:07.733
el enmarcarlo en términos de
cuántas personas vivirán

00:08:07.733 --> 00:08:09.867
o cuántas morirán.

00:08:09.867 --> 00:08:12.133
Por lo menos, a primera vista,

00:08:12.133 --> 00:08:14.933
no parece algo que
podría ser relevante.

00:08:14.933 --> 00:08:16.733
Están tomando exactamente
la misma decisión

00:08:16.733 --> 00:08:18.333
enmarcada de 2 formas distintas.

00:08:18.333 --> 00:08:21.633
¿Cómo podría ser que eso
marcara la diferencia?

00:08:21.633 --> 00:08:24.867
Sunstein sugiere que el mecanismo

00:08:24.867 --> 00:08:27.767
que hay detrás del fenómeno
que acabo de describir

00:08:27.767 --> 00:08:31.200
sucede una y otra vez,

00:08:31.200 --> 00:08:35.800
no solo en los casos hipotéticos como
los problemas del tranvía,

00:08:35.800 --> 00:08:39.433
sino todo el tiempo en los
razonamientos morales

00:08:39.433 --> 00:08:42.367
que hacemos como ciudadanos
de una democracia,

00:08:42.367 --> 00:08:46.167
que intentan hacer juicios sobre la
distribución de recursos

00:08:46.167 --> 00:08:50.000
y sobre el tipo de leyes
que debería haber

00:08:50.000 --> 00:08:53.867
para regular o incentivar ciertos
tipos de comportamiento.

00:08:53.867 --> 00:08:59.467
Sunstein afirma que en los
siguientes 4 dominios

00:08:59.467 --> 00:09:03.200
con frecuencia nos centramos en
la heurística, es decir,

00:09:03.200 --> 00:09:05.433
en los rasgos
superficiales del
fenómeno,

00:09:05.433 --> 00:09:07.933
en vez de en los atributos de
destino, es decir,

00:09:07.933 --> 00:09:09.633
en lo que realmente nos interesa.

00:09:09.633 --> 00:09:11.367
Recuerden que la clase pasada

00:09:11.367 --> 00:09:13.667
hablábamos de ponerle una
funda a sus teléfonos

00:09:13.667 --> 00:09:15.500
para reconocerlos fácilmente

00:09:15.500 --> 00:09:19.633
y así tener un acceso
heurístico a su teléfono.

00:09:19.633 --> 00:09:24.267
Obviamente, esa decoración es
útil como un medio

00:09:24.267 --> 00:09:26.667
para encontrar el teléfono
solo mientras sirva

00:09:26.667 --> 00:09:29.300
para encontrar el atributo de
destino que les importa,

00:09:29.300 --> 00:09:30.800
por ejemplo, encontrar el teléfono

00:09:30.800 --> 00:09:34.067
que tiene los números de
contactos que les interesan.

00:09:34.067 --> 00:09:36.267
Cuando los atributos de destino y la
heurística no coinciden,

00:09:36.267 --> 00:09:37.567
estamos en problemas.

00:09:37.567 --> 00:09:39.400
Sunstein se pregunta qué debemos hacer,

00:09:39.400 --> 00:09:42.933
cuando pensamos en las
reglas de seguridad,

00:09:42.933 --> 00:09:47.067
con el hecho de que como seres humanos

00:09:47.067 --> 00:09:51.067
muchas cosas que hacemos tienen el
potencial de causar daño,

00:09:51.067 --> 00:09:56.800
pero no queremos pasarnos nuestras
vidas envueltos en plástico,

00:09:56.800 --> 00:09:58.967
caminando muy lento por el mundo

00:09:58.967 --> 00:10:00.467
para no chocar contra nada.

00:10:00.467 --> 00:10:03.533
Siendo que deseamos tomar riesgos,

00:10:03.533 --> 00:10:08.233
¿cómo interactúa nuestra
tendencia a usar la heurística

00:10:08.233 --> 00:10:09.967
con la regulación de los riesgos?

00:10:09.967 --> 00:10:11.833
En el caso del castigo,

00:10:11.833 --> 00:10:14.267
y este es el primer tema que veremos
después de las vacaciones,

00:10:14.267 --> 00:10:18.000
Sunstein piensa que
usamos la heurística

00:10:18.000 --> 00:10:21.233
de forma contraproducente

00:10:21.233 --> 00:10:24.233
al castigar tanto a
individuos como a grupos.

00:10:24.233 --> 00:10:30.333
Sunstein piensa que cuando
dudamos en ciertas decisiones

00:10:30.333 --> 00:10:34.200
sobre medicina reproductiva,
corremos el riesgo

00:10:34.200 --> 00:10:38.467
de confundir la heurística con
los atributos de destino.

00:10:38.467 --> 00:10:43.800
Y al tomar muy en serio la distinción de
la omisión del acto, corremos

00:10:43.800 --> 00:10:47.433
el riesgo de confundir la heurística
con los atributos de destino.

00:10:47.433 --> 00:10:51.600
Volveremos al tema del castigo
después de las vacaciones

00:10:51.600 --> 00:10:56.367
y al tema de la omisión del acto
más adelante en esta clase.

00:10:56.367 --> 00:10:59.733
Ahora quiero explicar 3 ejemplos

00:10:59.733 --> 00:11:03.800
de la regulación del riesgo
mediante el análisis de Sunstein.

00:11:03.800 --> 00:11:06.667
El tercer ejemplo… tengo
mucha curiosidad

00:11:06.667 --> 00:11:09.500
de ver cómo salen los
resultados de las encuestas.

00:11:09.500 --> 00:11:13.667
Sunstein señala, y parece tener razón,

00:11:13.667 --> 00:11:18.833
que es más probable que las
personas condenen a una empresa

00:11:18.833 --> 00:11:21.100
cuando el comportamiento de esta
se describe con factores

00:11:21.100 --> 00:11:24.433
que implican seguridad en vez de riesgo.

00:11:24.433 --> 00:11:26.233
Por ejemplo, la empresa A

00:11:26.233 --> 00:11:29.733
fabrica un producto que usarán
10 millones de personas,

00:11:29.733 --> 00:11:32.333
pero matará a 10 personas.

00:11:32.333 --> 00:11:36.000
De los 10 millones de personas
que usarán este producto,

00:11:36.000 --> 00:11:40.833
10 tendrán una reacción de algún
tipo al producto y morirán.

00:11:40.833 --> 00:11:47.900
El costo de eliminar ese riesgo por
completo sería de $100 millones.

00:11:47.900 --> 00:11:53.433
Muchas personas se inclinan a pensar

00:11:53.433 --> 00:11:58.133
que la empresa debería gastar ese
dinero para deshacerse del riesgo;

00:11:58.133 --> 00:12:00.500
que es inaceptable que
fabrique un producto

00:12:00.500 --> 00:12:02.633
que provocará la muerte de 10 personas.

00:12:02.633 --> 00:12:09.433
En cambio, si enmarcamos el caso en
términos de probabilidades,

00:12:09.433 --> 00:12:11.533
que 10 millones de personas
usan el producto,

00:12:11.533 --> 00:12:15.267
que produce un riesgo de muerte
en 1 de cada 10 millones

00:12:15.267 --> 00:12:18.133
y que la eliminación del riesgo
cuesta exactamente lo mismo,

00:12:18.133 --> 00:12:22.867
este es el tipo de cosa que
permitimos todo el tiempo.

00:12:22.867 --> 00:12:29.833
Sin esta tolerancia al riesgo no
habría innovaciones tecnológicas

00:12:29.833 --> 00:12:32.233
y la mayoría de los bienes y recursos

00:12:32.233 --> 00:12:36.933
que damos por sentado nunca
se habrían logrado.

00:12:36.933 --> 00:12:39.100
Sunstein sostiene que

00:12:39.100 --> 00:12:43.667
aunque en ambos casos los atributos
de destino son idénticos,

00:12:43.667 --> 00:12:46.033
en este caso, 10 personas morirán

00:12:46.033 --> 00:12:48.367
y salvarlos costaría $100 millones.

00:12:48.367 --> 00:12:52.100
En este caso, 10 personas
morirán y salvarlos costaría

00:12:52.100 --> 00:12:55.767
$100 millones, los atributos de
destino son idénticos.

00:12:55.767 --> 00:12:58.333
En ambos casos 10 personas morirán

00:12:58.333 --> 00:13:01.433
y salvarlos costaría la
cantidad especificada.

00:13:01.433 --> 00:13:04.167
Los atributos
heurísticos son
distintos.

00:13:04.167 --> 00:13:06.400
Este caso se enmarca en
términos de certeza;

00:13:06.400 --> 00:13:08.800
este otro en términos de riesgo.

00:13:08.800 --> 00:13:13.433
Y tenemos una buena
heurística que dice así:

00:13:13.433 --> 00:13:16.033
Si 10 personas van a morir a
consecuencia de lo que ustedes hagan,

00:13:16.033 --> 00:13:17.367
no lo hagan.

00:13:17.367 --> 00:13:23.700
Sunstein sostiene que la
asimetría en nuestras respuestas

00:13:23.700 --> 00:13:27.233
a estos casos es irracional.

00:13:27.233 --> 00:13:29.867
De hecho, si en el segundo caso
aumentáramos el riesgo de muerte

00:13:29.867 --> 00:13:33.067
de 1 a 2 personas por millón

00:13:33.067 --> 00:13:37.000
y en el primer caso tuviéramos la
certeza de que mueren 10,

00:13:37.000 --> 00:13:46.233
aun así la gente
condenaría el primer caso,

00:13:46.233 --> 00:13:52.167
aunque el segundo caso sea
peor evidentemente.

00:13:52.167 --> 00:13:57.133
Al confundir los atributos
heurísticos con los de destino,

00:13:57.133 --> 00:14:02.000
permitimos comportamientos equivocados.

00:14:02.000 --> 00:14:04.567
Sunstein piensa que esto
es lo que sucede

00:14:04.567 --> 00:14:07.800
en el caso del comercio de
emisiones (límites y comercio)

00:14:07.800 --> 00:14:09.533
del que fue uno de los
primeros defensores.

00:14:09.533 --> 00:14:12.067
En el modelo de comercio de emisiones,

00:14:12.067 --> 00:14:14.400
los contaminadores obtienen una licencia

00:14:14.400 --> 00:14:17.167
para contaminar el aire
con "n" unidades.

00:14:17.167 --> 00:14:21.267
Esas licencias se pueden comerciar en
el mercado de tal forma que

00:14:21.267 --> 00:14:25.167
posiblemente haya menos
contaminación a menor costo.

00:14:25.167 --> 00:14:28.267
Vamos conceder a Sunstein la razón
en el aspecto económico.

00:14:28.267 --> 00:14:33.400
Aun así, hay oposición a los
límites y comercio.

00:14:33.400 --> 00:14:36.033
Aunque estemos dispuestos a reconocer

00:14:36.033 --> 00:14:37.867
que el atributo de destino
está presente,

00:14:37.867 --> 00:14:40.700
es decir, que se ha reducido
la contaminación,

00:14:40.700 --> 00:14:43.033
el atributo heurístico

00:14:43.033 --> 00:14:45.800
nos parece problemático:

00:14:45.800 --> 00:14:49.633
"¿La gente está pagando
por contaminar?

00:14:49.633 --> 00:14:53.000
¡No se debería poder pagar para
librarse de una mala acción!".

00:14:53.000 --> 00:14:55.433
Es un fenómeno interesante

00:14:55.433 --> 00:14:59.400
que la oposición a este
tipo de razonamiento,

00:14:59.400 --> 00:15:05.200
tanto de la izquierda como de la
derecha, dependa del contexto.

00:15:05.200 --> 00:15:12.567
La izquierda se opone a
comercializar las cosas

00:15:12.567 --> 00:15:19.333
y la derecha se resiste a cierto tipo

00:15:19.333 --> 00:15:23.533
de marcos que sugieren que sus
respuestas a cuestiones

00:15:23.533 --> 00:15:26.933
como las tecnologías
reproductivas y la clonación,

00:15:26.933 --> 00:15:31.300
se deben, según Sunstein, a la
heurística de "no jugar a ser Dios".

00:15:31.300 --> 00:15:35.800
Cuando se enfrentan a la
afirmación de que

00:15:35.800 --> 00:15:43.500
"están usando la heurística", ambos
bandos responden con hostilidad

00:15:43.500 --> 00:15:47.900
al sabihondo análisis académico.

00:15:47.900 --> 00:15:51.533
En la década de los 70, era común

00:15:51.533 --> 00:15:57.033
que los defensores de la
acumulación de arsenales nucleares

00:15:57.033 --> 00:16:00.100
apelaran a la noción llamada
"destrucción mutua garantizada",

00:16:00.100 --> 00:16:03.733
de la que hablaremos cuando
abordemos el dilema del prisionero.

00:16:03.733 --> 00:16:07.467
La idea básica es que si ambos
lados tienen suficientes armas

00:16:07.467 --> 00:16:12.700
para acabar con el enemigo,
ninguno de ellos las usará,

00:16:12.700 --> 00:16:16.500
porque la disuasión es
bastante poderosa.

00:16:16.500 --> 00:16:21.667
La izquierda se oponía a este análisis

00:16:21.667 --> 00:16:25.933
porque parecía demasiado astuto.

00:16:25.933 --> 00:16:30.667
Ambos bandos se oponen al
análisis de Sunstein,

00:16:30.667 --> 00:16:35.367
porque va en contra de la idea

00:16:35.367 --> 00:16:39.767
de que introspectivamente
seamos transparentes,

00:16:39.767 --> 00:16:42.600
de que nuestros juicios sean indicativos

00:16:42.600 --> 00:16:45.100
de las cosas que nos importan.

00:16:45.100 --> 00:16:49.367
El último ejemplo que quiero
darles de Sunstein

00:16:49.367 --> 00:16:52.233
es nuestra encuesta.

00:16:52.233 --> 00:16:56.700
La hipótesis de Sunstein es...
¿Están funcionando sus clickers?

00:16:56.700 --> 00:17:02.633
La hipótesis de Sunstein es
que nos incomoda más

00:17:02.633 --> 00:17:05.933
que nos lastimen las cosas que se
supone que deben protegernos,

00:17:05.933 --> 00:17:10.267
que las cosas cuya función
no sea protegernos.

00:17:10.267 --> 00:17:14.233
Sugiere que hay datos que demuestran

00:17:14.233 --> 00:17:16.733
que si a las personas se les da la
opción de elegir entre dos carros:

00:17:16.733 --> 00:17:19.533
con el primer carro hay un
2% de probabilidad

00:17:19.533 --> 00:17:23.200
de morir por el volante
si se accidentan.

00:17:23.200 --> 00:17:26.267
Y en el segundo carro tienen
1% de probabilidad

00:17:26.267 --> 00:17:28.967
de morir por el volante si se
accidentan, y además,

00:17:28.967 --> 00:17:34.467
tienen 1/10 de 1% de probabilidad de
morir por la bolsa de aire.

00:17:34.467 --> 00:17:37.867
La pregunta es: ¿Qué carro eligen?

00:17:37.867 --> 00:17:41.667
El carro donde hay 2% de
probabilidad de morir por el
volante,

00:17:41.667 --> 00:17:46.133
o el carro donde hay 1% de
probabilidad de morir por el volante

00:17:46.133 --> 00:17:50.433
y 1/10 de 1% de probabilidad de
morir por la bolsa de aire,

00:17:50.433 --> 00:17:52.633
que se supone que debía protegerlos.

00:17:52.633 --> 00:17:55.000
Veamos los resultados.

00:17:55.000 --> 00:17:56.233
Estoy haciendo esta encuesta

00:17:56.233 --> 00:17:59.533
porque mis intuiciones no se alinearon
con lo que Sunstein propone

00:17:59.533 --> 00:18:01.933
y me da curiosidad saber si
las de ustedes sí.

00:18:01.933 --> 00:18:06.233
Veamos los resultados.

00:18:06.233 --> 00:18:12.133
El 15% quiere comprar el carro A

00:18:12.133 --> 00:18:15.900
y el 85% quiere comprar el carro B.

00:18:15.900 --> 00:18:18.167
El 85% eligió

00:18:18.167 --> 00:18:21.633
la opción racional estadísticamente.

00:18:21.633 --> 00:18:28.400
Pero una buena parte están
dispuestos a arriesgarse más

00:18:28.400 --> 00:18:32.833
para evitar ese sentimiento de
traición de la bolsa de aire,

00:18:32.833 --> 00:18:35.500
que se supone que debía protegerlos.

00:18:35.500 --> 00:18:42.867
Para resumir, Sunstein sugiere que
en el razonamiento moral,

00:18:42.867 --> 00:18:46.367
con frecuencia, sustituimos los
atributos heurísticos

00:18:46.367 --> 00:18:48.233
por los atributos de destino.

00:18:48.233 --> 00:18:51.700
Y hacer esto es un error.

00:18:51.700 --> 00:18:55.133
¿Qué sugieren las 3 respuestas
al problema del tranvía

00:18:55.133 --> 00:18:57.233
que hemos considerado?

00:18:57.233 --> 00:19:00.133
Thomson dice lo siguiente:

00:19:00.133 --> 00:19:03.833
Reconsiderar nuestras intuiciones a
la luz de casos alternativos,

00:19:03.833 --> 00:19:05.933
como el caso alternativo del transeúnte

00:19:05.933 --> 00:19:09.900
donde nos imaginamos a nosotros
mismos en las vías,

00:19:09.900 --> 00:19:12.667
puede hacernos cambiar

00:19:12.667 --> 00:19:17.000
nuestra evaluación de dichos casos.

00:19:17.000 --> 00:19:23.033
Ella considera que esos cambios
en nuestras respuestas

00:19:23.033 --> 00:19:27.567
revelan algo significativo moralmente.

00:19:27.567 --> 00:19:33.233
Podemos aprender al reflexionar
sobre esos casos específicos

00:19:33.233 --> 00:19:38.267
qué es lo que la moralidad nos exige.

00:19:38.267 --> 00:19:40.700
Greene y Sunstein, por el contrario,

00:19:40.700 --> 00:19:43.833
afirman que nuestras respuestas
intuitivas a los casos

00:19:43.833 --> 00:19:47.900
con frecuencia obedecen a aspectos que
son moralmente irrelevantes,

00:19:47.900 --> 00:19:50.067
por lo tanto,

00:19:50.067 --> 00:19:55.100
esos aspectos no revelan nada
significativo moralmente.

00:19:55.100 --> 00:19:58.633
La pregunta es la siguiente:

00:19:58.633 --> 00:20:05.400
¿Esto representa algún
problema para Mill y Kant?

00:20:05.400 --> 00:20:14.300
Volvamos a las primeras páginas del
tratado sobre el utilitarismo de Mill.

00:20:14.300 --> 00:20:17.400
No les pedí que le
leyeran este fragmento,

00:20:17.400 --> 00:20:19.633
así que no hay razón para que
ya sepan lo que dice.

00:20:19.633 --> 00:20:21.433
Mill escribe: "Aunque en la ciencia,

00:20:21.433 --> 00:20:24.200
las verdades particulares
anteceden a la teoría general,

00:20:24.200 --> 00:20:28.700
se puede esperar lo contrario en el
caso de las artes prácticas,

00:20:28.700 --> 00:20:31.033
como la moral o la legislación."

00:20:31.033 --> 00:20:33.067
En la ciencia, observamos
casos particulares.

00:20:33.067 --> 00:20:37.067
Tiramos este objeto y cae con
una aceleración A,

00:20:37.067 --> 00:20:40.033
tiramos este objeto y cae con
una aceleración A,

00:20:40.033 --> 00:20:42.467
tiramos este objeto y cae con
una aceleración A.

00:20:42.467 --> 00:20:46.400
Entonces, concluimos que la ley que
gobierna la caída de los cuerpos

00:20:46.400 --> 00:20:48.567
es que caen con una aceleración A.

00:20:48.567 --> 00:20:49.867
"Aunque en la ciencia,

00:20:49.867 --> 00:20:51.567
las verdades particulares
anteceden a la teoría general,

00:20:51.567 --> 00:20:54.233
se puede esperar lo contrario en el
caso de las artes prácticas,

00:20:54.233 --> 00:20:55.800
como la moral…

00:20:55.800 --> 00:20:59.000
Una prueba del bien o el mal debe
ser un medio para determinar

00:20:59.000 --> 00:21:00.633
lo que está bien o mal…

00:21:00.633 --> 00:21:05.133
y no una consecuencia de ya
haberlo determinado".

00:21:05.133 --> 00:21:08.667
Mill dice: "La dificultad" (de construir
una teoría a partir de juicios)

00:21:08.667 --> 00:21:12.467
"no se evita por recurrir a"

00:21:12.467 --> 00:21:15.733
lo que a veces se llama sentido moral,

00:21:15.733 --> 00:21:18.233
"una facultad natural que
distingue lo bueno de lo malo

00:21:18.233 --> 00:21:21.600
en un caso particular a mano,

00:21:21.600 --> 00:21:23.067
mientras nuestros otros sentidos

00:21:23.067 --> 00:21:26.000
perciben lo que se ve y se
escucha realmente".

00:21:26.000 --> 00:21:29.333
Como si así pudieran ver si un
caso está mal moralmente.

00:21:29.333 --> 00:21:35.167
"Más bien, el razonamiento
moral, la comprensión moral,

00:21:35.167 --> 00:21:40.133
es una división de nuestra razón, no
de nuestra facultad emocional.

00:21:40.133 --> 00:21:42.333
La moralidad de una
acción individual…

00:21:42.333 --> 00:21:48.000
es una cuestión de la aplicación de
la ley a un caso individual…

00:21:48.000 --> 00:21:52.933
Como resultado, cualquier
perseverancia o constancia

00:21:52.933 --> 00:21:55.367
adquirida por nuestras creencias morales

00:21:55.367 --> 00:22:03.467
se debe a una influencia tácita de este
estándar disponible reflexivamente".

00:22:03.467 --> 00:22:10.600
Mill está construyendo teoría a partir de
teoría, no teoría a partir de casos.

00:22:10.600 --> 00:22:17.367
Kant: "No puede prestarse peor
servicio a la moralidad

00:22:17.367 --> 00:22:21.100
que un intento de hacer que se
derive de ejemplos.

00:22:21.100 --> 00:22:24.000
Porque cada ejemplo de
moralidad en sí mismo

00:22:24.000 --> 00:22:27.300
debe tener los principios
de la moralidad

00:22:27.300 --> 00:22:33.567
para ver si puede funcionar
como un modelo".

00:22:33.567 --> 00:22:40.267
De cierta manera, hemos expresado
aquí el diálogo de este curso.

00:22:40.267 --> 00:22:45.367
¿En qué medida nuestra
capacidad de reflexión racional

00:22:45.367 --> 00:22:50.600
es la mejor manera de responder a las
preguntas que nos interesan?

00:22:50.600 --> 00:22:57.033
¿En qué medida nuestra capacidad
de respuesta emocional,

00:22:57.033 --> 00:23:01.467
de sensación, de juicio instintivo

00:23:01.467 --> 00:23:05.567
basado en la presentación de
casos particulares,

00:23:05.567 --> 00:23:10.267
es indicativa de las respuestas a las
preguntas que nos interesan?

00:23:10.267 --> 00:23:14.867
Con esto concluimos el debate de
los casos del tranvía.

00:23:14.867 --> 00:23:18.200
En la segunda parte de la clase

00:23:18.200 --> 00:23:21.333
quiero repasar 2 tipos de acertijos

00:23:21.333 --> 00:23:27.000
que persisten sin importar cuál de
esas 2 actitudes adoptemos.

00:23:27.000 --> 00:23:29.667
El primer acertijo es un pagaré

00:23:29.667 --> 00:23:33.500
que les prometí en la primera clase,
porque es uno de los textos

00:23:33.500 --> 00:23:36.467
más divertidos que vamos a
leer en todo el semestre.

00:23:36.467 --> 00:23:41.667
Es el documento de Roy Sorensen
y Christopher Boorse

00:23:41.667 --> 00:23:43.567
sobre agacharse o sacrificar.

00:23:43.567 --> 00:23:45.733
Recordarán que ese fin de semana

00:23:45.733 --> 00:23:51.333
habían disparado al senador de Arizona

00:23:51.333 --> 00:23:52.767
y no quise dar el ejemplo con balas.

00:23:52.767 --> 00:23:57.100
El ejemplo que les di es que
estaban parados en una fila.

00:23:57.100 --> 00:23:58.100
Ustedes son la persona amarilla.

00:23:58.100 --> 00:24:00.867
Un oso corre hacia ustedes
a toda velocidad.

00:24:00.867 --> 00:24:02.800
Ustedes se quitan de la fila

00:24:02.800 --> 00:24:05.667
y el oso se come a la persona
que tenían detrás.

00:24:05.667 --> 00:24:09.200
Comparen esto con otro caso donde
están parados en la fila.

00:24:09.200 --> 00:24:10.533
También aquí son la persona amarilla.

00:24:10.533 --> 00:24:12.300
Un oso va corriendo en su dirección.

00:24:12.300 --> 00:24:14.367
Ustedes se voltean, toman a
la persona de atrás

00:24:14.367 --> 00:24:18.100
y la ponen enfrente de ustedes,
y el oso se la come.

00:24:18.100 --> 00:24:22.933
El primero es el caso
clásico de agacharse.

00:24:22.933 --> 00:24:25.000
Están en una situación

00:24:25.000 --> 00:24:27.767
en la que un peligro
viene hacia ustedes.

00:24:27.767 --> 00:24:29.600
Ustedes se quitan del camino del daño

00:24:29.600 --> 00:24:32.100
y el daño va contra alguien más.

00:24:32.100 --> 00:24:35.233
El segundo es un caso
clásico de sacrificio.

00:24:35.233 --> 00:24:36.800
Un daño se dirige hacia ustedes

00:24:36.800 --> 00:24:41.533
y usan a otra persona como
escudo para protegerse.

00:24:41.533 --> 00:24:43.933
Agacharse es evitar un daño

00:24:43.933 --> 00:24:45.967
y permitir que caiga sobre alguien más.

00:24:45.967 --> 00:24:48.967
El sacrificio es evitar un daño
haciendo que caiga sobre alguien más,

00:24:48.967 --> 00:24:54.067
usando a otra persona como un escudo.

00:24:54.067 --> 00:24:58.267
Esto es análogo a la distinción
de la omisión del acto,

00:24:58.267 --> 00:24:59.933
que ya vimos,

00:24:59.933 --> 00:25:04.033
pero que está completamente
dentro del reino de los actos.

00:25:04.033 --> 00:25:08.300
Sorensen y Boorse destacan
en su artículo

00:25:08.300 --> 00:25:12.867
lo resistente que es este fenómeno

00:25:12.867 --> 00:25:17.033
aunque se cambien los
marcos de los casos.

00:25:17.033 --> 00:25:19.867
Presentan el ejemplo del pistolero
en el centro comercial.

00:25:19.867 --> 00:25:21.667
Hay una bala dirigiéndose hacia ustedes

00:25:21.667 --> 00:25:23.467
y tienen la opción de quitarse

00:25:23.467 --> 00:25:27.033
o de poner a alguien enfrente de
ustedes para evitar la bala.

00:25:27.033 --> 00:25:28.900
Está el caso del camión a
exceso de velocidad.

00:25:28.900 --> 00:25:31.267
Están en una fila de carros.

00:25:31.267 --> 00:25:34.467
Hay un camión que viene desde
atrás a tal velocidad

00:25:34.467 --> 00:25:36.033
que chocará contra ustedes.

00:25:36.033 --> 00:25:38.167
Tienen 2 opciones.

00:25:38.167 --> 00:25:39.667
La primera es cambiarse de carril

00:25:39.667 --> 00:25:42.100
y el camión chocará con el carro
que está enfrente de ustedes.

00:25:42.100 --> 00:25:46.367
La segunda es hacerle una
señal al carro de atrás

00:25:46.367 --> 00:25:47.667
para que se pase al carril
donde están ustedes

00:25:47.667 --> 00:25:50.000
y el camión choque contra él.

00:25:50.000 --> 00:25:52.633
Está el caso de los terroristas.

00:25:52.633 --> 00:25:54.233
Están en un avión.

00:25:54.233 --> 00:25:55.533
En este ejemplo, unos terroristas libios

00:25:55.533 --> 00:25:58.233
-es bastante oportuno
hablar de Libia ahora-

00:25:58.233 --> 00:25:59.900
están en el mismo avión

00:25:59.900 --> 00:26:04.967
y amenazan con matar a todos
los estadounidenses.

00:26:04.967 --> 00:26:07.667
Ustedes son representantes

00:26:07.667 --> 00:26:11.367
del Departamento de Estado de
los Estados Unidos

00:26:11.367 --> 00:26:13.600
y su portafolio tiene una etiqueta
del Departamento de Estado.

00:26:13.600 --> 00:26:15.133
Los terroristas van por el pasillo y
están a punto de dispararles.

00:26:15.133 --> 00:26:16.267
Tienen 2 posibilidades.

00:26:16.267 --> 00:26:21.233
La primera, es cubrir su etiqueta con
otra de la aerolínea libia,

00:26:21.233 --> 00:26:25.400
así los terroristas los ignorarán

00:26:25.400 --> 00:26:26.300
y dispararán a la mujer que
está detrás en la fila.

00:26:26.300 --> 00:26:30.533
La otra opción es cambiar su
portafolio con la persona de al lado

00:26:30.533 --> 00:26:35.000
para que le disparen a ella
en vez de a ustedes.

00:26:35.000 --> 00:26:37.433
O el caso de los barcos hundiéndose.

00:26:37.433 --> 00:26:39.200
Están en el océano.

00:26:39.200 --> 00:26:42.467
Están intentando obtener su señal,

00:26:42.467 --> 00:26:44.167
su barco y el barco de al lado
se están hundiendo.

00:26:44.167 --> 00:26:45.767
Están intentando enviar su señal

00:26:45.767 --> 00:26:48.233
a un avión que está
sobrevolando para que los
rescate.

00:26:48.233 --> 00:26:50.900
Tienen 2 opciones.

00:26:50.900 --> 00:26:53.400
Pueden aumentar su señal, ¿verdad?

00:26:53.400 --> 00:26:58.133
Hacer que su luz sea muy fuerte y que
el avión venga a rescatarlos.

00:26:58.133 --> 00:27:01.033
O pueden interferir la
señal del otro barco

00:27:01.033 --> 00:27:03.667
para que su señal sea un
poco más fuerte

00:27:03.667 --> 00:27:06.800
y el avión venga a
rescatarlos a ustedes.

00:27:06.800 --> 00:27:10.033
Sorensen presenta varios
casos como estos.

00:27:10.033 --> 00:27:13.233
Si los escarabajos se están
comiendo sus rosas,

00:27:13.233 --> 00:27:17.333
está bien usar insecticida

00:27:17.333 --> 00:27:19.900
para que los escarabajos se
vayan a la casa del vecino,

00:27:19.900 --> 00:27:25.633
pero no está bien poner atrayente
de escarabajos en sus rosas.

00:27:25.633 --> 00:27:31.533
Tenemos esta extraña
tendencia, una y otra vez,

00:27:31.533 --> 00:27:38.833
a pensar que está bien
agacharnos, pero no protegernos.

00:27:38.833 --> 00:27:44.300
Sorensen y Boorse
consideran desconcertante

00:27:44.300 --> 00:27:48.800
que parezca que no hay una
forma sistemática

00:27:48.800 --> 00:27:53.900
para explicar este tipo de
discrepancias en la intuición.

00:27:53.900 --> 00:27:55.100
Podrían pensar...

00:27:55.100 --> 00:27:56.933
El problema con estos casos

00:27:56.933 --> 00:28:01.733
es que cuando amarran los
zapatos de su oponente,

00:28:01.733 --> 00:28:03.233
cuando intentan escaparse del oso,

00:28:03.233 --> 00:28:05.867
o cuando ponen a otra persona
enfrente en el caso de las balas,

00:28:05.867 --> 00:28:08.867
piensan que están
interfiriendo de forma justa.

00:28:08.867 --> 00:28:15.167
Y que la competencia justa es lo
importante en este tipo de circunstancias.

00:28:15.167 --> 00:28:18.900
Obviamente, en muchas de
estas circunstancias

00:28:18.900 --> 00:28:23.467
la competencia era injusta
desde el principio.

00:28:23.467 --> 00:28:27.700
Sin embargo, parece problemático.

00:28:27.700 --> 00:28:32.500
Aunque la persona que están
intentando dejar atrás ante el oso

00:28:32.500 --> 00:28:37.300
sea más lenta corriendo y tengan la
certeza de que le ganarán,

00:28:37.300 --> 00:28:41.900
sigue pareciendo incorrecto
amarrarle sus zapatos entre sí.

00:28:41.900 --> 00:28:44.200
No parece que la justicia
de la competencia

00:28:44.200 --> 00:28:46.200
sea lo que impulse la intuición.

00:28:46.200 --> 00:28:51.200
Dicen que tal vez, en todos
los casos de empujones,

00:28:51.200 --> 00:28:54.433
lo que hacen ustedes es una
especie de mal incluido.

00:28:54.433 --> 00:28:58.100
Está mal agarrar a alguien y
ponerlo adelante de ustedes.

00:28:58.100 --> 00:29:01.867
Pero está bien solo agacharse para que
algo le pegue a otra persona.

00:29:01.867 --> 00:29:04.300
Está mal robar el
portafolio de alguien.

00:29:04.300 --> 00:29:06.567
Está mal interferir la
señal de alguien.

00:29:06.567 --> 00:29:12.167
Señalan que al parecer es igual de malo

00:29:12.167 --> 00:29:16.733
poner a otra persona enfrente de
ustedes de forma amigable

00:29:16.733 --> 00:29:19.900
preguntándole si quiere
apreciar la hermosa vista.

00:29:19.900 --> 00:29:23.633
Es igual de malo que agarrarla y
ponerla delante de ustedes.

00:29:23.633 --> 00:29:28.600
Asustar a alguien

00:29:28.600 --> 00:29:34.533
para que salte de un
acantilado gritándole "E=mc2"

00:29:34.533 --> 00:29:37.267
es igual de problemático que
sorprenderlo para que salte

00:29:37.267 --> 00:29:40.300
gritándole algún apodo racial.

00:29:40.300 --> 00:29:46.533
No parece que el mal incluido
explique nuestra respuesta.

00:29:46.533 --> 00:29:50.933
Dejaré que lean estas
respuestas por su cuenta

00:29:50.933 --> 00:29:53.633
si aún no han podido hacerlo.

00:29:53.633 --> 00:29:56.367
Tampoco parece que la distinción
de la omisión del acto,

00:29:56.367 --> 00:30:00.500
ni la distinción de la aceptación del
acto basten como explicación.

00:30:00.500 --> 00:30:03.467
Tampoco parece bastar la idea
de que sea importante

00:30:03.467 --> 00:30:06.333
si fueron el locus de una cadena causal,

00:30:06.333 --> 00:30:09.533
los creadores de alguna
secuencia de causalidad.

00:30:09.533 --> 00:30:12.233
La doctrina del efecto doble

00:30:12.233 --> 00:30:15.833
tampoco parece explicar estos casos.

00:30:15.833 --> 00:30:20.433
Tampoco parece que la noción
de los derechos de Kant

00:30:20.433 --> 00:30:25.533
comparada con la noción
utilitaria expliquen estos casos.

00:30:25.533 --> 00:30:31.667
Sorensen y Boorse concluyen
no muy satisfechos

00:30:31.667 --> 00:30:33.367
y con escepticismo que:

00:30:33.367 --> 00:30:36.967
es un rasgo desconcertante

00:30:36.967 --> 00:30:39.300
sobre nuestra psicología.

00:30:39.300 --> 00:30:43.000
Pero nosotros, habiendo escuchado la
primera mitad de esta clase,

00:30:43.000 --> 00:30:45.533
tenemos otra explicación alternativa.

00:30:45.533 --> 00:30:47.967
No les garantizo que
funcionará en todos los casos,

00:30:47.967 --> 00:30:49.900
pero parece bastante prometedora.

00:30:49.900 --> 00:30:52.967
Esta explicación dice que en los
casos de agacharse o protegerse

00:30:52.967 --> 00:30:56.167
hay un exceso de aplicación
de la heurística.

00:30:56.167 --> 00:31:01.967
En general, parece que quitarse
del camino de un daño

00:31:01.967 --> 00:31:03.867
no es algo malo,

00:31:03.867 --> 00:31:08.000
mientras que poner a alguien en
el camino de un daño

00:31:08.000 --> 00:31:10.767
sí es malo.

00:31:10.767 --> 00:31:14.300
Tal vez, este primer
conjunto de acertijos

00:31:14.300 --> 00:31:19.200
se puede explicar mediante
la heurística.

00:31:19.200 --> 00:31:24.033
En estos 15 minutos quiero centrarme
en una serie de acertijos

00:31:24.033 --> 00:31:26.100
que pienso que no pueden explicarse
mediante la heurística.

00:31:26.100 --> 00:31:28.867
Para esto, vamos a
necesitar los clickers.

00:31:28.867 --> 00:31:33.233
Comencemos con cuatro conductores.

00:31:33.233 --> 00:31:36.300
El primero es Alberto
Afortunado o Atento Afortunado,

00:31:36.300 --> 00:31:38.400
que hace lo siguiente:

00:31:38.400 --> 00:31:39.700
Se sube a su carro.

00:31:39.700 --> 00:31:43.167
Tiene su carro en perfecto estado.

00:31:43.167 --> 00:31:46.800
Respeta todos los semáforos.

00:31:46.800 --> 00:31:49.533
Maneja de forma sumamente segura.

00:31:49.533 --> 00:31:54.800
Y al final del día vuelve a su
casa después del trabajo.

00:31:54.800 --> 00:31:57.100
Eso es todo. Ese es Atento Afortunado.

00:31:57.100 --> 00:32:01.000
Pregunta: Cuando Atento
Afortunado maneja hacia su casa,

00:32:01.000 --> 00:32:07.233
sin tomar en cuenta que lleva a
su amante en su carro,

00:32:07.233 --> 00:32:10.133
sin tomar en cuenta que compró un carro

00:32:10.133 --> 00:32:16.433
que produce altas emisiones en
vez de uno híbrido.

00:32:16.433 --> 00:32:18.633
Al manejar hacia su casa,
sin tomar en cuenta

00:32:18.633 --> 00:32:22.133
todas las otras cosas que Atento
podría haber hecho mal moralmente,

00:32:22.133 --> 00:32:26.000
¿hizo algo reprobable moralmente al
ir del trabajo a su casa,

00:32:26.000 --> 00:32:28.067
al tener en buenas condiciones su carro

00:32:28.067 --> 00:32:31.200
y no lastimar a nadie en el camino?

00:32:31.200 --> 00:32:32.733
No es una pregunta con trampa.

00:32:32.733 --> 00:32:34.800
Si piensan que Atento Afortunado

00:32:34.800 --> 00:32:36.100
hizo algo reprobable
moralmente al conducir,

00:32:36.100 --> 00:32:37.333
sin tomar en cuenta todas las cosas

00:32:37.333 --> 00:32:41.133
que son reprobables
moralmente, pulsen 1.

00:32:41.133 --> 00:32:44.300
Si piensan que no hizo nada
reprobable moralmente,

00:32:44.300 --> 00:32:45.600
pulsen 2.

00:32:45.600 --> 00:32:48.700
Están juzgando la acción de
manejar del trabajo a la casa,

00:32:48.700 --> 00:32:51.433
si no pasa nada malo,

00:32:51.433 --> 00:32:53.833
¿es una acción
problemática moralmente?

00:32:53.833 --> 00:32:55.833
Esperemos que...

00:32:55.833 --> 00:32:57.600
Siempre hay este 5%.

00:32:57.600 --> 00:32:59.333
Esas multitudes que están en
contra de los carros.

00:32:59.333 --> 00:33:00.500
Son los que irán a la
escuela de medicina

00:33:00.500 --> 00:33:03.800
y descuartizarán a una pobre persona
sana en la sala de espera.

00:33:03.800 --> 00:33:06.267
El 95% piensa que Atento Afortunado

00:33:06.267 --> 00:33:08.567
no hizo nada reprobable moralmente.

00:33:08.567 --> 00:33:14.233
Conozcamos al gemelo de Atento
Afortunado, Atento Desafortunado.

00:33:14.233 --> 00:33:16.233
Aquí está lo que hizo
Atento Desafortunado.

00:33:16.233 --> 00:33:19.333
Exactamente lo mismo que
Atento Afortunado.

00:33:19.333 --> 00:33:24.000
Excepto que cerca de su casa,

00:33:24.000 --> 00:33:28.733
un niño corrió enfrente de su
carro y lo atropelló.

00:33:28.733 --> 00:33:31.067
¿De acuerdo? Atento Desafortunado

00:33:31.067 --> 00:33:34.567
hizo exactamente lo mismo que
Atento Afortunado.

00:33:34.567 --> 00:33:39.167
Salió del trabajo, revisó sus
llantas, estuvo atento todo el tiempo

00:33:39.167 --> 00:33:41.367
y manejó a una velocidad
segura y adecuada.

00:33:41.367 --> 00:33:47.633
Pero debido a su mala suerte, mató
a un niño en el camino.

00:33:47.633 --> 00:33:53.333
Pregunta: ¿Atento Desafortunado hizo
algo reprobable moralmente?

00:33:53.333 --> 00:33:57.233
Pulsen 1 si consideran que sí.

00:33:57.233 --> 00:33:59.033
Pulsen 2 si consideran que no.

00:33:59.033 --> 00:34:02.933
Voy a apuntar los resultados
del primer caso: 5 y 95.

00:34:02.933 --> 00:34:06.967
Ok. Veamos los resultados.

00:34:06.967 --> 00:34:11.933
El 81% piensa que no hizo nada
reprobable moralmente,

00:34:11.933 --> 00:34:13.967
pero pasamos del 5%

00:34:13.967 --> 00:34:18.667
al 19% que piensa que sí hizo
algo reprobable moralmente.

00:34:18.667 --> 00:34:20.100
Pasemos a nuestro tercer caso.

00:34:20.100 --> 00:34:22.133
Aquí tenemos al Sr. Celular Afortunado.

00:34:22.133 --> 00:34:23.967
Esto es lo que hace el Sr.
Celular Afortunado.

00:34:23.967 --> 00:34:26.800
Se sube a su carro y maneja
del trabajo a su casa.

00:34:26.800 --> 00:34:29.067
Y en el camino del trabajo a su
casa habla en su celular,

00:34:29.067 --> 00:34:31.200
pero no pasa nada.

00:34:31.200 --> 00:34:35.100
Llega a su casa sin haber
lastimado a nadie.

00:34:35.100 --> 00:34:39.567
Pregunta: ¿Celular Afortunado hizo
algo reprobable moralmente

00:34:39.567 --> 00:34:44.433
al hablar por teléfono
mientras manejaba?

00:34:44.433 --> 00:34:47.600
Veamos los resultados.

00:34:51.733 --> 00:34:55.233
Ok. Este es su veredicto:

00:34:55.233 --> 00:34:58.133
El 78% piensa que Celular
Afortunado hizo algo…

00:34:58.133 --> 00:35:00.100
¡No puedo creerlo!

00:35:00.100 --> 00:35:02.667
¡Se están adelantando al
siguiente caso!

00:35:02.667 --> 00:35:06.033
Todos ustedes hablan en sus celulares

00:35:06.033 --> 00:35:07.567
cuando van manejando todo el tiempo

00:35:07.567 --> 00:35:09.200
y no piensan que están haciendo
algo reprobable moralmente.

00:35:09.200 --> 00:35:11.633
Ok. Estos datos no son válidos.

00:35:11.633 --> 00:35:15.200
Esto se debe a dónde están
insertados en este experimento.

00:35:15.200 --> 00:35:16.933
Bien.

00:35:16.933 --> 00:35:19.400
Como ya respondieron a la
cuarta pregunta,

00:35:19.400 --> 00:35:21.600
déjenme preguntarles si...

00:35:21.600 --> 00:35:23.600
Celular Desafortunado maneja
del trabajo a su casa

00:35:23.600 --> 00:35:24.700
hablando por teléfono.

00:35:24.700 --> 00:35:26.667
Un niño corre enfrente de su carro y…

00:35:26.667 --> 00:35:29.367
Pregunta:

00:35:29.367 --> 00:35:32.933
¿Celular Desafortunado hizo
algo reprobable moralmente

00:35:32.933 --> 00:35:34.933
en su camino a casa?

00:35:34.933 --> 00:35:38.633
Veamos los resultados.

00:35:41.200 --> 00:35:42.000
Bien.

00:35:42.000 --> 00:35:46.067
Veamos cómo sale la gran barra roja
de Celular Desafortunado.

00:35:46.067 --> 00:35:49.967
Ok. Tenemos un cambio total
comparado con el caso original

00:35:49.967 --> 00:35:52.767
y también es diferente
del caso anterior.

00:35:52.767 --> 00:35:56.467
Ok.Estos ejemplos demuestran

00:35:56.467 --> 00:36:00.467
un fenómeno conocido como suerte moral.

00:36:00.467 --> 00:36:04.767
Tenemos dos personas, Atento
Afortunado y Atento Desafortunado,

00:36:04.767 --> 00:36:07.600
que hacen exactamente lo mismo,

00:36:07.600 --> 00:36:09.700
pero las acciones de
Atento Desafortunado

00:36:09.700 --> 00:36:12.300
causan la muerte de una
víctima inocente.

00:36:12.300 --> 00:36:14.933
Mientras solo el 5% piensa

00:36:14.933 --> 00:36:16.733
que Atento Afortunado hizo algo malo,

00:36:16.733 --> 00:36:21.367
el 19% piensa que Atento
Desafortunado hizo algo malo.

00:36:21.367 --> 00:36:24.000
Tenemos algo similar,

00:36:24.000 --> 00:36:28.000
alguien que de forma muy sutil
ha tomado un riesgo,

00:36:28.000 --> 00:36:31.267
que en un caso no tuvo
malas consecuencias

00:36:31.267 --> 00:36:34.567
y en el otro tuvo graves consecuencias.

00:36:34.567 --> 00:36:39.400
El 92% condena a Celular Desafortunado.

00:36:39.400 --> 00:36:44.700
Esto ilustra el fenómeno
conocido como suerte moral.

00:36:44.700 --> 00:36:48.400
Los casos donde un agente
recibe una culpa moral

00:36:48.400 --> 00:36:50.233
por una acción o sus consecuencias

00:36:50.233 --> 00:36:53.833
aunque dicho agente no
tenga control total

00:36:53.833 --> 00:36:55.800
sobre la acción o sus consecuencias.

00:36:55.800 --> 00:37:01.400
¿De acuerdo? No es que Atento
Desafortunado y Celular Desafortunado

00:37:01.400 --> 00:37:04.900
quisieran que el niño corriera
enfrente de sus carros.

00:37:04.900 --> 00:37:08.167
No es que Atento Desafortunado y
Celular Desafortunado

00:37:08.167 --> 00:37:12.800
hubieran podido hacer algo
distinto en ese momento.

00:37:12.800 --> 00:37:18.200
El niño estaba enfrente del carro
y el carro lo atropelló.

00:37:18.200 --> 00:37:21.633
La suerte moral es desconcertante

00:37:21.633 --> 00:37:26.933
porque parece que tenemos 2
compromisos que compiten entre sí

00:37:26.933 --> 00:37:29.667
cuando pensamos en la
responsabilidad moral.

00:37:29.667 --> 00:37:32.533
Por un lado, parece que aceptamos el
llamado principio de control

00:37:32.533 --> 00:37:34.800
de que no debería asignarse
mérito ni culpa moral

00:37:34.800 --> 00:37:39.133
en los casos donde la acción
o las consecuencias

00:37:39.133 --> 00:37:44.400
están más allá del
control del agente.

00:37:44.400 --> 00:37:49.267
Puedo ver que la mayoría de ustedes se
suscribe al principio del control,

00:37:49.267 --> 00:37:55.167
porque el 81% piensan que
Atento Desafortunado

00:37:55.167 --> 00:37:57.267
no hizo nada malo moralmente,

00:37:57.267 --> 00:38:00.000
aunque haya matado a un
niño con su carro.

00:38:00.000 --> 00:38:02.133
La razón por la que se
inclinan a pensar

00:38:02.133 --> 00:38:04.767
que no hizo nada malo,
sospecho que se debe

00:38:04.767 --> 00:38:09.067
a que su juicio en ese caso,
como decía Mill,

00:38:09.067 --> 00:38:13.567
está regulado por un principio al
que se suscriben tácitamente.

00:38:13.567 --> 00:38:17.500
Concretamente, algo como el
principio de control.

00:38:17.500 --> 00:38:20.167
Nagel dice que es
verosímil intuitivamente

00:38:20.167 --> 00:38:22.633
que la gente no pueda ser
evaluada moralmente

00:38:22.633 --> 00:38:24.267
por algo que no es su culpa

00:38:24.267 --> 00:38:26.800
o que se debe a factores que van
más allá de su control.

00:38:26.800 --> 00:38:29.100
Si chocan contra mí y me tropiezo,

00:38:29.100 --> 00:38:32.167
y accidentalmente caigo encima del
botón rojo que va a ocasionar

00:38:32.167 --> 00:38:36.333
el inicio de la guerra nuclear en
el planeta, no es mi culpa.

00:38:36.333 --> 00:38:40.467
Es horrible que el planeta
quede destruido,

00:38:40.467 --> 00:38:43.533
pero yo solo me tropecé.

00:38:43.533 --> 00:38:49.100
Por el contrario,
directamente en competencia

00:38:49.100 --> 00:38:52.900
con el principio del control, el
principio de la suerte moral estipula,

00:38:52.900 --> 00:38:55.133
que en algunos casos,

00:38:55.133 --> 00:38:58.267
el mérito y la culpa
moral deben asignarse

00:38:58.267 --> 00:39:00.567
aunque la acción o sus consecuencias

00:39:00.567 --> 00:39:03.433
estén más allá del
control del agente.

00:39:03.433 --> 00:39:06.267
La diferencia en nuestras respuestas

00:39:06.267 --> 00:39:08.933
entre los casos afortunados
y desafortunados

00:39:08.933 --> 00:39:14.133
indica el grado en el que nos
suscribimos tácitamente a eso.

00:39:14.133 --> 00:39:21.100
Pasaron del 95% al 81% de no reprobable.

00:39:21.100 --> 00:39:26.133
El 15% cambió su opinión

00:39:26.133 --> 00:39:29.700
por algo que está más allá
del control del agente.

00:39:29.700 --> 00:39:31.400
En el caso del celular,

00:39:31.400 --> 00:39:35.733
más o menos el 15% cambió de opinión.

00:39:35.733 --> 00:39:39.300
El problema es que es muy difícil

00:39:39.300 --> 00:39:42.300
deshacerse de ambos principios.

00:39:42.300 --> 00:39:47.967
El principio del control se basa en el
siguiente tipo de razonamiento.

00:39:47.967 --> 00:39:50.867
Por lo general, tenemos un
sentido bastante bueno

00:39:50.867 --> 00:39:56.067
de qué tipo de factores aumentan la
culpa o el mérito de una acción.

00:39:56.067 --> 00:39:59.100
Por lo general, si una
acción es involuntaria,

00:39:59.100 --> 00:40:03.433
si no se hace por
coacción ni por error.

00:40:03.433 --> 00:40:07.867
Si eligen específicamente realizar
la acción que han realizado,

00:40:07.867 --> 00:40:11.533
obtendrán más mérito por hacerla

00:40:11.533 --> 00:40:12.933
si fue una buena acción

00:40:12.933 --> 00:40:16.033
y más culpa si fue una mala acción.

00:40:16.033 --> 00:40:18.933
De la misma forma, si contaban
con toda la información,

00:40:18.933 --> 00:40:23.033
si eran conscientes de las
consecuencias posibles,

00:40:23.033 --> 00:40:26.500
si sabían que lo que daban de beber a
alguien era agua o cianuro,

00:40:26.500 --> 00:40:31.967
esto aumenta el grado de
mérito o de culpa.

00:40:31.967 --> 00:40:36.000
Estas son respuestas bastante sólidas

00:40:36.000 --> 00:40:39.200
que van en contra no solo de
nuestro análisis de casos,

00:40:39.200 --> 00:40:43.067
sino también de nuestra
comprensión de los principios

00:40:43.067 --> 00:40:46.367
que parecen sustentar la
responsabilidad moral.

00:40:46.367 --> 00:40:50.133
De la misma manera, parece que la
ausencia de esas características

00:40:50.133 --> 00:40:53.200
disminuye la culpa o el mérito moral.

00:40:53.200 --> 00:40:57.533
Si hacen algo bajo coacción
o accidentalmente,

00:40:57.533 --> 00:41:01.167
tienen menos responsabilidad.

00:41:01.167 --> 00:41:03.233
Si hacen algo por falta de
información, por ejemplo,

00:41:03.233 --> 00:41:06.667
si pienso que les estoy dando algo
completamente saludable

00:41:06.667 --> 00:41:09.133
y termino dándoles algo
que les hace daño,

00:41:09.133 --> 00:41:14.700
solemos pensar que el grado
de culpa se mitiga.

00:41:14.700 --> 00:41:18.233
El principio del control
simplemente dice

00:41:18.233 --> 00:41:20.533
que si no hubiera una
diferencia en estos factores,

00:41:20.533 --> 00:41:24.400
¿cómo podría haber una diferencia
en la culpa o el mérito?

00:41:24.400 --> 00:41:27.067
Si estos factores se
mantuvieran constantes,

00:41:27.067 --> 00:41:30.867
estaríamos en una situación

00:41:30.867 --> 00:41:33.667
en la que no habría diferencia en
la responsabilidad moral.

00:41:33.667 --> 00:41:38.133
Por el contrario, el principio de la suerte
moral también es bastante fuerte.

00:41:38.133 --> 00:41:42.167
Parece innegable que en
algunos casos evaluamos

00:41:42.167 --> 00:41:44.733
el mérito moral y la culpa en
la ausencia de control.

00:41:44.733 --> 00:41:46.233
Como el caso del conductor.

00:41:46.233 --> 00:41:49.367
Uno de los sujetos que usan el celular
atropella al niño, el otro no.

00:41:49.367 --> 00:41:51.167
Al primero se le culpa moralmente.

00:41:51.167 --> 00:41:54.633
Dejo la estufa prendida en mi
casa o en su casa.

00:41:54.633 --> 00:41:56.867
Voy a visitarlos y dejo la
estufa prendida en su casa.

00:41:56.867 --> 00:41:57.833
Salgo de la casa todo el día.

00:41:57.833 --> 00:42:00.833
Cuando no tengo suerte,
la casa se quema.

00:42:00.833 --> 00:42:02.500
Cuando tengo suerte no se quema.

00:42:02.500 --> 00:42:05.333
Al parecer, aunque piensen que
ambas cosas son malas,

00:42:05.333 --> 00:42:09.500
es peor dejar la estufa
prendida y quemar la casa

00:42:09.500 --> 00:42:12.333
que dejar la estufa
prendida simplemente.

00:42:12.333 --> 00:42:15.900
Nagel da el ejemplo de dejar la
tina llenándose de agua

00:42:15.900 --> 00:42:17.467
con el bebé adentro.

00:42:17.467 --> 00:42:23.500
Es una acción muy irresponsable,
pero mucho más problemática

00:42:23.500 --> 00:42:26.667
cuando el bebé se ahoga.

00:42:26.667 --> 00:42:31.633
O el caso en el que ustedes y yo
tenemos carácteres similares.

00:42:31.633 --> 00:42:34.567
Yo me quedo en Alemania, ustedes no.

00:42:34.567 --> 00:42:37.633
Es la década de 1930. Me vuelvo nazi;

00:42:37.633 --> 00:42:42.433
ustedes viven sin las
exigencias morales nazis.

00:42:42.433 --> 00:42:45.500
Hay 3 tipos de respuestas
que podemos dar

00:42:45.500 --> 00:42:46.567
a los casos de suerte moral.

00:42:46.567 --> 00:42:48.433
Podemos ofrecer una
respuesta racionalista.

00:42:48.433 --> 00:42:49.167
Podemos decir que la suerte

00:42:49.167 --> 00:42:52.333
simplemente no puede jugar un papel
en la evaluación moral.

00:42:52.333 --> 00:42:59.067
Podemos adoptar el extremo de eso,
una visión meramente kantiana

00:42:59.067 --> 00:43:03.333
de que el agente solo es
responsable de su voluntad

00:43:03.333 --> 00:43:06.133
y de aquellas cosas sobre las que
tiene un control total.

00:43:06.133 --> 00:43:09.833
O pueden adoptar una
versión extrema de Mill.

00:43:09.833 --> 00:43:13.367
Que el agente es responsable de
todas las consecuencias

00:43:13.367 --> 00:43:16.567
de sus acciones y que la actitud no
hace ninguna diferencia.

00:43:16.567 --> 00:43:19.600
Pueden adoptar una actitud
irracional hacia esto.

00:43:19.600 --> 00:43:19.967
Pueden decir

00:43:19.967 --> 00:43:23.667
que la suerte puede jugar un papel
en la evaluación moral.

00:43:23.667 --> 00:43:27.867
Aunque pienso que esta opción es
muy difícil de mantener,

00:43:27.867 --> 00:43:34.033
podrían decir que nunca
sabemos qué tan responsable

00:43:34.033 --> 00:43:38.867
es una persona de una acción hasta
que vemos sus consecuencias.

00:43:38.867 --> 00:43:43.733
Que cuando hacía la hipótesis de que
estos casos eran idénticos,

00:43:43.733 --> 00:43:47.800
estaba idealizando de forma ilegítima.

00:43:47.800 --> 00:43:53.567
Parece que la tercera
respuesta podría funcionar

00:43:53.567 --> 00:43:56.433
para los casos clásicos de suerte
moral que he estado describiendo,

00:43:56.433 --> 00:43:58.633
casos que podríamos considerar
de suerte resultante,

00:43:58.633 --> 00:44:01.733
en los que hay suerte en el
resultado de la acción.

00:44:01.733 --> 00:44:06.567
Realizo una acción que fracasa de
una forma que yo no esperaba.

00:44:06.567 --> 00:44:09.533
Ese es un tipo de caso de suerte moral.

00:44:09.533 --> 00:44:13.133
Pero es difícil ver cómo
podemos usar esa explicación

00:44:13.133 --> 00:44:15.667
para algunos casos más profundos.

00:44:15.667 --> 00:44:19.000
Consideremos la suerte constitutiva.

00:44:19.000 --> 00:44:21.467
Algunos de ustedes nacieron con genes

00:44:21.467 --> 00:44:24.633
que les facilitan comportarse
de forma altruista,

00:44:24.633 --> 00:44:26.200
algunos no.

00:44:26.200 --> 00:44:28.067
Algunos fueron criados en familias

00:44:28.067 --> 00:44:30.967
que apoyaban ciertos aspectos morales

00:44:30.967 --> 00:44:32.867
y algunos no.

00:44:32.867 --> 00:44:36.867
Si su carácter es el
resultado de esos aspectos,

00:44:36.867 --> 00:44:39.033
¿ustedes son responsables de eso?

00:44:39.033 --> 00:44:43.100
En caso de que no, ¿por qué el
mérito o la culpa moral

00:44:43.100 --> 00:44:45.000
se evalúan con respecto al carácter?

00:44:45.000 --> 00:44:46.467
Consideremos la suerte circunstancial,

00:44:46.467 --> 00:44:50.433
que Jonathan Shay discutía en
Achilles in Vietnam,

00:44:50.433 --> 00:44:53.200
la suerte con respecto al
entorno del agente.

00:44:53.200 --> 00:44:55.900
A veces, las circunstancias en
las que se encuentran

00:44:55.900 --> 00:45:00.933
crean o revelan aspectos
ocultos de su carácter.

00:45:00.933 --> 00:45:06.933
Debido a que son en parte una
cuestión de suerte, ¿eso significa

00:45:06.933 --> 00:45:11.200
que ustedes no son moralmente
responsables de lo que hicieron?

00:45:11.200 --> 00:45:16.133
Por último, si pensamos en
nuestras acciones

00:45:16.133 --> 00:45:19.667
desde la perspectiva del libre
albedrío, se vuelve difícil encontrar

00:45:19.667 --> 00:45:24.233
un espacio en el que seamos
responsables de lo que hacemos.

00:45:24.233 --> 00:45:30.067
Es un aspecto general del mundo que las
acciones y las consecuencias

00:45:30.067 --> 00:45:34.433
están determinadas en parte
por aspectos externos,

00:45:34.433 --> 00:45:38.467
que están fuera del control del agente.

00:45:38.467 --> 00:45:42.067
Comenzamos a pensar por qué
respondemos de esa manera

00:45:42.067 --> 00:45:42.967
en los casos del tranvía y
resulta que se debe

00:45:42.967 --> 00:45:45.267
a que se activa nuestra parte
emocional del cerebro.

00:45:45.267 --> 00:45:46.600
¿Por qué pasa eso?

00:45:46.600 --> 00:45:49.467
Porque el flujo de sangre
sucede de cierta forma

00:45:49.467 --> 00:45:51.367
en nuestro cerebro.

00:45:51.367 --> 00:45:52.500
¿Y por qué pasa eso?

00:45:52.500 --> 00:45:54.167
Porque la sangre fluye de cierta manera

00:45:54.167 --> 00:45:56.767
por las acciones de cierto
tipo de moléculas.

00:45:56.767 --> 00:46:01.767
A medida que reflexionamos sobre
esto, la zona de agencia,

00:46:01.767 --> 00:46:07.267
dice Nagel, parece encogerse
a un punto infinito.

00:46:07.267 --> 00:46:09.767
Dejo que se vayan de vacaciones

00:46:09.767 --> 00:46:12.600
con la siguiente
no-solución desconcertante

00:46:12.600 --> 00:46:15.667
a un problema moral profundo.

00:46:15.667 --> 00:46:19.167
Nagel sugiere que el problema de la
suerte no tiene solución

00:46:19.167 --> 00:46:23.067
porque algo en la idea de
concebirnos como agentes

00:46:23.067 --> 00:46:28.500
es incompatible con el hecho innegable

00:46:28.500 --> 00:46:31.500
de que las acciones son eventos y
las personas son cosas.

00:46:31.500 --> 00:46:35.000
"A medida que los determinantes
externos de la acción de alguien

00:46:35.000 --> 00:46:38.300
exponen gradualmente su efecto
en las consecuencias,

00:46:38.300 --> 00:46:42.567
en el carácter y en la misma
elección, vemos que efectivamente

00:46:42.567 --> 00:46:48.133
las acciones son eventos y las
personas son cosas.

00:46:48.133 --> 00:46:49.667
Como resultado,

00:46:49.667 --> 00:46:53.300
no queda nada que pueda imputarse
al sujeto responsable,

00:46:53.300 --> 00:46:56.833
y nos quedamos solo con una
porción de una larga secuencia

00:46:56.833 --> 00:47:00.133
de eventos que puede
lamentarse o celebrarse,

00:47:00.133 --> 00:47:03.267
pero no puede ser objeto de
mérito o de culpa".

00:47:03.267 --> 00:47:10.000
Sin embargo, renunciar al
lenguaje del mérito o la culpa

00:47:10.000 --> 00:47:13.100
es eliminar de nuestro
repertorio conceptual

00:47:13.100 --> 00:47:17.267
tal vez la herramienta más
importante que tenemos.

00:47:17.267 --> 00:47:20.500
Llegar a una perspectiva
estable de estos asuntos

00:47:20.500 --> 00:47:23.933
parece sumamente difícil.

00:47:23.933 --> 00:47:27.267
Nos vemos después de las vacaciones.

