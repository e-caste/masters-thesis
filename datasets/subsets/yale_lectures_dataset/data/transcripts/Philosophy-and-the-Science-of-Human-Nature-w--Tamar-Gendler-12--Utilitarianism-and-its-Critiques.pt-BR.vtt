WEBVTT
Kind: captions
Language: pt-BR

00:00:01.367 --> 00:00:08.100
Então, o que eu quero fazer na aula
de hoje é mudar um pouco

00:00:08.100 --> 00:00:11.133
o panorama sobre o que falamos
na 1ª unidade do curso.

00:00:11.133 --> 00:00:15.467
Como vocês sabes, a 1ª unidade
do curso foi enfocada numa

00:00:15.467 --> 00:00:21.167
série de textos que se preocupavam
com a prosperidade humana.

00:00:21.167 --> 00:00:25.733
E embora o nosso texto de
abertura, o desafio de Glauco

00:00:25.733 --> 00:00:29.300
em A República de Platão, se
preocupe com a moralidade

00:00:29.300 --> 00:00:33.533
e como a moral contribui para a
prosperidade humana, nós não

00:00:33.533 --> 00:00:39.067
temos dado, até agora, muita
atenção para o que os filósofos

00:00:39.067 --> 00:00:42.667
têm a dizer sobre a
natureza da moralidade.

00:00:42.667 --> 00:00:49.167
E o objetivo desta unidade é, de uma
forma extremamente rápida,

00:00:49.167 --> 00:00:55.567
apresentar hoje e na próxima
terça, 2 das teorias morais

00:00:55.567 --> 00:00:59.867
mais importantes na tradição
ocidental e, em seguida, nas sessões

00:00:59.867 --> 00:01:02.667
restantes antes da pausa de
março, comentar

00:01:02.667 --> 00:01:06.500
sobre algumas pesquisas
empíricas sobre essas questões.

00:01:06.500 --> 00:01:10.467
E eu sei que esta turma possui uma
grande diversidade de experiências.

00:01:10.467 --> 00:01:13.200
Alguns de vocês estão no seu
1º curso de Filosofia.

00:01:13.200 --> 00:01:17.633
Outros estão tendo um curso
completo sobre ética.

00:01:17.633 --> 00:01:20.467
Por isso, eu tentei preparar
a aula de tal forma

00:01:20.467 --> 00:01:26.700
que ela contemple a todos, mas de
tal modo que, eu espero,

00:01:26.700 --> 00:01:30.533
não aborreça àqueles que
já viram isso antes.

00:01:30.533 --> 00:01:34.600
Em especial, para compensar o fato
de que há pouca psicologia

00:01:34.600 --> 00:01:39.933
empírica nesta aula, temos 6
slides de enquetes.

00:01:39.933 --> 00:01:42.100
Eles vão surgir na metade da
aula, justamente no momento

00:01:42.100 --> 00:01:43.533
em que vocês começarem a cochilar

00:01:43.533 --> 00:01:45.800
porque dormiram apenas 2
horas na noite anterior.

00:01:45.800 --> 00:01:48.233
Mesmo se vocês não tiverem
prestado atenção à 1ª parte,

00:01:48.233 --> 00:01:49.767
terão de votar.

00:01:49.767 --> 00:01:55.767
Tudo bem, então o que a filosofia
moral se propõe a fazer?

00:01:55.767 --> 00:02:00.867
O que é isso de fornecer uma
explicação filosófica da moralidade?

00:02:00.867 --> 00:02:07.067
A filosofia moral é o esforço
sistemático para compreender os

00:02:07.067 --> 00:02:12.233
conceitos morais e justificar os
princípios morais e teorias.

00:02:12.233 --> 00:02:17.833
Ou seja: a filosofia moral, mesmo
que acabe dando uma resposta

00:02:17.833 --> 00:02:23.800
não-sistemática de como a moralidade
funciona e o que a moralidade

00:02:23.800 --> 00:02:30.067
faz, faz isso dentro do esforço
de pensar sistematicamente

00:02:30.067 --> 00:02:32.867
sobre a natureza da moralidade.

00:02:32.867 --> 00:02:34.867
O que eu quero dizer com moralidade?

00:02:34.867 --> 00:02:39.367
Quero dizer que as teorias morais
objetivam proporcionar modelos

00:02:39.367 --> 00:02:43.200
de termos como "certo" e "errado",
"admissível" e "inadmissível ",

00:02:43.200 --> 00:02:48.500
"deve" e "não deve", "proibido",
"bom", "ruim" e análogos...

00:02:48.500 --> 00:02:55.633
e fornecer um modelo de comportamentos
a que esses termos se aplicam.

00:02:55.633 --> 00:03:00.433
Ela é, fundamentalmente – para
mencionar uma distinção terminológica

00:03:00.433 --> 00:03:01.733
que fizemos antes –, uma
iniciativa normativa

00:03:01.733 --> 00:03:06.167
em vez de uma descritiva.

00:03:06.167 --> 00:03:10.600
A teoria moral filosófica
não pretende dizer

00:03:10.600 --> 00:03:12.133
como as pessoas agem.

00:03:12.133 --> 00:03:17.367
Ela se destina a dizer como
as pessoas devem agir

00:03:17.367 --> 00:03:19.733
se desejam estar em
conformidade com as
restrições

00:03:19.733 --> 00:03:22.867
que a moralidade deposita nelas.

00:03:22.867 --> 00:03:27.700
Em particular, a filosofia moral
se preocupa em fornecer

00:03:27.700 --> 00:03:33.267
uma resposta baseada em princípios
para 3 tipos de perguntas.

00:03:33.267 --> 00:03:39.033
O 1º tipo já foi encontrado no
contexto do desafio de Glauco.

00:03:39.033 --> 00:03:42.033
É a questão da motivação moral.

00:03:42.033 --> 00:03:45.567
"Por que queremos agir

00:03:45.567 --> 00:03:48.967
de acordo com o que a
moralidade exige de nós?"

00:03:48.967 --> 00:03:51.767
E, daqui a 1 minuto, eu vou dar a
vocês uma noção da variedade

00:03:51.767 --> 00:03:54.133
de respostas que foram
fornecidas para essa pergunta.

00:03:54.133 --> 00:03:57.433
Assim, a 1ª questão que a
filosofia moral coloca é

00:03:57.433 --> 00:04:01.100
por que nós ainda queremos ser morais.

00:04:01.100 --> 00:04:04.767
Em seguida, ela faz o
questionamento particular:

00:04:04.767 --> 00:04:09.700
"O que devemos fazer à medida que
buscamos agir moralmente?"

00:04:09.700 --> 00:04:13.467
E sobre isso nós tivemos muito
pouco a dizer até agora.

00:04:13.467 --> 00:04:16.000
Sabemos que, segundo
Aristóteles, para ser corajoso

00:04:16.000 --> 00:04:17.900
você tem de atuar como um corajoso.

00:04:17.900 --> 00:04:20.833
Mas Aristóteles apresentou a
coragem como uma virtude

00:04:20.833 --> 00:04:25.433
sem qualquer explicação sobre o
que faz a coragem estar

00:04:25.433 --> 00:04:29.967
na categoria de virtudes e a
covardia na categoria de vícios,

00:04:29.967 --> 00:04:34.333
com nada mais que uma análise
muito geral do significado.

00:04:34.333 --> 00:04:38.733
E nós ainda não analisamos todas
as alegações específicas

00:04:38.733 --> 00:04:43.533
sobre as ações particulares serem
moralmente aceitáveis ou não.

00:04:43.533 --> 00:04:46.167
Portanto, o 2º tipo de coisa que
uma teoria moral tenta fazer

00:04:46.167 --> 00:04:47.967
– e, mais uma vez, vou dar alguns
exemplos daqui a pouco –

00:04:47.967 --> 00:04:51.233
é dar respostas
específicas para a
pergunta:

00:04:51.233 --> 00:04:55.800
"Esta ação está moralmente OK?".

00:04:55.800 --> 00:05:00.833
Além disso, o que uma teoria
moral visa fazer é dizer

00:05:00.833 --> 00:05:06.500
por que deu as respostas
feitas na questão 2.

00:05:06.500 --> 00:05:10.967
"Em virtude de que
características comuns

00:05:10.967 --> 00:05:14.133
as ações enquadradas na
categoria de moral

00:05:14.133 --> 00:05:19.500
se distinguem das ações
enquadradas na categoria de imoral?"

00:05:19.500 --> 00:05:24.800
Então, quais seriam as respostas
para essas 3 perguntas?

00:05:24.800 --> 00:05:26.267
Vamos começar,

00:05:26.267 --> 00:05:30.167
uma vez que já encontramos isso na
questão da motivação moral.

00:05:30.167 --> 00:05:36.100
Uma classe de respostas
possíveis para o por quê

00:05:36.100 --> 00:05:40.700
seria moral agir de acordo com as
restrições da moralidade

00:05:40.700 --> 00:05:43.100
é um modelo do interesse próprio.

00:05:43.100 --> 00:05:45.900
Alguém pode apresentar um
modelo que diz:

00:05:45.900 --> 00:05:51.133
quando você se comporta
moralmente, as coisas funcionam
bem.

00:05:51.133 --> 00:05:54.333
Como Sócrates argumenta na
resposta a Glauco,

00:05:54.333 --> 00:05:58.533
quando você se comporta de acordo com
as restrições da moralidade,

00:05:58.533 --> 00:06:02.400
há harmonia na sua alma.

00:06:02.400 --> 00:06:07.433
E isso proporciona a você a possibilidade
de um certo tipo de prosperidade.

00:06:07.433 --> 00:06:12.133
Ou você pode ter o que está
implícito no argumento inicial

00:06:12.133 --> 00:06:16.133
de Glauco, uma visão de
que a moralidade

00:06:16.133 --> 00:06:19.600
fornece um certo tipo de
estabilidade na sociedade.

00:06:19.600 --> 00:06:23.467
Cada um de nós se comportando
de maneira pró-social

00:06:23.467 --> 00:06:26.433
aumenta a probabilidade dos
outros, ao nosso redor,

00:06:26.433 --> 00:06:28.533
se comportarem de maneira pró-social.

00:06:28.533 --> 00:06:31.600
E assim chegamos a uma espécie
de estado de equilíbrio

00:06:31.600 --> 00:06:34.467
em que as coisas funcionam bem,

00:06:34.467 --> 00:06:37.067
se todo mundo se comporta
pró-socialmente.

00:06:37.067 --> 00:06:38.733
E vamos falar sobre isso novamente

00:06:38.733 --> 00:06:41.400
no início da seção de
filosofia política.

00:06:41.400 --> 00:06:44.233
Portanto, a teoria do interesse próprio

00:06:44.233 --> 00:06:48.600
apela para um certo tipo de
coordenação, seja uma coordenação

00:06:48.600 --> 00:06:54.467
entre as partes da alma ou entre os
indivíduos de uma sociedade.

00:06:54.467 --> 00:06:56.833
Um 2º tipo de teoria do
interesse próprio

00:06:56.833 --> 00:06:59.767
é o que podemos chamar de
"obter as coisas boas".

00:06:59.767 --> 00:07:03.600
Ela está no cerne de algumas
tradições religiosas.

00:07:03.600 --> 00:07:08.033
Aqui está o que você recebe se agir
de acordo com as restrições

00:07:08.033 --> 00:07:13.700
da moralidade: você obtém a vida
eterna num lugar muito agradável.

00:07:13.700 --> 00:07:15.667
Aqui está o que você
recebe se não agir

00:07:15.667 --> 00:07:17.633
de acordo com as
restrições da moralidade:

00:07:17.633 --> 00:07:22.400
você obtém a eternidade num
lugar muito desagradável.

00:07:22.400 --> 00:07:27.033
Portanto, a noção de que há
alguma recompensa além da Terra

00:07:27.033 --> 00:07:29.433
por se comportar de maneiras morais

00:07:29.433 --> 00:07:33.800
é um exemplo de uma justificação de
interesse próprio da moralidade.

00:07:33.800 --> 00:07:37.067
Ou alguém pode dar o tipo
de justificativa

00:07:37.067 --> 00:07:41.800
que Adimanto dá no desafio de Glauco.

00:07:41.800 --> 00:07:44.400
Adimanto salienta que uma das
coisas que a moralidade

00:07:44.400 --> 00:07:47.733
fornece é uma reputação melhor.

00:07:47.733 --> 00:07:49.600
Como resultado de se comportar

00:07:49.600 --> 00:07:51.767
de acordo com os padrões de moralidade,

00:07:51.767 --> 00:07:55.433
você é percebido como tendo se
comportado dessa forma

00:07:55.433 --> 00:07:58.733
e essa reputação aporta
algum valor para você.

00:07:58.733 --> 00:08:03.867
Ou pode ser, como Aristóteles
pondera no final do Livro X,

00:08:03.867 --> 00:08:07.667
que a sociedade está
estruturada de alguma forma

00:08:07.667 --> 00:08:11.133
que motiva as pessoas a agirem de
acordo com as restrições

00:08:11.133 --> 00:08:15.767
da moralidade, afinal é uma
forma de evitar a punição.

00:08:15.767 --> 00:08:18.933
Muitos de nós obedecemos a
lei de velocidade

00:08:18.933 --> 00:08:21.300
precisamente por essa razão.

00:08:21.300 --> 00:08:28.500
Nós a obedecemos mais quando há
luzes de sirene por perto.

00:08:28.500 --> 00:08:31.267
Mas também podemos ter uma
versão interiorizada

00:08:31.267 --> 00:08:34.233
da redução da punição.

00:08:34.233 --> 00:08:38.133
Parte da imagem freudiana que
ouvimos falar na aula

00:08:38.133 --> 00:08:42.900
sobre a alma dividida discutia o
desenvolvimento da consciência

00:08:42.900 --> 00:08:45.633
como uma interiorização
de regras externas,

00:08:45.633 --> 00:08:50.367
em que o superego fica chateado
quando o id se comporta

00:08:50.367 --> 00:08:53.600
de um jeito em desacordo com as
restrições da moralidade.

00:08:53.600 --> 00:08:55.833
E é possível ter uma
versão não-freudiana

00:08:55.833 --> 00:08:59.000
disso quando se apela para a
noção de consciência.

00:08:59.000 --> 00:09:02.400
Assim, a ideia de o que a
moralidade traz para você

00:09:02.400 --> 00:09:05.300
é tanto a possibilidade de
salvação quanto melhor reputação,

00:09:05.300 --> 00:09:09.400
tanto a possibilidade de não ser
punido por leis externas

00:09:09.400 --> 00:09:12.200
quanto a possibilidade de não ser punido
por sua própria consciência,

00:09:12.200 --> 00:09:16.233
é uma outra versão da teoria
do interesse próprio.

00:09:16.233 --> 00:09:18.567
É um tipo de justificação

00:09:18.567 --> 00:09:22.700
que alguém pode prever ao se
comportar de maneiras morais.

00:09:22.700 --> 00:09:25.800
Um 2º tipo diferente de justificação

00:09:25.800 --> 00:09:28.433
diz que o motivo para agir moralmente

00:09:28.433 --> 00:09:30.867
é porque as características normativas

00:09:30.867 --> 00:09:33.933
são características
fundamentais do mundo.

00:09:33.933 --> 00:09:37.600
Existe um grande "deve" lá fora.

00:09:37.600 --> 00:09:42.233
É um fato sobre a realidade que o que
estamos moralmente obrigados

00:09:42.233 --> 00:09:47.500
a fazer é agir de acordo com as
exigências da moralidade

00:09:47.500 --> 00:09:50.433
e não por interesse próprio,

00:09:50.433 --> 00:09:57.167
mas simplesmente porque nós
somos responsivos

00:09:57.167 --> 00:10:01.967
a essa característica do mundo,
estamos motivados a agir moralmente.

00:10:01.967 --> 00:10:08.233
Um 3º tipo de justificação, o 3º tipo
de explicação da motivação,

00:10:08.233 --> 00:10:11.067
é o que podemos chamar de
uma teoria factível

00:10:11.067 --> 00:10:15.800
que diz, basicamente, que as
pessoas são assim.

00:10:15.800 --> 00:10:19.200
Portanto, os modelos evolutivos dizem

00:10:19.200 --> 00:10:24.333
que os comportamentos
pró-sociais foram os
escolhidos,

00:10:24.333 --> 00:10:28.067
talvez porque possibilitam a resolução
de problemas de coordenação.

00:10:28.067 --> 00:10:29.733
Mas seja qual for a explicação,

00:10:29.733 --> 00:10:32.867
o comportamento pró-social diz que
foi essa teoria a escolhida.

00:10:32.867 --> 00:10:36.467
É um fato bruto do mundo que nos
comportamos de forma pró-social...

00:10:36.467 --> 00:10:38.700
não um fato normativo bruto do
mundo, e sim um fato

00:10:38.700 --> 00:10:42.733
descritivo bruto do mundo que nos
comportamos desse jeito.

00:10:42.733 --> 00:10:46.567
Ou você pode ter, não uma versão
evolutiva com base nisso,

00:10:46.567 --> 00:10:48.033
mas uma versão que diz:

00:10:48.033 --> 00:10:51.967
esta é a maneira que a alma
humana se manifesta

00:10:51.967 --> 00:10:56.233
quando ela está em conformidade
com o seu estado natural.

00:10:56.233 --> 00:11:00.167
Então, você pode ter uma
teoria da moralidade que diz

00:11:00.167 --> 00:11:02.567
que a razão para se comportar
moralmente é o interesse próprio.

00:11:02.567 --> 00:11:04.800
Você pode ter uma teoria da
moralidade que diz

00:11:04.800 --> 00:11:07.467
que a razão para se comportar
moralmente é o altruísmo.

00:11:07.467 --> 00:11:09.767
Você pode ter uma teoria da
moralidade que diz que a razão

00:11:09.767 --> 00:11:13.400
para se comportar moralmente é a
maneira que nos comportamos.

00:11:13.400 --> 00:11:19.133
Ou você pode ter algum tipo
de teoria combinada.

00:11:19.133 --> 00:11:23.900
E nós já falamos sobre a 1ª: a
teoria do interesse próprio.

00:11:23.900 --> 00:11:26.633
E como esta seção do curso continua,

00:11:26.633 --> 00:11:31.433
vamos comentar mais sobre outros
tipos de explicações.

00:11:31.433 --> 00:11:34.733
Esses são alguns exemplos
dos tipos de respostas

00:11:34.733 --> 00:11:39.633
que são dadas à 1ª questão,
a da motivação moral.

00:11:39.633 --> 00:11:42.400
Que tipos de problemas surgem

00:11:42.400 --> 00:11:45.833
quando refletimos sobre o
comportamento moral?

00:11:45.833 --> 00:11:49.467
Bom, vocês viram um monte
de exemplos disso

00:11:49.467 --> 00:11:52.333
na leitura que fizemos para hoje.

00:11:52.333 --> 00:11:57.400
Um tipo de questionamento que as
teorias morais estabelecem

00:11:57.400 --> 00:12:01.000
para buscar respostas é se
é moralmente exigido

00:12:01.000 --> 00:12:02.867
ou moralmente permitido

00:12:02.867 --> 00:12:08.500
prejudicar 1 pessoa a fim de
ajudar a várias.

00:12:08.500 --> 00:12:12.167
Ou seja, a história de Bernard
Williams – "Jim e os índios" –,

00:12:12.167 --> 00:12:14.433
coloca Jim numa situação

00:12:14.433 --> 00:12:18.400
em que se ele atira em 1
dos 20 prisioneiros,

00:12:18.400 --> 00:12:21.200
os outros 19 são libertados,

00:12:21.200 --> 00:12:25.600
mas se ele recusa em atirar em 1,
todos os 20 serão mortos.

00:12:25.600 --> 00:12:29.967
Ou a história de Omelas, a
história de uma sociedade

00:12:29.967 --> 00:12:34.100
cuja prosperidade depende do
sofrimento de 1 criança.

00:12:34.100 --> 00:12:39.200
Ou os dilemas do Trolley,
apresentados por mim na 1ª aula,

00:12:39.200 --> 00:12:42.800
onde um trem está em
direção a 5 pessoas

00:12:42.800 --> 00:12:46.167
e nós temos a possibilidade de
desviá-lo do seu caminho,

00:12:46.167 --> 00:12:49.200
o que levaria à morte de 1 pessoa.

00:12:49.200 --> 00:12:54.367
Esses são exemplos de
representações esquemáticas

00:12:54.367 --> 00:12:55.733
dos tipos de questões

00:12:55.733 --> 00:12:58.867
que a teoria moral
confronta o tempo todo.

00:12:58.867 --> 00:13:00.500
Sempre que pensamos

00:13:00.500 --> 00:13:02.533
sobre diferimentos de ameaça

00:13:02.533 --> 00:13:05.100
– é certo colocar em
quarentena uma população

00:13:05.100 --> 00:13:07.700
que sofre de uma doença,
causando danos a ela,

00:13:07.700 --> 00:13:11.367
mas beneficiando o resto
da sociedade? –

00:13:11.367 --> 00:13:14.800
estamos diante desses
tipos de questões.

00:13:14.800 --> 00:13:19.333
Um tipo de questão que a
filosofia moral visa responder

00:13:19.333 --> 00:13:22.567
é se esta espécie de troca

00:13:22.567 --> 00:13:26.000
é moralmente exigida ou
moralmente permitida.

00:13:26.000 --> 00:13:29.767
Uma versão particularmente
profunda da questão

00:13:29.767 --> 00:13:31.967
surge quando pensamos sobre quais são

00:13:31.967 --> 00:13:36.567
os nossos deveres morais perante
os menos afortunados.

00:13:36.567 --> 00:13:43.800
O filósofo Peter Singer tem um
argumento de que toda a estrutura

00:13:43.800 --> 00:13:50.900
do 1º Mundo e do 3º Mundo é
moralmente ilegítima, pois

00:13:50.900 --> 00:13:56.833
envolve uma falta de vontade por parte
dos que estão no 1º Mundo

00:13:56.833 --> 00:14:02.100
para fazer o que é
moralmente exigido deles,

00:14:02.100 --> 00:14:07.133
isto é, pegar uma grande proporção
dos recursos e redistribuí-los

00:14:07.133 --> 00:14:14.333
para as pessoas que sofrem de
doenças bastante fáceis de curar.

00:14:14.333 --> 00:14:16.667
As pessoas que não têm
telas mosquiteiras,

00:14:16.667 --> 00:14:19.000
as pessoas que não têm vacinas,

00:14:19.000 --> 00:14:21.633
as pessoas que não têm água potável,

00:14:21.633 --> 00:14:25.500
as pessoas que não têm acesso a
cuidados médicos básicos

00:14:25.500 --> 00:14:28.033
nos primeiros 5 anos... tudo
isso, por exemplo,

00:14:28.033 --> 00:14:30.700
evitaria a chance de
cegueira ao longo da vida.

00:14:30.700 --> 00:14:33.367
Portanto, outra questão que
a teoria moral pede

00:14:33.367 --> 00:14:36.533
– algumas vezes na versão da
pergunta anterior – é,

00:14:36.533 --> 00:14:41.200
em geral, o que são os nossos deveres
perante os menos afortunados.

00:14:41.200 --> 00:14:43.800
Ela também faz perguntas como:

00:14:43.800 --> 00:14:46.433
Estes tipos de comportamentos são
moralmente compulsórios?

00:14:46.433 --> 00:14:49.267
É moralmente compulsório para
nós o comportamento

00:14:49.267 --> 00:14:52.300
que ajuda o meio ambiente – a
reciclagem, digamos?

00:14:52.300 --> 00:14:55.067
É moralmente compulsório para nós
agirmos de determinadas maneiras

00:14:55.067 --> 00:14:59.167
em relação aos animais não-humanos,
talvez virando vegetarianos?

00:14:59.167 --> 00:15:03.933
É moralmente obrigatório para
nós adorarmos uma divindade

00:15:03.933 --> 00:15:06.767
de alguma forma? O culto religioso é
algo moralmente compulsório?

00:15:06.767 --> 00:15:10.133
O respeito pelos mais
velhos, parte fundamental

00:15:10.133 --> 00:15:14.167
das estruturas morais tradicionais, é
algo moralmente compulsório?

00:15:14.167 --> 00:15:16.633
E as teorias morais também
fazem perguntas como:

00:15:16.633 --> 00:15:18.967
Esses tipos de coisas são
moralmente admissíveis?

00:15:18.967 --> 00:15:20.533
O aborto é moralmente admissível?

00:15:20.533 --> 00:15:22.167
A eutanásia é moralmente admissível?

00:15:22.167 --> 00:15:24.533
A pena capital é
moralmente admissível?

00:15:24.533 --> 00:15:26.133
E o sexo antes do casamento?

00:15:26.133 --> 00:15:28.767
E mentir em função de uma
ou outra motivação?

00:15:28.767 --> 00:15:31.733
Que tal, como Kant discute
na próxima leitura,

00:15:31.733 --> 00:15:35.100
deixar de cultivar o talento de alguém.

00:15:35.100 --> 00:15:38.767
Kant acha que se trata de uma
violação do mandato moral?

00:15:38.767 --> 00:15:40.933
Então, esses são os tipos de perguntas

00:15:40.933 --> 00:15:43.767
que as teorias morais buscam respostas.

00:15:43.767 --> 00:15:47.833
E isso pode parecer um
bocado heterogêneo.

00:15:47.833 --> 00:15:52.200
Mas dá um senso da
totalidade de explicações

00:15:52.200 --> 00:15:54.833
que as teorias morais tentam oferecer.

00:15:54.833 --> 00:16:00.300
Então, vamos nos focar nas 4 grandes
teorias morais na tradição

00:16:00.300 --> 00:16:05.133
ocidental e pensar como elas poderiam
fornecer respostas categóricas

00:16:05.133 --> 00:16:09.000
a essa grande variedade de questões.

00:16:09.000 --> 00:16:14.400
O tipo de teoria moral que vamos
discutir na aula de hoje é,

00:16:14.400 --> 00:16:18.767
essencialmente, uma teoria moral
conhecida como utilitarismo.

00:16:18.767 --> 00:16:20.833
Ela afirma que um ato é moral

00:16:20.833 --> 00:16:24.500
na medida em que produz o maior
bem para o maior número.

00:16:24.500 --> 00:16:27.300
Ela toma como noção
fundamental a noção do bem.

00:16:27.300 --> 00:16:29.833
E isso nos dá respostas
para as perguntas

00:16:29.833 --> 00:16:33.133
feitas anteriormente,
contanto que a gente saiba

00:16:33.133 --> 00:16:35.833
como os bens são
distribuídos em resposta a
elas.

00:16:35.833 --> 00:16:43.300
Portanto, se sabemos o que produz a
felicidade nos seres sencientes,

00:16:43.300 --> 00:16:47.000
então o utilitarismo nos
dará uma resposta

00:16:47.000 --> 00:16:51.167
para se ser vegetariano é
moralmente compulsório.

00:16:51.167 --> 00:16:53.133
Ele vai nos dizer para pegar a
quantidade de felicidade

00:16:53.133 --> 00:16:55.100
que é distribuída entre
os seres sencientes

00:16:55.100 --> 00:17:00.800
e observar qual distribuição
maximiza a quantidade de felicidade.

00:17:00.800 --> 00:17:03.300
O utilitarismo nos dá

00:17:03.300 --> 00:17:07.367
uma espécie de resposta
sistemática para a questão.

00:17:07.367 --> 00:17:09.800
Um 2º tipo de resposta,

00:17:09.800 --> 00:17:12.367
que vamos discutir na aula da terça,

00:17:12.367 --> 00:17:16.233
é a resposta dada por Kant e a
tradição deontológica.

00:17:16.233 --> 00:17:20.733
O que Kant afirma é que um ato é moral

00:17:20.733 --> 00:17:23.633
na medida em que ele é executado
como resultado de agir

00:17:23.633 --> 00:17:27.300
com o tipo correto de motivação.

00:17:27.300 --> 00:17:31.633
Ele toma como noção primária não
a noção de benevolência,

00:17:31.633 --> 00:17:35.367
mas a noção de retidão.

00:17:35.367 --> 00:17:40.133
E, com base nisso, Kant fornece
um monte de respostas

00:17:40.133 --> 00:17:41.867
às nossas perguntas específicas.

00:17:41.867 --> 00:17:45.867
Em especial, ele argumenta
que não está OK

00:17:45.867 --> 00:17:50.033
sacrificar o bem de 1
pelo bem de muitos.

00:17:50.033 --> 00:17:54.733
E ele argumenta que mentir é
moralmente inaceitável.

00:17:54.733 --> 00:17:56.600
Na próxima aula, comentaremos

00:17:56.600 --> 00:17:59.300
como a partir de um princípio
muito abstrato como esse

00:17:59.300 --> 00:18:03.000
é possível derivar essas
respostas específicas.

00:18:03.000 --> 00:18:09.233
Já conferimos a resposta tradicional
antiga para isso em Aristóteles,

00:18:09.233 --> 00:18:12.433
que um ato é moral na medida
em que ele é executado

00:18:12.433 --> 00:18:14.800
como o resultado de ter um
caráter virtuoso.

00:18:14.800 --> 00:18:18.167
Portanto, o que
Aristóteles diz é para
ver

00:18:18.167 --> 00:18:20.867
como o ser virtuoso se comportaria.

00:18:20.867 --> 00:18:26.833
E ao ver o que um virtuoso
faz, você pode aprender

00:18:26.833 --> 00:18:32.867
pelo exemplo dele o que a
moralidade exige de nós.

00:18:32.867 --> 00:18:37.133
E uma tradição final sobre a qual
não temos muito a dizer

00:18:37.133 --> 00:18:43.200
nesta aula é, naturalmente, uma base
para a moralidade que atingiu

00:18:43.200 --> 00:18:49.433
o centro da cultura ocidental há
pelo menos 2.000 anos:

00:18:49.433 --> 00:18:52.900
a ideia de que um ato é moral na medida

00:18:52.900 --> 00:18:58.000
em que está em conformidade com o
que a divindade exige de nós.

00:18:58.000 --> 00:19:02.867
Pode-se dar uma explicação,
como faz o utilitarismo,

00:19:02.867 --> 00:19:05.700
que apela para a noção
da benevolência.

00:19:05.700 --> 00:19:10.100
Pode-se dar uma justificação,
como faz a deontologia,

00:19:10.100 --> 00:19:11.900
que apela para a noção da retidão.

00:19:11.900 --> 00:19:15.700
Pode-se dar uma justificação,
como faz a ética da virtude,

00:19:15.700 --> 00:19:17.900
que apela para a noção da virtuosismo.

00:19:17.900 --> 00:19:23.133
Ou pode-se dar um modelo, como
faz a ética religiosa,

00:19:23.133 --> 00:19:27.300
que apela para a noção da
exigência divina.

00:19:27.300 --> 00:19:29.533
Então vamos refletir um pouco mais

00:19:29.533 --> 00:19:34.800
sobre a relação entre essas 3
teorias particulares,

00:19:34.800 --> 00:19:38.033
as que vamos nos focar no
contexto desta classe,

00:19:38.033 --> 00:19:41.967
tentando compreender a teoria particular

00:19:41.967 --> 00:19:45.133
que abordamos hoje, ou
seja, o utilitarismo.

00:19:45.133 --> 00:19:52.967
Portanto, a ética da virtude
centra a sua atenção no ator,

00:19:52.967 --> 00:19:55.267
não a pessoa que está no palco

00:19:55.267 --> 00:19:56.900
e recita Hamlet,

00:19:56.900 --> 00:20:03.867
mas o ator que executa um ato
que será moral ou não.

00:20:03.867 --> 00:20:09.533
A deontologia centra a sua
atenção no ato.

00:20:09.533 --> 00:20:12.467
Ela não analisa quem o realiza,

00:20:12.467 --> 00:20:20.500
mas sim qual ato é executado e
sob qual descrição.

00:20:20.500 --> 00:20:26.400
O consequencialismo, pelo
contrário, não analisa

00:20:26.400 --> 00:20:31.400
quem executa o ato nem a descrição
em que o ato é executado,

00:20:31.400 --> 00:20:37.433
e sim as consequências
que o ato acarreta.

00:20:37.433 --> 00:20:43.567
E encontramos a teoria da
virtude na voz de...

00:20:43.567 --> 00:20:52.867
vejam se vocês reconhecem este
senhor... Aristóteles.

00:20:52.867 --> 00:21:02.133
Encontraremos a deontologia na
voz de Immanuel Kant.

00:21:02.133 --> 00:21:06.433
E o que vamos discutir hoje –
o consequencialismo e,

00:21:06.433 --> 00:21:12.933
em particular, o utilitarismo –
na voz de John Stuart Mill.

00:21:12.933 --> 00:21:17.900
Portanto, vamos ver o que
Mill tem a dizer

00:21:17.900 --> 00:21:22.733
sobre a natureza
fundamental da moralidade.

00:21:22.733 --> 00:21:25.067
Mill afirma...

00:21:25.067 --> 00:21:27.500
e é melhor eu avisar que estamos
perto do slide do clicker,

00:21:27.500 --> 00:21:28.933
por isso, se você está
um pouco disperso,

00:21:28.933 --> 00:21:30.700
é hora de pegar o seu clicker.

00:21:30.700 --> 00:21:35.300
Daqui a 4 ou 5 minutos nós vamos
realizar algumas enquetes.

00:21:35.300 --> 00:21:42.333
Então, Mill afirma que o
panorama correto para pensar

00:21:42.333 --> 00:21:45.633
sobre as teorias morais é o
consequencialista,

00:21:45.633 --> 00:21:48.300
não aquele que olha para o ator
como faz a teoria da virtude,

00:21:48.300 --> 00:21:50.767
não aquele que olha para o ato
como faz a deontologia,

00:21:50.767 --> 00:21:52.833
mas sim aquele que olha para
as consequências

00:21:52.833 --> 00:21:54.733
como faz o consequencialismo.

00:21:54.733 --> 00:21:56.967
O grau de retidão moral de um ato

00:21:56.967 --> 00:21:59.567
é determinado pelas suas
consequências.

00:21:59.567 --> 00:22:03.233
E Mill fornece uma versão
particular disso.

00:22:03.233 --> 00:22:06.100
Ele diz que o grau de retidão
moral de um ato é determinado

00:22:06.100 --> 00:22:08.667
por um tipo particular de consequência,

00:22:08.667 --> 00:22:12.967
ou seja, a utilidade que o ato produz.

00:22:12.967 --> 00:22:15.467
Você pode ter uma teoria
consequencialista que diz

00:22:15.467 --> 00:22:17.367
que o grau de retidão moral de um ato

00:22:17.367 --> 00:22:20.367
é determinado pelas suas
consequências, digamos,

00:22:20.367 --> 00:22:25.233
a quantidade de bananas que ele produz.

00:22:25.233 --> 00:22:27.200
Seria uma teoria moral estranha,

00:22:27.200 --> 00:22:30.933
mas seria uma teoria
consequencialista que diz que o
grau de

00:22:30.933 --> 00:22:33.700
retidão moral de um ato é
determinado pelas suas consequências,

00:22:33.700 --> 00:22:37.633
em especial, pelo seu grau de
produção de bananas.

00:22:37.633 --> 00:22:42.500
Isso seria um tipo muito geral de
teoria consequencialista.

00:22:42.500 --> 00:22:47.167
As teorias utilitaristas são um tipo
particular de teoria consequencialista

00:22:47.167 --> 00:22:49.900
que afirmam que o grau de
retidão moral de um ato

00:22:49.900 --> 00:22:51.700
é determinado pelas suas
consequências,

00:22:51.700 --> 00:22:55.933
em particular

00:22:55.933 --> 00:22:59.767
pela quantidade de utilidade
produzida por ele

00:22:59.767 --> 00:23:04.800
– serventia, bem-estar no modelo de
Mill, o qual estamos interessados.

00:23:04.800 --> 00:23:08.900
Isso significa que, lembrando das
apostilas que vocês receberam

00:23:08.900 --> 00:23:14.567
na seção desta semana, ser
utilitário é uma condição
suficiente

00:23:14.567 --> 00:23:21.267
para ser consequencialista, mas não
uma condição necessária.

00:23:21.267 --> 00:23:23.100
E ser um consequencialista

00:23:23.100 --> 00:23:29.000
é uma condição necessária para ser um
utilitário, mas não suficiente.

00:23:29.000 --> 00:23:33.567
E se o que eu disse não é
completamente óbvio para vocês,

00:23:33.567 --> 00:23:36.800
deem uma olhada na apostila

00:23:36.800 --> 00:23:38.833
que vocês receberam.

00:23:38.833 --> 00:23:44.167
Portanto, Mill não só traça um
compromisso utilitário,

00:23:44.167 --> 00:23:47.733
no rumo para cumprir esse compromisso,

00:23:47.733 --> 00:23:53.067
ele faz 2 afirmações muito
particulares que eu quero pedir

00:23:53.067 --> 00:23:57.167
que vocês pensem à luz de
alguns casos particulares.

00:23:57.167 --> 00:24:03.067
A 1ª é a famosa formulação do
princípio da maior felicidade,

00:24:03.067 --> 00:24:08.767
que no texto de vocês aparece logo
no início da página 77.

00:24:08.767 --> 00:24:11.300
Mill diz: "As ações são corretas

00:24:11.300 --> 00:24:13.733
na medida em que tendem a
promover felicidade

00:24:13.733 --> 00:24:17.400
e erradas se tendem a
promover infelicidade".

00:24:17.400 --> 00:24:21.367
E ele continua algumas páginas
depois para esclarecer que não

00:24:21.367 --> 00:24:26.567
se trata da felicidade do próprio agente,
mas a de todos os interessados.

00:24:26.567 --> 00:24:31.600
E em 1 minuto nós vamos pensar
no que isso implica.

00:24:31.600 --> 00:24:35.767
O 2º compromisso de Mill que eu
quero que vocês reflitam

00:24:35.767 --> 00:24:40.733
é um que vai contra

00:24:40.733 --> 00:24:43.333
o que falamos em Aristóteles
na semana passada.

00:24:43.333 --> 00:24:47.633
Mill afirma que o motivo
não tem nada a ver

00:24:47.633 --> 00:24:49.500
com a moralidade da ação.

00:24:49.500 --> 00:24:53.833
"Aquele que salva um
semelhante de se afogar faz o

00:24:53.833 --> 00:24:57.967
que está moralmente certo seja
o seu motivo o dever,

00:24:57.967 --> 00:25:02.600
seja a esperança de ser
pago pelo incômodo.

00:25:02.600 --> 00:25:06.533
" A motivação com que um ato é
realizado, defende Mill,

00:25:06.533 --> 00:25:09.500
não nos diz nada sobre a
moralidade do ato.

00:25:09.500 --> 00:25:13.000
Ele não nega que isso nos
diz algo sobre o ator.

00:25:13.000 --> 00:25:16.533
Ele é perfeitamente ditoso em
afirmar que quem atua

00:25:16.533 --> 00:25:18.133
com a esperança de ser pago é,

00:25:18.133 --> 00:25:21.933
de alguma forma, diferente da pessoa

00:25:21.933 --> 00:25:26.467
que atua com um senso de dever
ou obrigação moral.

00:25:26.467 --> 00:25:32.267
Mas uma vez que o valor moral do
próprio ato está em causa,

00:25:32.267 --> 00:25:36.000
Mill acha que não há diferença.

00:25:36.000 --> 00:25:39.067
Portanto, este é o 1º
questionamento que eu quero fazer.

00:25:39.067 --> 00:25:42.267
Vamos pegar o caso descrito por Mill.

00:25:42.267 --> 00:25:46.467
Você vê alguém se afogando num lago.

00:25:46.467 --> 00:25:48.933
E a questão é:

00:25:48.933 --> 00:25:54.067
o seu ato de salvar essa pessoa
é moralmente correto,

00:25:54.067 --> 00:25:59.033
moralmente virtuoso ou moral só se
for feito por obrigação?

00:25:59.033 --> 00:26:01.800
Eu quero salvar esta pessoa porque
é a coisa certa a fazer

00:26:01.800 --> 00:26:03.933
ou por algum outro tipo de
motivação pró-social...

00:26:03.933 --> 00:26:06.167
Se você acha isso, aperte 1.

00:26:06.167 --> 00:26:10.167
Ou o ato é moralmente correto
independentemente do seu motivo,

00:26:10.167 --> 00:26:14.567
mesmo se você fizer isso porque há
uma placa que diz: "Salvar

00:26:14.567 --> 00:26:18.400
uma pessoa se afogando:
Recompensa de US$ 10.000".

00:26:18.400 --> 00:26:20.733
E você pensa: "US$ 10.000
é muito dinheiro!"

00:26:20.733 --> 00:26:22.833
E você pula na água.

00:26:22.833 --> 00:26:26.867
Tudo bem, eu vou pressionar o
temporizador de 10 segundos.

00:26:26.867 --> 00:26:29.633
Temos umas 50, 70 pessoas aqui.

00:26:29.633 --> 00:26:31.333
Muito bem, os números estão subindo.

00:26:31.333 --> 00:26:35.167
Vamos ver se esta sala

00:26:35.167 --> 00:26:38.533
está cheia de kantianos ou de
consequencialistas.

00:26:38.533 --> 00:26:42.233
Curiosamente, há um
equilíbrio nas respostas.

00:26:42.233 --> 00:26:46.800
A maioria de vocês parece
estar com Mill...

00:26:46.800 --> 00:26:50.000
que um ato é moralmente correto
independentemente do motivo.

00:26:50.000 --> 00:26:55.000
Mas uma parcela considerável
ficará satisfeita

00:26:55.000 --> 00:26:58.833
quando lermos Kant, que oferece a
resposta escolhida por vocês.

00:26:58.833 --> 00:27:03.233
E uma das tarefas que faremos na
seção da próxima semana

00:27:03.233 --> 00:27:06.900
será que vocês, que optaram por um
ou outro lado da questão,

00:27:06.900 --> 00:27:10.767
conversem com outras pessoas

00:27:10.767 --> 00:27:16.033
e digam por que é que decidiram
por este grupo ou por este.

00:27:16.033 --> 00:27:18.967
Até agora Mill está se
saindo muito bem.

00:27:18.967 --> 00:27:21.533
Ele tem uma pequena maioria
da sala do lado dele.

00:27:21.533 --> 00:27:26.433
Agora eu quero apresentar uma
série de casos e perguntar

00:27:26.433 --> 00:27:29.833
o que vocês pensam sobre o
princípio da maior felicidade.

00:27:29.833 --> 00:27:32.367
Lembrem-se, Mill diz que um ato é moral

00:27:32.367 --> 00:27:36.333
na medida em que produz a maior
felicidade para o maior número,

00:27:36.333 --> 00:27:38.767
onde não estamos interessados

00:27:38.767 --> 00:27:42.933
com a forma que a felicidade é
distribuída entre os indivíduos.

00:27:42.933 --> 00:27:47.333
Portanto, vamos começar
com o seguinte caso.

00:27:47.333 --> 00:27:49.867
Existe um ato possível de realizar

00:27:49.867 --> 00:27:55.567
em que você ganhará 100
unidades de felicidade.

00:27:55.567 --> 00:27:58.000
Cada um destes rostos sorridentes...

00:27:58.000 --> 00:28:00.767
vocês não se sentem
pró-sociais vendo eles?

00:28:00.767 --> 00:28:02.467
Cada um desses rostos
sorridentes representa

00:28:02.467 --> 00:28:03.833
10 unidades de felicidade.

00:28:03.833 --> 00:28:05.733
Então, vamos supor que você
tenha um ventilador.

00:28:05.733 --> 00:28:07.000
É um dia muito quente

00:28:07.000 --> 00:28:09.133
e você tem um ventilador.

00:28:09.133 --> 00:28:11.200
E o refresco desse ventilador

00:28:11.200 --> 00:28:13.533
proporciona a você 100
unidades de felicidade.

00:28:13.533 --> 00:28:15.933
Ou vamos supor que você tenha
alguns biscoitos deliciosos

00:28:15.933 --> 00:28:19.500
e comê-los proporciona a você
100 unidades de felicidade.

00:28:19.500 --> 00:28:24.567
Além disso, realizar esse ato
dá a 100 outras pessoas

00:28:24.567 --> 00:28:27.100
1 unidade de felicidade para cada 1.

00:28:27.100 --> 00:28:30.267
Vamos supor que o seu ventilador
sopre um pouco fora da sua sala,

00:28:30.267 --> 00:28:33.133
de modo que, além de proporcionar
a você as 100 unidades,

00:28:33.133 --> 00:28:36.200
ele refresque as pessoas à
volta com 1 unidade cada.

00:28:36.200 --> 00:28:39.567
Ou vamos supor que quando você
termine de comer os 100 biscoitos,

00:28:39.567 --> 00:28:43.300
sobrem outros 100, e cada 1 das 100
pessoas come 1 biscoito,

00:28:43.300 --> 00:28:46.167
proporcionando a elas 1
unidade de felicidade.

00:28:46.167 --> 00:28:47.867
OK. Então, esse é o 1º ato.

00:28:47.867 --> 00:28:50.333
Existe um total de 200
unidades de felicidade.

00:28:50.333 --> 00:28:53.600
Você fica com 100 unidades e cada
1 das outras 100 pessoas

00:28:53.600 --> 00:28:55.367
recebe 1 unidade.

00:28:55.367 --> 00:28:59.367
Portanto, a sua escolha é entre
realizar esse ato e realizar

00:28:59.367 --> 00:29:01.433
um ato que eu chamo de 2º ato,

00:29:01.433 --> 00:29:04.033
que tem exatamente os mesmos
efeitos para você, certo?

00:29:04.033 --> 00:29:06.400
Você recebe 100 unidades de felicidade.

00:29:06.400 --> 00:29:09.100
Então, aqui está você com
as suas 100 unidades.

00:29:09.100 --> 00:29:14.600
Mas, neste caso, se você mudar um
pouco o ângulo do ventilador,

00:29:14.600 --> 00:29:19.267
por exemplo, você estaria tão
fresco quanto no 1º caso.

00:29:19.267 --> 00:29:23.833
Mas isso dobraria a quantidade de
felicidade nas pessoas à volta,certo?

00:29:23.833 --> 00:29:25.900
Você ajusta um pouco o
ângulo do ventilador.

00:29:25.900 --> 00:29:27.633
E em vez de 1 unidade de refrigeração,

00:29:27.633 --> 00:29:29.733
as pessoas recebem 2 unidades.

00:29:29.733 --> 00:29:32.033
Ou, em vez de jogar os biscoitos no lixo

00:29:32.033 --> 00:29:33.433
para que as pessoas só obtenham

00:29:33.433 --> 00:29:35.667
1 unidade de felicidade,

00:29:35.667 --> 00:29:37.867
você deixa os outros biscoitos por aí

00:29:37.867 --> 00:29:40.300
e todo mundo recebe 2
unidades de felicidade.

00:29:40.300 --> 00:29:44.533
Neste caso, realizando um ato que
não tem consequências

00:29:44.533 --> 00:29:47.867
diferentes para você, tanto quanto a
felicidade está em causa,

00:29:47.867 --> 00:29:52.433
você duplicou a felicidade de 100
pessoas envolvidas no ato.

00:29:52.433 --> 00:29:55.367
Portanto, a questão é
simplesmente esta.

00:29:55.367 --> 00:29:58.367
Dada a escolha entre o 1º ato,

00:29:58.367 --> 00:30:00.933
que traz um total de 200
unidades de felicidade,

00:30:00.933 --> 00:30:02.200
100 unidades para você

00:30:02.200 --> 00:30:06.833
e 1 unidade para cada 1 das
outras 100 pessoas;

00:30:06.833 --> 00:30:09.133
ou o 2º ato, que traz a mesma
quantidade de felicidade para você,

00:30:09.133 --> 00:30:12.067
mas 200 unidades de
felicidade para os outros

00:30:12.067 --> 00:30:13.467
e, portanto, um total de 300...

00:30:13.467 --> 00:30:18.300
pressione 1 se você acha que
só o 1º ato é moral.

00:30:18.300 --> 00:30:21.100
Esse é aquele em que você
tem 100 unidades

00:30:21.100 --> 00:30:22.833
e todo mundo recebe 1.

00:30:22.833 --> 00:30:25.600
Pressione 2 se você acha que
só o 2º ato é moral,

00:30:25.600 --> 00:30:27.833
aquele em que você ajusta o ventilador,

00:30:27.833 --> 00:30:30.167
ou seja lá o que você fizer,
isso dobra a felicidade

00:30:30.167 --> 00:30:32.700
das pessoas à volta; ou 3º,

00:30:32.700 --> 00:30:36.867
que qualquer um deles é um ato moral.

00:30:36.867 --> 00:30:39.867
OK. E eu vou voltar o nosso contador

00:30:39.867 --> 00:30:43.967
para termos 10 segundos e vermos como é
que vai a sua 1ª experiência

00:30:43.967 --> 00:30:46.767
com o princípio da maior
felicidade de Mill.

00:30:46.767 --> 00:30:50.400
And let's see how the numbers come out.

00:30:50.400 --> 00:30:55.333
OK. Então, muito poucos de vocês
pensam como um ato moral

00:30:55.333 --> 00:30:58.400
aquele em que você obtém 100
unidades de felicidade

00:30:58.400 --> 00:31:01.300
e os outros 100 obtêm 1 unidade.

00:31:01.300 --> 00:31:05.800
Mas vocês estão divididos mais ou
menos iguais nas exigências

00:31:05.800 --> 00:31:11.233
da moralidade em que você
redistribui os recursos

00:31:11.233 --> 00:31:15.867
de tal modo que eles também
vão para os outros.

00:31:15.867 --> 00:31:21.067
Portanto, a maior parte da nossa
discussão dos slides seguintes

00:31:21.067 --> 00:31:28.767
se interessa em quando estes 44% se
deslocarão para outro lugar...

00:31:28.767 --> 00:31:32.767
Mas eu estou interessada em ver
como tudo isso se desenrola.

00:31:32.767 --> 00:31:34.667
OK. Então, esse foi o nosso 1º caso;

00:31:34.667 --> 00:31:37.367
o caso em que, sem qualquer
custo para você,

00:31:37.367 --> 00:31:40.133
é possível trazer
felicidade para os outros.

00:31:40.133 --> 00:31:43.067
Agora, vamos contrastar o 1º caso.

00:31:43.067 --> 00:31:44.367
Você fica com 100
unidades de felicidade;

00:31:44.367 --> 00:31:46.000
outros 100 recebem 1 unidade de cada 1.

00:31:46.000 --> 00:31:48.900
Há um total de 200
unidades, como no 2º caso,

00:31:48.900 --> 00:31:55.333
que vamos chamar de 3º ato, onde, a
fim de redirecionar os bens,

00:31:55.333 --> 00:31:59.433
você faz a sua própria
felicidade cair para 50 unidades.

00:31:59.433 --> 00:32:01.500
Portanto, a fim de
reajustar o ventilador

00:32:01.500 --> 00:32:04.400
de tal forma que as outras pessoas
tenham 2 unidades cada 1,

00:32:04.400 --> 00:32:08.400
você tem uma ligeira redução na
quantidade de utilidade para você.

00:32:08.400 --> 00:32:13.300
Mas isso, em geral, ainda é
benéfico para você.

00:32:13.300 --> 00:32:15.600
Então, no 1º ato você obtém
100 unidade de felicidade,

00:32:15.600 --> 00:32:17.200
as outras pessoas conseguem 1.

00:32:17.200 --> 00:32:20.500
2º ato: você reduziu a sua
felicidade, redirecionou o ventilador,

00:32:20.500 --> 00:32:24.533
comeu menos biscoitos, mas você
distribuiu tudo de tal modo

00:32:24.533 --> 00:32:27.333
que os outros recebem 2 unidades.

00:32:27.333 --> 00:32:30.333
OK. Portanto, a questão é:

00:32:30.333 --> 00:32:32.967
1º ato, onde você fica com 100
unidades e todo mundo com 1;

00:32:32.967 --> 00:32:36.933
2º ato, onde você obtém 50
unidades e todos obtêm 2,

00:32:36.933 --> 00:32:41.500
mas o total é maior; ou nenhum dos 2?

00:32:41.500 --> 00:32:43.733
E, mais uma vez, vamos abrir a votação

00:32:43.733 --> 00:32:45.000
com o temporizador em 10 segundos.

00:32:45.000 --> 00:32:47.800
E vamos ver como saem os números.

00:32:56.033 --> 00:32:57.133
Tudo bem.

00:32:57.133 --> 00:33:01.967
Um pouco de mudança sobre um
ou outro ato ser moral.

00:33:01.967 --> 00:33:06.133
Há mais gente que pensa que
é moralmente exigido

00:33:06.133 --> 00:33:09.600
aumentar a felicidade das
pessoas ao redor

00:33:09.600 --> 00:33:11.967
quando não há nenhum
dano para si mesmo

00:33:11.967 --> 00:33:17.467
que quando existe algum prejuízo.

00:33:17.467 --> 00:33:24.000
Observem que Mill é muito claro
que o moralmente exigido

00:33:24.000 --> 00:33:29.933
é o número 2 aqui, que somente o ato
que traz a maior quantidade

00:33:29.933 --> 00:33:36.700
de utilidade para a comunidade
é o moralmente exigido.

00:33:36.700 --> 00:33:38.567
Vamos para o 3º caso.

00:33:38.567 --> 00:33:40.467
A 1ª versão é a mesma que antes.

00:33:40.467 --> 00:33:41.433
Você fica com 100 unidades.

00:33:41.433 --> 00:33:42.733
Todo mundo fica 1.

00:33:42.733 --> 00:33:46.167
Agora, a fim de fazer o
bem para os outros

00:33:46.167 --> 00:33:50.900
você tem de provar algum
tipo de desconforto.

00:33:50.900 --> 00:33:53.033
Você vira o ventilador
totalmente para o outro lado.

00:33:53.033 --> 00:33:54.267
Mas o resultado

00:33:54.267 --> 00:33:57.367
disso é que os outros 100
recebem 3 unidades cada.

00:33:57.367 --> 00:33:59.833
Portanto, a questão agora é esta.

00:33:59.833 --> 00:34:05.233
É o ato moralmente permitido, ou é
o ato, que é um ato moral,

00:34:05.233 --> 00:34:10.800
aquele que apresentamos inicialmente,

00:34:10.800 --> 00:34:13.067
aquele em que você tem
algum desconforto,

00:34:13.067 --> 00:34:15.233
mas as outras pessoas
experimentam a utilidade?

00:34:15.233 --> 00:34:18.067
Ou eles têm valor igual?

00:34:18.067 --> 00:34:21.233
Observem o total de 200
unidades, 250 unidades.

00:34:21.233 --> 00:34:23.633
O 1º caso, o nosso caso clássico;

00:34:23.633 --> 00:34:27.067
o 2º caso, aquele em que você
sente algum desconforto.

00:34:27.067 --> 00:34:29.200
Mas, em troca do desconforto,

00:34:29.200 --> 00:34:33.700
as outras pessoas, não você,
experimentam algo de bom.

00:34:33.700 --> 00:34:36.100
OK. Vamos acionar o tempo

00:34:36.100 --> 00:34:38.767
e ver como isso sai.

00:34:47.533 --> 00:34:50.900
OK. Portanto, neste caso,

00:34:50.900 --> 00:34:57.700
parece que poucos de vocês
estão ao lado de Mill.

00:34:57.700 --> 00:34:59.867
Uns quantos estão aqui,

00:34:59.867 --> 00:35:02.300
dizendo que o que precisamos
fazer é oferecer

00:35:02.300 --> 00:35:04.033
o maior bem para o maior número.

00:35:04.033 --> 00:35:08.833
E uma parcela
considerável está
pensando

00:35:08.833 --> 00:35:14.200
que a moralidade talvez não
exija tanto sacrifício.

00:35:14.200 --> 00:35:18.900
Seguimos adiante. O próximo caso é
exatamente como o último,

00:35:18.900 --> 00:35:22.667
exceto que é outra pessoa com 50
unidades de desconforto,

00:35:22.667 --> 00:35:25.900
a fim de distribuir 3 unidades de
utilidade para os outros.

00:35:25.900 --> 00:35:26.900
Então aqui está o caso.

00:35:26.900 --> 00:35:29.167
Ou você obtém 100
unidades de felicidade

00:35:29.167 --> 00:35:33.033
e os outros 1 unidade cada
para um total de 200.

00:35:33.033 --> 00:35:34.567
Ou vamos supor que você preserve

00:35:34.567 --> 00:35:36.000
as suas 100 unidades de felicidade aqui.

00:35:36.000 --> 00:35:37.867
Deixamos esta parte fora da equação.

00:35:37.867 --> 00:35:38.800
E a pergunta agora é esta.

00:35:38.800 --> 00:35:41.867
Vamos supor que você esteja
distribuindo recursos

00:35:41.867 --> 00:35:43.867
para a sociedade como um todo.

00:35:43.867 --> 00:35:48.333
Há um caso em que o 1º ato
tem de ser outra pessoa,

00:35:48.333 --> 00:35:51.800
de modo que alguém recebe 100
unidades de felicidade

00:35:51.800 --> 00:35:54.033
e os outros 100 obtêm 1
unidade cada ou alguém

00:35:54.033 --> 00:35:58.200
perde 50 unidades de felicidade,

00:35:58.200 --> 00:36:01.400
mas 100 outros ficam com
3 unidades cada.

00:36:01.400 --> 00:36:05.033
OK, então vamos substituir você no
1º ato por outra pessoa,

00:36:05.033 --> 00:36:10.433
e escolher entre uma distribuição
de recursos na sociedade

00:36:10.433 --> 00:36:14.500
que produz 200 unidades de
bem nesse formato

00:36:14.500 --> 00:36:17.200
ou uma distribuição de
recursos na sociedade

00:36:17.200 --> 00:36:20.633
que produz 250 unidades de
bem neste formato,

00:36:20.633 --> 00:36:23.700
menos 50 unidades de sofrimento

00:36:23.700 --> 00:36:27.000
de 1 para 300 unidades de
benefícios para os outros.

00:36:27.000 --> 00:36:32.233
Qual desses você escolhe para ser
o que exige a moralidade?

00:36:32.233 --> 00:36:37.700
E 5, 4, 3, 2, 1...

00:36:37.700 --> 00:36:40.533
E vamos ver se há alguma mudança em
relação ao caso anterior.

00:36:40.533 --> 00:36:47.333
OK. De repente, temos uma
mudança radical nos gráficos.

00:36:47.333 --> 00:36:52.167
Quase 50% de vocês estão certos

00:36:52.167 --> 00:36:56.900
de que o ato que requer trazer o
sofrimento para 1 pessoa,

00:36:56.900 --> 00:37:02.000
uma redução da utilidade não é
moralmente compulsória.

00:37:02.000 --> 00:37:09.267
Mais tarde, na seção da classe
que estamos tendo agora,

00:37:09.267 --> 00:37:13.533
vamos considerar se existe um ponto fixo

00:37:13.533 --> 00:37:18.033
onde está a linha base e se
essa mudança radical

00:37:18.033 --> 00:37:22.700
que temos quando aumentamos e
diminuímos a utilidade

00:37:22.700 --> 00:37:23.900
na psicologia das pessoas

00:37:23.900 --> 00:37:26.400
sobre o que exige a
moralidade é, de fato,

00:37:26.400 --> 00:37:30.900
captada numa diferença artificial.

00:37:30.900 --> 00:37:35.533
Vamos passar ao nosso último caso.

00:37:35.533 --> 00:37:39.167
Portanto, o nosso último caso

00:37:39.167 --> 00:37:41.367
é o de alguém que recebe 100
unidades de felicidade

00:37:41.367 --> 00:37:43.333
e os outros 100 obtêm 1 unidade cada.

00:37:43.333 --> 00:37:46.100
Então, há 200 unidades de felicidade

00:37:46.100 --> 00:37:51.767
ou um caso onde se retira de alguém
5.000 unidades de felicidade,

00:37:51.767 --> 00:37:55.300
mas outras 100 pessoas obtêm
500 unidades cada,

00:37:55.300 --> 00:37:59.533
de modo que há 45.000
unidades de felicidade

00:37:59.533 --> 00:38:02.300
produzidas pelo desenrolar do 6º ato.

00:38:02.300 --> 00:38:08.567
O caso aqui é uma posição onde
ninguém é prejudicado,

00:38:08.567 --> 00:38:12.367
mas o total de unidades de
felicidade são apenas 200;

00:38:12.367 --> 00:38:16.133
ou 1 pessoa é muito prejudicada,

00:38:16.133 --> 00:38:21.300
mas o total de unidades de
felicidade são 45.000.

00:38:21.300 --> 00:38:28.067
OK. E vamos habilitar a enquete e dispor
de 10, 9, 8, 7, 6 segundos...

00:38:28.067 --> 00:38:32.533
e ver como é que vocês
saem nesta questão.

00:38:35.400 --> 00:38:36.567
Tudo bem.

00:38:36.567 --> 00:38:40.800
Nesta questão, e eu já sei
muitas das respostas de vocês

00:38:40.800 --> 00:38:43.500
para o caso Omelas, em que
isso é modelado,

00:38:43.500 --> 00:38:50.167
parece claro que o sofrimento de 1

00:38:50.167 --> 00:38:54.900
não é algo que a
moralidade exija de nós,

00:38:54.900 --> 00:39:02.067
mesmo que o resultado seja o
aumento da utilidade geral.

00:39:02.067 --> 00:39:07.467
Agora, como vocês sabem, a
história de Omelas fala sobre

00:39:07.467 --> 00:39:10.767
uma sociedade onde há uma
comunidade de pessoas,

00:39:10.767 --> 00:39:14.600
cada uma dos quais tem milhares
de unidades de utilidade.

00:39:14.600 --> 00:39:18.133
Elas são incrivelmente
felizes da forma que vivem.

00:39:18.133 --> 00:39:26.667
Mas essa sociedade existe tal qual só
porque há 1 criança trancada

00:39:26.667 --> 00:39:31.267
cujo sofrimento permite a
alegria da sociedade.

00:39:31.267 --> 00:39:36.567
E como vocês sabem pela história,
quando as crianças atingem

00:39:36.567 --> 00:39:39.867
a idade adulta, elas são levadas para
ver o sofrimento da criança.

00:39:39.867 --> 00:39:43.700
E a maioria delas retorna à comunidade

00:39:43.700 --> 00:39:48.233
ciente disso, moldada por isso,

00:39:48.233 --> 00:39:50.400
mas disposta a tolerar essa situação.

00:39:50.400 --> 00:39:58.467
Um pequeno número, ao ver isso,
abandona a sociedade.

00:39:58.467 --> 00:40:03.400
A questão que eu quero postular
agora, com base nas respostas

00:40:03.400 --> 00:40:07.900
dadas sobre o que é
exigido pela moralidade,

00:40:07.900 --> 00:40:11.367
é sobre algumas coisas que parecem ter

00:40:11.367 --> 00:40:14.900
a mesma estrutura da
história de Omelas.

00:40:14.900 --> 00:40:21.633
Vamos supor que em algum momento
nos últimos 18 anos,

00:40:21.633 --> 00:40:27.367
alguém escondeu de vocês que o prazer

00:40:27.367 --> 00:40:35.800
ao comer carne depende, assim
como a alegria de Omelas,

00:40:35.800 --> 00:40:42.367
do sofrimento de um grande número
de animais não-humanos.

00:40:42.367 --> 00:40:48.367
Imagino que vocês notaram na
semana passada e na anterior,

00:40:48.367 --> 00:40:51.200
quando a neve caía no
campus e havia trilhas

00:40:51.200 --> 00:40:55.300
para vocês chegarem às aulas, que
a possibilidade de você

00:40:55.300 --> 00:41:00.567
andar pelo campus dependia de um
grande número de pessoas,

00:41:00.567 --> 00:41:05.533
cujas vidas já são difíceis,

00:41:05.533 --> 00:41:08.967
levantarem cedo pela manhã

00:41:08.967 --> 00:41:15.333
e abrirem caminho no frio da neve.

00:41:15.333 --> 00:41:20.467
Espero que alguém tenha
escondido de vocês

00:41:20.467 --> 00:41:23.433
que as roupas que vocês vestem

00:41:23.433 --> 00:41:27.033
e da qual vocês desfilam uma
certa quantidade de prazer,

00:41:27.033 --> 00:41:29.833
são, em grande número de casos,

00:41:29.833 --> 00:41:36.300
produzidas a partir de algo muito
próximo com a história de Omelas,

00:41:36.300 --> 00:41:38.500
ou seja, o trabalho infantil.

00:41:38.500 --> 00:41:43.467
Na verdade, eu assumo que a maioria
de vocês estão cientes

00:41:43.467 --> 00:41:45.733
de que a estrutura do mundo
moderno apresenta

00:41:45.733 --> 00:41:51.100
uma semelhança bastante chocante
com a história de Omelas.

00:41:51.100 --> 00:41:56.167
A possibilidade de
prosperidade no 1º Mundo

00:41:56.167 --> 00:42:00.600
é, em muitos aspectos,
uma consequência

00:42:00.600 --> 00:42:06.633
de uma estrutura desigual em
relação ao 3º Mundo.

00:42:06.633 --> 00:42:11.367
Agora, quase todos vocês
deram uma resposta

00:42:11.367 --> 00:42:14.233
que dizia que esse tipo de estrutura

00:42:14.233 --> 00:42:19.667
é, no mínimo, esquematicamente
moralmente aceitável.

00:42:19.667 --> 00:42:24.800
E a questão é o que está
acontecendo aí...

00:42:24.800 --> 00:42:28.700
Na história, Le Guin
sugeriu que vocês,

00:42:28.700 --> 00:42:30.833
como estudantes universitários,

00:42:30.833 --> 00:42:35.800
estão exatamente na idade em
que a relevância disto

00:42:35.800 --> 00:42:37.700
pode afetá-los mais profundamente.

00:42:37.700 --> 00:42:42.333
Ela escreve que, depois de terem sido
expostos a estes tipos de fatos,

00:42:42.333 --> 00:42:45.433
"os jovens muitas vezes voltam para
casa chorando, ou tão furiosos

00:42:45.433 --> 00:42:49.733
que não podem chorar, após verem
a criança cujo sofrimento

00:42:49.733 --> 00:42:52.233
depende o destino da sociedade e
enfrentarem esse terrível paradoxo.

00:42:52.233 --> 00:42:55.067
Eles talvez meditem sobre isso
por semanas ou anos.

00:42:55.067 --> 00:42:59.633
Mas com o tempo começam a compreender

00:42:59.633 --> 00:43:04.267
que ainda que a criança fosse
libertada ela não seria brindada

00:43:04.267 --> 00:43:06.967
com muitas coisas: o prazer vago
do calor e da comida,

00:43:06.967 --> 00:43:08.933
sem dúvida, mas não muito mais".

00:43:08.933 --> 00:43:12.267
Agora, uma das coisas
interessantes sobre a literatura

00:43:12.267 --> 00:43:13.933
em contraste com a filosofia

00:43:13.933 --> 00:43:20.467
é que ela deixa para você
interpretar o que está acontecendo.

00:43:20.467 --> 00:43:25.467
E a questão fundamental da
história de Omelas,

00:43:25.467 --> 00:43:27.933
penso eu, está nesta frase:

00:43:27.933 --> 00:43:31.167
"Mas com o tempo começam a compreender

00:43:31.167 --> 00:43:33.000
que ainda que a criança fosse
libertada ela não seria brindada

00:43:33.000 --> 00:43:34.867
com muitas coisas: o prazer vago
do calor e da comida,

00:43:34.867 --> 00:43:37.067
sem dúvida, mas não muito mais".

00:43:37.067 --> 00:43:44.967
É verdade... ou talvez seja o
tipo de racionalização

00:43:44.967 --> 00:43:51.767
que o reconhecimento do próprio
conforto traz com ele.

00:43:51.767 --> 00:43:56.667
Ela continua, talvez
explicando, talvez protestando,

00:43:56.667 --> 00:43:59.500
para dizer o seguinte: "Está
muito degradada e estúpida

00:43:59.500 --> 00:44:02.367
para desfrutar da alegria real.

00:44:02.367 --> 00:44:05.700
Ela temeu por muito tempo para
estar livre do medo.

00:44:05.700 --> 00:44:07.200
Os seus hábitos são demasiado rudes

00:44:07.200 --> 00:44:09.067
para responderem a um
tratamento humano".

00:44:09.067 --> 00:44:13.433
Aliás, pensem sobre trazer democracias

00:44:13.433 --> 00:44:16.167
a países sem tradição de democracia.

00:44:16.167 --> 00:44:18.800
"Depois de tanto tempo, é
provável que fosse infeliz

00:44:18.800 --> 00:44:22.300
sem paredes para protegê-la, sem a
escuridão para os seus olhos,

00:44:22.300 --> 00:44:24.533
sem excrementos onde se sentar.

00:44:24.533 --> 00:44:27.567
As lágrimas derramadas por
esta injustiça atroz

00:44:27.567 --> 00:44:32.200
se secam quando começam a entender a
terrível justiça da realidade,

00:44:32.200 --> 00:44:35.500
e a aceitam."

00:44:35.500 --> 00:44:40.100
Eu não tenho uma resposta
para qual das 2 leituras

00:44:40.100 --> 00:44:43.667
propostas é a adequada
para o caso Le Guin.

00:44:43.667 --> 00:44:48.433
Ela está argumentando ou
ajudando vocês a reconhecerem

00:44:48.433 --> 00:44:53.067
que aquela sensação inicial
de raiva com o fato

00:44:53.067 --> 00:44:57.800
de que o seu bem-estar depende
do sofrimento dos outros

00:44:57.800 --> 00:45:02.900
é, de fato, uma resposta imatura
para uma estrutura inevitável

00:45:02.900 --> 00:45:05.333
da desigualdade no mundo?

00:45:05.333 --> 00:45:12.833
Ou ela está sugerindo que ao
pensar dessa forma você

00:45:12.833 --> 00:45:19.267
está abandonando a sua única chance
para o comportamento moral,

00:45:19.267 --> 00:45:23.633
que é quando você está
profundamente exposto à injustiça

00:45:23.633 --> 00:45:27.600
e ela golpeia você na forma
de lágrimas ou fúria

00:45:27.600 --> 00:45:33.367
e diz que você está em posição de
trazer a moral para sua vida?

00:45:33.367 --> 00:45:38.633
De qualquer modo, ela
sugere que viver a vida

00:45:38.633 --> 00:45:43.200
com os olhos abertos para o fato
de que o seu bem-estar

00:45:43.200 --> 00:45:48.833
depende do sofrimento dos outros é
moralmente compulsório.

00:45:48.833 --> 00:45:51.233
Ela continua: "Essas
lágrimas e essa fúria,

00:45:51.233 --> 00:45:54.300
a generosidade posta à prova e a
aceitação da impotência,

00:45:54.300 --> 00:45:58.133
são, talvez, a verdadeira fonte do
esplendor de suas vidas.

00:45:58.133 --> 00:46:01.300
Eles sabem que eles, como a
criança, não são livres",

00:46:01.300 --> 00:46:04.733
que vivem num mundo de
interdependência mútua.

00:46:04.733 --> 00:46:07.133
"Eles conhecem a compaixão.

00:46:07.133 --> 00:46:11.133
É por causa da consciência de
sofrimento no mundo – ela escreve –

00:46:11.133 --> 00:46:15.667
... É por causa da criança que eles
tratam tão bem os seus filhos.

00:46:15.667 --> 00:46:17.367
Eles sabem que se a miserável

00:46:17.367 --> 00:46:20.400
não estivesse submergido
na escuridão",

00:46:20.400 --> 00:46:24.000
se nós não tivéssemos os
recursos que permitem

00:46:24.000 --> 00:46:26.167
ao 1º Mundo prosperar como ele faz,

00:46:26.167 --> 00:46:29.667
"o outro, o flautista não
faria música alegre".

00:46:29.667 --> 00:46:31.833
Todas as coisas que nos beneficiamos,

00:46:31.833 --> 00:46:35.533
a grandeza desta
universidade, não estaria
aqui.

00:46:35.533 --> 00:46:37.267
"Não faria música alegre enquanto
os jovens e belos jóqueis

00:46:37.267 --> 00:46:39.600
se alinham para a corrida ao sol

00:46:39.600 --> 00:46:41.800
da primeira manhã de verão."

00:46:41.800 --> 00:46:47.167
Portanto, eu quero deixar isso
como uma das muitas coisas

00:46:47.167 --> 00:46:52.667
que podemos absorver da história de
Omelas e como uma introdução

00:46:52.667 --> 00:46:59.533
para a alegação feita por Mill.

00:46:59.533 --> 00:47:02.200
E na próxima aula vamos abordar,

00:47:02.200 --> 00:47:07.667
no contexto de Kant, algumas
críticas sistemáticas

00:47:07.667 --> 00:47:11.300
para o quadro utilitarista que oferecido

00:47:11.300 --> 00:47:14.600
nos escritos de Bernard Williams
e a nossa alternativa,

00:47:14.600 --> 00:47:17.900
oferecido nos escritos de Immanuel Kant.

00:47:17.900 --> 00:47:18.300
Portanto, vejo vocês na terça.

