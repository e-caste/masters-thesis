WEBVTT
Kind: captions
Language: en

00:00:00.510 --> 00:00:05.360
PROFESSOR: So we left ourselves
at the end of the

00:00:05.360 --> 00:00:10.110
last lecture in a somewhat
perplexing situation.

00:00:10.110 --> 00:00:15.780
We had thought through the
particular scenarios that Judy

00:00:15.780 --> 00:00:19.310
Thomson presents us with
in her trolley paper.

00:00:19.310 --> 00:00:21.810
And we had discovered the
following apparently

00:00:21.810 --> 00:00:26.640
perplexing feature about
the class's responses.

00:00:26.640 --> 00:00:31.650
In what's called the Classic
Bystander case--

00:00:31.650 --> 00:00:35.820
the case where there's a
bystander standing next to a

00:00:35.820 --> 00:00:39.860
trolley that's hurtling down
a track about to hit five

00:00:39.860 --> 00:00:43.780
people, and the bystander could
if he chose turn the

00:00:43.780 --> 00:00:45.810
trolley onto a track where
the trolley will

00:00:45.810 --> 00:00:47.480
only hit one person--

00:00:47.480 --> 00:00:50.510
your responses were
as follows.

00:00:50.510 --> 00:00:54.820
Roughly 15% of you thought he
was morally required to turn

00:00:54.820 --> 00:00:57.480
the trolley from the
five to the one.

00:00:57.480 --> 00:01:01.300
70% of you thought he was
morally permitted to do so.

00:01:01.300 --> 00:01:05.130
And only 15% of you thought that
it's a morally prohibited

00:01:05.130 --> 00:01:07.790
act for him to turn
the trolley from

00:01:07.790 --> 00:01:10.600
the five to the one.

00:01:10.600 --> 00:01:14.700
By contrast, we ended
class with Thomson's

00:01:14.700 --> 00:01:17.030
famous Fat Man case.

00:01:17.030 --> 00:01:20.360
This is a case where our
bystander is standing next to

00:01:20.360 --> 00:01:23.720
the trolley as before, the
trolley is hurtling down the

00:01:23.720 --> 00:01:27.940
track about to kill the five,
and the bystander has

00:01:27.940 --> 00:01:31.590
available to him a means for
stopping the trolley.

00:01:31.590 --> 00:01:34.260
In this case, rather than
turning it onto a different

00:01:34.260 --> 00:01:37.920
track, the means he has
available to him is to push a

00:01:37.920 --> 00:01:41.480
fat man off a bridge,
thereby stopping the

00:01:41.480 --> 00:01:43.340
trolley in its tracks.

00:01:43.340 --> 00:01:47.810
And your responses in this
case exhibited a highly

00:01:47.810 --> 00:01:51.680
different distribution than they
did in the first case.

00:01:51.680 --> 00:01:54.950
Whereas in the first case,
15% of you thought it was

00:01:54.950 --> 00:01:59.520
prohibited to stop the trolley
from hitting the five by

00:01:59.520 --> 00:02:02.960
killing the one or by causing
the trolley to kill the one,

00:02:02.960 --> 00:02:08.580
in the Fat Man case,
78% of you--

00:02:08.580 --> 00:02:10.940
4/5 of the class--

00:02:10.940 --> 00:02:14.420
thought that the act of turning
or of stopping the

00:02:14.420 --> 00:02:19.820
trolley by putting in its way
another person was morally

00:02:19.820 --> 00:02:21.100
prohibited.

00:02:21.100 --> 00:02:24.220
Now the puzzle that this raises,
as you know from the

00:02:24.220 --> 00:02:27.860
end of last class, is that
it seems that in both the

00:02:27.860 --> 00:02:31.100
Bystander case where one--

00:02:31.100 --> 00:02:31.820
sorry.

00:02:31.820 --> 00:02:32.740
The puzzle is this.

00:02:32.740 --> 00:02:37.920
In the Bystander case, it seems
clear to most people

00:02:37.920 --> 00:02:41.980
that killing one person
is bad, but that

00:02:41.980 --> 00:02:45.940
letting five die is worse.

00:02:45.940 --> 00:02:49.870
Whereas in the Fat Man
case, it seems

00:02:49.870 --> 00:02:52.800
to be just the inverse.

00:02:52.800 --> 00:02:57.860
So what Thomson asks us at the
end of that paper, having run

00:02:57.860 --> 00:03:00.330
through a number of cases,
including some that I didn't

00:03:00.330 --> 00:03:03.900
go over in this summary right
now, is what could possibly

00:03:03.900 --> 00:03:07.590
explain the difference in our
reactions to the Bystander

00:03:07.590 --> 00:03:09.400
case and the Fat Man case?

00:03:09.400 --> 00:03:13.880
And what she suggests is that
whereas utility prohibits

00:03:13.880 --> 00:03:15.940
letting the five die--

00:03:15.940 --> 00:03:20.260
that is, it would be better for
the number of lives saved

00:03:20.260 --> 00:03:23.930
if we saved five than
if we saved one--

00:03:23.930 --> 00:03:29.720
the notion of a right is what
prohibits killing the one in

00:03:29.720 --> 00:03:31.350
the Fat Man case.

00:03:31.350 --> 00:03:34.890
So what has to happen, says
Thomson in the Fat Man case,

00:03:34.890 --> 00:03:39.120
is that you interfere with his
right not to have his person

00:03:39.120 --> 00:03:43.220
used as a means to the end
of saving another.

00:03:43.220 --> 00:03:48.920
Whereas in the case of
Bystander, there's no right

00:03:48.920 --> 00:03:51.300
that is infringed upon.

00:03:51.300 --> 00:03:57.019
And, suggests Thomson, rights
trump utilities.

00:03:57.019 --> 00:04:02.120
So what the right prohibits
is what is mandated

00:04:02.120 --> 00:04:04.480
in the Fat Man case.

00:04:04.480 --> 00:04:10.600
So that's where we were at the
end of class last time.

00:04:10.600 --> 00:04:14.180
And the solution that Thomson
proposed there is what we

00:04:14.180 --> 00:04:19.100
might call a classic solution
to trolley-type dilemmas.

00:04:19.100 --> 00:04:23.000
It's a solution that assumes
that Fat Man case and the

00:04:23.000 --> 00:04:27.270
Bystander case carry different
moral mandates, and that the

00:04:27.270 --> 00:04:30.550
reason they carry those
different mandates is because

00:04:30.550 --> 00:04:35.940
of a deep moral difference
that those cases encode.

00:04:35.940 --> 00:04:39.370
So the difference between our
response to Fat Man and our

00:04:39.370 --> 00:04:43.870
response to Bystander, says
Thomson in that 1985 article,

00:04:43.870 --> 00:04:46.030
is one that we should respect.

00:04:46.030 --> 00:04:49.080
And the reason we should respect
that difference, she

00:04:49.080 --> 00:04:52.300
contends, is that that
difference is tracking a

00:04:52.300 --> 00:04:57.277
profound moral difference
between them, namely that in

00:04:57.277 --> 00:05:00.730
the case of Fat Man but not in
the case of Bystander, the

00:05:00.730 --> 00:05:05.240
rights of an individual
are violated.

00:05:05.240 --> 00:05:09.600
What I want to do in class today
is to go through with

00:05:09.600 --> 00:05:16.000
you three non-classic responses
to the trolley case.

00:05:16.000 --> 00:05:19.420
And I'll be giving you the
chance to use your clickers in

00:05:19.420 --> 00:05:22.160
the first and third of these.

00:05:22.160 --> 00:05:25.200
So if you get your clickers
out, we'll be prepared for

00:05:25.200 --> 00:05:26.840
what's going to happen
in a few minutes.

00:05:26.840 --> 00:05:29.290
So what are the three
classic responses?

00:05:29.290 --> 00:05:32.170
Remember, in a classic response,
the claim is that

00:05:32.170 --> 00:05:35.130
Fat Man and Bystander carry
different moral mandates, and

00:05:35.130 --> 00:05:37.900
that that difference can be
traced to a deeper, morally

00:05:37.900 --> 00:05:40.270
relevant difference
between them.

00:05:40.270 --> 00:05:44.490
So two of the responses that
we'll consider today are ones

00:05:44.490 --> 00:05:48.430
that suggest that Fat Man and
Bystander in fact don't carry

00:05:48.430 --> 00:05:50.740
different moral mandates.

00:05:50.740 --> 00:05:54.380
So the first example that I'm
going to run through with you

00:05:54.380 --> 00:06:00.590
is Judy Thomson's rethinking
of trolley cases in a 2008

00:06:00.590 --> 00:06:05.960
paper in which she ends up
assimilating the Bystander

00:06:05.960 --> 00:06:08.700
case to the Fat Man case.

00:06:08.700 --> 00:06:14.280
And suggesting that in neither
of the cases is it permissible

00:06:14.280 --> 00:06:17.960
to kill the one to
save the five.

00:06:17.960 --> 00:06:23.170
The second view that we'll
consider is Josh Greene's view

00:06:23.170 --> 00:06:27.220
that the right thing to do in
the Fat Man case is the same

00:06:27.220 --> 00:06:31.050
thing as the right thing to do
in the Bystander case, namely

00:06:31.050 --> 00:06:35.710
that in both cases, the right
thing to do is to stop the

00:06:35.710 --> 00:06:39.030
trolley from hitting the
five and cause it

00:06:39.030 --> 00:06:41.430
instead to kill the one.

00:06:41.430 --> 00:06:42.900
And finally--

00:06:42.900 --> 00:06:46.080
I'm shoe-horning this a bit,
because in truth, Sunstein is

00:06:46.080 --> 00:06:48.890
a little closer to Greene
than he is to Thomson.

00:06:48.890 --> 00:06:55.360
But we might use his thinking to
maintain the position that

00:06:55.360 --> 00:07:01.900
though our responses to the
cases differ, the cases are in

00:07:01.900 --> 00:07:04.400
some more fundamental
sense the same.

00:07:04.400 --> 00:07:07.160
And what Sunstein is going to
suggest we need to do is to

00:07:07.160 --> 00:07:08.960
push the fat man.

00:07:08.960 --> 00:07:12.050
So what we have are
three views here.

00:07:12.050 --> 00:07:14.480
Thomson's saying the cases come
together, and they come

00:07:14.480 --> 00:07:17.100
together in telling us
never to kill the

00:07:17.100 --> 00:07:18.780
one to save the five.

00:07:18.780 --> 00:07:21.120
Greene's saying the cases come
together, and they come

00:07:21.120 --> 00:07:25.220
together in telling us always
kill the one to save the five.

00:07:25.220 --> 00:07:29.490
And then, perhaps, Sunstein's
view telling us that the cases

00:07:29.490 --> 00:07:31.450
come apart.

00:07:31.450 --> 00:07:34.320
But these three non-classic
responses are interesting not

00:07:34.320 --> 00:07:37.700
just for the difference in their
content, I think they're

00:07:37.700 --> 00:07:42.470
interesting for the purposes of
this class because each of

00:07:42.470 --> 00:07:46.000
them makes use of a slightly
different kind of

00:07:46.000 --> 00:07:48.560
argumentative methodology.

00:07:48.560 --> 00:07:52.340
And there's no reason that the
methodologies and the answers

00:07:52.340 --> 00:07:55.490
needed to line up in the
way that they did.

00:07:55.490 --> 00:07:58.590
So one of the things that I want
you to think about as we

00:07:58.590 --> 00:08:04.280
go through today's lecture is
what use might be made of each

00:08:04.280 --> 00:08:06.710
of these methodologies
to make one of

00:08:06.710 --> 00:08:08.510
the alternative arguments.

00:08:08.510 --> 00:08:13.280
So Thomson's contention that
in the Bystander case we

00:08:13.280 --> 00:08:16.990
shouldn't turn the trolley is
one that she makes on the

00:08:16.990 --> 00:08:19.179
basis of inviting you--

00:08:19.179 --> 00:08:20.850
as I will do in a moment--

00:08:20.850 --> 00:08:25.410
to consider additional
hypothetical cases, and then

00:08:25.410 --> 00:08:34.290
asking you to be consistent
about your responses to cases

00:08:34.290 --> 00:08:36.960
that fail to differ
in moral ways.

00:08:36.960 --> 00:08:41.070
So Thomson's methodology is the
same as it was in her 1985

00:08:41.070 --> 00:08:45.270
paper, there's just a new case
that she's thought about.

00:08:45.270 --> 00:08:50.030
Sunstein's methodology is to
canvas a large array of

00:08:50.030 --> 00:08:53.870
literature in the heuristics and
biases tradition, and to

00:08:53.870 --> 00:08:58.640
suggest that moral reasoning is
no different than any other

00:08:58.640 --> 00:09:00.710
sort of reasoning.

00:09:00.710 --> 00:09:05.790
And Josh Greene's method is
of course to make use of

00:09:05.790 --> 00:09:10.970
neuroimaging results and on that
basis to argue in favor

00:09:10.970 --> 00:09:16.760
of his view that what is morally
mandated of us is a

00:09:16.760 --> 00:09:19.770
certain kind of utilitarian
stance.

00:09:19.770 --> 00:09:21.540
So let's start--

00:09:21.540 --> 00:09:23.870
and here you'll need
your clickers--

00:09:23.870 --> 00:09:27.730
with the additional hypothetical
cases that

00:09:27.730 --> 00:09:32.410
convinced Judy Thomson, and may
convince you, that it's

00:09:32.410 --> 00:09:36.650
not OK to turn the trolley
in Bystander.

00:09:36.650 --> 00:09:40.330
So the case that Thomson
presents us with is a case

00:09:40.330 --> 00:09:44.290
that we'll call Bystander's
Three Options.

00:09:44.290 --> 00:09:48.680
So here's poor Jim, deeply
regretting that he ever

00:09:48.680 --> 00:09:53.590
enrolled in this class, standing
by the trolley in a

00:09:53.590 --> 00:09:56.720
usual Bystander dilemma where
the trolley is about to hit

00:09:56.720 --> 00:09:59.350
the five and Jim has the
possibility of deflecting it

00:09:59.350 --> 00:10:01.220
to hit the one.

00:10:01.220 --> 00:10:06.480
But because Jim lives his life
in Judy Thomson's thought

00:10:06.480 --> 00:10:12.020
experiment, she has, in rather
dastardly fashion, introduced

00:10:12.020 --> 00:10:16.810
a third track at the end of
which, rather unfortunately

00:10:16.810 --> 00:10:22.790
for Jim, Jim is standing.

00:10:22.790 --> 00:10:27.210
Now here's Jim's three-way
dilemma.

00:10:27.210 --> 00:10:36.820
One, allow the trolley to
continue on its original path

00:10:36.820 --> 00:10:40.330
killing the five.

00:10:40.330 --> 00:10:44.840
Option two, deflect the
trolley so that it

00:10:44.840 --> 00:10:46.620
hits the other guy.

00:10:49.120 --> 00:10:51.990
Option three, deflect
the trolley from

00:10:51.990 --> 00:10:53.990
the five to the one--

00:10:53.990 --> 00:10:57.940
oh, except the one is Jim.

00:10:57.940 --> 00:11:00.070
Question.

00:11:00.070 --> 00:11:07.010
In three-way Bystander, if Jim
decides to turn the trolley--

00:11:07.010 --> 00:11:08.510
so we're ignoring the
case where he

00:11:08.510 --> 00:11:11.150
lets it hit the five--

00:11:11.150 --> 00:11:15.010
he's made the decision to turn
the trolley, the question is

00:11:15.010 --> 00:11:16.500
the following.

00:11:16.500 --> 00:11:21.880
Is it morally required for him
to turn the trolley onto the

00:11:21.880 --> 00:11:25.430
track where it hits the other
guy instead of himself?

00:11:25.430 --> 00:11:29.820
Is it morally permitted, but not
morally required, for him

00:11:29.820 --> 00:11:33.140
to turn the trolley onto the
track where it hits the other

00:11:33.140 --> 00:11:35.450
guy instead of himself?

00:11:35.450 --> 00:11:40.220
Or is it morally prohibited for
him to turn the trolley

00:11:40.220 --> 00:11:43.570
onto the track instead
of to himself?

00:11:43.570 --> 00:11:47.190
So we're assuming that Jim has
made the decision to turn the

00:11:47.190 --> 00:11:48.080
trolley from the five.

00:11:48.080 --> 00:11:51.740
After all, it's a straight
Bystander case.

00:11:51.740 --> 00:11:53.310
If he doesn't turn
the trolley, it's

00:11:53.310 --> 00:11:54.810
going to hit the five.

00:11:54.810 --> 00:11:59.280
78% of you have previously told
me that what one ought to

00:11:59.280 --> 00:12:01.650
do, or at least what one is
permitted to do in this case,

00:12:01.650 --> 00:12:02.830
is to turn the trolley.

00:12:02.830 --> 00:12:06.828
How come there's no responses
coming, guys?

00:12:06.828 --> 00:12:08.110
STUDENTS: [INTERPOSING VOICES].

00:12:08.110 --> 00:12:09.360
PROFESSOR: It's not working?

00:12:09.360 --> 00:12:10.100
Oh, my goodness.

00:12:10.100 --> 00:12:10.410
All right.

00:12:10.410 --> 00:12:12.700
So why is it not open for you?

00:12:12.700 --> 00:12:13.420
Let's try.

00:12:13.420 --> 00:12:14.663
Is it open now?

00:12:14.663 --> 00:12:15.610
STUDENT: No.

00:12:15.610 --> 00:12:16.710
PROFESSOR: Tragic.

00:12:16.710 --> 00:12:19.280
This is really, really,
very, very horrible.

00:12:19.280 --> 00:12:20.620
That did not work.

00:12:20.620 --> 00:12:26.290
OK, the whole lecture today
depends upon these working.

00:12:26.290 --> 00:12:29.240
So let's try this again.

00:12:29.240 --> 00:12:34.710
And tell me now whether
this works.

00:12:34.710 --> 00:12:36.130
Is it working?

00:12:36.130 --> 00:12:37.740
OK.

00:12:37.740 --> 00:12:38.990
Is it working now?

00:12:41.810 --> 00:12:42.460
No?

00:12:42.460 --> 00:12:43.710
Still no?

00:12:45.890 --> 00:12:47.140
No?

00:12:49.240 --> 00:12:50.660
All right.

00:12:50.660 --> 00:12:51.790
Hm.

00:12:51.790 --> 00:12:54.550
We're going to have to run--

00:12:54.550 --> 00:12:56.900
I think there's nothing
I can do.

00:12:56.900 --> 00:13:00.330
I'm going to try resetting
once more

00:13:00.330 --> 00:13:01.340
and see if that works.

00:13:01.340 --> 00:13:03.190
And I'm going to try
removing and then

00:13:03.190 --> 00:13:06.340
returning this receiver.

00:13:06.340 --> 00:13:07.620
And then--

00:13:07.620 --> 00:13:10.210
if not-- we're going to do the
old-fashioned show of hands

00:13:10.210 --> 00:13:13.060
and all my beautifully
constructed slides will turn

00:13:13.060 --> 00:13:16.500
out not to be useful, but
that's all right.

00:13:16.500 --> 00:13:18.870
Worse things have happened
in the world.

00:13:18.870 --> 00:13:19.330
All right.

00:13:19.330 --> 00:13:20.980
Try it again.

00:13:20.980 --> 00:13:21.900
Yay!

00:13:21.900 --> 00:13:22.370
Awesome.

00:13:22.370 --> 00:13:23.970
I have no idea what I changed.

00:13:23.970 --> 00:13:24.530
OK.

00:13:24.530 --> 00:13:26.050
So, answering this question.

00:13:26.050 --> 00:13:26.290
Wow.

00:13:26.290 --> 00:13:27.400
There's 64 of you.

00:13:27.400 --> 00:13:28.550
There's 71 of you.

00:13:28.550 --> 00:13:29.630
We'll do the countdown.

00:13:29.630 --> 00:13:30.820
10, 9, 8--

00:13:30.820 --> 00:13:35.050
so let's see how the numbers
come out in--

00:13:35.050 --> 00:13:38.050
4, 3, 2, 1 seconds.

00:13:38.050 --> 00:13:39.580
Oh, and it's so exciting.

00:13:39.580 --> 00:13:41.270
Especially because we
had to suffer first.

00:13:41.270 --> 00:13:42.900
The contrast. OK.

00:13:42.900 --> 00:13:48.150
So in this case, 6% of you think
it's morally required

00:13:48.150 --> 00:13:51.280
for Jim to turn the trolley
onto the other man.

00:13:51.280 --> 00:13:56.140
But you were the 6% who continue
to be outliers, or

00:13:56.140 --> 00:13:57.090
perhaps you're different
people.

00:13:57.090 --> 00:13:59.800
But let's look at
what's going on.

00:13:59.800 --> 00:14:03.610
61% of you think it's morally
permitted for him to turn the

00:14:03.610 --> 00:14:06.710
trolley onto the other man.

00:14:06.710 --> 00:14:10.490
And 32% of you think it's
morally prohibited for him to

00:14:10.490 --> 00:14:12.560
turn the trolley onto
the other man.

00:14:12.560 --> 00:14:17.660
Now interestingly, Judy Thomson
expects that more of

00:14:17.660 --> 00:14:21.250
you will fall into
this category.

00:14:21.250 --> 00:14:23.950
So it's an interesting question
for us to think about

00:14:23.950 --> 00:14:28.660
as a class why it is that she
is under the impression that

00:14:28.660 --> 00:14:34.230
it's rather surprising that this
is the response that you

00:14:34.230 --> 00:14:38.930
gave. But in any case, let's
move to a second contrast case

00:14:38.930 --> 00:14:40.570
and see how this goes.

00:14:40.570 --> 00:14:41.540
OK.

00:14:41.540 --> 00:14:46.340
Suppose now that we have
only a two-way case.

00:14:46.340 --> 00:14:52.460
In the two-way case, bystander
Jim has only two options.

00:14:52.460 --> 00:14:58.570
Either the trolley is going to
hit the five or he can deflect

00:14:58.570 --> 00:15:01.510
the trolley in such a way
that it hits him.

00:15:01.510 --> 00:15:04.200
I want to go back for a second
and just get the numbers that

00:15:04.200 --> 00:15:06.540
I got on the last slide, because
I forgot to record

00:15:06.540 --> 00:15:11.080
those for myself, thrown off
as I was by our situation.

00:15:11.080 --> 00:15:12.810
So let me just record these.

00:15:12.810 --> 00:15:15.200
6%, 61%, 32%.

00:15:15.200 --> 00:15:15.690
OK.

00:15:15.690 --> 00:15:17.400
So moving on to the new case.

00:15:17.400 --> 00:15:19.500
It's a two-way trolley, and
the question is this.

00:15:19.500 --> 00:15:24.020
In Bystander's Two Options, is
it morally required for him to

00:15:24.020 --> 00:15:27.190
let the trolley hit the five
instead of himself, is it

00:15:27.190 --> 00:15:30.490
morally permitted for him to let
the trolley hit the five

00:15:30.490 --> 00:15:34.910
instead of himself, or is it
morally prohibited for him to

00:15:34.910 --> 00:15:38.030
let the trolley hit the five
instead of himself?

00:15:38.030 --> 00:15:38.810
OK?

00:15:38.810 --> 00:15:40.190
So let's think through
that case.

00:15:40.190 --> 00:15:41.640
So remember, it's
a two-way case.

00:15:41.640 --> 00:15:43.590
The trolley's heading down
towards the five.

00:15:43.590 --> 00:15:47.050
And the question is: is it
required, permitted, or

00:15:47.050 --> 00:15:49.810
prohibited for him to turn
the trolley from

00:15:49.810 --> 00:15:53.460
the five to hit himself?

00:15:53.460 --> 00:15:54.710
OK.

00:15:54.710 --> 00:15:57.200
And let's see how the numbers
come out here.

00:15:57.200 --> 00:16:00.290
We've got roughly 10 seconds
to find out whether your

00:16:00.290 --> 00:16:04.470
distribution is going to be
similar or different here.

00:16:07.180 --> 00:16:13.540
OK, so here's how the numbers
come out: 8%, 70%, 22%.

00:16:13.540 --> 00:16:20.160
Now, the case with which we want
to contrast this is the

00:16:20.160 --> 00:16:22.480
classic Bystander case.

00:16:22.480 --> 00:16:26.690
In the classic Bystander case,
more of you thought he was

00:16:26.690 --> 00:16:30.670
morally required to turn the
trolley than you think in this

00:16:30.670 --> 00:16:32.490
particular case.

00:16:32.490 --> 00:16:36.430
In the classic Bystander case,
interestingly, you had roughly

00:16:36.430 --> 00:16:40.710
the same view about whether
it was morally permitted.

00:16:40.710 --> 00:16:45.540
And more of you think it's
morally prohibited for him to

00:16:45.540 --> 00:16:48.960
let the trolley hit the five
instead of himself.

00:16:48.960 --> 00:16:52.369
So the interesting difference
is this one here.

00:16:52.369 --> 00:16:56.390
You took a different attitude
with respect to whether it's

00:16:56.390 --> 00:16:59.250
morally required for him to
turn the trolley when the

00:16:59.250 --> 00:17:02.970
person it's going to hit is
himself than when the person

00:17:02.970 --> 00:17:06.180
it's going to hit is
another person.

00:17:06.180 --> 00:17:10.850
So let's go back and do just a
classic Bystander case and see

00:17:10.850 --> 00:17:14.410
whether, as a result of having
thought through this case, if

00:17:14.410 --> 00:17:16.840
there's any change in
your intuitions.

00:17:16.840 --> 00:17:19.870
So this is just the standard
Bystander case that you've

00:17:19.870 --> 00:17:21.130
seen before.

00:17:21.130 --> 00:17:25.640
In the classic two-way Bystander
case, do you think

00:17:25.640 --> 00:17:29.020
it's morally mandatory, morally
permitted, or morally

00:17:29.020 --> 00:17:32.020
prohibited for Jim to
turn the trolley?

00:17:44.530 --> 00:17:48.670
So we're 3, 2, 1.

00:17:48.670 --> 00:17:51.140
And let's see how the
numbers come out.

00:17:51.140 --> 00:17:54.240
20%, 65%, 15%.

00:17:54.240 --> 00:17:59.340
So as a result of having thought
about the first-person

00:17:59.340 --> 00:18:01.370
analogue, some--

00:18:01.370 --> 00:18:03.710
though many fewer than I
would have thought--

00:18:03.710 --> 00:18:07.970
some of you changed your view.

00:18:07.970 --> 00:18:11.670
Whereas originally, 15% of you
thought it was morally

00:18:11.670 --> 00:18:13.710
mandatory to turn
the trolley--

00:18:13.710 --> 00:18:16.780
oh, you've changed your view
exactly the direction against

00:18:16.780 --> 00:18:17.990
the one I would have
predicted.

00:18:17.990 --> 00:18:19.450
So here's a mystery.

00:18:19.450 --> 00:18:21.940
Here's a little bit of
experimental philosophy done

00:18:21.940 --> 00:18:23.090
in our classroom.

00:18:23.090 --> 00:18:27.010
What Judy Thomson was
predicting-- and we can talk

00:18:27.010 --> 00:18:29.170
in sections about why this
didn't happen --

00:18:29.170 --> 00:18:32.700
what Judy Thomson was predicting
is that you would

00:18:32.700 --> 00:18:34.600
react as follows.

00:18:34.600 --> 00:18:37.810
If it's not morally mandatory
for me to turn the trolley

00:18:37.810 --> 00:18:42.490
onto myself, then it's not
morally mandatory, indeed not

00:18:42.490 --> 00:18:46.140
morally permitted, for me
to turn the trolley

00:18:46.140 --> 00:18:49.470
onto another person.

00:18:49.470 --> 00:18:55.400
If I'm not willing to take a
hit myself in that case, I

00:18:55.400 --> 00:18:59.645
shouldn't be deciding on behalf
of another person that

00:18:59.645 --> 00:19:01.380
he take that hit.

00:19:01.380 --> 00:19:06.000
So I want you to think about
what it is in Thomson's

00:19:06.000 --> 00:19:10.660
thinking about this case that
made it feel to her so obvious

00:19:10.660 --> 00:19:13.130
that as the result of
considering the first-person

00:19:13.130 --> 00:19:15.920
case, people would be inclined
to rethink the

00:19:15.920 --> 00:19:17.340
third-person case.

00:19:17.340 --> 00:19:21.600
And I have to say, I myself in
reading Thomson's 2008 paper

00:19:21.600 --> 00:19:24.190
am very easily brought
into the mindset

00:19:24.190 --> 00:19:25.420
she describes there.

00:19:25.420 --> 00:19:29.220
So I find it surprising and
extremely interesting to see

00:19:29.220 --> 00:19:32.060
that that isn't what happened
in this context.

00:19:32.060 --> 00:19:37.380
Let's assume, however, that at
least for some of you, the

00:19:37.380 --> 00:19:40.060
intuition that you came to
have as the result of

00:19:40.060 --> 00:19:42.240
considering this case
was something

00:19:42.240 --> 00:19:44.350
like Thomson's intuition.

00:19:44.350 --> 00:19:48.420
So that whereas on the old view
in Bystander case, you

00:19:48.420 --> 00:19:51.830
thought the right thing was to
kill the one rather than to

00:19:51.830 --> 00:19:54.920
let the five die-- that is, in
the standard switch case, and

00:19:54.920 --> 00:19:57.530
this is in fact what most of
you think-- in the standard

00:19:57.530 --> 00:20:01.080
Bystander case, most of you
think that the right thing to

00:20:01.080 --> 00:20:05.690
do is to kill the one rather
than to let the five die.

00:20:05.690 --> 00:20:10.440
What Thomson says is that
in thinking through the

00:20:10.440 --> 00:20:14.870
first-person case, you ought to
realize that Bystander is a

00:20:14.870 --> 00:20:19.310
lot more like Fat Man than
you initially thought.

00:20:19.310 --> 00:20:24.330
To the extent that you reject
that intuition of Thomson's,

00:20:24.330 --> 00:20:28.100
you're in a position to
disagree with her.

00:20:28.100 --> 00:20:32.020
So let's move to the view with
which I take it most of you

00:20:32.020 --> 00:20:34.440
are going to end up agreeing,
since this is exactly the

00:20:34.440 --> 00:20:37.870
opposite of Thomson's, namely
Greene's argument that the

00:20:37.870 --> 00:20:39.570
assimilation ought to
go the other way.

00:20:39.570 --> 00:20:43.200
So just to remind you where we
are in the picture, the puzzle

00:20:43.200 --> 00:20:45.880
with which we began is that
people were giving a different

00:20:45.880 --> 00:20:50.980
response in Bystander than in
Fat Man, and Thomson tried to

00:20:50.980 --> 00:20:54.610
get rid of the problem by
causing you to assimilate

00:20:54.610 --> 00:20:57.380
Bystander to Fat Man.

00:20:57.380 --> 00:21:01.430
I was unable through Thomson's
cases to get you to shift your

00:21:01.430 --> 00:21:03.670
intuitions in that case.

00:21:03.670 --> 00:21:06.580
So we're stuck with a
residual difference

00:21:06.580 --> 00:21:08.310
between your responses.

00:21:08.310 --> 00:21:12.240
Most of you think it's OK to
turn the trolley in Bystander

00:21:12.240 --> 00:21:15.780
regardless of whether you
wouldn't do it on yourself,

00:21:15.780 --> 00:21:18.700
but that it's not OK to
push the man on the

00:21:18.700 --> 00:21:20.870
bridge in Fat Man.

00:21:20.870 --> 00:21:25.640
So Greene's going to give us a
second way of thinking about

00:21:25.640 --> 00:21:29.010
how it is that we might bring
those responses together.

00:21:29.010 --> 00:21:32.470
And his argument runs
as follows.

00:21:32.470 --> 00:21:37.450
In general, we're not in
a very good position to

00:21:37.450 --> 00:21:42.870
determine what really underlies
our reasoning.

00:21:42.870 --> 00:21:46.270
There's an entire tradition in
social psychology that I

00:21:46.270 --> 00:21:50.070
talked about in one of the early
lectures that aims to

00:21:50.070 --> 00:21:54.690
show that a lot of what people
engage in when they make

00:21:54.690 --> 00:22:00.490
decisions is post-facto
rationalization of intuitive

00:22:00.490 --> 00:22:04.790
responses that they had which
weren't in fact tracking what

00:22:04.790 --> 00:22:06.840
they would say are the relevant

00:22:06.840 --> 00:22:09.300
features of the situation.

00:22:09.300 --> 00:22:13.840
So famously, people are more
likely to choose an object

00:22:13.840 --> 00:22:18.200
that lies on the left-hand side
of a visual array than an

00:22:18.200 --> 00:22:20.070
object that lies on the
right-hand side

00:22:20.070 --> 00:22:21.610
of that visual array.

00:22:21.610 --> 00:22:25.840
But in making the choice, they
don't provide as their reason

00:22:25.840 --> 00:22:29.370
the location of the object, they
provide as their reason

00:22:29.370 --> 00:22:31.660
some other feature
of the object.

00:22:31.660 --> 00:22:35.230
And when we looked at, in
the second lecture, the

00:22:35.230 --> 00:22:38.970
confabulation results, whereby
subjects who had undergone

00:22:38.970 --> 00:22:39.810
commissurotomy--

00:22:39.810 --> 00:22:42.890
that is, whose corpus callosum
had been severed--

00:22:42.890 --> 00:22:45.710
so that the right and left
hemispheres of their brains

00:22:45.710 --> 00:22:49.030
weren't in communication, we
discovered that when they

00:22:49.030 --> 00:22:54.140
performed an act that was based
on stimulation of the

00:22:54.140 --> 00:22:58.600
right brain, the left brain,
which is the linguistic part,

00:22:58.600 --> 00:23:02.140
came up with an explanation for
what they were doing that

00:23:02.140 --> 00:23:07.020
was obviously not the real
source of their behavior.

00:23:07.020 --> 00:23:11.400
So there are many cases, Greene
points out, where our

00:23:11.400 --> 00:23:14.120
motivations our opaque us.

00:23:14.120 --> 00:23:16.610
Where we think we're responding
to one thing, but

00:23:16.610 --> 00:23:19.980
in fact we're responding
to something else.

00:23:19.980 --> 00:23:25.900
One of those cases, says Greene,
is the difference in

00:23:25.900 --> 00:23:29.390
our response to the Fat
Man case and to

00:23:29.390 --> 00:23:31.420
the Bystander case.

00:23:31.420 --> 00:23:35.050
So what happens in the
Bystander case--

00:23:35.050 --> 00:23:37.730
where we're trying to decide
whether to shift the trolley

00:23:37.730 --> 00:23:39.820
from the five to the one--

00:23:39.820 --> 00:23:41.570
is that our rational

00:23:41.570 --> 00:23:45.350
processing system gets activated.

00:23:45.350 --> 00:23:48.700
Whereas what happens,
hypothesizes Greene-- and

00:23:48.700 --> 00:23:51.800
we'll give some evidence in a
minute-- what happens in the

00:23:51.800 --> 00:23:57.296
Fat Man case is that our
emotional processing system

00:23:57.296 --> 00:23:59.660
gets activated.

00:23:59.660 --> 00:24:05.120
And says Greene, given the
choice between our rational

00:24:05.120 --> 00:24:11.120
system and our emotional system,
the rational system is

00:24:11.120 --> 00:24:17.990
the one whose outputs we ought
to trust. So says Greene, the

00:24:17.990 --> 00:24:23.360
morally right thing to
do in this case is to

00:24:23.360 --> 00:24:26.090
push the fat man.

00:24:26.090 --> 00:24:31.120
Notice that this is a multi-step
argument, some of

00:24:31.120 --> 00:24:34.090
whose premises are
a good deal more

00:24:34.090 --> 00:24:36.720
controversial than others.

00:24:36.720 --> 00:24:39.520
So the premise that our
motivations are often opaque

00:24:39.520 --> 00:24:45.390
to us is completely undisputed
by everyone.

00:24:45.390 --> 00:24:50.100
There's no question that often
we aren't aware of what's

00:24:50.100 --> 00:24:54.310
causing us to respond
in a particular way.

00:24:54.310 --> 00:24:59.130
I may be particularly irritable
because my feet are

00:24:59.130 --> 00:25:03.610
wet, and unaware of the fact
that the reason that I'm

00:25:03.610 --> 00:25:06.020
responding to you in a
short-tempered way is not

00:25:06.020 --> 00:25:09.100
because you are particularly
irritating, but because my

00:25:09.100 --> 00:25:11.160
feet are uncomfortable.

00:25:11.160 --> 00:25:15.280
This phenomenon is undeniable.

00:25:15.280 --> 00:25:18.000
The question of whether what
actually explains our

00:25:18.000 --> 00:25:22.340
different responses in these
two cases is an interesting

00:25:22.340 --> 00:25:24.530
empirical question.

00:25:24.530 --> 00:25:29.680
And there has been collected
over the last decade or so

00:25:29.680 --> 00:25:33.680
some pretty interesting
neuroimaging data suggesting

00:25:33.680 --> 00:25:38.270
that there are systematic
activation differences in what

00:25:38.270 --> 00:25:42.830
goes on when people give
utilitarian responses to cases

00:25:42.830 --> 00:25:46.770
and what goes on when people
give responses to cases that

00:25:46.770 --> 00:25:49.690
seem to involve the sorts
of notions to which

00:25:49.690 --> 00:25:51.210
deontologists appeal.

00:25:51.210 --> 00:25:54.050
Notions like rights.

00:25:54.050 --> 00:25:58.560
And there is a certain amount of
additional evidence coming

00:25:58.560 --> 00:26:02.440
from other research that the
areas that are differentially

00:26:02.440 --> 00:26:06.400
activated in those two cases
correspond on the one hand

00:26:06.400 --> 00:26:10.390
with what is often thought of
as a rational processing

00:26:10.390 --> 00:26:13.040
system-- a calculative
processing system--

00:26:13.040 --> 00:26:16.470
whereas in the other, they
correspond with areas of the

00:26:16.470 --> 00:26:20.940
brain that have been in
independent cases implicated

00:26:20.940 --> 00:26:22.460
in emotional processing.

00:26:22.460 --> 00:26:25.230
So the first premise
is uncontroversial.

00:26:25.230 --> 00:26:28.600
The second premise is reasonably
well-supported.

00:26:28.600 --> 00:26:32.690
There's controversy about the
data, but there is scientific

00:26:32.690 --> 00:26:35.990
evidence for which there's a
good argument to be made that

00:26:35.990 --> 00:26:39.590
what it shows is roughly
what's written here.

00:26:39.590 --> 00:26:45.150
The controversial question is
whether even if the first two

00:26:45.150 --> 00:26:51.350
premises are true, the third
normative premise is true.

00:26:51.350 --> 00:26:57.000
Is it the case that if our
responses to Fat Man are

00:26:57.000 --> 00:27:00.880
triggered by emotion, whereas
our responses to Bystander are

00:27:00.880 --> 00:27:05.350
triggered by the rational
system, is it the case that we

00:27:05.350 --> 00:27:08.840
ought to go with the
rational system?

00:27:08.840 --> 00:27:13.910
That is a normative claim,
not an empirical one.

00:27:13.910 --> 00:27:17.320
And even if the arguments that
we're going to consider in a

00:27:17.320 --> 00:27:21.250
minute successfully establish
the truth of the second

00:27:21.250 --> 00:27:26.580
premise, we don't yet have the
truth of the third premise

00:27:26.580 --> 00:27:29.060
thereby established.

00:27:29.060 --> 00:27:37.000
So let's talk about evidence
that Greene has found in favor

00:27:37.000 --> 00:27:41.140
of the premise that what goes on
in cases like Fat Man is an

00:27:41.140 --> 00:27:44.290
emotional response, whereas
what goes on in cases like

00:27:44.290 --> 00:27:47.770
classic Bystander is a
rational response.

00:27:47.770 --> 00:27:55.370
So Greene has for the last
decade or so put people into

00:27:55.370 --> 00:28:00.730
fMRI machines-- into scanners
which track where blood is

00:28:00.730 --> 00:28:02.670
flowing in the brain--

00:28:02.670 --> 00:28:05.730
and presented them in
the scanners with

00:28:05.730 --> 00:28:09.010
three kinds of dilemmas.

00:28:09.010 --> 00:28:12.310
The first kind of dilemma are
dilemmas that he calls

00:28:12.310 --> 00:28:15.020
moral/personal dilemmas.

00:28:15.020 --> 00:28:18.150
These are dilemmas like Fat Man
where you're being asked

00:28:18.150 --> 00:28:20.770
whether you want to push the
fat man off the bridge.

00:28:20.770 --> 00:28:23.760
Dilemmas like the doctor case,
which I presented, where we're

00:28:23.760 --> 00:28:26.620
considering whether to cut up a
healthy patient to save the

00:28:26.620 --> 00:28:27.770
lives of others.

00:28:27.770 --> 00:28:30.570
Dilemmas like a lifeboat case
where there's not enough food

00:28:30.570 --> 00:28:32.820
and water to go around on
the lifeboat and you're

00:28:32.820 --> 00:28:35.430
considering whether to throw off
one of the people on the

00:28:35.430 --> 00:28:38.920
lifeboat so as to leave enough
food and water to go around

00:28:38.920 --> 00:28:40.920
for the remaining subjects.

00:28:40.920 --> 00:28:44.030
So that's the first class of
cases that he has subjects

00:28:44.030 --> 00:28:46.510
consider in the scanner.

00:28:46.510 --> 00:28:49.090
The second class of cases that
he has people consider in the

00:28:49.090 --> 00:28:53.220
scanner are what he calls moral
and impersonal cases.

00:28:53.220 --> 00:28:56.270
So these are cases like
Bystander at the switch where

00:28:56.270 --> 00:28:59.490
you're facing a moral dilemma,
but not one where you are

00:28:59.490 --> 00:29:02.870
imagining, in an up-close and
personal way, causing

00:29:02.870 --> 00:29:05.920
particular harm to a particular
individual who's in

00:29:05.920 --> 00:29:07.110
your proximity.

00:29:07.110 --> 00:29:10.340
Cases like ones where you've
found a lost wallet and you

00:29:10.340 --> 00:29:12.590
need to decide whether
to return it.

00:29:12.590 --> 00:29:15.840
Cases where you're voting on a
policy that will have certain

00:29:15.840 --> 00:29:20.430
kinds of effects on people, but
where those effects are

00:29:20.430 --> 00:29:23.080
relatively remote from you.

00:29:23.080 --> 00:29:26.180
And finally, he presents people
with what he calls

00:29:26.180 --> 00:29:27.370
non-moral dilemmas.

00:29:27.370 --> 00:29:30.850
Questions like, if I'm trying
to get from Cleveland to

00:29:30.850 --> 00:29:34.740
Chicago, should I take the bus
or the train or a plane?

00:29:34.740 --> 00:29:37.890
Or if I'm trying to decide which
coupon to use on the

00:29:37.890 --> 00:29:42.240
internet to save on shipping,
should I do this or that?

00:29:42.240 --> 00:29:45.530
Cases that involve the same
kinds of objects, right?

00:29:45.530 --> 00:29:48.010
Fat Man involves trains.

00:29:48.010 --> 00:29:51.330
Bus versus train involves
trains.

00:29:51.330 --> 00:29:54.000
We might have a coupon-use case
where you're using the

00:29:54.000 --> 00:29:56.110
coupon to buy a boat.

00:29:56.110 --> 00:29:58.280
Lifeboat involves a boat.

00:29:58.280 --> 00:30:01.680
So he has the subjects in the
scanner and they're presented

00:30:01.680 --> 00:30:03.190
with these sorts of cases.

00:30:03.190 --> 00:30:06.330
And you'll notice that I've put
a little color-coded box

00:30:06.330 --> 00:30:09.820
here of black, grey,
and white.

00:30:09.820 --> 00:30:13.940
What Greene discovered
in the 2001 paper--

00:30:13.940 --> 00:30:16.520
and let me say some of these
data have since been

00:30:16.520 --> 00:30:20.630
re-analyzed, so some of the
details haven't held up, but

00:30:20.630 --> 00:30:22.190
many of them have--

00:30:22.190 --> 00:30:28.770
what he discovered is that if
one believes, as many do, that

00:30:28.770 --> 00:30:32.180
the brain areas listed here--

00:30:32.180 --> 00:30:35.940
brain areas like medial frontal
gyrus, and angular

00:30:35.940 --> 00:30:39.260
gyrus, and posterior
cingulate gyrus--

00:30:39.260 --> 00:30:44.320
if one believes that those are
areas associated with emotion,

00:30:44.320 --> 00:30:48.120
then we have good evidence that
in the moral/personal

00:30:48.120 --> 00:30:51.320
cases, the areas of the
brain associated

00:30:51.320 --> 00:30:53.800
with emotion are activated.

00:30:53.800 --> 00:30:56.910
Whereas in the moral/impersonal
and non-moral

00:30:56.910 --> 00:31:00.180
cases that doesn't occur.

00:31:00.180 --> 00:31:04.420
By contrast, it looks like
a bunch of areas that are

00:31:04.420 --> 00:31:07.820
traditionally associated
with working memory--

00:31:07.820 --> 00:31:10.450
parietal lobe, middle
frontal gyrus--

00:31:10.450 --> 00:31:15.680
are more active in the
impersonal case and the

00:31:15.680 --> 00:31:20.970
non-moral case than they are
in the personal case.

00:31:20.970 --> 00:31:24.900
And here's the famous image
from Greene's 2001 paper

00:31:24.900 --> 00:31:29.880
reproduced in many papers since
that shows the brain

00:31:29.880 --> 00:31:32.770
areas that exhibit differential
response in the

00:31:32.770 --> 00:31:35.710
moral/personal cases
as contrasted

00:31:35.710 --> 00:31:39.230
with the other cases.

00:31:39.230 --> 00:31:44.870
So it looks like there is
some, perhaps decisive,

00:31:44.870 --> 00:31:49.540
evidence in favor of Greene's
second premise.

00:31:49.540 --> 00:31:54.390
In favor of the premise that
what goes on in moral/personal

00:31:54.390 --> 00:31:58.110
cases is an activation of the
part of the brain associated

00:31:58.110 --> 00:32:01.710
with emotion, whereas what
goes on in cases like

00:32:01.710 --> 00:32:05.270
Bystander is an activation
of the part of the brain

00:32:05.270 --> 00:32:11.010
associated with reasoning
and other sorts of

00:32:11.010 --> 00:32:14.220
more-controlled processes.

00:32:14.220 --> 00:32:21.150
Moreover says Greene, there's
lots and lots of behavioral

00:32:21.150 --> 00:32:25.800
evidence that supports the
hypothesis that one of the

00:32:25.800 --> 00:32:30.810
things that goes on when we
respond to hypothetical moral

00:32:30.810 --> 00:32:34.670
dilemmas is that we track
features of the case that are

00:32:34.670 --> 00:32:37.250
not morally relevant.

00:32:37.250 --> 00:32:43.110
So for example, there's a study
from the early 2000s by

00:32:43.110 --> 00:32:47.175
behavioral economists Small and
Loewenstein that points

00:32:47.175 --> 00:32:53.090
out that in a very profound
sense, identifiable victims

00:32:53.090 --> 00:32:57.250
produce in us more powerful
emotional responses the

00:32:57.250 --> 00:33:00.910
non-identifiable victims. And
this isn't just the difference

00:33:00.910 --> 00:33:04.850
between a picture of the child
to whom your Oxfam donations

00:33:04.850 --> 00:33:07.740
will go versus a description
of the child to whom your

00:33:07.740 --> 00:33:09.990
Oxfam donations will go.

00:33:09.990 --> 00:33:16.750
There is in fact a strikingly
large difference between

00:33:16.750 --> 00:33:23.080
people's willingness to give
some of their rewards in a

00:33:23.080 --> 00:33:28.740
game in a laboratory to
person number four--

00:33:28.740 --> 00:33:32.670
right, so they draw a name from
a hat and it says person

00:33:32.670 --> 00:33:34.550
number four--

00:33:34.550 --> 00:33:38.800
than in cases where they're
told, decide how much money

00:33:38.800 --> 00:33:42.870
you want to give to the person
whose number you're about to

00:33:42.870 --> 00:33:44.220
draw from the hat.

00:33:47.715 --> 00:33:50.460
In neither of these instances do
they know who person number

00:33:50.460 --> 00:33:52.320
four is going to be.

00:33:52.320 --> 00:33:55.429
But the fact that in the first
case, the person they draw

00:33:55.429 --> 00:33:57.900
from the hat and it says person
number four, and they

00:33:57.900 --> 00:33:59.620
think, oh, I'll give
this amount of my

00:33:59.620 --> 00:34:01.139
proceeds to the person.

00:34:01.139 --> 00:34:04.860
Whereas in the second case,
they decide what amount of

00:34:04.860 --> 00:34:08.165
proceeds they want to give to
the person whose number they

00:34:08.165 --> 00:34:09.880
are about to draw.

00:34:09.880 --> 00:34:15.650
The fact that that produces
in subjects consistently

00:34:15.650 --> 00:34:21.360
different responses suggests
to Greene, and perhaps to

00:34:21.360 --> 00:34:28.010
others of you, that perhaps
using our intuitions about

00:34:28.010 --> 00:34:33.190
these sorts of cases to build
our moral theories may not be

00:34:33.190 --> 00:34:39.030
the best way to proceed, since
presumably there are few of

00:34:39.030 --> 00:34:42.890
you who think that there is a
relevant moral difference

00:34:42.890 --> 00:34:47.480
between whether you know the
number of the person to whom

00:34:47.480 --> 00:34:51.580
you're going to be giving the
gift or whether you're about

00:34:51.580 --> 00:34:55.080
to find out the number of the
person to whom you're going to

00:34:55.080 --> 00:34:57.270
be giving the gift.

00:34:57.270 --> 00:35:00.600
Here's something else that
appears to affect our moral

00:35:00.600 --> 00:35:01.910
responses to cases.

00:35:07.610 --> 00:35:10.630
This is work done by Jon Haidt,
author of The Happiness

00:35:10.630 --> 00:35:13.410
Hypothesis, with various
collaborators.

00:35:13.410 --> 00:35:15.950
If you're deciding how much
punishment to give somebody--

00:35:15.950 --> 00:35:19.420
if you're deciding how
wrong an act was--

00:35:19.420 --> 00:35:25.460
if you have been induced to
feel disgust, either by

00:35:25.460 --> 00:35:30.160
sitting at a dirty table or
by having been trained to

00:35:30.160 --> 00:35:34.560
associate certain terms with
disgust through a hypnotic

00:35:34.560 --> 00:35:40.390
suggestion, you will be harsher
in punishing people

00:35:40.390 --> 00:35:42.890
for their misdeeds.

00:35:42.890 --> 00:35:47.800
Now, I take it that most of you
don't think people deserve

00:35:47.800 --> 00:35:52.430
harsher punishment when you are
feeling disgust because

00:35:52.430 --> 00:35:54.550
the table in front
of you is dirty.

00:35:54.550 --> 00:35:58.480
I take it you think that how
bad an act is that somebody

00:35:58.480 --> 00:36:03.530
else has done is independent of
your feelings of disgust.

00:36:03.530 --> 00:36:06.810
But it looks like one of the
things that condemnation

00:36:06.810 --> 00:36:09.190
tracks is that feeling.

00:36:09.190 --> 00:36:12.710
And in a minute, I'll talk about
how that connects to

00:36:12.710 --> 00:36:16.810
Sunstein's more general
discussion of heuristics.

00:36:16.810 --> 00:36:22.720
Finally, some work by David
Pizarro, a Yale PhD, suggest

00:36:22.720 --> 00:36:27.560
that in specific trolley cases,
we can get people's

00:36:27.560 --> 00:36:32.300
intuitions to move around in
cases like Fat Man just by

00:36:32.300 --> 00:36:36.510
varying what most people would
say are morally irrelevant

00:36:36.510 --> 00:36:38.760
features of the situation.

00:36:38.760 --> 00:36:42.260
In particular, Pizarro presents
subjects with two

00:36:42.260 --> 00:36:44.740
different versions of
the Fat Man case.

00:36:44.740 --> 00:36:48.410
In the first, you're asked
whether it is morally

00:36:48.410 --> 00:36:52.480
permitted, required, or
prohibited, to push a man

00:36:52.480 --> 00:36:57.800
named Tyrone Peyton off the
bridge in order to save 100

00:36:57.800 --> 00:37:01.160
members of the New York
Philharmonic.

00:37:01.160 --> 00:37:04.110
And in the second, you're asked
whether it's morally

00:37:04.110 --> 00:37:08.900
acceptable to push a man named
Chip Ellsworth III off the

00:37:08.900 --> 00:37:15.400
bridge to save 100 members of
the Harlem Jazz Orchestra.

00:37:15.400 --> 00:37:18.700
So the question is whether
pushing a white man off the

00:37:18.700 --> 00:37:23.980
bridge to save 100 people of
African descent or pushing a

00:37:23.980 --> 00:37:27.910
black man off the bridge to save
100 people of European

00:37:27.910 --> 00:37:32.480
descent should produce
different responses.

00:37:32.480 --> 00:37:34.570
And interestingly--

00:37:34.570 --> 00:37:36.640
perhaps as the result
of a certain kind of

00:37:36.640 --> 00:37:38.600
self-correction--

00:37:38.600 --> 00:37:42.610
liberals say it is less morally
acceptable to push

00:37:42.610 --> 00:37:45.130
Tyrone Peyton off the
bridge than to

00:37:45.130 --> 00:37:47.990
produce Chip Ellsworth.

00:37:47.990 --> 00:37:50.650
Regardless of which direction
the numbers come out, what's

00:37:50.650 --> 00:37:54.610
interesting is the numbers come
out differently, tracking

00:37:54.610 --> 00:37:59.130
a feature which most of us would
think isn't a morally

00:37:59.130 --> 00:38:01.080
relevant feature.

00:38:01.080 --> 00:38:07.110
So it looks like strengthening
Greene's second premise-- and

00:38:07.110 --> 00:38:11.110
this is an argument that he
makes in more detail in a

00:38:11.110 --> 00:38:14.560
paper, from which we'll read
excerpts after the break,

00:38:14.560 --> 00:38:18.600
called "The Secret Joke of
Kant's Soul"-- it looks like

00:38:18.600 --> 00:38:22.810
there's pretty good reason to
think at least some of our

00:38:22.810 --> 00:38:26.990
responses to these cases are
tracking features which we

00:38:26.990 --> 00:38:29.760
wouldn't reflectively endorse.

00:38:29.760 --> 00:38:34.740
And Greene thinks in particular
in Fat Man, our

00:38:34.740 --> 00:38:39.950
reluctance to push the fat man
off the bridge is tracking one

00:38:39.950 --> 00:38:43.390
of those morally irrelevant
features.

00:38:43.390 --> 00:38:47.540
Deontological judgments, says
Greene-- those where we're

00:38:47.540 --> 00:38:50.560
unwilling to make the
utilitarian move--

00:38:50.560 --> 00:38:53.020
deontological judgments, says
Greene, are driven by

00:38:53.020 --> 00:38:54.810
emotional responses.

00:38:54.810 --> 00:38:58.270
Consequentialist judgments are
driven by cognitive ones.

00:38:58.270 --> 00:39:02.480
And the deontological responses,
he says, lack moral

00:39:02.480 --> 00:39:03.960
significance.

00:39:03.960 --> 00:39:10.950
In fact, deontology itself is a
kind of moral confabulation.

00:39:10.950 --> 00:39:14.590
I'm going to give Kant the last
word in this lecture.

00:39:14.590 --> 00:39:18.660
So those of you who are crying
for the sage of Konigsberg,

00:39:18.660 --> 00:39:22.340
realize that he will get the
very last word in today's

00:39:22.340 --> 00:39:25.870
lecture complete with a
beautiful image of his face.

00:39:25.870 --> 00:39:30.720
But before I do that I want to
spend the final 10 substantive

00:39:30.720 --> 00:39:35.450
minutes of the lecture talking
you through the third article

00:39:35.450 --> 00:39:39.070
which we are considering
for today, namely

00:39:39.070 --> 00:39:41.640
Cass Sunstein's article.

00:39:41.640 --> 00:39:45.360
So Sunstein, in somewhat similar
vein to Greene, though

00:39:45.360 --> 00:39:49.400
drawing on a slightly different
literature, argues

00:39:49.400 --> 00:39:54.460
that a good portion of our moral
reasoning operates in

00:39:54.460 --> 00:39:58.640
exactly the same way that our
regular reasoning does, namely

00:39:58.640 --> 00:40:02.960
by making use of heuristics,
which we know about from the

00:40:02.960 --> 00:40:06.210
lecture on January 20
on dual processing.

00:40:06.210 --> 00:40:10.320
Heuristics are fast and frugal
tools for dealing with the

00:40:10.320 --> 00:40:13.160
complexity of the world
when we're faced with

00:40:13.160 --> 00:40:17.500
time-sensitive, decision-making
tasks.

00:40:17.500 --> 00:40:22.560
And the way that heuristics
work is really smart.

00:40:22.560 --> 00:40:25.640
They work by means of something
called attribute

00:40:25.640 --> 00:40:27.130
substitution.

00:40:27.130 --> 00:40:30.350
We're interested in a
target attribute--

00:40:30.350 --> 00:40:32.890
something that's relatively
hard to find

00:40:32.890 --> 00:40:34.820
out about the world.

00:40:34.820 --> 00:40:40.430
And we focus our attention
instead on a heuristic

00:40:40.430 --> 00:40:41.310
attribute--

00:40:41.310 --> 00:40:43.740
something that's relatively
easy to find

00:40:43.740 --> 00:40:45.680
out about the world.

00:40:45.680 --> 00:40:50.270
So some of you may make use of
this when you're trying to

00:40:50.270 --> 00:40:52.530
distinguish your telephone
from other people's

00:40:52.530 --> 00:40:53.910
telephones.

00:40:53.910 --> 00:40:55.300
The target attribute--

00:40:55.300 --> 00:40:57.200
the thing you're really
interested in is--

00:40:57.200 --> 00:40:59.220
is this my phone?

00:40:59.220 --> 00:41:01.360
Something which you're only
going to be able to determine

00:41:01.360 --> 00:41:03.630
by turning on the phone and
looking to see whether the

00:41:03.630 --> 00:41:05.810
numbers in it are the numbers
that you've placed

00:41:05.810 --> 00:41:07.450
into it, let's say.

00:41:07.450 --> 00:41:11.480
But you might make your life
easy by putting a cover on

00:41:11.480 --> 00:41:14.410
your phone or a sticker on your
phone or some surface

00:41:14.410 --> 00:41:18.580
feature on your phone that will
let you find your phone

00:41:18.580 --> 00:41:21.250
quickly and well.

00:41:21.250 --> 00:41:21.535
Right?

00:41:21.535 --> 00:41:25.680
So you're going to make use of
an easy to find attribute

00:41:25.680 --> 00:41:30.650
rather than a difficult to
determine attribute.

00:41:30.650 --> 00:41:34.910
In general, this is an
extraordinarily good way to

00:41:34.910 --> 00:41:36.930
navigate the world.

00:41:36.930 --> 00:41:40.500
Target and heuristic attributes
generally coincide.

00:41:40.500 --> 00:41:44.060
That's how the heuristic
attributes came to be the ones

00:41:44.060 --> 00:41:47.360
which you're using as the
markers of the target.

00:41:47.360 --> 00:41:52.060
And it takes much less effort to
process surface features of

00:41:52.060 --> 00:41:56.060
the world than to spend your
time working through the

00:41:56.060 --> 00:41:59.350
details of each of the
things that you want

00:41:59.350 --> 00:42:01.980
to make sense of.

00:42:01.980 --> 00:42:06.240
So I observed myself this
morning making use of a target

00:42:06.240 --> 00:42:09.730
attribute on my way
into school.

00:42:09.730 --> 00:42:12.970
I was stopped at a stoplight,
and I noticed out of the

00:42:12.970 --> 00:42:15.440
corner of my eye that
the car next to me

00:42:15.440 --> 00:42:17.640
had started to move.

00:42:17.640 --> 00:42:21.380
Now obviously, the attribute in
which I was interested in

00:42:21.380 --> 00:42:25.470
was whether the light
had turned green.

00:42:25.470 --> 00:42:29.070
But because I couldn't quite see
the green light from where

00:42:29.070 --> 00:42:33.470
I was sitting, I was able to use
instead the motion of the

00:42:33.470 --> 00:42:36.720
car next to me as an indicator
of the thing that I was

00:42:36.720 --> 00:42:38.500
concerned with.

00:42:38.500 --> 00:42:40.990
Now of course, the heuristic
could have

00:42:40.990 --> 00:42:42.590
misfired in this case.

00:42:42.590 --> 00:42:44.880
It could have been that that car
was moving even though the

00:42:44.880 --> 00:42:45.660
light was still red.

00:42:45.660 --> 00:42:48.050
It could have been that that
car was moving in the left

00:42:48.050 --> 00:42:50.770
lane and had a special
light that I didn't.

00:42:50.770 --> 00:42:56.370
But for the most part, we make
use of heuristics all the time

00:42:56.370 --> 00:42:58.290
and they help us.

00:42:58.290 --> 00:43:01.630
Now Sunstein's argument is
that in non-moral cases,

00:43:01.630 --> 00:43:03.800
people often use heuristics.

00:43:03.800 --> 00:43:07.770
That though these are useful,
they may also lead to errors.

00:43:07.770 --> 00:43:12.100
And that in moral cases,
people often use

00:43:12.100 --> 00:43:13.350
heuristics as well.

00:43:15.840 --> 00:43:20.400
But just as they may lead to
errors in the non-moral cases,

00:43:20.400 --> 00:43:24.960
so too may they lead to errors
in the moral cases.

00:43:24.960 --> 00:43:30.500
And in particular, he thinks
they do in a number of cases

00:43:30.500 --> 00:43:31.890
that he goes on to discuss.

00:43:35.680 --> 00:43:37.420
And I realize, I think we're
going to close--

00:43:37.420 --> 00:43:39.850
I said Kant would get the last
word, but Kant's going to get

00:43:39.850 --> 00:43:41.830
the last word on Thursday.

00:43:41.830 --> 00:43:43.920
We're going to go through
Sunstein and one of the

00:43:43.920 --> 00:43:45.580
examples, and then we'll
get to Kant.

00:43:45.580 --> 00:43:48.700
So Sunstein points out, for
example, that there's a

00:43:48.700 --> 00:43:51.410
heuristic called the
availability heuristic.

00:43:51.410 --> 00:43:54.230
That's a heuristic that says,
look, I'm trying to figure out

00:43:54.230 --> 00:43:56.650
how likely something is to
happen, and here's a good way

00:43:56.650 --> 00:43:59.020
to determine how likely
something is to happen.

00:43:59.020 --> 00:44:01.930
I think about how easy it is for
me to think of cases where

00:44:01.930 --> 00:44:03.100
that did happen.

00:44:03.100 --> 00:44:05.510
So whenever I'm worried that
my children are going to be

00:44:05.510 --> 00:44:09.820
kidnapped, I think for myself,
how many friends do I know

00:44:09.820 --> 00:44:11.490
whose children were kidnapped?

00:44:11.490 --> 00:44:13.830
How many people do I know whose
children were kidnapped?

00:44:13.830 --> 00:44:18.340
And when I discover that the
answer is none, I relax.

00:44:18.340 --> 00:44:21.560
This kind of heuristic
is often correct, but

00:44:21.560 --> 00:44:22.990
it can lead us astray.

00:44:22.990 --> 00:44:28.550
Suppose, for example, you're
asked whether it's more likely

00:44:28.550 --> 00:44:30.670
whether there are more words in
the English language that

00:44:30.670 --> 00:44:34.280
end in I-N-G or more words in
the English language whose

00:44:34.280 --> 00:44:45.300
second-to-last letter is N. It's
much easier to think of

00:44:45.300 --> 00:44:49.360
words that end with I-N-G, and
so people tend to say that

00:44:49.360 --> 00:44:52.420
there are more words ending
in I-N-G than words whose

00:44:52.420 --> 00:44:57.220
second-to-last letter is N. But
of course, every word that

00:44:57.220 --> 00:45:01.870
ends with I-N-G is a word whose
second-to-last letter is

00:45:01.870 --> 00:45:08.360
N. You've been bamboozled by
the availability heuristic.

00:45:08.360 --> 00:45:10.920
Or suppose you make use of
what's sometimes called the

00:45:10.920 --> 00:45:12.610
representative heuristic.

00:45:12.610 --> 00:45:15.930
That the probability of
something occurring tracks its

00:45:15.930 --> 00:45:18.300
degree of typicality.

00:45:18.300 --> 00:45:20.900
This too is often correct.

00:45:20.900 --> 00:45:24.360
What it is to be a typical
instance is to be one of the

00:45:24.360 --> 00:45:27.790
instances that occurs
more frequently.

00:45:27.790 --> 00:45:30.590
But as you know from the Linda
the bank teller case or the

00:45:30.590 --> 00:45:35.330
farmer with a tractor case,
if I ask you of our random

00:45:35.330 --> 00:45:39.090
resident of Iowa whether it's
more likely that that resident

00:45:39.090 --> 00:45:44.750
is a farmer or a farmer with a
tractor, the representative

00:45:44.750 --> 00:45:47.540
heuristic is going to draw you
towards saying that it's more

00:45:47.540 --> 00:45:50.830
likely that the person is
a farmer with a tractor.

00:45:50.830 --> 00:45:53.250
But obviously, every
farmer with a

00:45:53.250 --> 00:45:57.290
tractor is also a farmer.

00:45:57.290 --> 00:46:03.070
Now remember that Sunstein's
arguments for one and two are

00:46:03.070 --> 00:46:06.230
easy to make because we have
an independent way of

00:46:06.230 --> 00:46:09.380
determining whether somebody
has made an

00:46:09.380 --> 00:46:11.290
error in those cases.

00:46:11.290 --> 00:46:14.340
We can see what went wrong in
the availability heuristic and

00:46:14.340 --> 00:46:17.520
in the representative heuristic,
because we can see

00:46:17.520 --> 00:46:21.580
that it is in fact more likely
that the second-to-last letter

00:46:21.580 --> 00:46:26.120
of a word is N than it is that
the last three letters of the

00:46:26.120 --> 00:46:30.200
word are I-N-G. We can see that
it's more likely that

00:46:30.200 --> 00:46:33.060
somebody's a farmer than
it is that somebody's a

00:46:33.060 --> 00:46:34.770
farmer with a tractor.

00:46:34.770 --> 00:46:37.940
Because in both of those
cases, one of them is a

00:46:37.940 --> 00:46:40.980
special instance of the other.

00:46:40.980 --> 00:46:45.290
Sunstein's argument for moral
heuristics is going to take

00:46:45.290 --> 00:46:47.300
more steps.

00:46:47.300 --> 00:46:51.150
Because it's not enough for
him to show what we'll

00:46:51.150 --> 00:46:52.340
establish--

00:46:52.340 --> 00:46:56.840
that in moral cases, people
often use heuristics--

00:46:56.840 --> 00:47:01.740
he's also going to need to show
that in so doing, they're

00:47:01.740 --> 00:47:03.380
making mistakes.

00:47:03.380 --> 00:47:06.610
Where the question of how we get
an independent handle on

00:47:06.610 --> 00:47:11.900
what it is to make a mistake is
a rather complicated one.

00:47:11.900 --> 00:47:16.310
But let's first think about what
his argument in favor of

00:47:16.310 --> 00:47:19.580
the claim that in moral
cases people often

00:47:19.580 --> 00:47:21.040
use heuristics is.

00:47:21.040 --> 00:47:23.080
And I'm going to close today's
lecture with two

00:47:23.080 --> 00:47:24.220
examples that he gives.

00:47:24.220 --> 00:47:27.870
And then we'll begin on Thursday
by running through

00:47:27.870 --> 00:47:31.190
some particular cases where
I'll ask you to respond.

00:47:31.190 --> 00:47:34.610
So one of the examples that he
provides is again some work by

00:47:34.610 --> 00:47:38.640
Jonathan Haidt on a phenomenon
known as moral dumbfounding.

00:47:38.640 --> 00:47:43.960
As you know from reading
Sunstein's paper, people often

00:47:43.960 --> 00:47:48.270
respond to the question: "Is
it morally acceptable for a

00:47:48.270 --> 00:47:52.440
brother and sister to engage
in consensual, harm-free

00:47:52.440 --> 00:47:57.640
sibling incestuous relations?"
by saying that it is morally

00:47:57.640 --> 00:47:59.530
unacceptable.

00:47:59.530 --> 00:48:05.550
But when asked to provide
reasons for that, subjects

00:48:05.550 --> 00:48:08.820
find it difficult to do so.

00:48:08.820 --> 00:48:11.580
Likewise, many people are
inclined to think there's

00:48:11.580 --> 00:48:15.930
something morally problematic
about wiping the floor of your

00:48:15.930 --> 00:48:22.200
bathroom with a flag, or about
eating your dog if he's been

00:48:22.200 --> 00:48:28.410
hit by a car, but they find it
difficult to articulate what

00:48:28.410 --> 00:48:32.570
their reasons are for
those responses.

00:48:32.570 --> 00:48:38.120
Sunstein suggests the reason
is an overextension of

00:48:38.120 --> 00:48:39.740
heuristics.

00:48:39.740 --> 00:48:43.040
Likewise, he points out in
moral framing cases-- and

00:48:43.040 --> 00:48:45.660
we'll start with this
next lecture--

00:48:45.660 --> 00:48:49.900
cases like the Asian disease
case that I presented you with

00:48:49.900 --> 00:48:55.040
in our lecture on January 20,
whether you present a moral

00:48:55.040 --> 00:48:59.770
dilemma as involving
lives saved, or by

00:48:59.770 --> 00:49:02.500
contrast lives lost--

00:49:02.500 --> 00:49:06.340
even when those are just
complementary descriptions of

00:49:06.340 --> 00:49:08.220
the same outcome--

00:49:08.220 --> 00:49:12.970
people are likely to have
different responses.

00:49:12.970 --> 00:49:18.600
And Sunstein concludes on that
basis that people make use of

00:49:18.600 --> 00:49:22.720
heuristics in moral reasoning
just as they do

00:49:22.720 --> 00:49:25.240
in non-moral reasoning.

00:49:25.240 --> 00:49:29.430
And we'll begin on Thursday with
Sunstein's discussion of

00:49:29.430 --> 00:49:32.980
those cases, and then we'll let
Kant and Mill get the last

00:49:32.980 --> 00:49:34.610
words in the trolley debate.

00:49:34.610 --> 00:49:36.280
I'll see you then.

