WEBVTT
Kind: captions
Language: en

00:00:00.510 --> 00:00:03.810
PROFESSOR: So there's two things
that we need to do in

00:00:03.810 --> 00:00:04.800
today's lecture.

00:00:04.800 --> 00:00:09.590
The first is to finish up our
discussion of deontology,

00:00:09.590 --> 00:00:11.950
which was necessarily
quite rushed.

00:00:11.950 --> 00:00:14.380
We're trying to do Kant
in roughly a lecture.

00:00:14.380 --> 00:00:18.610
But I do want to get to the
end of that discussion.

00:00:18.610 --> 00:00:22.090
And the second, which will allow
you to use your clickers

00:00:22.090 --> 00:00:25.730
and express your opinions, is to
talk through the structure

00:00:25.730 --> 00:00:30.020
of Judy Thomson's trolley
problem paper.

00:00:30.020 --> 00:00:35.470
So you recall from last lecture
that our goal in

00:00:35.470 --> 00:00:39.100
understanding the very brief
selection from Kant's

00:00:39.100 --> 00:00:42.800
Groundwork to the Metaphysics of
Morals that we read was to

00:00:42.800 --> 00:00:48.840
try to make sense of the three
claims that he makes in the

00:00:48.840 --> 00:00:53.820
first book, first chapter,
of that volume.

00:00:53.820 --> 00:00:57.130
And those claims, to which
I've now added some

00:00:57.130 --> 00:00:59.441
underlining, are
the following.

00:00:59.441 --> 00:01:02.954
The first is the claim that in
order to have moral worth, an

00:01:02.954 --> 00:01:07.630
action needs to be
done from duty.

00:01:07.630 --> 00:01:10.390
And the distinction that Kant
is making there is the

00:01:10.390 --> 00:01:13.970
distinction between doing
something in keeping with

00:01:13.970 --> 00:01:18.390
duty, that is, something that
conforms to what morality

00:01:18.390 --> 00:01:21.570
demands of you, and doing
something not merely in

00:01:21.570 --> 00:01:25.560
keeping with, but
also from duty.

00:01:25.560 --> 00:01:29.970
And Kant's picture is that the
moral worth of an action is

00:01:29.970 --> 00:01:35.060
determined not merely by it
being in keeping with duty.

00:01:35.060 --> 00:01:39.320
That's a necessary but not
a sufficient condition.

00:01:39.320 --> 00:01:42.270
What determines the moral worth
of an action is that it

00:01:42.270 --> 00:01:47.350
be done in keeping with duty
for the sake of being in

00:01:47.350 --> 00:01:52.512
keeping with duty, that is,
it be done from duty.

00:01:52.512 --> 00:01:56.020
The second thing that Kant says,
the second proposition

00:01:56.020 --> 00:01:59.595
which he seeks to defend in the
Groundwork, is the claim

00:01:59.595 --> 00:02:01.480
that an action done
from duty--

00:02:01.480 --> 00:02:03.630
that's the thing we were talking
about in the first

00:02:03.630 --> 00:02:07.580
claim-- an action done from duty
has its moral worth not

00:02:07.580 --> 00:02:12.440
in the purpose that is to be
attained by it, not in what

00:02:12.440 --> 00:02:17.390
the Greeks would call its telos,
its goal, its aim, but

00:02:17.390 --> 00:02:22.050
rather in the maxim according
to which the action is

00:02:22.050 --> 00:02:23.210
determined.

00:02:23.210 --> 00:02:26.090
That is, what determines the
morality of the action on the

00:02:26.090 --> 00:02:29.850
Kantian picture is the
description under which the

00:02:29.850 --> 00:02:32.620
action is performed.

00:02:32.620 --> 00:02:35.900
Now, a number of you came to
office hours yesterday, and we

00:02:35.900 --> 00:02:39.370
had a rather lively discussion
of how it is that one goes

00:02:39.370 --> 00:02:43.450
about determining what things
count as maxims. And I

00:02:43.450 --> 00:02:46.070
encourage those of you who are
interested in that question to

00:02:46.070 --> 00:02:49.710
take an ethics course, where
you can work through Kant's

00:02:49.710 --> 00:02:52.430
writings on this question
in more detail.

00:02:52.430 --> 00:02:56.360
For the purpose of our class,
all we need to hold on to is

00:02:56.360 --> 00:03:00.310
the simple idea that what Kant
is interested in here are acts

00:03:00.310 --> 00:03:03.704
under a description, and that
that description is going to

00:03:03.704 --> 00:03:07.140
have to satisfy a certain
sort of test, as we'll

00:03:07.140 --> 00:03:08.645
find out in a moment.

00:03:08.645 --> 00:03:11.054
So that's Kant's second claim.

00:03:11.054 --> 00:03:15.160
Kant's third claim is that duty,
which is the central

00:03:15.160 --> 00:03:17.390
notion to deontology --

00:03:17.390 --> 00:03:21.010
deon, duty is at the core
of deontology --

00:03:21.010 --> 00:03:27.890
duty is "the necessity of an
action done out of respect for

00:03:27.890 --> 00:03:33.855
the law." And the idea here is
that not only do you need to

00:03:33.855 --> 00:03:38.030
act with the goal of conforming
to the law in mind,

00:03:38.030 --> 00:03:42.070
not only do you need to do so
in a way that you articulate

00:03:42.070 --> 00:03:48.990
your actions as falling under
that norm, but you do so

00:03:48.990 --> 00:03:55.490
because you take the moral law
to be morally binding upon

00:03:55.490 --> 00:04:01.120
you, because you recognize that
it is what rationality

00:04:01.120 --> 00:04:03.480
demands of you.

00:04:03.480 --> 00:04:11.300
The moral law turns out be the
law of governing your behavior

00:04:11.300 --> 00:04:17.100
that you set for yourself
as a rational being.

00:04:17.100 --> 00:04:21.790
It is the only aspect of your
behavior on the Kantian

00:04:21.790 --> 00:04:26.380
picture that isn't determined
by the contingent forces of

00:04:26.380 --> 00:04:28.156
the world around you.

00:04:28.156 --> 00:04:34.540
It's determined by your
recognition of your role as

00:04:34.540 --> 00:04:40.070
somebody capable of binding
themselves to a law that they

00:04:40.070 --> 00:04:43.380
themselves set.

00:04:43.380 --> 00:04:48.540
So as I said, the reason we were
interested in these three

00:04:48.540 --> 00:04:53.160
principles was to get to Kant's
famous formulation of

00:04:53.160 --> 00:04:54.860
the categorical imperative.

00:04:54.860 --> 00:05:00.130
And we closed lecture last time
by meeting one member of

00:05:00.130 --> 00:05:05.110
the categorical imperative
family in response to Kant's

00:05:05.110 --> 00:05:10.560
question: "What sort of law
can it be, the thought of

00:05:10.560 --> 00:05:13.223
which must determine the will
without reference to any

00:05:13.223 --> 00:05:16.540
expected effects so that the
will can be called absolutely

00:05:16.540 --> 00:05:20.263
good without qualification?"
Kant sets himself, as I said,

00:05:20.263 --> 00:05:25.080
this cliffhanger of a question,
and answers it with

00:05:25.080 --> 00:05:28.910
the first member of the
categorical imperative family.

00:05:28.910 --> 00:05:32.580
It's the will's "universal
conformity of its actions to

00:05:32.580 --> 00:05:38.870
law as such." Only thereby,
says Kant, can one act

00:05:38.870 --> 00:05:43.910
autonomously and not
heteronomously.

00:05:43.910 --> 00:05:46.780
What does it mean to act
autonomously as opposed to

00:05:46.780 --> 00:05:48.180
heteronomously?

00:05:48.180 --> 00:05:52.910
Let's look at the words:
autonomous, heteronomous.

00:05:52.910 --> 00:05:55.560
You'll notice that they both
have here the word

00:05:55.560 --> 00:05:58.800
nomos, that is, law.

00:05:58.800 --> 00:06:03.950
And they distinguish the law to
which you're subjected by

00:06:03.950 --> 00:06:08.690
saying that in one case, it's
an auto-nomos, and in the

00:06:08.690 --> 00:06:13.160
other case, it's
a hetero-nomos.

00:06:13.160 --> 00:06:15.040
Auto-nomos.

00:06:15.040 --> 00:06:16.730
What could that be?

00:06:16.730 --> 00:06:18.620
Let's think of other
words where we

00:06:18.620 --> 00:06:20.030
have this prefix auto-.

00:06:20.030 --> 00:06:22.820
How about automobile?

00:06:22.820 --> 00:06:24.720
What's an automobile?

00:06:24.720 --> 00:06:31.680
An automobile is something
that is self-propelled.

00:06:31.680 --> 00:06:35.570
It is propelled by
its own strength.

00:06:35.570 --> 00:06:41.650
To act autonomously, on Kant's
picture, is to act on the

00:06:41.650 --> 00:06:47.090
basis of a law that you
yourself have imposed.

00:06:47.090 --> 00:06:55.450
You are auto-nomos, subject to
a law that comes from within.

00:06:55.450 --> 00:07:01.130
By contrast, what is it
to be hetero-nomos?

00:07:01.130 --> 00:07:06.110
Well, what is it to
be heterosexual?

00:07:06.110 --> 00:07:11.790
What it is to be heterosexual is
to be attracted sexually to

00:07:11.790 --> 00:07:17.130
individuals that have a
different gender than you do.

00:07:17.130 --> 00:07:19.910
So what is it to be
hetero-nomos?

00:07:19.910 --> 00:07:24.004
It's to have law given unto you
that comes from outside of

00:07:24.004 --> 00:07:29.250
you, that comes from something
different from you.

00:07:29.250 --> 00:07:36.980
So Kant's picture is that
autonomy, self-lawgiving, is

00:07:36.980 --> 00:07:42.720
possible only when the law
to which you conform your

00:07:42.720 --> 00:07:48.950
behavior comes not from the
contingencies of the world,

00:07:48.950 --> 00:07:52.310
but comes from within.

00:07:52.310 --> 00:07:56.820
In this way, Kant is concerned
with the same sorts of

00:07:56.820 --> 00:08:01.630
questions that Epictetus
and Boethius are.

00:08:01.630 --> 00:08:05.490
Both of them are profoundly
concerned with how it is that

00:08:05.490 --> 00:08:11.520
human freedom is possible, and
Kant's picture is that human

00:08:11.520 --> 00:08:18.570
freedom becomes possible when
you govern your actions on the

00:08:18.570 --> 00:08:25.409
basis of what do you yourself
decide to be, norms that you

00:08:25.409 --> 00:08:28.419
want to conform to.

00:08:28.419 --> 00:08:33.020
In particular, when you conform
your actions to the

00:08:33.020 --> 00:08:37.290
categorical imperative, which
says, in the formulation that

00:08:37.290 --> 00:08:40.350
we see at the beginning of this
section, that you should

00:08:40.350 --> 00:08:46.110
"never act in such a way that
you cannot also will that your

00:08:46.110 --> 00:08:49.560
maxim"--there's your maxim
again--"should become a

00:08:49.560 --> 00:08:56.310
universal law." When you act in
such a way that you don't

00:08:56.310 --> 00:09:00.470
take the contingencies of
your situation into

00:09:00.470 --> 00:09:05.800
consideration--but rather that
you think of yourself as one

00:09:05.800 --> 00:09:11.750
among any number of beings who
in your situation would do

00:09:11.750 --> 00:09:18.210
exactly what you do--only then
do you become free of the

00:09:18.210 --> 00:09:21.430
contingencies of circumstance.

00:09:21.430 --> 00:09:26.090
So the picture is that in some
ways, by stepping beyond the

00:09:26.090 --> 00:09:29.710
bounds of the contingent
features of your experience,

00:09:29.710 --> 00:09:36.160
by stepping beyond the bounds of
yourself, you thereby gain

00:09:36.160 --> 00:09:42.342
freedom from the contingencies
of the world around you.

00:09:42.342 --> 00:09:48.940
And Kant suggests that if you
take this on has a picture,

00:09:48.940 --> 00:09:54.590
you will come to see that it
conforms with the rules of

00:09:54.590 --> 00:09:56.801
rationality.

00:09:56.801 --> 00:10:00.010
So, he says, suppose you're
confronted with a very

00:10:00.010 --> 00:10:04.440
particular case of an act that
you want to perform under a

00:10:04.440 --> 00:10:06.170
particular maxim.

00:10:06.170 --> 00:10:09.520
The maxim you set for yourself
is, "when I make a promise and

00:10:09.520 --> 00:10:11.840
it's going to be a pain in the
neck for me to keep that

00:10:11.840 --> 00:10:14.930
promise, I'll break
that promise.

00:10:14.930 --> 00:10:20.670
That is, the maxim is, it's
OK for me to make lying

00:10:20.670 --> 00:10:22.730
promises."

00:10:22.730 --> 00:10:27.310
And Kant asks, suppose that
you made, under that

00:10:27.310 --> 00:10:31.921
description, a promise that
you didn't intend to keep.

00:10:31.921 --> 00:10:36.560
Can that maxim and
be universalized?

00:10:36.560 --> 00:10:38.490
Well, he says, suppose
that it were.

00:10:38.490 --> 00:10:44.450
Suppose everybody, when they
made promises, did so only

00:10:44.450 --> 00:10:45.920
with the thought that they
would keep them when

00:10:45.920 --> 00:10:50.240
convenient and not when they
were inconvenient.

00:10:50.240 --> 00:10:55.430
Were that to happen, says Kant,
there would be no such

00:10:55.430 --> 00:10:58.980
thing as reliable promising.

00:10:58.980 --> 00:11:00.740
Why?

00:11:00.740 --> 00:11:06.690
Well, because promises
are like balconies.

00:11:06.690 --> 00:11:12.460
We don't step out on a balcony
if there's a good chance that

00:11:12.460 --> 00:11:16.460
the balcony will break when
we step out on it.

00:11:16.460 --> 00:11:20.720
And we don't step out on
promises, on commitments that

00:11:20.720 --> 00:11:27.040
others make to us, unless we are
close to certain that that

00:11:27.040 --> 00:11:30.876
promise will be preserved.

00:11:30.876 --> 00:11:36.930
So, says Kant, since the
practice of promising would

00:11:36.930 --> 00:11:44.130
break down if everybody who
found it convenient made lying

00:11:44.130 --> 00:11:51.010
promises, it is not in keeping
with what the moral law

00:11:51.010 --> 00:11:57.582
demands of us that we make
a lying promise.

00:11:57.582 --> 00:12:04.580
And, Kant suggests, this
framework can be extended to

00:12:04.580 --> 00:12:10.130
all the kinds of duties
that there are.

00:12:10.130 --> 00:12:15.370
There are, suggests Kant, two
categories of obligation that

00:12:15.370 --> 00:12:18.490
we have towards ourselves
and to others.

00:12:18.490 --> 00:12:23.935
We have duties to ourself and
duties to other people.

00:12:23.935 --> 00:12:27.140
And, in addition, says Kant, we
have perfect duties, things

00:12:27.140 --> 00:12:32.630
that we need always to do, and
imperfect duties, things that

00:12:32.630 --> 00:12:35.900
we need sometimes to do.

00:12:35.900 --> 00:12:40.550
In all four of these cases, says
Kant in the reading that

00:12:40.550 --> 00:12:47.160
we did, we can see that the
categorical imperative gives

00:12:47.160 --> 00:12:51.020
us guidance as to whether
an action

00:12:51.020 --> 00:12:54.044
under a maxim is permitted.

00:12:54.044 --> 00:12:57.460
The action under the maxim is
going to be permitted if it

00:12:57.460 --> 00:13:00.341
can be universalized,
and it's going to be

00:13:00.341 --> 00:13:03.030
prohibited if it can't.

00:13:03.030 --> 00:13:07.540
So if we ask this question, is
it all right to make lying

00:13:07.540 --> 00:13:10.370
promises, and we say to ourself,
suppose everybody

00:13:10.370 --> 00:13:15.500
made lying promises, we discover
that the act of lying

00:13:15.500 --> 00:13:19.660
promises is prohibited by the
categorical imperative,

00:13:19.660 --> 00:13:22.250
because it can't be
universalized.

00:13:22.250 --> 00:13:26.890
Suppose we ask the question,
"is it OK to commit suicide

00:13:26.890 --> 00:13:32.250
when feeling frustrated with
the world?" And Kant says,

00:13:32.250 --> 00:13:36.040
suppose everybody did that.

00:13:36.040 --> 00:13:39.560
The practice, the convoluted
argument would break down,

00:13:39.560 --> 00:13:43.632
because there would be nobody
enough to kill themselves.

00:13:43.632 --> 00:13:45.520
So goes the argument.

00:13:45.520 --> 00:13:49.410
Suppose we ask ourselves whether
we have a duty to

00:13:49.410 --> 00:13:52.335
cultivate our talent?

00:13:52.335 --> 00:13:56.420
Kant says: suppose nobody
cultivated their talents.

00:13:56.420 --> 00:14:02.330
The world in which we live would
be one in which nobody

00:14:02.330 --> 00:14:04.590
would want to live.

00:14:04.590 --> 00:14:09.708
And consequently, we have a
moral obligation to do.

00:14:09.708 --> 00:14:14.020
Finally, he asks, "do we have an
obligation to give money to

00:14:14.020 --> 00:14:18.680
those in need?" And asks again,
what would happen if it

00:14:18.680 --> 00:14:21.550
were a universal law
that nobody gave

00:14:21.550 --> 00:14:23.225
money to those in need?

00:14:23.225 --> 00:14:28.440
And again we discover a
breakdown of an ordered world

00:14:28.440 --> 00:14:30.790
in which we want to survive.

00:14:30.790 --> 00:14:34.270
Now, there's room for
questioning--

00:14:34.270 --> 00:14:37.210
in fact, there's room for
questioning all four of these

00:14:37.210 --> 00:14:40.465
derivations, though it's
generally accepted that the

00:14:40.465 --> 00:14:45.040
lying promise derivation is the
most effective of them.

00:14:45.040 --> 00:14:51.490
But let's look instead at what
Kant says about them, if it

00:14:51.490 --> 00:14:55.476
were to turn out that these
derivations worked.

00:14:55.476 --> 00:15:00.480
Kant says, "these are some of
the many actual duties whose

00:15:00.480 --> 00:15:05.590
derivation from the single
principle above is clear."

00:15:05.590 --> 00:15:08.480
"It's clear" is a bit of a
stretch, but we can see how

00:15:08.480 --> 00:15:10.840
that derivation would go.

00:15:10.840 --> 00:15:15.146
What does this tell
us about morality?

00:15:15.146 --> 00:15:22.322
Kant says it tells us that when
we take an act and try to

00:15:22.322 --> 00:15:26.910
determine whether it's moral,
we need to check to see

00:15:26.910 --> 00:15:31.270
whether we're making an
exception for ourselves.

00:15:31.270 --> 00:15:34.810
When we act, we need to be able
to will that a maxim of

00:15:34.810 --> 00:15:38.490
our actions become
a universal law.

00:15:38.490 --> 00:15:43.500
When we transgress, says Kant,
we don't will that our maxim

00:15:43.500 --> 00:15:46.720
should become a universal law,
but rather that the opposite

00:15:46.720 --> 00:15:51.400
of this maxim should remain
a law universally.

00:15:51.400 --> 00:15:57.793
So suppose you like to sit in
the last two rows of this

00:15:57.793 --> 00:16:03.210
classroom, even though you
arrive not late to class.

00:16:03.210 --> 00:16:08.300
Can you will that this become
a universal law, or is this

00:16:08.300 --> 00:16:13.390
something that has works for you
only if others are willing

00:16:13.390 --> 00:16:15.480
to sit further in so that
there's room for

00:16:15.480 --> 00:16:18.770
people on the stairs?

00:16:18.770 --> 00:16:24.055
Kant would say that the moral
law demands of you that you

00:16:24.055 --> 00:16:29.080
move inward, because you're
sitting in those last two

00:16:29.080 --> 00:16:34.250
rows, despite your not late
arrival, depends on other

00:16:34.250 --> 00:16:38.220
people doing something
different.

00:16:38.220 --> 00:16:42.630
Stealing depends
on other people

00:16:42.630 --> 00:16:46.750
respecting the laws of properly.

00:16:46.750 --> 00:16:51.740
Your not paying the toll on the
subway depends on other

00:16:51.740 --> 00:16:55.250
people paying the toll so that
there's enough money to keep

00:16:55.250 --> 00:16:57.520
up the subway.

00:16:57.520 --> 00:17:04.069
When you make an exception for
yourself, says Kant, you

00:17:04.069 --> 00:17:06.750
violate the moral law.

00:17:06.750 --> 00:17:11.000
And we'll return to this at the
opening of our discussion

00:17:11.000 --> 00:17:13.700
of political philosophy,
when we talk

00:17:13.700 --> 00:17:16.840
about prisoner's dilemma.

00:17:16.840 --> 00:17:21.870
Now, I mentioned in passing that
you had met one member of

00:17:21.870 --> 00:17:26.545
the categorical imperative
family, that you had met

00:17:26.545 --> 00:17:29.740
what's sometimes called the
formula of the universal law:

00:17:29.740 --> 00:17:33.130
that one should "act only in
accordance with that maxim

00:17:33.130 --> 00:17:36.110
through which you can, at the
same time, will that it become

00:17:36.110 --> 00:17:39.798
a universal law."

00:17:39.798 --> 00:17:46.770
Kant put the categorical
imperative four different ways

00:17:46.770 --> 00:17:48.520
for a number of reasons.

00:17:48.520 --> 00:17:53.400
One of which, he says, it that
in certain cases, it's easier

00:17:53.400 --> 00:17:57.470
to see how to apply the
categorical imperative if we

00:17:57.470 --> 00:18:01.240
frame it in a slightly
different way.

00:18:01.240 --> 00:18:05.780
And I want to introduce you to
the other three, largely

00:18:05.780 --> 00:18:09.710
because the second of these is
going to play a central role

00:18:09.710 --> 00:18:14.280
in the second half of
today's lecture.

00:18:14.280 --> 00:18:21.940
So Kant claims that it is
equivalent to saying that "you

00:18:21.940 --> 00:18:24.620
should act only in such a way
that you can will your maxim

00:18:24.620 --> 00:18:30.410
to be universal" to say that
"you should act so as to treat

00:18:30.410 --> 00:18:34.490
humanity, whether in your own
person or that of any other,

00:18:34.490 --> 00:18:43.690
in every case as an end, and
never merely as a means only."

00:18:43.690 --> 00:18:51.020
Do not use yourself as a means
to an end, and do not use

00:18:51.020 --> 00:18:58.020
others in your interactions
with them merely as means.

00:18:58.020 --> 00:19:04.420
Treat humanity and all others
as ends in themselves.

00:19:08.100 --> 00:19:12.890
Equivalent to that, says Kant,
is the formula of autonomy,

00:19:12.890 --> 00:19:15.582
which we've already talked
about recently.

00:19:15.582 --> 00:19:19.390
"Act so that through your
maxims, you could be a

00:19:19.390 --> 00:19:24.870
legislator of universal laws."
Act in such a way that you are

00:19:24.870 --> 00:19:30.380
self-lawgiver with respect to
rules that reason endorses.

00:19:30.380 --> 00:19:34.990
And finally, a rather
complicated notion, sometimes

00:19:34.990 --> 00:19:38.240
called the kingdom of ends
formulation, "act in

00:19:38.240 --> 00:19:42.650
accordance with the maxims of a
member giving universal laws

00:19:42.650 --> 00:19:47.260
for a possible kingdom of
ends,"--a harmonious society

00:19:47.260 --> 00:19:49.330
in which everybody exists
according to the

00:19:49.330 --> 00:19:52.080
laws that you give.

00:19:52.080 --> 00:19:56.650
As I said, we have only about an
hour's worth of Kant, so we

00:19:56.650 --> 00:19:59.310
won't focus on the third
and the fourth.

00:19:59.310 --> 00:20:02.980
But I do want to call your
attention to the formula of

00:20:02.980 --> 00:20:06.580
humanity, because as I said,
it's going to play a central

00:20:06.580 --> 00:20:12.460
roll in Judy Thomson's ultimate
diagnosis of what may

00:20:12.460 --> 00:20:17.090
be going on in our intuitions
about trolley cases.

00:20:17.090 --> 00:20:22.050
So let me close the discussion
of Kant by trying to connect

00:20:22.050 --> 00:20:28.240
it back to the mini-unit
of which it is a part.

00:20:28.240 --> 00:20:32.240
You'll recall that we began this
section of the course on

00:20:32.240 --> 00:20:36.720
the seventeenth, that is, last
Thursday, with thinking about

00:20:36.720 --> 00:20:40.412
consequentialism as
a moral theory.

00:20:40.412 --> 00:20:46.660
And the question that I want to
ask is: what is there that

00:20:46.660 --> 00:20:52.700
is common to the two concrete
moral theories that we've

00:20:52.700 --> 00:20:56.350
taken a look at the beginning
of this unit?

00:20:56.350 --> 00:20:59.860
We've looked so far as some
of the differences between

00:20:59.860 --> 00:21:01.760
consequentialism on
the one hand and

00:21:01.760 --> 00:21:03.355
deontology on the other.

00:21:03.355 --> 00:21:07.090
But I think it's important, in
moving on to some of their

00:21:07.090 --> 00:21:09.240
practical applications,
to think about what

00:21:09.240 --> 00:21:10.920
they have in common.

00:21:10.920 --> 00:21:16.820
And what they have in common
is that both teleology,

00:21:16.820 --> 00:21:19.300
consequentialism, utilitarianism
in the

00:21:19.300 --> 00:21:24.540
particular form that we found
it--and deontology prohibit

00:21:24.540 --> 00:21:28.245
first-person exceptionalism.

00:21:28.245 --> 00:21:35.030
Kant says: my desire may serve
at bases for willed actions

00:21:35.030 --> 00:21:39.420
only if I can, at the same,
coherently will that others in

00:21:39.420 --> 00:21:43.710
similar circumstances would
act in a way that I am

00:21:43.710 --> 00:21:45.720
choosing to act.

00:21:45.720 --> 00:21:50.620
I'm only allowed to do things
that I'm going to assume other

00:21:50.620 --> 00:21:53.940
people are also allowed to do.

00:21:53.940 --> 00:21:58.920
And Bentham, quoted in Mill,
Bentham, the great grandfather

00:21:58.920 --> 00:22:04.540
of utilitarianism, says,
"everyone is to count for one,

00:22:04.540 --> 00:22:08.630
no one for more than one."
Mill, in his greatest

00:22:08.630 --> 00:22:13.270
happiness principle, speaks of
the happiness of all, not the

00:22:13.270 --> 00:22:17.020
happiness from the subjective
perspective.

00:22:17.020 --> 00:22:25.340
So the challenge of morality is
that of viewing the world

00:22:25.340 --> 00:22:31.750
not from the stance of your own
needs as the most central

00:22:31.750 --> 00:22:36.350
set of needs in the world, but
rather from the perspective of

00:22:36.350 --> 00:22:44.780
your own needs as one set of
needs among those of six

00:22:44.780 --> 00:22:49.940
billion equally sentient
beings.

00:22:49.940 --> 00:22:56.220
Now the problem for morality is
that the tendency towards

00:22:56.220 --> 00:23:00.420
first person exceptionalism, the
tendency to take one's own

00:23:00.420 --> 00:23:07.330
needs as more important than the
needs of anybody else is

00:23:07.330 --> 00:23:11.730
perhaps the most widespread
and pervasive

00:23:11.730 --> 00:23:14.750
psychological bias.

00:23:14.750 --> 00:23:19.310
And when we get to the unit on
political philosophy after

00:23:19.310 --> 00:23:24.550
March break, we'll talk about
ways in which social

00:23:24.550 --> 00:23:30.110
structures are put into place
to help deal with

00:23:30.110 --> 00:23:34.000
this sort of tension.

00:23:34.000 --> 00:23:38.180
Even in the passages from Mill
that we read for last

00:23:38.180 --> 00:23:43.410
Thursday, Mill talks about what
sorts of attitudes it's

00:23:43.410 --> 00:23:48.145
important to cultivate in
individuals so that they begin

00:23:48.145 --> 00:23:52.200
to view the world from
a moral perspective.

00:23:52.200 --> 00:23:56.150
In the selections that we read
from book ten of Aristotle's

00:23:56.150 --> 00:24:01.420
Nicomachean Ethics, Aristotle
asked, in what way should

00:24:01.420 --> 00:24:05.070
society be structured
to make it easy for

00:24:05.070 --> 00:24:07.860
people to act morally?

00:24:07.860 --> 00:24:11.060
And in some ways, the question
with which we're going to

00:24:11.060 --> 00:24:16.070
close the course--how does
rational versus nonrational

00:24:16.070 --> 00:24:21.010
persuasion work, what are the
roles of literary as opposed

00:24:21.010 --> 00:24:23.300
to argumentative representations
of the good

00:24:23.300 --> 00:24:32.070
life--is a version of this
dilemma, operationalized.

00:24:32.070 --> 00:24:36.520
How is it, given an inevitable
human tendency to take one's

00:24:36.520 --> 00:24:41.520
needs as more important than
others, how is it possible to

00:24:41.520 --> 00:24:44.400
structure society in
such a way that the

00:24:44.400 --> 00:24:47.510
needs of all are met?

00:24:47.510 --> 00:24:51.510
So that's what I want to
say by way of the Kant.

00:24:51.510 --> 00:24:54.570
And in the remainder of lecture,
I want to ask you to

00:24:54.570 --> 00:25:02.120
take out your clickers
and enjoy the ride.

00:25:02.120 --> 00:25:09.400
So as you know, the paper which
we read for today is a

00:25:09.400 --> 00:25:15.320
great and intricate paper by the
philosopher Judith Jarvis

00:25:15.320 --> 00:25:21.310
Thomson written in the mid-1980s
in response to an

00:25:21.310 --> 00:25:25.170
earlier paper by another
mid-century woman philosopher,

00:25:25.170 --> 00:25:27.145
Philippa Foot.

00:25:27.145 --> 00:25:31.980
And what Philippa Foot and Judy
Thomson are interested in

00:25:31.980 --> 00:25:38.810
in these papers is a systematic
exploration of a

00:25:38.810 --> 00:25:46.276
number of cases which seem to
evoke, in most subjects,

00:25:46.276 --> 00:25:50.300
pretty powerful intuitions about
what the right thing to

00:25:50.300 --> 00:25:56.085
do is, but which seem to
adduce intuitions the

00:25:56.085 --> 00:26:02.850
explanation for which is
hard to systematize.

00:26:02.850 --> 00:26:06.750
So as you know well, the
first case, which we'll

00:26:06.750 --> 00:26:09.020
call trolley driver--

00:26:09.020 --> 00:26:10.420
there's the driver--

00:26:10.420 --> 00:26:12.250
is this.

00:26:12.250 --> 00:26:17.310
There's a trolley, hurtling down
a track, in such a way

00:26:17.310 --> 00:26:22.430
that going to kill
five people.

00:26:22.430 --> 00:26:27.950
But it turns out that there is
a second track onto which the

00:26:27.950 --> 00:26:30.720
trolley could be
diverted, where

00:26:30.720 --> 00:26:35.260
there is only one person.

00:26:35.260 --> 00:26:36.660
And the question is this.

00:26:36.660 --> 00:26:40.240
The trolley driver is
driving the trolley.

00:26:40.240 --> 00:26:44.010
It's heading towards the five
people in such a way that he

00:26:44.010 --> 00:26:46.730
is going to kill the five.

00:26:46.730 --> 00:26:51.010
Should he, is he morally
required to, morally

00:26:51.010 --> 00:26:56.690
prohibited from, or perhaps
neither prohibited nor

00:26:56.690 --> 00:27:00.750
required but rather just
permitted, to turn the trolley

00:27:00.750 --> 00:27:04.925
onto the track where there
is only the one?

00:27:04.925 --> 00:27:08.480
Now notice that though this
is framed as an idealized

00:27:08.480 --> 00:27:14.270
problem, the diversion of
threat is something that

00:27:14.270 --> 00:27:18.750
decision-makers face
all the time.

00:27:18.750 --> 00:27:24.230
Suppose there is an airplane
that has become incapacitated,

00:27:24.230 --> 00:27:27.620
that's falling in such a way
that it's going to land on a

00:27:27.620 --> 00:27:32.020
large city, and it would be
possible to divert the plane

00:27:32.020 --> 00:27:37.530
so that it falls on a less
populated area instead.

00:27:37.530 --> 00:27:42.500
Suppose that there is an illness
which is taking the

00:27:42.500 --> 00:27:48.650
lives of many people, but if one
quarantines those who are

00:27:48.650 --> 00:27:53.520
ill, causing that number
to die, the rest of the

00:27:53.520 --> 00:27:57.140
population will be spared.

00:27:57.140 --> 00:28:02.662
In case after case, we face
dilemmas with roughly the

00:28:02.662 --> 00:28:05.580
structure of trolley.

00:28:05.580 --> 00:28:10.070
So though these cases are
idealized, in the sense that

00:28:10.070 --> 00:28:14.060
we're granting ourselves that
we know with certainty what

00:28:14.060 --> 00:28:20.190
the outcomes will be, it is, I
think, not a useless exercise,

00:28:20.190 --> 00:28:23.840
even if our concern is
real-world morality, to think

00:28:23.840 --> 00:28:27.890
through what the right thing
to do is in these cases.

00:28:27.890 --> 00:28:33.610
So let's start with the case of
the trolley driver, driver

00:28:33.610 --> 00:28:34.920
of the trolley.

00:28:34.920 --> 00:28:38.210
The trolley is heading down the
track in such a way that

00:28:38.210 --> 00:28:42.910
the driver will, with the
trolley, kill five people if

00:28:42.910 --> 00:28:44.740
he doesn't turn it.

00:28:44.740 --> 00:28:48.990
He faces the choice of turning
the trolley onto the track

00:28:48.990 --> 00:28:50.750
where there is the one.

00:28:50.750 --> 00:28:51.930
Question.

00:28:51.930 --> 00:28:56.660
Is it morally mandatory for him
to turn a trolley from the

00:28:56.660 --> 00:28:58.660
five to the one?

00:28:58.660 --> 00:29:02.720
Is it morally permitted, but not
morally mandatory, for him

00:29:02.720 --> 00:29:05.470
to turn the trolley from
the five to the one?

00:29:05.470 --> 00:29:09.880
Or is it morally prohibited for
him to turn the trolley

00:29:09.880 --> 00:29:12.180
from the five to the one?

00:29:21.830 --> 00:29:23.290
OK.

00:29:23.290 --> 00:29:26.170
So let's see how the
numbers come out.

00:29:26.170 --> 00:29:28.240
I'm going to write these down,
because we're going to need

00:29:28.240 --> 00:29:29.490
them for later.

00:29:29.490 --> 00:29:35.060
So 7% of you, very, very small
number, think that it's

00:29:35.060 --> 00:29:39.110
morally prohibited for him
to turn the trolley.

00:29:39.110 --> 00:29:43.601
The vast majority of you, close
to two thirds, think

00:29:43.601 --> 00:29:46.800
that it's morally permitted, but
not morally mandatory, for

00:29:46.800 --> 00:29:48.580
him to turn the trolley.

00:29:48.580 --> 00:29:52.650
And about 30% of you, roughly
a third, think that he is

00:29:52.650 --> 00:29:57.590
morally required to
make that turn.

00:29:57.590 --> 00:30:00.670
Case number two.

00:30:00.670 --> 00:30:01.920
Transplant.

00:30:03.600 --> 00:30:06.390
You're running a hospital.

00:30:06.390 --> 00:30:11.560
Five people show up at the
hospital, all of them destined

00:30:11.560 --> 00:30:15.800
to die, because one needs a
lung, and one needs a heart,

00:30:15.800 --> 00:30:19.710
and one needs a kidney, and
one needs a liver, and one

00:30:19.710 --> 00:30:20.960
needs a brain.

00:30:23.470 --> 00:30:27.080
So they're all going to die,
and you are the doctor.

00:30:27.080 --> 00:30:33.030
And into the emergency room
walks a perfectly healthy

00:30:33.030 --> 00:30:36.550
young man who has a heart and
a lung and a liver and a

00:30:36.550 --> 00:30:40.861
kidney and a really good,
active brain.

00:30:40.861 --> 00:30:47.300
And if you were to chop up that
man and give his parts to

00:30:47.300 --> 00:30:52.070
the five suffering individuals,
you could save

00:30:52.070 --> 00:30:54.860
the five at the cost
of the one.

00:30:54.860 --> 00:30:55.550
Question.

00:30:55.550 --> 00:31:00.325
For the doctor, is it A, morally
mandatory to chop up

00:31:00.325 --> 00:31:04.700
the healthy man to save the
five, B, morally permitted but

00:31:04.700 --> 00:31:09.770
not morally mandatory to chop
up the healthy man, or C,

00:31:09.770 --> 00:31:13.590
morally prohibited to chop
up the healthy man?

00:31:17.318 --> 00:31:21.191
So let's see how numbers
so come out.

00:31:26.370 --> 00:31:27.620
So.

00:31:30.300 --> 00:31:35.950
85% of you think it is morally
prohibited to cut up the one

00:31:35.950 --> 00:31:37.950
to save the five.

00:31:37.950 --> 00:31:40.820
9% of you think it is
morally permitted,

00:31:40.820 --> 00:31:44.104
but not morally mandatory.

00:31:44.104 --> 00:31:46.620
And 6% of you--

00:31:46.620 --> 00:31:47.870
off to med school--

00:31:50.360 --> 00:31:53.930
think that it is morally
mandatory to chop up the

00:31:53.930 --> 00:31:55.760
healthy man.

00:31:55.760 --> 00:31:59.096
So what's going on here?

00:31:59.096 --> 00:32:02.930
So Philippa Foot, who was the
person who initially presented

00:32:02.930 --> 00:32:05.150
this juxtaposition,
has a hypothesis.

00:32:05.150 --> 00:32:07.440
And her hypothesis is this.

00:32:07.440 --> 00:32:10.435
That in the trolley driver
case, the choice that the

00:32:10.435 --> 00:32:15.750
driver faces is between killing
one and killing five.

00:32:15.750 --> 00:32:19.750
Whereas in the transplant case,
the choice that the

00:32:19.750 --> 00:32:22.400
doctor faces is between
killing one and

00:32:22.400 --> 00:32:24.590
letting five die.

00:32:24.590 --> 00:32:27.510
And if we were to graph these
on what I'll call the

00:32:27.510 --> 00:32:32.950
bad-o-meter, which tells us how
bad things are, we would

00:32:32.950 --> 00:32:42.870
discover that letting five die
is bad, but killing one is

00:32:42.870 --> 00:32:48.310
worse, and killing five
is even worse.

00:32:48.310 --> 00:32:51.350
And so this seems to give us the
answer that since killing

00:32:51.350 --> 00:32:54.100
five is worse than killing
one, then in the trolley

00:32:54.100 --> 00:32:58.540
driver case, it's OK for him to
turn the trolley, but since

00:32:58.540 --> 00:33:01.640
killing one is worse than
letting five die, then in the

00:33:01.640 --> 00:33:06.000
doctor case, it's not OK
to chop up the one man.

00:33:06.000 --> 00:33:08.710
Because in the doctor case, you
have to kill once to save

00:33:08.710 --> 00:33:12.440
five, whereas in the trolley
case, the driver has to kill

00:33:12.440 --> 00:33:16.740
one in order not to kill five.

00:33:16.740 --> 00:33:19.225
And that seems to accord
pretty well with your

00:33:19.225 --> 00:33:20.520
intuitions.

00:33:20.520 --> 00:33:24.040
93% of you think it's permitted,
in the trolley

00:33:24.040 --> 00:33:28.770
case, to turn the trolley,
whereas only 14% of you think

00:33:28.770 --> 00:33:33.430
it's permitted in the doctor's
case to kill the one.

00:33:33.430 --> 00:33:36.860
So it looks like this
bad-o-meter is pretty well

00:33:36.860 --> 00:33:38.745
capturing the intuition
of those

00:33:38.745 --> 00:33:40.425
of you in this classroom.

00:33:40.425 --> 00:33:43.480
And in fact, empirical studies
that have been done on

00:33:43.480 --> 00:33:46.156
thousands and thousands of
people throughout the world

00:33:46.156 --> 00:33:48.870
suggest that your intuitions are
pretty much in line with

00:33:48.870 --> 00:33:51.280
the intuitions of most.

00:33:51.280 --> 00:33:54.790
But there's a problem.

00:33:54.790 --> 00:33:58.290
Case number three, trolley
bystander.

00:33:58.290 --> 00:33:59.295
Here's Jim.

00:33:59.295 --> 00:34:00.530
Poor Jim.

00:34:00.530 --> 00:34:01.560
Really bad luck.

00:34:01.560 --> 00:34:03.860
First he shows up in this Latin
American town, and he's

00:34:03.860 --> 00:34:05.429
supposed to shoot
some Indians.

00:34:05.429 --> 00:34:08.500
Now here he is, next to a
trolley which is hurtling down

00:34:08.500 --> 00:34:11.239
a track, about to kill
five people.

00:34:11.239 --> 00:34:14.840
But here there is, in the middle
of the track, a switch

00:34:14.840 --> 00:34:19.582
that if Him turns will cause the
trolley to kill the one.

00:34:19.582 --> 00:34:20.659
Question.

00:34:20.659 --> 00:34:26.610
For Jim, the bystander, is it
morally mandatory for him to

00:34:26.610 --> 00:34:29.519
turn the trolley so that instead
of the trolley hitting

00:34:29.519 --> 00:34:32.215
the five, it hits the one?

00:34:32.215 --> 00:34:36.147
Is it morally permitted, but not
morally mandatory for him

00:34:36.147 --> 00:34:37.350
to turn the trolley?

00:34:37.350 --> 00:34:38.770
That's answer two.

00:34:38.770 --> 00:34:45.920
Or is it morally prohibited for
him to turn the trolley?

00:34:45.920 --> 00:34:48.120
See how those numbers
come out.

00:34:58.280 --> 00:34:59.050
OK.

00:34:59.050 --> 00:35:00.400
And here are your numbers.

00:35:00.400 --> 00:35:02.940
15, 70, 15.

00:35:02.940 --> 00:35:08.020
These are very, very similar to
the distribution of answers

00:35:08.020 --> 00:35:11.050
that you gave in the
driver case.

00:35:11.050 --> 00:35:15.350
In the driver case, 63% of you
thought it was morally

00:35:15.350 --> 00:35:17.940
permitted, whereas here
70% of you think

00:35:17.940 --> 00:35:19.770
it's morally permitted.

00:35:19.770 --> 00:35:22.630
In the driver case,
30% of you thought

00:35:22.630 --> 00:35:24.240
it was morally mandatory.

00:35:24.240 --> 00:35:25.660
Here slightly fewer
of you think it's

00:35:25.660 --> 00:35:29.200
morally mandatory, 15%.

00:35:29.200 --> 00:35:32.880
And in the driver case, 7% of
you thought it was prohibited.

00:35:32.880 --> 00:35:36.270
Here 15% of you think it's
morally prohibited.

00:35:36.270 --> 00:35:40.330
So there's a little change,
but not a lot of change.

00:35:40.330 --> 00:35:43.900
Here's the problem.

00:35:43.900 --> 00:35:49.200
Remember that in Foot's analysis
of the case, we knew

00:35:49.200 --> 00:35:53.120
that letting five die was a
little bit bad, that killing

00:35:53.120 --> 00:35:56.650
one was worse, and
that killing five

00:35:56.650 --> 00:35:59.120
was worse than that.

00:35:59.120 --> 00:36:02.080
Trolley driver faced the choice
of killing one versus

00:36:02.080 --> 00:36:02.920
killing five.

00:36:02.920 --> 00:36:06.160
In transplant, you face the
choice of killing ones versus

00:36:06.160 --> 00:36:08.370
letting five die.

00:36:08.370 --> 00:36:14.220
But what's going on in
the bystander case?

00:36:14.220 --> 00:36:19.550
Well, in the bystander case,
Jim, Jim of the bad luck,

00:36:19.550 --> 00:36:24.790
faces a choice between killing
one, diverting the trolley

00:36:24.790 --> 00:36:31.310
onto the track in such a way
that Jim kills that guy--or

00:36:31.310 --> 00:36:34.460
letting five die.

00:36:34.460 --> 00:36:38.070
Letting the trolley hit the five
that it's going to hit

00:36:38.070 --> 00:36:40.420
inevitably.

00:36:40.420 --> 00:36:46.400
But in contrast to the doctor
case, where 85% of you thought

00:36:46.400 --> 00:36:51.460
it was prohibited to kill the
one in order to save the five

00:36:51.460 --> 00:36:58.472
who would otherwise die, in this
case, 85% of you think

00:36:58.472 --> 00:37:04.340
it's at least permitted to kill
the one in order to let

00:37:04.340 --> 00:37:05.430
the five die.

00:37:05.430 --> 00:37:07.370
Let me do that again for you.

00:37:07.370 --> 00:37:13.230
85% of you--watch the
bad-o-meter--think that it

00:37:13.230 --> 00:37:15.560
goes the other way.

00:37:15.560 --> 00:37:16.800
Now what's going on?

00:37:16.800 --> 00:37:19.140
We thought we had a solution
to the problem.

00:37:19.140 --> 00:37:22.530
The solution to the problem that
differentiated transplant

00:37:22.530 --> 00:37:25.730
from trolley driver was the
distinction between killing

00:37:25.730 --> 00:37:27.320
and letting die.

00:37:27.320 --> 00:37:30.500
And all of a sudden, there's a
whole bunch of you who seem to

00:37:30.500 --> 00:37:35.890
be saying about bystander that
letting five die is worse than

00:37:35.890 --> 00:37:37.270
killing one.

00:37:37.270 --> 00:37:39.750
You must think that, or you
wouldn't think that it's

00:37:39.750 --> 00:37:45.780
morally, at least, permitted for
him to turn the trolley.

00:37:45.780 --> 00:37:52.190
Moreover, stuff gets
even worse.

00:37:52.190 --> 00:37:58.500
Suppose that the hospital case
comes about as follows.

00:37:58.500 --> 00:38:03.540
Five healthy individuals show
up at the hospitals, and a

00:38:03.540 --> 00:38:08.690
doctor--either because he's
tired or because he wants to

00:38:08.690 --> 00:38:12.850
get the insurance benefits of
which he is a beneficiary if

00:38:12.850 --> 00:38:14.420
there are a lot of sick
patients in his

00:38:14.420 --> 00:38:18.980
hospital--poisons the five who
show up, in such a way that

00:38:18.980 --> 00:38:20.720
one of them needs a liver, one
of them needs a kidney, one of

00:38:20.720 --> 00:38:22.905
them needs lungs, one of them
needs a heart, and one of them

00:38:22.905 --> 00:38:24.440
needs a brain.

00:38:24.440 --> 00:38:30.590
And so as a result of what that
man has done, these five

00:38:30.590 --> 00:38:33.620
individuals will die.

00:38:33.620 --> 00:38:37.422
And it's a few hours later,
and he thinks, "Oh!

00:38:37.422 --> 00:38:39.670
I forgot about the categorical
imperative!

00:38:39.670 --> 00:38:41.730
Shoot!

00:38:41.730 --> 00:38:46.620
What am I going to do?" And up
shows a healthy individual,

00:38:46.620 --> 00:38:50.700
and he thinks, "Oh, God, I've
got a solution, here, I can

00:38:50.700 --> 00:38:51.450
chop him up.

00:38:51.450 --> 00:38:56.320
Heart, lung, kidneys,
liver, brain, and I

00:38:56.320 --> 00:38:58.480
can save the five.

00:38:58.480 --> 00:38:59.760
OK."

00:38:59.760 --> 00:39:00.260
Question.

00:39:00.260 --> 00:39:02.250
For the doctor who has poisoned
the five individuals

00:39:02.250 --> 00:39:04.900
who earlier showed up at the
hospital, who now faces the

00:39:04.900 --> 00:39:09.560
option of saving their lives by
killing the one, is it A,

00:39:09.560 --> 00:39:13.905
morally mandatory to chop up
the healthy man, B, morally

00:39:13.905 --> 00:39:16.190
permitted, but not morally
mandatory, to chop up the

00:39:16.190 --> 00:39:20.360
healthy man, or C, morally
prohibited to chop up the

00:39:20.360 --> 00:39:20.990
healthy man?

00:39:20.990 --> 00:39:23.830
And let's see how the
numbers come out.

00:39:32.130 --> 00:39:33.510
All right.

00:39:33.510 --> 00:39:35.490
So.

00:39:35.490 --> 00:39:38.710
82%, 11%, 7%.

00:39:38.710 --> 00:39:41.550
So your numbers here are almost
identical to what they

00:39:41.550 --> 00:39:44.580
were in the original
doctor case.

00:39:44.580 --> 00:39:50.050
There it was 6, 9, 85,
here it's 7, 11, 82.

00:39:50.050 --> 00:39:52.910
Almost no difference.

00:39:52.910 --> 00:39:57.000
But let's go back to
our bad-o-meter.

00:39:57.000 --> 00:39:59.880
We are going to set aside the
killing and letting die

00:39:59.880 --> 00:40:03.620
question, and just look at the
kill one versus kill five.

00:40:03.620 --> 00:40:07.020
So we know from trolley
driver that killing

00:40:07.020 --> 00:40:09.360
one is pretty bad.

00:40:09.360 --> 00:40:14.940
But according to most of you,
according to you 93% of you,

00:40:14.940 --> 00:40:20.000
killing five is worse
than killing one.

00:40:20.000 --> 00:40:21.720
OK.

00:40:21.720 --> 00:40:24.090
Poison doctor.

00:40:24.090 --> 00:40:28.600
So here's what choice
the doctor faces.

00:40:28.600 --> 00:40:31.880
He can kill five--

00:40:31.880 --> 00:40:32.550
right?

00:40:32.550 --> 00:40:36.194
He's poisoned them, and now
they're going to die.

00:40:36.194 --> 00:40:39.090
Or he can kill just one--that
one healthy guy

00:40:39.090 --> 00:40:40.782
who just showed up.

00:40:40.782 --> 00:40:44.550
And then the other
five won't die.

00:40:44.550 --> 00:40:52.520
82% of you told me that it was
better for him to kill five

00:40:52.520 --> 00:40:53.810
than to kill one.

00:40:53.810 --> 00:40:57.370
Let me show you this again,
on the bad-o-meter.

00:40:57.370 --> 00:41:03.230
85% of you thought that it was
better for him to kill five

00:41:03.230 --> 00:41:06.300
than to kill one.

00:41:06.300 --> 00:41:09.020
So we have these two
super-duper, excellent

00:41:09.020 --> 00:41:12.345
principles that seem to explain
what was going on in

00:41:12.345 --> 00:41:13.820
the trolley case.

00:41:13.820 --> 00:41:17.460
On the one hand, that killing
one was worse than letting

00:41:17.460 --> 00:41:19.550
five die, and then all of a
sudden, bystander made us

00:41:19.550 --> 00:41:22.971
think, oh no, we don't
have that intuition.

00:41:22.971 --> 00:41:26.230
And then we have the intuition
that at least killing one was

00:41:26.230 --> 00:41:30.340
better than killing five, and
the poison case made us

00:41:30.340 --> 00:41:33.150
rethink that as well.

00:41:33.150 --> 00:41:37.660
Now, there's an obvious issue
that maybe making the moral

00:41:37.660 --> 00:41:38.990
difference here.

00:41:38.990 --> 00:41:42.390
There's a temporal difference
between the killing of the one

00:41:42.390 --> 00:41:45.190
and the killing of the
five happened.

00:41:45.190 --> 00:41:49.440
And perhaps, says Thomson,
perhaps that's what explains

00:41:49.440 --> 00:41:53.450
our intuition in the
doctor case.

00:41:53.450 --> 00:41:58.280
Perhaps it is because the
killing of the five has become

00:41:58.280 --> 00:42:02.910
a letting die, as the result of
time, that it's misleading

00:42:02.910 --> 00:42:08.020
to describe this as a kill one
versus kill five case.

00:42:08.020 --> 00:42:13.180
But the temporal is not going to
help us with the transplant

00:42:13.180 --> 00:42:15.880
versus bystander case.

00:42:15.880 --> 00:42:21.770
Those seem pretty clearly to be
both cases where one faces

00:42:21.770 --> 00:42:26.920
the choice between killing
one and letting five die.

00:42:26.920 --> 00:42:30.530
And whereas it seemed pretty
clear to most of you in

00:42:30.530 --> 00:42:33.370
transplant that killing one was
worse than letting five

00:42:33.370 --> 00:42:36.810
die, it seems pretty clear
that for most of you in

00:42:36.810 --> 00:42:41.830
bystander, it's the
other way around.

00:42:41.830 --> 00:42:46.920
So the trolley problem is the
problem raised by these

00:42:46.920 --> 00:42:49.691
dancing arrows.

00:42:49.691 --> 00:42:55.710
How is it that we systematize
our intuitions about killing

00:42:55.710 --> 00:42:59.360
and letting die, given that they
appear to come apart in

00:42:59.360 --> 00:43:01.150
these cases?

00:43:01.150 --> 00:43:06.390
So Thomson suggests that whereas
the transplant case

00:43:06.390 --> 00:43:08.680
gives us a choice between
killing one and letting five

00:43:08.680 --> 00:43:13.740
die, as does bystander, there
is a potentially relevant

00:43:13.740 --> 00:43:16.190
difference between them.

00:43:16.190 --> 00:43:20.910
And that is that in the case of
transplant, you are using

00:43:20.910 --> 00:43:25.050
the one as a means.

00:43:25.050 --> 00:43:29.717
You're using the one as a way
to achieve the outcome of

00:43:29.717 --> 00:43:31.720
saving the other five.

00:43:31.720 --> 00:43:35.370
Whereas in bystander, when
you're diverting the trolley

00:43:35.370 --> 00:43:38.750
onto the track where that one
individual is, you're not

00:43:38.750 --> 00:43:41.290
using that individual as a
means, and--oh my goodness, I

00:43:41.290 --> 00:43:43.770
told you we'd get back
to Kant, and we have!

00:43:43.770 --> 00:43:45.805
What did Kant's formula
of humanity say?

00:43:45.805 --> 00:43:49.400
Kant's formula of humanity
said, "so act as to treat

00:43:49.400 --> 00:43:52.910
humanity, whether in your own
person or that of any other,

00:43:52.910 --> 00:43:58.370
in every case as an end, and
never merely as a means only."

00:43:58.370 --> 00:44:00.680
So maybe that's our solution.

00:44:00.680 --> 00:44:05.690
Maybe the problem in bystander
is that since you're not

00:44:05.690 --> 00:44:09.450
treating him as a means, it's
OK to kill the one.

00:44:09.450 --> 00:44:11.994
Whereas in transplant, since
you are treating him as a

00:44:11.994 --> 00:44:16.700
means, it's not OK to kill the
one, and consequently, you're

00:44:16.700 --> 00:44:21.210
morally obliged to what
the five die.

00:44:21.210 --> 00:44:25.960
Well, says Thomson, that
can't be quite right.

00:44:25.960 --> 00:44:30.200
Suppose that you're Jim,
standing next to the trolley.

00:44:30.200 --> 00:44:32.420
Trolley's on its usual path
to kill the five.

00:44:32.420 --> 00:44:35.490
But here, instead of the
straight track on which the

00:44:35.490 --> 00:44:40.300
one sits, there's a looped
track, and the one is in the

00:44:40.300 --> 00:44:44.840
middle of the track in such a
way that if you divert the

00:44:44.840 --> 00:44:49.770
trolley, it will hit him,
thereby saving the five.

00:44:49.770 --> 00:44:51.020
Question.

00:44:51.020 --> 00:44:56.790
In the case that Thomson calls
"loop," is it A, morally

00:44:56.790 --> 00:45:00.000
mandatory to turn the
trolley, that's one.

00:45:00.000 --> 00:45:03.400
B, morally permitted, but not
morally mandatory, or C,

00:45:03.400 --> 00:45:04.150
morally prohibited?

00:45:04.150 --> 00:45:06.360
So remember, trolley's
heading down the

00:45:06.360 --> 00:45:08.450
track towards the five.

00:45:08.450 --> 00:45:09.800
You're Jim.

00:45:09.800 --> 00:45:12.807
The trolley's going to hit the
five, or you can divert the

00:45:12.807 --> 00:45:16.760
trolley onto the track with the
one, and because the one

00:45:16.760 --> 00:45:23.710
is there, you will cause
the trolley to stop.

00:45:23.710 --> 00:45:23.925
OK?

00:45:23.925 --> 00:45:26.940
So let's see how the numbers
come out on this.

00:45:37.910 --> 00:45:39.420
OK.

00:45:39.420 --> 00:45:43.830
So 68, 18, 14.

00:45:43.830 --> 00:45:47.250
Not a lot of difference.

00:45:47.250 --> 00:45:52.280
You've answered bystander in
loop in almost the same way

00:45:52.280 --> 00:45:54.750
that you've answered all our
other trolley cases.

00:45:54.750 --> 00:45:58.900
Generally the distribution
has been 15, 70, 15.

00:45:58.900 --> 00:46:00.150
That was bystander.

00:46:00.150 --> 00:46:03.370
Here it's 14, 68, 18.

00:46:03.370 --> 00:46:08.100
But notice that in this case,
you were using the guy on the

00:46:08.100 --> 00:46:10.260
loop track as a means!

00:46:10.260 --> 00:46:12.580
You're using him as a
means to your end.

00:46:12.580 --> 00:46:17.650
You're trying to stop the
trolley by using his body.

00:46:17.650 --> 00:46:21.400
Kant didn't help us enough!

00:46:21.400 --> 00:46:22.380
Restock.

00:46:22.380 --> 00:46:25.100
Let's take stock again,
says Thomson.

00:46:25.100 --> 00:46:28.640
Perhaps some of the work
is being done by

00:46:28.640 --> 00:46:32.200
some notion of rights.

00:46:32.200 --> 00:46:36.470
Perhaps what's going on in the
transplant case, the one where

00:46:36.470 --> 00:46:39.110
you guys are not going to let
the doctor chop up the healthy

00:46:39.110 --> 00:46:42.860
man, is that you would be
violating that man's rights.

00:46:42.860 --> 00:46:46.970
And perhaps it's true that
"rights trump utility." That

00:46:46.970 --> 00:46:50.830
is, that when somebody has a
right to bodily integrity,

00:46:50.830 --> 00:46:54.750
that takes precedence over
the needs of the many.

00:46:54.750 --> 00:46:58.330
The utilities of the five
that are going to saved.

00:46:58.330 --> 00:47:02.190
And we'll close this lecture
with the final example, one

00:47:02.190 --> 00:47:05.600
that's meant to test
that hypothesis.

00:47:05.600 --> 00:47:08.580
And we'll begin next lecture
like by talking about some of

00:47:08.580 --> 00:47:11.810
the reasons that people tend
to give this response.

00:47:11.810 --> 00:47:14.730
So suppose now that instead
of the looping

00:47:14.730 --> 00:47:17.740
track, there's a bridge.

00:47:17.740 --> 00:47:22.170
And suppose that on
that bridge is our

00:47:22.170 --> 00:47:24.870
fairly large gentleman.

00:47:24.870 --> 00:47:29.390
And you are now faced with
the following dilemma.

00:47:29.390 --> 00:47:31.020
The trolley's heading
down the track.

00:47:31.020 --> 00:47:32.950
It's about to kill the five.

00:47:32.950 --> 00:47:36.970
And here's how Jim the bystander
could stop it.

00:47:36.970 --> 00:47:41.930
He could push the fat man off
the bridge, and thereby cause

00:47:41.930 --> 00:47:45.850
the trolley to be stopped in
its tracks by his weight.

00:47:45.850 --> 00:47:47.170
Question.

00:47:47.170 --> 00:47:51.770
For the bystander in fat man,
is it morally mandatory to

00:47:51.770 --> 00:47:55.020
push the fat man, morally
permitted but not morally

00:47:55.020 --> 00:47:58.170
mandatory, or morally
prohibited to

00:47:58.170 --> 00:47:59.890
push the fact man?

00:47:59.890 --> 00:48:02.990
And let's take responses, and
I'll leave you with those

00:48:02.990 --> 00:48:07.760
numbers and a remark about
them as our close.

00:48:07.760 --> 00:48:13.050
So let's see whether we get any
shift in the fat man case.

00:48:13.050 --> 00:48:14.256
My goodness!

00:48:14.256 --> 00:48:16.560
That looks awfully different.

00:48:16.560 --> 00:48:18.370
What is going on?

00:48:18.370 --> 00:48:21.115
So remember, our classic
distribution is that we have

00:48:21.115 --> 00:48:25.170
roughly 70% here, and no more
than 10% in the prohibited.

00:48:25.170 --> 00:48:28.620
All of a sudden, 78% of you
think it's prohibited.

00:48:28.620 --> 00:48:30.232
What's going on?

00:48:30.232 --> 00:48:31.600
Cliffhanger!

00:48:31.600 --> 00:48:34.420
We'll talk about
it on Tuesday.

