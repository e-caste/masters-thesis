WEBVTT
Kind: captions
Language: en

00:00:00.160 --> 00:00:00.330
PROFESSOR: OK.

00:00:00.330 --> 00:00:04.680
So I want to start out by
finishing off the discussion

00:00:04.680 --> 00:00:10.540
that we began last class about
ways of thinking about the

00:00:10.540 --> 00:00:13.350
perplexity that the trolley
case gives rise to.

00:00:13.350 --> 00:00:16.290
And you'll remember that the
perplexities that the trolley

00:00:16.290 --> 00:00:20.600
case gives rise to is that
there's an apparent asymmetry

00:00:20.600 --> 00:00:24.670
in our responses to the
bystander case and the fat man

00:00:24.670 --> 00:00:29.720
case, even though both of them
seem arguably to involve

00:00:29.720 --> 00:00:33.700
killing one in order
to save five.

00:00:33.700 --> 00:00:38.540
And we looked, last class, at
Judy Thomson's response, which

00:00:38.540 --> 00:00:42.100
says, look, there's no asymmetry
in the two cases,

00:00:42.100 --> 00:00:45.960
because when we reflect on the
additional hypothetical case

00:00:45.960 --> 00:00:48.790
where there's a third track on
which you, yourself, are

00:00:48.790 --> 00:00:52.290
standing, we come to recognize
that it's not morally

00:00:52.290 --> 00:00:55.390
acceptable to turn in Bystander,
just as it's not

00:00:55.390 --> 00:00:58.500
morally acceptable to
push in Fat Man.

00:00:58.500 --> 00:01:01.090
At the other extreme, we
looked at Josh Greene's

00:01:01.090 --> 00:01:04.990
response, which was that just as
it's morally acceptable to

00:01:04.990 --> 00:01:08.600
turn the trolley in Bystander,
it's morally acceptable to

00:01:08.600 --> 00:01:10.600
push the man in Fat Man.

00:01:10.600 --> 00:01:13.330
And to the extent that we're
getting differential responses

00:01:13.330 --> 00:01:17.800
in those cases, says Greene,
it's due to the fact that the

00:01:17.800 --> 00:01:22.500
emotional part of our brain
response mechanism is

00:01:22.500 --> 00:01:26.210
activated by the up close and
personal nature of the fat man

00:01:26.210 --> 00:01:31.300
case, and as a result, we give
an answer that he thinks

00:01:31.300 --> 00:01:33.830
remains morally unjustified.

00:01:33.830 --> 00:01:36.860
And what we started to think
about at the end of last

00:01:36.860 --> 00:01:39.570
lecture was a third possibility,
which lies

00:01:39.570 --> 00:01:42.700
somewhere in between the Thomson
and the Greene, though

00:01:42.700 --> 00:01:44.080
closer to the Greene.

00:01:44.080 --> 00:01:47.780
And that's Cass Sunstein's
argument that though our

00:01:47.780 --> 00:01:51.520
responses differ, and perhaps
differ in ways that will be

00:01:51.520 --> 00:01:55.070
impossible for us to change,
the cases are

00:01:55.070 --> 00:01:57.090
the same, deep down.

00:01:57.090 --> 00:02:01.900
And he's inclined, though not as
certain as Josh Greene is,

00:02:01.900 --> 00:02:05.420
to think that if we want the
cases to come together, what

00:02:05.420 --> 00:02:07.210
we ought to do is to
push the fat man.

00:02:07.210 --> 00:02:10.150
And you'll recall that
his argument there

00:02:10.150 --> 00:02:11.370
proceeded as follows.

00:02:11.370 --> 00:02:13.900
He suggested that in that
in non-moral cases, it's

00:02:13.900 --> 00:02:17.420
uncontroversial that we make
use of heuristics, and that

00:02:17.420 --> 00:02:21.210
those heuristics, though useful,
frequently lead us to

00:02:21.210 --> 00:02:26.320
errors, and then went on to
contend that just as this

00:02:26.320 --> 00:02:29.330
occurs in non-moral cases,
so too does it

00:02:29.330 --> 00:02:30.840
occur in moral cases.

00:02:30.840 --> 00:02:35.820
And we left at the end of last
class thinking about what goes

00:02:35.820 --> 00:02:40.710
on in Sunstein argument that
in moral cases people often

00:02:40.710 --> 00:02:42.210
use heuristics.

00:02:42.210 --> 00:02:45.530
And you will call that he gave
a couple of examples from

00:02:45.530 --> 00:02:50.860
Jonathan Haidt's work of cases
where people were expressing

00:02:50.860 --> 00:02:53.950
moral disapprobation toward
actions for which they could

00:02:53.950 --> 00:02:55.730
find no justification.

00:02:55.730 --> 00:02:59.110
So consensual incest between
siblings, cleaning your

00:02:59.110 --> 00:03:01.760
bathroom floor with the American
flag: People were

00:03:01.760 --> 00:03:04.190
inclined to find those morally
problematic, and to find them

00:03:04.190 --> 00:03:08.470
morally problematic even when,
if pushed, they were unable to

00:03:08.470 --> 00:03:13.030
articulate what moral rule
those things violated.

00:03:13.030 --> 00:03:18.510
And what Sunstein suggests in
the paper is that in general,

00:03:18.510 --> 00:03:22.330
we can look at the heuristics
and biases literature and see

00:03:22.330 --> 00:03:27.790
instance after instance where
the framing of a case affects

00:03:27.790 --> 00:03:31.600
our response to the case in ways
that they do in non-moral

00:03:31.600 --> 00:03:34.340
cases, in moral cases as well.

00:03:34.340 --> 00:03:38.910
So you'll recall back in the
third lecture, right when we

00:03:38.910 --> 00:03:40.910
were learning how to use our
clickers, which I should tell

00:03:40.910 --> 00:03:43.420
you, we're going to use a bit in
this lecture, so you should

00:03:43.420 --> 00:03:45.110
take out your clickers.

00:03:45.110 --> 00:03:47.709
When we were first starting to
learn our clickers, we were

00:03:47.709 --> 00:03:50.700
presented with the famous Asian
disease case, which is

00:03:50.700 --> 00:03:52.310
the case that runs as follows.

00:03:52.310 --> 00:03:55.010
A terrible disease has
struck 600 people

00:03:55.010 --> 00:03:55.940
in your town, right?

00:03:55.940 --> 00:03:59.390
So there's 600 people in your
town who are destined to die.

00:03:59.390 --> 00:04:01.610
You are the mayor, and two
courses of treatment are

00:04:01.610 --> 00:04:04.510
available, plan A or plan B.

00:04:04.510 --> 00:04:07.690
And I asked half of you to look
at the green side of the

00:04:07.690 --> 00:04:11.220
description, which says that
plan A is the one where 200

00:04:11.220 --> 00:04:14.150
people will live, whereas plan
B is one where there's a

00:04:14.150 --> 00:04:17.460
one-third probability that 600
will live, and a two-thirds

00:04:17.460 --> 00:04:19.290
probability that no
one will live.

00:04:19.290 --> 00:04:23.890
And the other half of you looked
at the exact same plan,

00:04:23.890 --> 00:04:27.230
but described not in terms of
who will live, but in terms of

00:04:27.230 --> 00:04:28.180
who will die.

00:04:28.180 --> 00:04:34.480
So plan A says, 200 of the 600
people will live, which means

00:04:34.480 --> 00:04:36.100
400 will die.

00:04:36.100 --> 00:04:41.230
And here plan A means 400 of the
600 people will die, which

00:04:41.230 --> 00:04:43.970
means 200 of the people
will live.

00:04:43.970 --> 00:04:47.560
Nonetheless, and this was our
very first clicker response,

00:04:47.560 --> 00:04:51.680
the attitudes that you had
towards the cases differed,

00:04:51.680 --> 00:04:54.760
whereas when it was presented
as the number of people who

00:04:54.760 --> 00:04:59.880
will live, 66% of you went with
plan A, and only 34% with

00:04:59.880 --> 00:05:04.750
plan B. When we inverted the
framing, the numbers came out

00:05:04.750 --> 00:05:06.720
exactly the opposite.

00:05:06.720 --> 00:05:10.880
So 66 of you favored plan A in
the green case, 64 of you

00:05:10.880 --> 00:05:12.840
favored plan A in
the blue case.

00:05:12.840 --> 00:05:17.500
But plan A and plan B are
mathematically identical.

00:05:17.500 --> 00:05:22.690
So perhaps something like this
is what's going on in the

00:05:22.690 --> 00:05:24.000
trolley cases.

00:05:24.000 --> 00:05:26.950
And this is not a real clicker
example, but imagine you were

00:05:26.950 --> 00:05:28.530
presented with the
following case.

00:05:28.530 --> 00:05:32.190
A terrible trolley is hurtling
down the tracks towards six

00:05:32.190 --> 00:05:33.530
people in your town.

00:05:33.530 --> 00:05:36.600
You are the mayor, and two
courses for the trolley are

00:05:36.600 --> 00:05:38.890
available for you, plan
A and plan B.

00:05:38.890 --> 00:05:43.850
And then I present you plan A,
one person will be spared,

00:05:43.850 --> 00:05:46.810
which means, of course, that
five people will die.

00:05:46.810 --> 00:05:50.180
Or plan B, that one person
will die, which means, of

00:05:50.180 --> 00:05:52.610
course, that five people
will be spared.

00:05:52.610 --> 00:05:56.510
And there's an inclination, I
think, to go with plan B in

00:05:56.510 --> 00:05:59.170
the blue case and plan A in
the A [correction: green]

00:05:59.170 --> 00:05:59.976
case.

00:05:59.976 --> 00:06:02.870
And this generalizes.

00:06:02.870 --> 00:06:06.760
Depending on who we're focusing
on in these moral

00:06:06.760 --> 00:06:10.180
dilemmas, we have different
responses to them.

00:06:10.180 --> 00:06:13.150
If we think about Josh Greene's
crying baby case,

00:06:13.150 --> 00:06:17.470
where you're locked in a
basement with 19 others and

00:06:17.470 --> 00:06:21.920
your crying baby, surrounded
by enemy soldiers who will

00:06:21.920 --> 00:06:25.890
kill you if you are found, the
dilemma that Greene presents

00:06:25.890 --> 00:06:30.820
subjects with is, should you
smother the baby, whose cries

00:06:30.820 --> 00:06:34.440
will call the soldiers with
certainty to your hiding

00:06:34.440 --> 00:06:38.150
location and cause them
to kill all 20 of you?

00:06:38.150 --> 00:06:41.430
So very much like the Jim and
the Indians case, but with an

00:06:41.430 --> 00:06:44.750
even more painful premise.

00:06:44.750 --> 00:06:48.570
If you focus your attention in
that case to the experience of

00:06:48.570 --> 00:06:53.250
putting your hand over the mouth
of your screaming child,

00:06:53.250 --> 00:06:58.370
it is virtually impossible to
judge that as the thing that

00:06:58.370 --> 00:07:00.460
is morally required.

00:07:00.460 --> 00:07:04.260
But if you redirect your
attention even a tiny bit

00:07:04.260 --> 00:07:07.410
towards the two year old next to
you, and the four year old

00:07:07.410 --> 00:07:10.570
next to her, and the old man
in the other corner of the

00:07:10.570 --> 00:07:14.530
room, all of whom will die if
you don't take this action

00:07:14.530 --> 00:07:20.130
towards the baby, your response
to the case shifts.

00:07:20.130 --> 00:07:26.480
And the shiftiness in the
direction of our attention is

00:07:26.480 --> 00:07:30.110
something that's going to
be endemic to all of

00:07:30.110 --> 00:07:32.660
these kinds of cases.

00:07:32.660 --> 00:07:36.440
To some extent, we're able only
to focus on part of the

00:07:36.440 --> 00:07:39.060
world at a time.

00:07:39.060 --> 00:07:43.550
And As a result of that, it's
incredibly difficult to hold

00:07:43.550 --> 00:07:48.440
in focus in a way that makes
them seem stable--these kinds

00:07:48.440 --> 00:07:51.270
of moral dilemmas.

00:07:51.270 --> 00:07:55.990
So Sunstein's suggestion is that
this phenomenon, whereby

00:07:55.990 --> 00:07:58.270
features that have got to
be morally irrelevant--

00:07:58.270 --> 00:07:58.560
right?

00:07:58.560 --> 00:08:01.800
It can't be morally relevant to
what's the right thing to

00:08:01.800 --> 00:08:05.730
do in the trolley case whether
you frame it in terms of the

00:08:05.730 --> 00:08:09.270
number who will live or the
number who will die.

00:08:09.270 --> 00:08:12.880
At least, prima facie, that
doesn't seem like the kind of

00:08:12.880 --> 00:08:13.929
thing that could be relevant.

00:08:13.929 --> 00:08:16.890
You're making exactly the same
decision framed in two

00:08:16.890 --> 00:08:17.690
different ways.

00:08:17.690 --> 00:08:21.360
How could that be what
makes the difference?

00:08:21.360 --> 00:08:25.060
Sunstein's suggestion is that
the mechanism that underlies

00:08:25.060 --> 00:08:28.830
the phenomenon that I've just
described happens over and

00:08:28.830 --> 00:08:33.100
over and over again, not just
in hypothetical trolley

00:08:33.100 --> 00:08:38.000
problem-style cases, but all the
time in the kind of moral

00:08:38.000 --> 00:08:41.890
reasoning that we engage in as
citizens of a democracy,

00:08:41.890 --> 00:08:45.590
trying to make judgments about
distributions of resources,

00:08:45.590 --> 00:08:48.480
trying to make judgments about
what sorts of laws should be

00:08:48.480 --> 00:08:51.050
put in place to regulate
or incentivize

00:08:51.050 --> 00:08:53.670
certain kinds of behavior.

00:08:53.670 --> 00:08:58.840
So in each of the following four
domains, says Sunstein,

00:08:58.840 --> 00:09:02.470
we very often focus on
heuristics, that is, the

00:09:02.470 --> 00:09:05.590
surface features of the
phenomenon, rather than the

00:09:05.590 --> 00:09:08.230
target attributes, that is, the
thing that we ultimately

00:09:08.230 --> 00:09:08.770
care about.

00:09:08.770 --> 00:09:12.510
Remember I talked last class
about putting a skin on your

00:09:12.510 --> 00:09:15.830
phone so that it's easily
recognizable, that gives you

00:09:15.830 --> 00:09:18.830
heuristic access to which
phone is yours.

00:09:18.830 --> 00:09:23.190
But of course that decoration
on your phone is useful as a

00:09:23.190 --> 00:09:26.660
way of finding your phone only
in so far as it tracks the

00:09:26.660 --> 00:09:29.670
target attribute that you care
about, namely, finding the

00:09:29.670 --> 00:09:32.330
phone which has in it the phone
numbers that you care

00:09:32.330 --> 00:09:33.130
about having.

00:09:33.130 --> 00:09:35.980
And when targets and heuristics
come apart, we're

00:09:35.980 --> 00:09:37.450
in trouble.

00:09:37.450 --> 00:09:41.160
So, says Sunstein, when we're
thinking about risk

00:09:41.160 --> 00:09:45.290
regulation, that is, what do
we do with the fact that as

00:09:45.290 --> 00:09:48.580
human beings, lots and lots
of the stuff we do has the

00:09:48.580 --> 00:09:51.820
potential for causing harm, but
we don't want to spend our

00:09:51.820 --> 00:09:56.940
lives wrapped in large amount
of Styrofoam, moving very

00:09:56.940 --> 00:10:00.020
slowly through the world so as
not to bump into things.

00:10:00.020 --> 00:10:04.630
Given that we are willing to
take risks, how is it that our

00:10:04.630 --> 00:10:07.810
tendency to use heuristics
interacts with our

00:10:07.810 --> 00:10:09.590
regulation of them?

00:10:09.590 --> 00:10:12.220
In cases of punishment, and this
is the first topic that

00:10:12.220 --> 00:10:16.220
we'll turn to after break,
Sunstein thinks we use

00:10:16.220 --> 00:10:19.030
heuristics in ways that
cause us to behave in

00:10:19.030 --> 00:10:22.670
counterproductive ways in
punishing both individuals and

00:10:22.670 --> 00:10:24.110
aggregates.

00:10:24.110 --> 00:10:29.640
In our hesitation to make
certain kinds of choices in

00:10:29.640 --> 00:10:34.340
the area of reproductive
medicine, thinks Sunstein, we

00:10:34.340 --> 00:10:37.990
risk mistaking the heuristics
for the target.

00:10:37.990 --> 00:10:43.400
And in taking the act-omission
distinction so seriously, we

00:10:43.400 --> 00:10:46.950
risk mistaking heuristics
for targets.

00:10:46.950 --> 00:10:50.750
So we'll turn to the issue of
punishment right after break,

00:10:50.750 --> 00:10:54.760
and we'll turn to the issue of
act-omission in the later part

00:10:54.760 --> 00:10:55.640
of the lecture.

00:10:55.640 --> 00:10:59.130
What I want to do right now is
to run through three examples

00:10:59.130 --> 00:11:03.160
of risk regulation via
Sunstein's analysis.

00:11:03.160 --> 00:11:04.440
And the third of these--

00:11:04.440 --> 00:11:07.360
I'm actually really curious, and
so I want to see how the

00:11:07.360 --> 00:11:09.240
clicker numbers come out.

00:11:09.240 --> 00:11:12.570
So Sunstein points out, and it
seems to me that he's exactly

00:11:12.570 --> 00:11:17.920
right, that people are more
likely to condemn a company

00:11:17.920 --> 00:11:20.910
when their behavior is described
in ways that involve

00:11:20.910 --> 00:11:23.820
certainty than in ways
that involve risk.

00:11:23.820 --> 00:11:27.320
So take company A, which
produces a product that 10

00:11:27.320 --> 00:11:31.590
million people use, which
will kill 10 people.

00:11:31.590 --> 00:11:35.390
Of the 10 million people who
make use of this product, 10

00:11:35.390 --> 00:11:38.850
of them will have a reaction
to it of a kind that will

00:11:38.850 --> 00:11:40.570
cause them to die.

00:11:40.570 --> 00:11:45.150
And the cost of eliminating that
risk entirely would be

00:11:45.150 --> 00:11:47.480
$100 million.

00:11:47.480 --> 00:11:51.460
There is a feeling, an
inclination, at least, on the

00:11:51.460 --> 00:11:55.060
part of many, to think that the
company ought to spend its

00:11:55.060 --> 00:11:58.430
money getting rid of that risk;
that it's unacceptable

00:11:58.430 --> 00:12:02.290
to produce a product when 10
people are going to die.

00:12:02.290 --> 00:12:07.160
By contrast, if you frame
the case in terms of

00:12:07.160 --> 00:12:11.270
probabilities, that 10 million
people use the product, that

00:12:11.270 --> 00:12:15.150
it produces a risk of death of
one per million, and the risk

00:12:15.150 --> 00:12:19.030
elimination is exactly as
costly, this is the sort of

00:12:19.030 --> 00:12:22.010
thing that we allow
all the time.

00:12:22.010 --> 00:12:27.280
Without this sort of risk
tolerance, there would be no

00:12:27.280 --> 00:12:30.750
technological innovation, and
most of the goods and

00:12:30.750 --> 00:12:33.950
resources that all of us have
come to take for granted would

00:12:33.950 --> 00:12:36.570
never have come to be.

00:12:36.570 --> 00:12:39.910
So Sunstein's contention here
is that though the target

00:12:39.910 --> 00:12:44.050
attributes are identical in the
two cases, in this case,

00:12:44.050 --> 00:12:46.050
10 people are going to die,
and saving them would have

00:12:46.050 --> 00:12:49.470
cost $100 million, in this case,
10 people are going to

00:12:49.470 --> 00:12:53.150
die, and saving them would cost
$100 million, the target

00:12:53.150 --> 00:12:55.250
attributes are identical.

00:12:55.250 --> 00:12:59.300
In both cases, 10 people die,
and saving them would cost the

00:12:59.300 --> 00:13:00.740
amount specified.

00:13:00.740 --> 00:13:03.370
The heuristic attributes
differ.

00:13:03.370 --> 00:13:06.790
This one is framed in terms of
certainty, this one is framed

00:13:06.790 --> 00:13:07.980
in terms of risk.

00:13:07.980 --> 00:13:12.760
And we have a very good
heuristic that goes like this.

00:13:12.760 --> 00:13:14.700
If 10 people are going to
die from what you're

00:13:14.700 --> 00:13:17.230
doing, don't do it.

00:13:17.230 --> 00:13:22.150
And Sunstein's contention is
that the asymmetry in our

00:13:22.150 --> 00:13:26.460
response to these cases
is irrational.

00:13:26.460 --> 00:13:28.330
Indeed, if we lifted
this [second]

00:13:28.330 --> 00:13:32.695
one to a risk of two deaths
per million, and had this

00:13:32.695 --> 00:13:32.820
[first]

00:13:32.820 --> 00:13:39.470
one with a certainty of 10,
people would still be inclined

00:13:39.470 --> 00:13:47.370
to condemn the first choice,
even though in that case, the

00:13:47.370 --> 00:13:51.580
second choice is clearly
the worse one.

00:13:51.580 --> 00:13:55.280
So as a result of mistaking the
heuristic attributes for

00:13:55.280 --> 00:13:59.300
the target one, we make mistakes
in what sorts of

00:13:59.300 --> 00:14:01.710
behaviors we permit.

00:14:01.710 --> 00:14:05.210
Sunstein thinks that this is
what's going on in the case of

00:14:05.210 --> 00:14:07.760
emissions trading -- cap and
trade -- of which he was an

00:14:07.760 --> 00:14:09.370
early advocate.

00:14:09.370 --> 00:14:13.060
In the model of emissions
trading, polluters get given a

00:14:13.060 --> 00:14:17.090
license to pollute n units of
pollution into the air, and

00:14:17.090 --> 00:14:19.970
those licenses then get to be
traded on the market in such a

00:14:19.970 --> 00:14:24.780
way that, arguably, there's less
pollution at lower cost.

00:14:24.780 --> 00:14:28.130
Let's grant Sunstein the
economics there.

00:14:28.130 --> 00:14:32.890
Even so, there is resistance
to cap and trade.

00:14:32.890 --> 00:14:36.500
Because even if we're willing
to concede that the target

00:14:36.500 --> 00:14:37.170
attribute--

00:14:37.170 --> 00:14:39.750
namely, that we've reduced
the amount of pollution--

00:14:39.750 --> 00:14:42.590
is present, the heuristic
attribute--

00:14:42.590 --> 00:14:44.900
"People are paying to pollute?

00:14:44.900 --> 00:14:48.080
You shouldn't be able to pay
your way out of serious

00:14:48.080 --> 00:14:49.130
wrongdoing!" --

00:14:49.130 --> 00:14:52.850
strikes us as problematic.

00:14:52.850 --> 00:14:56.890
Now, it's an interesting
phenomenon that resistance to

00:14:56.890 --> 00:15:01.720
this sort of reasoning happens
depending on the context from

00:15:01.720 --> 00:15:04.980
both the right and the left.

00:15:04.980 --> 00:15:11.110
So there is resistance to
commoditization of things from

00:15:11.110 --> 00:15:16.920
the left, and there is
resistance from the right to

00:15:16.920 --> 00:15:20.990
certain other sorts of framing
that suggests that their

00:15:20.990 --> 00:15:24.370
responses in cases, for example,
of reproductive

00:15:24.370 --> 00:15:28.470
technologies like cloning, are
due, says Sunstein, to the

00:15:28.470 --> 00:15:31.310
heuristic, "don't play God."

00:15:31.310 --> 00:15:36.820
And when confronted with the
suggestion, you're just using

00:15:36.820 --> 00:15:43.620
a heuristic there, both sides
respond with hostility to the

00:15:43.620 --> 00:15:47.470
smarty-pants academic
analysis.

00:15:47.470 --> 00:15:53.830
In 1970s, it was common for
advocates of the buildup of

00:15:53.830 --> 00:15:57.270
nuclear arsenals to make appeal
to a notion called

00:15:57.270 --> 00:16:01.230
"mutually assured destruction"
that we'll talk about when we

00:16:01.230 --> 00:16:03.080
talk about the prisoners'
dilemma.

00:16:03.080 --> 00:16:07.030
The basic idea is that if both
sides have enough weapons to

00:16:07.030 --> 00:16:12.130
knock the other side out, then
neither will make use of them,

00:16:12.130 --> 00:16:16.370
because the deterrence function
is too great.

00:16:16.370 --> 00:16:21.220
There was resistance to that
analysis from the left,

00:16:21.220 --> 00:16:25.470
because it felt too clever.

00:16:25.470 --> 00:16:28.500
There is resistance to the
sort of analysis that

00:16:28.500 --> 00:16:32.890
Sunstein's posing here from both
sides, because it cuts

00:16:32.890 --> 00:16:39.109
against the idea that we are
introspectively transparent in

00:16:39.109 --> 00:16:42.730
such a way that our judgments
are indicative of the things

00:16:42.730 --> 00:16:45.099
that we care about.

00:16:45.099 --> 00:16:49.369
So the last example that I want
to give you from Sunstein

00:16:49.369 --> 00:16:51.900
is our poll.

00:16:51.900 --> 00:16:54.010
Sunstein hypothesizes--

00:16:54.010 --> 00:16:56.280
and are your clickers working?

00:16:56.280 --> 00:17:02.700
Sunstein hypothesizes that we
are more uncomfortable being

00:17:02.700 --> 00:17:06.240
harmed by things which are meant
to protect us than being

00:17:06.240 --> 00:17:09.840
harmed by things which aren't
meant to protect us.

00:17:09.840 --> 00:17:13.970
And he suggests that there is
data showing that if people

00:17:13.970 --> 00:17:16.240
are given a choice between
two cars--

00:17:16.240 --> 00:17:19.100
the first car is one where
there's a 2% chance if you're

00:17:19.100 --> 00:17:22.810
in an accident that you'll be
killed by the steering wheel.

00:17:22.810 --> 00:17:25.660
And the second is a car where
there's a 1% chance if you're

00:17:25.660 --> 00:17:28.250
in an accident, you'll be killed
by the steering wheel,

00:17:28.250 --> 00:17:31.800
but in addition, there's a 1/10
of 1% chance that the

00:17:31.800 --> 00:17:34.390
airbag will kill you.

00:17:34.390 --> 00:17:37.460
And the question is, which
car do you choose?

00:17:37.460 --> 00:17:39.840
The one where there's a 2%
chance that you'll be killed

00:17:39.840 --> 00:17:43.750
by the steering wheel, or the
one where there's a 1% chance

00:17:43.750 --> 00:17:45.840
that you'll be killed by the
steering wheel, but a 10th of

00:17:45.840 --> 00:17:49.900
a percent chance that you'll be
killed by the airbag, which

00:17:49.900 --> 00:17:51.460
was meant to protect you.

00:17:51.460 --> 00:17:54.170
And let's see how those
numbers come out.

00:17:54.170 --> 00:17:57.470
I have to say, I'm doing this
poll because my intuitions

00:17:57.470 --> 00:17:59.900
didn't line up with Sunstein's,
and I'm curious

00:17:59.900 --> 00:18:01.890
whether yours do.

00:18:01.890 --> 00:18:02.710
OK.

00:18:02.710 --> 00:18:06.010
Let's see how the numbers
came out.

00:18:06.010 --> 00:18:13.970
So 15% of you want to buy a car
A, and 85% of you want to

00:18:13.970 --> 00:18:19.090
buy car B. So 85% of you
are doing what is the

00:18:19.090 --> 00:18:21.360
statistically rational choice.

00:18:21.360 --> 00:18:26.920
But a good proportion of you
are willing to risk greater

00:18:26.920 --> 00:18:32.690
harm so as to avoid this feeling
of betrayal by that

00:18:32.690 --> 00:18:35.830
which is meant to protect.

00:18:35.830 --> 00:18:41.270
So Sunstein's suggestion, just
to sum up, is that in moral

00:18:41.270 --> 00:18:44.870
reasoning, frequently, we
substitute heuristic

00:18:44.870 --> 00:18:47.220
attributes for target ones.

00:18:47.220 --> 00:18:51.310
And to do so is a mistake.

00:18:51.310 --> 00:18:54.290
So what do the three responses
to the trolley problem that

00:18:54.290 --> 00:18:57.160
we've considered suggest?

00:18:57.160 --> 00:18:59.220
Well, what Thomson
says is this.

00:18:59.220 --> 00:19:01.940
She says, reconsidering our
intuitions in light of

00:19:01.940 --> 00:19:05.210
alternative cases, like the
alternative bystander case

00:19:05.210 --> 00:19:08.470
where you imagine yourself to
be one of the people on the

00:19:08.470 --> 00:19:11.170
track, reconsidering our
intuitions in light of

00:19:11.170 --> 00:19:15.150
alternative cases can lead to
shifts in our assessment of

00:19:15.150 --> 00:19:16.890
those cases.

00:19:16.890 --> 00:19:23.900
And those shifts in our
responses, she thinks, reveal

00:19:23.900 --> 00:19:26.750
something morally significant.

00:19:26.750 --> 00:19:31.110
We can learn from the
contemplation of those

00:19:31.110 --> 00:19:38.210
specific cases what it is that
morality demands of us.

00:19:38.210 --> 00:19:40.870
Greene and Sunstein, by
contrast, contend that our

00:19:40.870 --> 00:19:45.430
intuitive responses to cases
frequently track features that

00:19:45.430 --> 00:19:49.660
are morally irrelevant, and that
as a consequence, those

00:19:49.660 --> 00:19:55.400
features fail to reveal
something morally significant.

00:19:55.400 --> 00:19:58.470
The question is this.

00:19:58.470 --> 00:20:05.370
Is any of this a problem
for Mill and Kant?

00:20:05.370 --> 00:20:10.750
Let's look back to the very
opening pages of Mill's

00:20:10.750 --> 00:20:13.760
treatise on utilitarianism.

00:20:13.760 --> 00:20:16.640
He writes there, and I didn't
have you read this passage so

00:20:16.640 --> 00:20:19.210
there's no reason you should
know that he says it.

00:20:19.210 --> 00:20:22.630
"Though in science, the
particular truths precede the

00:20:22.630 --> 00:20:26.420
general theory, the contrary
might be expected to be the

00:20:26.420 --> 00:20:29.330
case with a practical art,
such as morals or

00:20:29.330 --> 00:20:30.460
legislation."

00:20:30.460 --> 00:20:32.580
So in science, we look at
particular instances.

00:20:32.580 --> 00:20:34.670
We discover we drop this object
and it falls with

00:20:34.670 --> 00:20:37.980
acceleration a, we drop this
object and we discover it

00:20:37.980 --> 00:20:40.020
falls with acceleration a, we
drop this object and we

00:20:40.020 --> 00:20:41.580
discover it falls with
acceleration a.

00:20:41.580 --> 00:20:45.095
And from that, we conclude that
the law governing fall of

00:20:45.095 --> 00:20:48.050
bodies is that they fall
with acceleration a.

00:20:48.050 --> 00:20:50.130
So "though in science,
particular truths precede the

00:20:50.130 --> 00:20:52.270
general theory, the contrary
might be expected to be the

00:20:52.270 --> 00:20:55.030
case with a practical art,
such as morals...

00:20:55.030 --> 00:20:58.350
A test of right and wrong must
be the means of ascertaining

00:20:58.350 --> 00:20:59.880
what is right or wrong...

00:20:59.880 --> 00:21:03.145
and not a consequence of already
having ascertained

00:21:03.145 --> 00:21:07.160
it." "The difficulty" (of
building a theory out of

00:21:07.160 --> 00:21:11.570
judgments) "the difficulty is
not avoided by recourse to"

00:21:11.570 --> 00:21:15.790
what is sometimes now called the
moral sense --"a natural

00:21:15.790 --> 00:21:19.010
faculty," says Mill, "that
discerns what's right or wrong

00:21:19.010 --> 00:21:23.030
in a particular case in hand,
as our other senses discern

00:21:23.030 --> 00:21:26.790
the sight or sound actually
present." So as if you can see

00:21:26.790 --> 00:21:28.870
whether a case is
morally wrong.

00:21:28.870 --> 00:21:33.510
"Rather," he says, "moral
reasoning, moral

00:21:33.510 --> 00:21:38.050
understanding, is a branch
of our reason, not of our

00:21:38.050 --> 00:21:39.560
sensitive faculty.

00:21:39.560 --> 00:21:41.880
The morality of an individual
action...

00:21:41.880 --> 00:21:45.640
is a question of the application
of the law to an

00:21:45.640 --> 00:21:47.520
individual case...

00:21:47.520 --> 00:21:52.420
As a result, whatever
steadfastness or constancy our

00:21:52.420 --> 00:21:59.420
moral belief has attained is due
to the tacit influence of

00:21:59.420 --> 00:22:02.760
this reflectively available
standard."

00:22:02.760 --> 00:22:07.720
So Mill is building theory
out of theory, not

00:22:07.720 --> 00:22:10.430
theory out of cases.

00:22:10.430 --> 00:22:12.320
Kant.

00:22:12.320 --> 00:22:17.330
"Worse service cannot be
rendered to morality than that

00:22:17.330 --> 00:22:20.550
an attempt to be made to derive
it from examples.

00:22:20.550 --> 00:22:24.470
For every example of morality
must itself first be charged

00:22:24.470 --> 00:22:28.300
according to principles of
morality in order to see

00:22:28.300 --> 00:22:33.580
whether it is fit to
serve as a model."

00:22:33.580 --> 00:22:36.610
We have here, in some
ways, embodied the

00:22:36.610 --> 00:22:40.150
dialogue of this course.

00:22:40.150 --> 00:22:45.280
To what extent is our capacity
for rational reflection the

00:22:45.280 --> 00:22:50.240
best way to get at answers to
questions that we care about?

00:22:50.240 --> 00:22:56.990
To what extent is our capacity
for emotional response, for

00:22:56.990 --> 00:23:02.430
sensation, for instinctive
judgment on the basis of

00:23:02.430 --> 00:23:07.240
presentation of particular
cases, indicative of answers

00:23:07.240 --> 00:23:10.470
to the questions
we care about?

00:23:10.470 --> 00:23:14.010
So that closes the discussion
of the trolley cases.

00:23:14.010 --> 00:23:18.270
And when I want to do in the
second half of lecture is run

00:23:18.270 --> 00:23:23.230
through two kinds of puzzles
which persist regardless of

00:23:23.230 --> 00:23:26.680
which of those attitudes
that we take.

00:23:26.680 --> 00:23:29.360
So the first is something that
I presented you with as

00:23:29.360 --> 00:23:32.510
promissory note in the very
first lecture, because this is

00:23:32.510 --> 00:23:36.050
one of the most fun papers that
we're reading all term.

00:23:36.050 --> 00:23:41.710
And this is Roy Sorensen's paper
with Boorse on ducking

00:23:41.710 --> 00:23:42.730
and sacrificing.

00:23:42.730 --> 00:23:48.020
And you'll remember, that was
the weekend that the senator

00:23:48.020 --> 00:23:52.070
from Arizona had been shot, so I
couldn't do it with bullets.

00:23:52.070 --> 00:23:54.570
So you'll remember that the
case I gave you is, you're

00:23:54.570 --> 00:23:56.080
standing in a line.

00:23:56.080 --> 00:23:57.210
You're the yellow guy.

00:23:57.210 --> 00:24:00.250
And a bear is rushing
towards you.

00:24:00.250 --> 00:24:04.160
And you jump out of the line,
and the bear eats the person

00:24:04.160 --> 00:24:05.660
behind you.

00:24:05.660 --> 00:24:07.400
Contrast that with the
case where you're

00:24:07.400 --> 00:24:08.480
standing in a line.

00:24:08.480 --> 00:24:09.890
You're still the yellow guy.

00:24:09.890 --> 00:24:13.040
A bear is rushing towards you,
and you reach behind you, pick

00:24:13.040 --> 00:24:15.990
up the guy, and put him
in front of you, and

00:24:15.990 --> 00:24:17.730
the bear eats him.

00:24:17.730 --> 00:24:19.990
The first of these is
the classic what's

00:24:19.990 --> 00:24:22.260
called ducking case.

00:24:22.260 --> 00:24:26.270
That is, you're in a situation
where there's a harm moving in

00:24:26.270 --> 00:24:27.200
your direction.

00:24:27.200 --> 00:24:30.500
You move out of the harm's way,
and the harm hits someone

00:24:30.500 --> 00:24:31.680
else instead.

00:24:31.680 --> 00:24:34.610
The second it's a classic
sacrificing case.

00:24:34.610 --> 00:24:37.960
There's a harm moving towards
you, and you make you use of

00:24:37.960 --> 00:24:40.970
another person as a shield.

00:24:40.970 --> 00:24:44.540
So to duck is to avoid harm,
thereby allowing it to fall on

00:24:44.540 --> 00:24:45.120
someone else.

00:24:45.120 --> 00:24:48.530
To sacrifice is to avoid harm
by bringing about that the

00:24:48.530 --> 00:24:51.220
harm comes to someone
else if you use

00:24:51.220 --> 00:24:53.560
that person as a shield.

00:24:53.560 --> 00:24:57.850
And this is analogous to the
act-omission distinction, one

00:24:57.850 --> 00:25:01.030
that we've already looked at,
but it's wholly within the

00:25:01.030 --> 00:25:03.690
realm of acts.

00:25:03.690 --> 00:25:08.190
Now what Sorensen and Boorse
bring out in their article is

00:25:08.190 --> 00:25:13.780
how resilient this phenomenon
is, regardless of how you mess

00:25:13.780 --> 00:25:16.300
around with the framing
of the case.

00:25:16.300 --> 00:25:19.030
So they give you the example
of the mall gunman.

00:25:19.030 --> 00:25:21.407
There's a bullet coming towards
you, and your choice

00:25:21.407 --> 00:25:24.910
is to leap aside or to pull
somebody in front of you as a

00:25:24.910 --> 00:25:26.640
way of avoiding the bullet.

00:25:26.640 --> 00:25:28.320
There's the speeding
truck case.

00:25:28.320 --> 00:25:30.400
You're in a row of cars.

00:25:30.400 --> 00:25:33.840
There's a truck coming up behind
you in such a way that

00:25:33.840 --> 00:25:35.350
it's going to crash into you.

00:25:35.350 --> 00:25:37.490
And you have one of two
things that you do.

00:25:37.490 --> 00:25:40.100
In the first, you switch lanes,
and the truck hits the

00:25:40.100 --> 00:25:41.530
car that was in front of you.

00:25:41.530 --> 00:25:45.940
In the second, you signal to
a car that's behind you to

00:25:45.940 --> 00:25:49.810
switch into your lane, and
the truck hits him.

00:25:49.810 --> 00:25:51.820
There's the terrorist case.

00:25:51.820 --> 00:25:53.540
You're on an airplane.

00:25:53.540 --> 00:25:54.800
Libyan terrorists--

00:25:54.800 --> 00:25:57.430
quite timely to be speaking
about Libya--

00:25:57.430 --> 00:26:00.820
Libyan terrorists, in this
example, come onto your

00:26:00.820 --> 00:26:04.440
airplane and threaten to
kill all Americans.

00:26:04.440 --> 00:26:07.440
You're a U.S. State Department
representative, an on your

00:26:07.440 --> 00:26:11.140
briefcase is a U.S. State
Department sticker, and the

00:26:11.140 --> 00:26:12.890
terrorists are coming down
the aisle, and they're

00:26:12.890 --> 00:26:14.450
about to shoot you.

00:26:14.450 --> 00:26:15.430
Two possibilities.

00:26:15.430 --> 00:26:20.560
One, you cover your sticker with
a Libyan airline sticker,

00:26:20.560 --> 00:26:24.165
so they skip you and go and
shoot the woman sitting next

00:26:24.165 --> 00:26:25.660
to you, the next one in line.

00:26:25.660 --> 00:26:28.960
The other, you switch briefcases
with the person

00:26:28.960 --> 00:26:34.850
next to you, and so they shoot
her instead of shooting you.

00:26:34.850 --> 00:26:36.520
Or the sinking boats case.

00:26:36.520 --> 00:26:38.650
You're in an ocean.

00:26:38.650 --> 00:26:41.770
You're trying to get your
signal, your boat is sinking,

00:26:41.770 --> 00:26:43.265
the guy next to you's
boat is sinking.

00:26:43.265 --> 00:26:46.870
You're trying to signal to an
airplane above you to come and

00:26:46.870 --> 00:26:47.940
pick you up.

00:26:47.940 --> 00:26:50.350
And you can do one
of two things.

00:26:50.350 --> 00:26:52.600
You can strengthen your
signal, right?

00:26:52.600 --> 00:26:56.210
Make your light really strong,
and then the airplane will

00:26:56.210 --> 00:26:57.540
come and rescue you.

00:26:57.540 --> 00:27:01.210
Or you can jam the signal of
the other guy, making your

00:27:01.210 --> 00:27:05.040
signal relatively stronger so
that the airline comes and

00:27:05.040 --> 00:27:06.376
picks you up.

00:27:06.376 --> 00:27:09.280
Sorensen gives case after
case about this.

00:27:09.280 --> 00:27:14.700
If beetles are eating your
roses, it's OK to put beetle

00:27:14.700 --> 00:27:18.040
repellent on your roses, which
will cause them to go over to

00:27:18.040 --> 00:27:21.010
your neighbor's house, but
it's not OK to put beetle

00:27:21.010 --> 00:27:25.270
attractant on his roses.

00:27:25.270 --> 00:27:31.105
We have this strange tendency,
over and over and over, to

00:27:31.105 --> 00:27:38.380
think that ducking is OK and
that shielding is not.

00:27:38.380 --> 00:27:43.970
Now the perplexity that Sorensen
and Boorse consider

00:27:43.970 --> 00:27:48.700
is that it seems like there's
no systematic way to account

00:27:48.700 --> 00:27:53.450
for these kinds of discrepancies
in intuition.

00:27:53.450 --> 00:27:54.340
So you might think, look.

00:27:54.340 --> 00:27:59.970
The problem with these cases is
that when you tie up your

00:27:59.970 --> 00:28:02.480
opponent's feet, when you're
trying to outrun the bear, or

00:28:02.480 --> 00:28:05.810
when you push him in front of
you in the shooting cases, you

00:28:05.810 --> 00:28:08.050
interfere with fair
competition.

00:28:08.050 --> 00:28:13.050
And that fair competition is
what matters in these sorts of

00:28:13.050 --> 00:28:15.000
circumstances.

00:28:15.000 --> 00:28:18.190
But of course, there are plenty
of these circumstances

00:28:18.190 --> 00:28:22.980
where the competition was
unfair to begin with.

00:28:22.980 --> 00:28:27.120
And nonetheless, it
seems problematic.

00:28:27.120 --> 00:28:31.110
Even if the guy whom you're
trying outrun the bear in

00:28:31.110 --> 00:28:34.840
front of is a much slower runner
than you, so that you

00:28:34.840 --> 00:28:39.400
were certain to win, it still
doesn't seem OK to tie his

00:28:39.400 --> 00:28:41.410
shoes together.

00:28:41.410 --> 00:28:43.800
The fairness of the competition
doesn't seem to be

00:28:43.800 --> 00:28:45.330
what's driving the intuition.

00:28:45.330 --> 00:28:49.540
So perhaps, they say, it's that
in each of the shoving

00:28:49.540 --> 00:28:53.850
cases, what you do is somehow
an included wrong.

00:28:53.850 --> 00:28:56.100
It's wrong to pick somebody
up and carry

00:28:56.100 --> 00:28:57.360
them in front of you.

00:28:57.360 --> 00:29:00.030
Whereas, it's OK just
to duck down so that

00:29:00.030 --> 00:29:01.080
something hits them.

00:29:01.080 --> 00:29:03.380
It's wrong to steal somebody's
briefcase.

00:29:03.380 --> 00:29:06.300
It's wrong to jam somebody's
signal.

00:29:06.300 --> 00:29:12.290
But, they point out, it seems
just as bad to put the person

00:29:12.290 --> 00:29:16.660
in front of you in a friendly
way by saying, "wouldn't you

00:29:16.660 --> 00:29:20.740
like to see the beautiful view?"
as it does to pick him

00:29:20.740 --> 00:29:22.760
up and put him in
front of you.

00:29:22.760 --> 00:29:27.770
It's just as problematic, they
suggest, to scare somebody

00:29:27.770 --> 00:29:31.610
into jumping off a cliff by
yelling, "E equals mc

00:29:31.610 --> 00:29:35.450
squared!" to surprise them, as
it is to cause them to jump

00:29:35.450 --> 00:29:39.440
off the cliff by yelling
a racial epithet.

00:29:39.440 --> 00:29:43.690
The included wrong doesn't seem
to be what's explaining

00:29:43.690 --> 00:29:45.940
our response.

00:29:45.940 --> 00:29:49.650
So, too, and I'll leave you to
read these responses on your

00:29:49.650 --> 00:29:53.950
own if you haven't had the
chance already, so, too does

00:29:53.950 --> 00:29:56.280
the act-omission distinction
or the doing-allowing

00:29:56.280 --> 00:29:59.760
distinction not seem sufficient
to do the work.

00:29:59.760 --> 00:30:02.980
So, too, does the idea that what
matters is if you were

00:30:02.980 --> 00:30:07.050
the locus of a causal chain,
the originator of some

00:30:07.050 --> 00:30:08.910
sequence of causality.

00:30:08.910 --> 00:30:12.460
So, too, does the doctrine of
double effect not seem to

00:30:12.460 --> 00:30:14.870
account for all of
these cases.

00:30:14.870 --> 00:30:19.830
So, too, does appeal to Kant's
notion of rights in contrast

00:30:19.830 --> 00:30:24.970
to utilities not seem to explain
all of these cases.

00:30:24.970 --> 00:30:30.620
So Sorensen and Boorse somewhat
reluctantly consider

00:30:30.620 --> 00:30:32.200
a conclusion of skepticism.

00:30:32.200 --> 00:30:37.250
Which is roughly, this is a
perplexing feature of our

00:30:37.250 --> 00:30:38.920
psychology.

00:30:38.920 --> 00:30:42.370
But we, having listened to the
first half of this lecture,

00:30:42.370 --> 00:30:44.470
have one more alternative
explanation.

00:30:44.470 --> 00:30:46.870
And I don't promise that it
will work in every case,

00:30:46.870 --> 00:30:48.870
though it seems pretty
promising.

00:30:48.870 --> 00:30:51.320
Which is that what's going on
in the ducking and shielding

00:30:51.320 --> 00:30:55.550
cases is the overapplication
of a heuristic.

00:30:55.550 --> 00:31:00.000
In general, it does seem like
moving out of the way of a

00:31:00.000 --> 00:31:05.610
harm is not a bad thing to do,
whereas putting somebody into

00:31:05.610 --> 00:31:10.230
the track of a harm is
a bad thing to do.

00:31:10.230 --> 00:31:15.820
So perhaps these first set of
puzzles can be explained by

00:31:15.820 --> 00:31:18.330
means of heuristics.

00:31:18.330 --> 00:31:22.240
In the last fifteen minutes of
lecture, I want to focus on a

00:31:22.240 --> 00:31:25.080
set of puzzles which,
I think, can't.

00:31:25.080 --> 00:31:28.510
And for these, you'll
need your clickers.

00:31:28.510 --> 00:31:32.410
So let's start with
four drivers.

00:31:32.410 --> 00:31:36.270
The first of them, Lucky Albert,
or Lucky Alert, does

00:31:36.270 --> 00:31:37.450
the following.

00:31:37.450 --> 00:31:39.550
He gets into his car.

00:31:39.550 --> 00:31:42.500
He has his car in perfect
condition.

00:31:42.500 --> 00:31:46.050
He pays attention
at every light.

00:31:46.050 --> 00:31:48.940
He drives in an extremely
safe way.

00:31:48.940 --> 00:31:53.840
And at the end of the day,
gets home from work.

00:31:53.840 --> 00:31:54.790
That's it.

00:31:54.790 --> 00:31:55.920
That's lucky alert.

00:31:55.920 --> 00:31:58.060
Question.

00:31:58.060 --> 00:32:03.930
When Lucky Alert drives home,
setting aside whether he has

00:32:03.930 --> 00:32:07.830
his mistress in his car with
him, setting aside whether

00:32:07.830 --> 00:32:12.670
he's bought a car that has a
high rate of emissions as

00:32:12.670 --> 00:32:17.740
opposed to buying a Prius, in
driving home, setting aside

00:32:17.740 --> 00:32:20.590
all the other things that Alert
might have done morally

00:32:20.590 --> 00:32:24.130
wrong, did he do something
morally blameworthy, driving

00:32:24.130 --> 00:32:28.590
home from work, having fully
fixed his car, and doing no

00:32:28.590 --> 00:32:29.960
harm to anyone along the way?

00:32:29.960 --> 00:32:31.920
So this is not a
trick question.

00:32:31.920 --> 00:32:34.740
So if you think Lucky Alert
did something morally

00:32:34.740 --> 00:32:36.850
blameworthy, setting aside all
the things that are morally

00:32:36.850 --> 00:32:40.100
blameworthy about driving
a car, push one.

00:32:40.100 --> 00:32:42.480
Whereas if you think he didn't
do anything morally

00:32:42.480 --> 00:32:44.530
blameworthy, push two.

00:32:44.530 --> 00:32:48.330
So what you're judging is, is
driving home from work, all

00:32:48.330 --> 00:32:51.140
things considered, if nothing
bad happens, a morally

00:32:51.140 --> 00:32:52.880
problematic thing to do?

00:32:52.880 --> 00:32:54.540
And let's hope--

00:32:54.540 --> 00:32:54.850
OK.

00:32:54.850 --> 00:32:56.710
So there's always that 5%.

00:32:56.710 --> 00:32:58.370
Those anti-car crowds.

00:32:58.370 --> 00:33:00.740
You're the ones going to med
school and chopping up our

00:33:00.740 --> 00:33:02.930
poor healthy guy in
the waiting room.

00:33:02.930 --> 00:33:06.690
95% of you think Lucky Alert
did nothing morally

00:33:06.690 --> 00:33:07.350
blameworthy.

00:33:07.350 --> 00:33:13.530
Let's meet Lucky Alert's twin
brother, Unlucky Alert.

00:33:13.530 --> 00:33:15.610
Here's what Unlucky Alert did.

00:33:15.610 --> 00:33:18.736
Exactly what Lucky Alert did.

00:33:18.736 --> 00:33:25.380
Except as he neared his house,
a child ran out in front of

00:33:25.380 --> 00:33:28.350
his car and he hit the child.

00:33:28.350 --> 00:33:29.030
OK?

00:33:29.030 --> 00:33:34.360
Unlucky Alert did exactly
what Lucky Alert did.

00:33:34.360 --> 00:33:38.570
Left work, checked his tires,
stayed alert the entire time,

00:33:38.570 --> 00:33:40.230
drove at safe and
proper speeds.

00:33:40.230 --> 00:33:47.219
But due to bad luck, on his
way home killed a child.

00:33:47.219 --> 00:33:48.429
Question.

00:33:48.429 --> 00:33:53.080
Did Unlucky Alert do something
morally blameworthy?

00:33:53.080 --> 00:33:56.250
If yes, push one.

00:33:56.250 --> 00:33:57.719
If no, push two.

00:33:57.719 --> 00:34:00.510
And I'm going to write down the
numbers on the first case,

00:34:00.510 --> 00:34:02.800
which were 5 and 95.

00:34:02.800 --> 00:34:03.489
OK.

00:34:03.489 --> 00:34:06.139
So let's see how the numbers
come out on this.

00:34:06.139 --> 00:34:10.100
Here, 81% of you think he didn't
do something morally

00:34:10.100 --> 00:34:15.679
blameworthy, but we're up from
5% to 19% on people who think

00:34:15.679 --> 00:34:17.659
he did do something morally
blameworthy.

00:34:17.659 --> 00:34:19.659
Let's turn to our third case.

00:34:19.659 --> 00:34:21.110
Here's Mr. Lucky Cell Phone.

00:34:21.110 --> 00:34:23.100
Here's what Mr. Lucky
Cell Phone does.

00:34:23.100 --> 00:34:25.790
He gets into his car and starts
driving home from work.

00:34:25.790 --> 00:34:28.260
And on his way home from work,
he talks on his cell phone,

00:34:28.260 --> 00:34:30.660
but you know what, nothing
else happens.

00:34:30.660 --> 00:34:34.560
And he gets home from work
having harmed no one.

00:34:34.560 --> 00:34:35.760
Question.

00:34:35.760 --> 00:34:38.750
Did Lucky Cell Phone do
something morally blameworthy

00:34:38.750 --> 00:34:43.770
in driving home from work
talking on his cell phone?

00:34:43.770 --> 00:34:46.460
And let's see how these
numbers come out.

00:34:50.870 --> 00:34:51.560
OK.

00:34:51.560 --> 00:34:54.400
So your verdict here.

00:34:54.400 --> 00:34:57.620
78% of you think Lucky Cell
Phone did something-- you

00:34:57.620 --> 00:34:59.190
guys, I don't believe you.

00:34:59.190 --> 00:35:01.980
I mean, you're anticipating
the next case!

00:35:01.980 --> 00:35:04.640
All of you talk on your cell
phones when you drive all of

00:35:04.640 --> 00:35:07.300
the time, and you don't think of
yourself as doing something

00:35:07.300 --> 00:35:08.300
morally blameworthy!

00:35:08.300 --> 00:35:08.830
OK.

00:35:08.830 --> 00:35:10.580
These are not valid data.

00:35:10.580 --> 00:35:13.510
This has to do with where they
are embedded in this

00:35:13.510 --> 00:35:15.430
experiment.

00:35:15.430 --> 00:35:15.800
All right.

00:35:15.800 --> 00:35:19.040
So since you've already answered
question four, let me

00:35:19.040 --> 00:35:20.690
just ask it of you.

00:35:20.690 --> 00:35:23.280
Unlucky Cell Phone drives home
from work while talking on his

00:35:23.280 --> 00:35:23.690
cell phone.

00:35:23.690 --> 00:35:26.070
Child runs out in front
of his car, and--

00:35:26.070 --> 00:35:27.300
OK.

00:35:27.300 --> 00:35:28.340
Question.

00:35:28.340 --> 00:35:32.000
During his drive home, did
Unlucky Cell Phone do

00:35:32.000 --> 00:35:34.080
something morally blameworthy?

00:35:34.080 --> 00:35:37.622
And let's see how the
numbers come out.

00:35:40.340 --> 00:35:40.970
All right.

00:35:40.970 --> 00:35:43.320
Let's see where Unlucky
Cell Phone's big

00:35:43.320 --> 00:35:45.170
red line comes out.

00:35:45.170 --> 00:35:45.520
OK.

00:35:45.520 --> 00:35:48.890
So now we've got a complete
shift from the original one,

00:35:48.890 --> 00:35:51.870
and in fact different from
our previous case.

00:35:51.870 --> 00:35:52.390
OK.

00:35:52.390 --> 00:35:57.010
So what these examples
demonstrate is a phenomenon

00:35:57.010 --> 00:35:59.530
known as moral luck.

00:35:59.530 --> 00:36:04.240
We have two people here, Lucky
Alert and Unlucky Alert, who

00:36:04.240 --> 00:36:09.360
do exactly the same thing, but
Unlucky Alert's actions caused

00:36:09.360 --> 00:36:11.480
the death of an innocent
victim.

00:36:11.480 --> 00:36:14.680
And whereas only 5% of you
think Lucky Alert did

00:36:14.680 --> 00:36:19.340
something wrong, 19% of you
think Unlucky Alert did

00:36:19.340 --> 00:36:20.560
something wrong.

00:36:20.560 --> 00:36:24.180
Here we have, in similar
fashion, somebody who in a

00:36:24.180 --> 00:36:28.760
very slight way has taken a
risk, which in this case had

00:36:28.760 --> 00:36:32.660
no bad consequences and in this
case had very severe bad

00:36:32.660 --> 00:36:33.590
consequences.

00:36:33.590 --> 00:36:39.000
And 92% of you condemn
Unlucky Cell Phone.

00:36:39.000 --> 00:36:42.220
The phenomenon that this
illustrates is a phenomenon

00:36:42.220 --> 00:36:44.120
known as moral luck.

00:36:44.120 --> 00:36:47.692
Cases where an agent is assigned
moral blame for an

00:36:47.692 --> 00:36:51.610
action or its consequences, even
though the agent didn't

00:36:51.610 --> 00:36:54.810
have full control over that
action or its consequences.

00:36:54.810 --> 00:36:55.220
Right?

00:36:55.220 --> 00:37:00.350
It's not the case that Unlucky
Alert or Unlucky Cell Phone

00:37:00.350 --> 00:37:03.940
wanted the child to run out
in front of his car.

00:37:03.940 --> 00:37:07.350
It's not the case that Unlucky
Alert or Unlucky Cell Phone

00:37:07.350 --> 00:37:11.810
could have done it anything
different at that moment.

00:37:11.810 --> 00:37:14.760
The child was in front
of the car, and

00:37:14.760 --> 00:37:17.980
the car hit the child.

00:37:17.980 --> 00:37:23.820
Moral luck is perplexing because
we seem to have two

00:37:23.820 --> 00:37:27.290
competing commitments when
we think about moral

00:37:27.290 --> 00:37:29.310
responsibility.

00:37:29.310 --> 00:37:31.740
On the one hand, we seem to
accept something which we

00:37:31.740 --> 00:37:35.550
might call the control
principle: That moral praise

00:37:35.550 --> 00:37:39.440
and blame shouldn't be assigned
in cases where the

00:37:39.440 --> 00:37:43.760
action or the consequences lie
beyond the agent's control.

00:37:43.760 --> 00:37:47.530
And I can see that many of you
subscribe to the control

00:37:47.530 --> 00:37:53.730
principle, because 81% of you
thought that the unlucky alert

00:37:53.730 --> 00:37:57.400
driver did nothing morally
wrong, even though he killed a

00:37:57.400 --> 00:37:59.270
child with his car.

00:37:59.270 --> 00:38:01.950
And the reason you're inclined
to think that he did nothing

00:38:01.950 --> 00:38:06.350
wrong in that case, I suspect,
is because your judgment in

00:38:06.350 --> 00:38:10.650
that case, as Mill said, is
regulated by a principle to

00:38:10.650 --> 00:38:12.840
which you tacitly subscribe.

00:38:12.840 --> 00:38:16.510
Namely, something like the
control principle.

00:38:16.510 --> 00:38:20.250
It's intuitively plausible, says
Nagel, that people can't

00:38:20.250 --> 00:38:23.390
see morally assessed for what's
not their fault, or for

00:38:23.390 --> 00:38:25.790
what's due to factors beyond
their control.

00:38:25.790 --> 00:38:29.700
If you bump into me, and I trip,
and I accidentally fall

00:38:29.700 --> 00:38:33.050
on the red button that causes
the nuclear war to start all

00:38:33.050 --> 00:38:35.520
over the planet, it's
not my fault.

00:38:35.520 --> 00:38:39.070
I mean, it's really a terribly
bad thing that the planet is

00:38:39.070 --> 00:38:42.900
destroyed, but I just tripped.

00:38:42.900 --> 00:38:47.380
By contrast, and directly in
competition with the control

00:38:47.380 --> 00:38:52.410
principle, it seems, as the
moral luck principle states,

00:38:52.410 --> 00:38:56.560
that in some cases, moral praise
and blame shouldn't be

00:38:56.560 --> 00:39:00.800
assigned even where the action
or consequences lie beyond the

00:39:00.800 --> 00:39:02.870
agent's control.

00:39:02.870 --> 00:39:06.250
The difference in your responses
between the lucky

00:39:06.250 --> 00:39:11.060
and the unlucky cases indicate
the degree to which you

00:39:11.060 --> 00:39:13.420
tacitly subscribe to that.

00:39:13.420 --> 00:39:20.070
So you went from 95% of no
blameworthiness to 81%.

00:39:20.070 --> 00:39:27.090
So 15% of you shifted your view
as a result of something

00:39:27.090 --> 00:39:28.730
beyond his control.

00:39:28.730 --> 00:39:32.590
In the cell phone case, again,
roughly 15% of you

00:39:32.590 --> 00:39:35.330
shifted your view.

00:39:35.330 --> 00:39:39.000
The problem is that both
of these principles are

00:39:39.000 --> 00:39:41.820
incredibly difficult
to let go of.

00:39:41.820 --> 00:39:44.570
The control principle
relies on the

00:39:44.570 --> 00:39:47.270
following kind of reasoning.

00:39:47.270 --> 00:39:50.440
In general, we have a pretty
good sense of what kind of

00:39:50.440 --> 00:39:55.560
factors increase the blame- or
praiseworthiness of an action.

00:39:55.560 --> 00:39:58.330
In general, if an act is
voluntary, that is, if you've

00:39:58.330 --> 00:40:02.160
done it not out of coercion
and not out of mistake--

00:40:02.160 --> 00:40:02.420
right?

00:40:02.420 --> 00:40:06.020
If you specifically chose to
perform the action that you

00:40:06.020 --> 00:40:07.260
performed--

00:40:07.260 --> 00:40:11.160
then you get more praise for
doing it if it was a good

00:40:11.160 --> 00:40:13.470
action, and more blame
for doing it if

00:40:13.470 --> 00:40:15.320
it was a bad action.

00:40:15.320 --> 00:40:18.760
Likewise, if you had full
information, if you were aware

00:40:18.760 --> 00:40:22.050
of its likely consequences, you
knew that that was water,

00:40:22.050 --> 00:40:24.570
or you knew that that was
cyanide that you were giving

00:40:24.570 --> 00:40:29.470
the person to drink, it
increases the degree of praise

00:40:29.470 --> 00:40:31.030
or blameworthiness.

00:40:31.030 --> 00:40:36.500
And these are pretty robust
responses that fall out not

00:40:36.500 --> 00:40:40.140
merely of our analysis of cases,
but also out of our

00:40:40.140 --> 00:40:42.920
understanding of the principles
that seem to

00:40:42.920 --> 00:40:45.650
underlie moral responsibility.

00:40:45.650 --> 00:40:49.090
Correspondingly, it seems like
the absence of those features

00:40:49.090 --> 00:40:51.670
decreases moral blameworthiness
or

00:40:51.670 --> 00:40:52.300
praiseworthiness.

00:40:52.300 --> 00:40:55.480
If you do something under
coercion, if you do something

00:40:55.480 --> 00:41:00.080
accidentally, it's less
your responsibility.

00:41:00.080 --> 00:41:02.590
And you do something out of
lack of information, if I,

00:41:02.590 --> 00:41:05.850
fully thinking that I'm giving
you something totally healthy,

00:41:05.850 --> 00:41:09.060
end up giving you something
that harms you, we tend to

00:41:09.060 --> 00:41:14.180
think that the degree of
blameworthiness is mitigated.

00:41:14.180 --> 00:41:18.650
The control principle simply
says that without a difference

00:41:18.650 --> 00:41:21.730
in these factors, how could
there be a difference in

00:41:21.730 --> 00:41:23.420
blame- or praiseworthiness?

00:41:23.420 --> 00:41:28.940
If we hold these factors
constant, we must be in a

00:41:28.940 --> 00:41:31.410
situation where there's
no difference in moral

00:41:31.410 --> 00:41:33.280
responsibility.

00:41:33.280 --> 00:41:35.210
By contrast, the moral
luck principle

00:41:35.210 --> 00:41:36.910
is also really forceful.

00:41:36.910 --> 00:41:41.150
It seems undeniable that there
are cases where we assess

00:41:41.150 --> 00:41:43.970
moral praise and blame in
the absence of control.

00:41:43.970 --> 00:41:45.490
The driver case was one.

00:41:45.490 --> 00:41:47.590
One of the cell phone users
hits the child,

00:41:47.590 --> 00:41:48.280
the other one doesn't.

00:41:48.280 --> 00:41:50.300
The first is morally
blameworthy.

00:41:50.300 --> 00:41:53.510
I leave the stove on in my
house, or your house.

00:41:53.510 --> 00:41:55.770
I go to visit you, and I leave
the stove on in your house.

00:41:55.770 --> 00:41:57.670
I go out for the day.

00:41:57.670 --> 00:42:00.160
When I'm unlucky, it causes
your house to burn down.

00:42:00.160 --> 00:42:01.640
When I'm lucky, it doesn't.

00:42:01.640 --> 00:42:04.860
It seems, even if you think both
were bad things to do, a

00:42:04.860 --> 00:42:07.770
much worse thing to leave on the
stove and burn down your

00:42:07.770 --> 00:42:11.660
house than to leave on the
stove, simpliciter.

00:42:11.660 --> 00:42:15.000
Nagel gives the example of
leaving the bath running with

00:42:15.000 --> 00:42:16.530
the baby in it.

00:42:16.530 --> 00:42:21.230
An irresponsible thing to do,
but immeasurably more

00:42:21.230 --> 00:42:26.360
problematic when the baby
drowns as a result.

00:42:26.360 --> 00:42:31.180
Or the case where you and I
have similar characters.

00:42:31.180 --> 00:42:33.670
I stay in Germany, you don't.

00:42:33.670 --> 00:42:35.440
It's the 1930s.

00:42:35.440 --> 00:42:39.990
I become a Nazi, you live your
life in a way that makes no

00:42:39.990 --> 00:42:41.950
moral demands of you.

00:42:41.950 --> 00:42:44.800
So there's three kinds of
responses that we can give to

00:42:44.800 --> 00:42:45.600
moral luck cases.

00:42:45.600 --> 00:42:47.450
We can give a rationalist
response.

00:42:47.450 --> 00:42:50.620
We can say, luck simply can't
play a role in moral

00:42:50.620 --> 00:42:51.580
evaluation.

00:42:51.580 --> 00:42:56.210
And we can either take the
extreme that, one might think,

00:42:56.210 --> 00:43:01.380
a pure Kantian take, that all
the agent is responsible for

00:43:01.380 --> 00:43:03.520
is his will, and those
things over

00:43:03.520 --> 00:43:05.240
which he has full control.

00:43:05.240 --> 00:43:08.550
Or you can take what might be
an extreme Millian version.

00:43:08.550 --> 00:43:11.565
That the agent is responsible
for all the consequences of

00:43:11.565 --> 00:43:15.950
his action, and that the
attitude makes no difference.

00:43:15.950 --> 00:43:18.590
You can take an irrationalist
attitude towards this.

00:43:18.590 --> 00:43:23.290
You can say that luck can play
some role in moral evaluation.

00:43:23.290 --> 00:43:25.900
Or, though I think this is
ultimately difficult to

00:43:25.900 --> 00:43:30.970
maintain, you can say that as
a matter of fact, we never

00:43:30.970 --> 00:43:36.090
know how responsible somebody is
for an action until we see

00:43:36.090 --> 00:43:38.370
what its consequences are.

00:43:38.370 --> 00:43:43.260
That when I hypothesize that
these cases were identical, I

00:43:43.260 --> 00:43:48.450
was idealizing in an
illegitimate way.

00:43:48.450 --> 00:43:52.880
Now, it seems like that third
response might work for the

00:43:52.880 --> 00:43:55.390
classic cases of moral luck
which I've been describing,

00:43:55.390 --> 00:43:58.490
cases which we would call
resultant luck, where there's

00:43:58.490 --> 00:44:00.260
luck in the outcome
of the action.

00:44:00.260 --> 00:44:04.290
I perform an action, and it
happens to go awry in a way

00:44:04.290 --> 00:44:05.860
that I didn't expect.

00:44:05.860 --> 00:44:08.770
That's one class of cases
of moral luck.

00:44:08.770 --> 00:44:12.430
But it's harder to see how we
can use that explanation for

00:44:12.430 --> 00:44:15.200
some of the deep and
profound instances.

00:44:15.200 --> 00:44:18.940
So take constitutive luck.

00:44:18.940 --> 00:44:21.810
Some of you were born with genes
that make it easier for

00:44:21.810 --> 00:44:25.240
you to behave in altruistic
ways, and some of you weren't.

00:44:25.240 --> 00:44:28.270
Some of you were raised in
families that were supportive

00:44:28.270 --> 00:44:29.920
of certain kinds of
moral outlook,

00:44:29.920 --> 00:44:32.480
and some of you weren't.

00:44:32.480 --> 00:44:36.000
Is your character that resulted
from those features

00:44:36.000 --> 00:44:39.340
something for which you are
responsible, and if it's not,

00:44:39.340 --> 00:44:41.765
how is it something with respect
to which moral praise

00:44:41.765 --> 00:44:43.930
and blame can be assessed?

00:44:43.930 --> 00:44:47.240
Take circumstantial luck, which
Jonathan Shay discussed

00:44:47.240 --> 00:44:49.730
in our Achilles in Vietnam --

00:44:49.730 --> 00:44:52.620
luck regarding the agent's
surroundings.

00:44:52.620 --> 00:44:56.160
Sometimes the circumstances
you're in either create or

00:44:56.160 --> 00:45:00.770
reveal otherwise hidden features
of your character.

00:45:00.770 --> 00:45:06.610
Does that mean, since they are
in part a matter of luck, that

00:45:06.610 --> 00:45:11.540
you are not thereby morally
responsible for what you did?

00:45:11.540 --> 00:45:15.440
Finally, if we start thinking
about our actions from the

00:45:15.440 --> 00:45:20.470
perspective of free will, it
becomes hard to carve out any

00:45:20.470 --> 00:45:24.040
space in which we're responsible
for what we do.

00:45:24.040 --> 00:45:28.380
It's a general fact about the
world that actions and

00:45:28.380 --> 00:45:32.460
consequences are in general
determined partly by features

00:45:32.460 --> 00:45:37.750
outside, or at least outside
the control of the agent.

00:45:37.750 --> 00:45:40.590
So we start thinking about why
it is that we respond that way

00:45:40.590 --> 00:45:42.790
in Trolley, and it turns out
it's because the emotional

00:45:42.790 --> 00:45:44.380
part of our brain
is lighting up.

00:45:44.380 --> 00:45:45.680
But why is that happening?

00:45:45.680 --> 00:45:49.110
Well, that's happening because
of blood flow, happening in a

00:45:49.110 --> 00:45:50.320
certain way in our brain.

00:45:50.320 --> 00:45:51.640
And why is that happening?

00:45:51.640 --> 00:45:53.900
Well, the blood is flowing in a
certain way because of what

00:45:53.900 --> 00:45:56.160
certain kinds of molecules
are doing.

00:45:56.160 --> 00:46:00.740
And as we think this through,
the area of genuine agency,

00:46:00.740 --> 00:46:06.600
says Nagel, seems to shrink
to an extensionless point.

00:46:06.600 --> 00:46:10.570
So I leave you for March break
with the following perplexing

00:46:10.570 --> 00:46:15.200
non-solution to a really
profound moral problem.

00:46:15.200 --> 00:46:17.700
Nagel suggests that the
problem of luck has no

00:46:17.700 --> 00:46:20.640
solution because something
in the idea of conceiving

00:46:20.640 --> 00:46:24.390
ourselves as agents is
incompatible with the

00:46:24.390 --> 00:46:27.810
undeniable fact that
actions are event,

00:46:27.810 --> 00:46:30.760
and people are things.

00:46:30.760 --> 00:46:34.100
"As the external determinants
of what someone has done are

00:46:34.100 --> 00:46:37.390
gradually exposed in their
effects on consequences,

00:46:37.390 --> 00:46:41.380
character, and choice itself, it
becomes gradually clear to

00:46:41.380 --> 00:46:45.040
us that actions are indeed
events, and that people are

00:46:45.040 --> 00:46:47.610
indeed things.

00:46:47.610 --> 00:46:50.930
As a result of this, nothing
remains which can be ascribed

00:46:50.930 --> 00:46:54.210
to the responsible self, and
we're left with nothing but a

00:46:54.210 --> 00:46:57.260
portion of the larger sequence
of events which can be

00:46:57.260 --> 00:47:02.400
deplored or celebrated, but
not praised or blamed."

00:47:02.400 --> 00:47:09.300
Nonetheless, giving up the
language of praise and blame

00:47:09.300 --> 00:47:12.330
is to remove from our conceptual
repertoire what is

00:47:12.330 --> 00:47:17.355
perhaps the most important tool
that we have. And coming

00:47:17.355 --> 00:47:19.650
to a stable perspective
on these matters

00:47:19.650 --> 00:47:23.380
seems enormously difficult.

00:47:23.380 --> 00:47:26.910
So I'll see you all at
the end of vacation.

