WEBVTT
Kind: captions
Language: pt-BR

00:00:01.100 --> 00:00:05.400
Eu quero iniciar concluindo a discussão

00:00:05.400 --> 00:00:09.633
que começamos na última aula
sobre os modos de pensar

00:00:09.633 --> 00:00:14.233
sobre a hesitação levantada
pelo caso do Trolley.

00:00:14.233 --> 00:00:17.867
Vocês se lembram que a hesitação

00:00:17.867 --> 00:00:22.600
é que existe uma aparente
assimetria em nossas respostas

00:00:22.600 --> 00:00:26.000
para o caso da testemunha e o
caso do homem gordo,

00:00:26.000 --> 00:00:34.400
embora ambos envolvam matar
1 para salvar 5.

00:00:34.400 --> 00:00:40.033
E vimos, na última classe, a
resposta de Judy Thomson,

00:00:40.033 --> 00:00:42.800
que diz: não há
assimetria nos 2 casos,

00:00:42.800 --> 00:00:46.733
porque quando refletimos sobre o
caso hipotético adicional

00:00:46.733 --> 00:00:50.367
em que há uma 3ª via onde
está você, reconhecemos

00:00:50.367 --> 00:00:55.233
que não é moralmente aceitável
desviar o trem na testemunha,

00:00:55.233 --> 00:00:57.167
da mesma forma que não é
moralmente aceitável

00:00:57.167 --> 00:00:59.167
empurrar o homem gordo.

00:00:59.167 --> 00:01:02.800
No outro extremo, vimos a
resposta de Josh Greene,

00:01:02.800 --> 00:01:05.567
que diz: assim como é
moralmente aceitável

00:01:05.567 --> 00:01:07.633
desviar o trem na testemunha,

00:01:07.633 --> 00:01:11.500
é moralmente aceitável
empurrar o homem gordo.

00:01:11.500 --> 00:01:15.033
E na medida em que encontramos
diferentes respostas, diz Greene,

00:01:15.033 --> 00:01:19.533
isso se deve ao fato de que a parte
emocional do nosso mecanismo

00:01:19.533 --> 00:01:26.200
de resposta do cérebro é ativado pela
natureza pessoal da proximidade

00:01:26.200 --> 00:01:28.867
no caso do homem gordo e,
como resultado,

00:01:28.867 --> 00:01:34.567
damos uma resposta que, para ele, é
moralmente injustificada.

00:01:34.567 --> 00:01:40.067
E no final da última aula começamos a
refletir sobre uma 3ª possibilidade,

00:01:40.067 --> 00:01:43.300
que está em algum lugar
entre Thomson e Greene,

00:01:43.300 --> 00:01:45.067
embora mais perto de Greene.

00:01:45.067 --> 00:01:49.667
E se trata do argumento de
Cass Sunstein de que,

00:01:49.667 --> 00:01:51.400
apesar das respostas diferirem

00:01:51.400 --> 00:01:54.300
– e talvez diferirem em aspectos
impossíveis de serem mudados –,

00:01:54.300 --> 00:01:57.767
no fundo, os casos são iguais.

00:01:57.767 --> 00:02:02.600
E, embora não tão
determinado como Josh Greene,

00:02:02.600 --> 00:02:05.667
ele está inclinado a pensar que
se queremos que os casos

00:02:05.667 --> 00:02:08.000
sejam visto iguais, o que devemos
fazer é empurrar o homem gordo.

00:02:08.000 --> 00:02:12.200
Vocês se lembram que o argumento dele
procede da seguinte maneira:

00:02:12.200 --> 00:02:14.433
Ele sugeriu que é incontestável
utilizar as heurísticas

00:02:14.433 --> 00:02:17.467
nos casos não-morais, e que essas
heurísticas, embora úteis,

00:02:17.467 --> 00:02:22.533
muitas vezes nos conduzem a erros;

00:02:22.533 --> 00:02:28.933
e então ele destacou que, assim como
isso ocorre nos casos não-morais,

00:02:28.933 --> 00:02:31.667
também ocorre nos casos morais.

00:02:31.667 --> 00:02:35.367
E fomos embora da última
classe pensando

00:02:35.367 --> 00:02:40.500
sobre o argumento de Sunstein
de que nos casos morais

00:02:40.500 --> 00:02:42.933
as pessoas costumam usar
as heurísticas.

00:02:42.933 --> 00:02:46.033
Vocês se lembram que ele deu
alguns exemplos do trabalho

00:02:46.033 --> 00:02:51.567
de Jonathan Haidt de situações em
que as pessoas expressavam

00:02:51.567 --> 00:02:53.967
desaprovação moral para ações

00:02:53.967 --> 00:02:56.467
para as quais elas não podiam
encontrar justificativa.

00:02:56.467 --> 00:02:59.233
O incesto consensual entre os irmãos,

00:02:59.233 --> 00:03:02.000
limpar o chão do banheiro com
a bandeira do país...

00:03:02.000 --> 00:03:04.600
As pessoas estavam inclinadas a ver isso

00:03:04.600 --> 00:03:08.200
moralmente problemático mesmo quando,

00:03:08.200 --> 00:03:09.900
ao serem questionadas,

00:03:09.900 --> 00:03:13.800
não conseguiam dizer qual a regra
moral que essas coisas violam.

00:03:13.800 --> 00:03:19.400
E o que Sunstein afirma no
artigo é que, em geral,

00:03:19.400 --> 00:03:21.967
podemos analisar a literatura das
heurísticas e vieses e ver,

00:03:21.967 --> 00:03:26.867
instância após instância, que o
enquadramento de um caso

00:03:26.867 --> 00:03:30.900
afeta a nossa resposta

00:03:30.900 --> 00:03:34.933
tanto nos casos não-morais
quanto nos casos morais.

00:03:34.933 --> 00:03:37.967
Vocês se lembram que na 3ª aula,

00:03:37.967 --> 00:03:41.200
justo quando aprendíamos a
usar os clickers,

00:03:41.200 --> 00:03:43.333
eu disse que o usaríamos nesta aula...

00:03:43.333 --> 00:03:45.967
portanto, é hora de cumprir a promessa.

00:03:45.967 --> 00:03:48.267
Quando estávamos aprendendo a usá-los

00:03:48.267 --> 00:03:51.167
fomos apresentados ao caso da
doença asiática,

00:03:51.167 --> 00:03:52.900
que é o seguinte:

00:03:52.900 --> 00:03:56.700
Uma doença terrível atingiu 600
pessoas na sua cidade, certo?

00:03:56.700 --> 00:03:59.967
Portanto, há 600 pessoas na cidade
que estão destinadas a morrer.

00:03:59.967 --> 00:04:02.900
Você é o prefeito e estão
disponíveis 2 opções de tratamento:

00:04:02.900 --> 00:04:05.200
o plano A ou B.

00:04:05.200 --> 00:04:09.033
Eu pedi para metade de vocês olhar
para o lado verde da descrição,

00:04:09.033 --> 00:04:12.733
que diz que o plano A é aquele em
que 200 pessoas vão viver,

00:04:12.733 --> 00:04:16.200
enquanto que o plano B é aquele em
que há 1/3 de probabilidade

00:04:16.200 --> 00:04:19.867
de 600 pessoas viverem e 2/3
de ninguém viver.

00:04:19.867 --> 00:04:24.633
E a outra metade olhou
exatamente para o mesmo plano,

00:04:24.633 --> 00:04:27.133
descrito não nos termos
de quem vai viver,

00:04:27.133 --> 00:04:28.967
e sim de quem vai morrer.

00:04:28.967 --> 00:04:34.167
Assim, o plano A diz: 200 das
600 pessoas vão viver,

00:04:34.167 --> 00:04:36.733
o que significa a morte de 400.

00:04:36.733 --> 00:04:41.533
E aqui o plano A significa que 400
das 600 pessoas morrem,

00:04:41.533 --> 00:04:44.500
isto é, 200 seguem vivas.

00:04:44.500 --> 00:04:48.033
No entanto, e esta foi o nosso
1º uso dos clickers,

00:04:48.033 --> 00:04:52.267
as atitudes que vocês tiveram com os
casos diferem, ao passo que,

00:04:52.267 --> 00:04:56.133
quando foi apresentado o número
de pessoas que viverão,

00:04:56.133 --> 00:05:01.667
66% optaram pelo plano A e
apenas 34% escolheram o B.

00:05:01.667 --> 00:05:04.133
Quando invertemos o enquadramento,

00:05:04.133 --> 00:05:07.533
os números saíram exatamente o oposto.

00:05:07.533 --> 00:05:11.000
Então, 66 de vocês favoreceram
o plano A no caso verde,

00:05:11.000 --> 00:05:13.467
64 favoreceram o plano A no azul.

00:05:13.467 --> 00:05:17.833
Mas o plano A e o plano B são
matematicamente idênticos.

00:05:17.833 --> 00:05:24.800
Talvez isso seja o que está
acontecendo nos casos do trem.

00:05:24.800 --> 00:05:26.800
E mesmo não sendo um exemplo real,

00:05:26.800 --> 00:05:29.167
imaginem que vocês foram
apresentados ao seguinte:

00:05:29.167 --> 00:05:32.133
Um trem está indo
desenfreado em direção

00:05:32.133 --> 00:05:34.500
a 6 pessoas na sua cidade.

00:05:34.500 --> 00:05:38.367
Você é o prefeito e tem 2
opções disponíveis:

00:05:38.367 --> 00:05:39.600
o plano A e o plano B.

00:05:39.600 --> 00:05:44.600
E então eu apresento o plano A para
você: 1 pessoa será salva,

00:05:44.600 --> 00:05:47.267
o que significa que 5 pessoas morrem.

00:05:47.267 --> 00:05:49.800
Ou o plano B: em que 1
pessoa vai morrer,

00:05:49.800 --> 00:05:53.467
o que significa que 5
pessoas serão salvas.

00:05:53.467 --> 00:05:58.167
E há uma inclinação, penso eu, para
ir com o plano B no caso azul

00:05:58.167 --> 00:06:00.933
e o plano A no caso verde.

00:06:00.933 --> 00:06:03.533
E isso é generalizado.

00:06:03.533 --> 00:06:08.233
Dependendo em quem estamos
focando esses dilemas morais,

00:06:08.233 --> 00:06:10.700
obtemos diferentes respostas.

00:06:10.700 --> 00:06:14.033
Se pensarmos no caso do choro do
bebê de Josh Greene,

00:06:14.033 --> 00:06:17.533
em que você está trancado num
porão com outras 19 pessoas

00:06:17.533 --> 00:06:22.300
e o seu bebê chorando, cercados
por soldados inimigos

00:06:22.300 --> 00:06:24.733
que matarão a todos se
forem encontrados,

00:06:24.733 --> 00:06:30.233
o dilema é se você deve
asfixiar o bebê,

00:06:30.233 --> 00:06:36.200
cujo choro vai chamar a atenção

00:06:36.200 --> 00:06:39.300
para o esconderijo e causar a
morte de 20 pessoas?

00:06:39.300 --> 00:06:41.467
Parece com o caso de Jim e os índios,

00:06:41.467 --> 00:06:44.900
mas com uma premissa
ainda mais dolorosa.

00:06:44.900 --> 00:06:49.100
Se você focar a sua
atenção nesse caso

00:06:49.100 --> 00:06:53.633
para a experiência de
tapar a boca do bebê,

00:06:53.633 --> 00:06:57.667
é praticamente impossível julgar isso

00:06:57.667 --> 00:07:00.767
como a coisa moralmente obrigatória.

00:07:00.767 --> 00:07:05.267
Mas se você redirecionar o foco

00:07:05.267 --> 00:07:09.167
para a criança de 2 anos que
está ao lado, a de 4 anos,

00:07:09.167 --> 00:07:11.700
o idoso no outro canto da sala...

00:07:11.700 --> 00:07:16.333
todos eles vão morrer se você não
fizer nada com o bebê...

00:07:16.333 --> 00:07:20.900
a sua resposta muda.

00:07:20.900 --> 00:07:26.633
E a mudança no sentido da nossa
atenção é algo endêmico

00:07:26.633 --> 00:07:33.133
em todos esses tipos de casos.

00:07:33.133 --> 00:07:36.233
Nós somos capazes de nos
concentrarmos apenas

00:07:36.233 --> 00:07:39.567
em partes do mundo ao mesmo tempo.

00:07:39.567 --> 00:07:45.600
E, como resultado, é
incrivelmente difícil manter o
foco

00:07:45.600 --> 00:07:51.567
de modo que esses tipos de dilemas
morais pareçam estáveis.

00:07:51.567 --> 00:07:55.533
Portanto, Sunstein sugere que esse
fenômeno, pelo que ele caracteriza,

00:07:55.533 --> 00:07:59.400
têm de ser moralmente
irrelevante... certo?

00:07:59.400 --> 00:08:01.433
Ele não pode ser moralmente relevante

00:08:01.433 --> 00:08:04.300
na coisa certa a se fazer
no caso do Trolley

00:08:04.300 --> 00:08:07.733
se você enquadrá-lo segundo o
número de pessoas que vão viver

00:08:07.733 --> 00:08:09.867
ou o número de mortes.

00:08:09.867 --> 00:08:12.133
Pelo menos, prima facie,

00:08:12.133 --> 00:08:14.933
isso não parece ser o tipo de coisa
que poderia ser relevante.

00:08:14.933 --> 00:08:16.733
Você está tomando
exatamente a mesma decisão

00:08:16.733 --> 00:08:18.333
enquadrada em 2 aspectos diferentes.

00:08:18.333 --> 00:08:21.633
Como isso poderia ser o que
faz a diferença?

00:08:21.633 --> 00:08:24.867
A sugestão de Sunstein é
que o mecanismo

00:08:24.867 --> 00:08:27.767
que fundamenta o fenômeno que
eu acabei de descrever

00:08:27.767 --> 00:08:31.200
ocorre repetidamente,

00:08:31.200 --> 00:08:35.800
não apenas em casos hipotéticos no
estilo do problema do Trolley,

00:08:35.800 --> 00:08:39.433
mas todo o tempo no tipo de
raciocínio moral

00:08:39.433 --> 00:08:42.367
que nos envolvemos como
cidadãos de uma democracia,

00:08:42.367 --> 00:08:46.167
tentando fazer julgamentos sobre a
distribuição de recursos,

00:08:46.167 --> 00:08:50.000
sobre que tipos de leis devem
ser postas em prática

00:08:50.000 --> 00:08:53.867
para regular ou incentivar certos
tipos de comportamento.

00:08:53.867 --> 00:08:59.467
Assim, em cada um dos 4 domínios
seguintes, diz Sunstein,

00:08:59.467 --> 00:09:03.200
nos concentramos muitas vezes nas
heurísticas, ou seja,

00:09:03.200 --> 00:09:05.433
nas características da
superfície do fenômeno,

00:09:05.433 --> 00:09:07.933
em vez dos atributos-alvo, isto é,

00:09:07.933 --> 00:09:09.633
a coisa que nos preocupamos
numa última análise.

00:09:09.633 --> 00:09:11.367
Lembrem-se que na última aula

00:09:11.367 --> 00:09:13.667
eu falei sobre colocar uma
capa no celular

00:09:13.667 --> 00:09:15.500
para que ele seja
facilmente identificável,

00:09:15.500 --> 00:09:19.633
que isso dá acesso heurístico a
qual telefone é o seu.

00:09:19.633 --> 00:09:24.267
Mas claro que esse adorno é útil

00:09:24.267 --> 00:09:26.667
para encontrar o seu
celular apenas na medida

00:09:26.667 --> 00:09:29.300
em que ele controla o atributo-alvo
que é importante para você,

00:09:29.300 --> 00:09:30.800
ou seja, encontrar o telefone

00:09:30.800 --> 00:09:34.067
que possui os contatos que são
importantes para você.

00:09:34.067 --> 00:09:36.267
E quando alvos e
heurísticas se separam,

00:09:36.267 --> 00:09:37.567
estamos em apuros.

00:09:37.567 --> 00:09:39.400
Portanto, afirma Sunstein,

00:09:39.400 --> 00:09:42.933
quando estamos pensando na
regulação do risco, isto é,

00:09:42.933 --> 00:09:47.067
o que fazemos com o fato de
que, como seres humanos,

00:09:47.067 --> 00:09:51.067
muitas coisas feitas tem o
potencial de causar dano,

00:09:51.067 --> 00:09:56.800
mas nós não queremos passar as
nossas vidas envoltos em isopor,

00:09:56.800 --> 00:09:58.967
movendo-nos muito lentamente pelo mundo,

00:09:58.967 --> 00:10:00.467
para não se chocar com as coisas.

00:10:00.467 --> 00:10:03.533
Considerando que estamos
dispostos a assumir riscos,

00:10:03.533 --> 00:10:08.233
como é que a nossa tendência de
usar as heurísticas interage

00:10:08.233 --> 00:10:09.967
com a nossa regulação?

00:10:09.967 --> 00:10:11.833
Nos casos de punição, e
esse é o 1º tema

00:10:11.833 --> 00:10:14.267
que vamos tratar depois da pausa,

00:10:14.267 --> 00:10:18.000
Sunstein acha que usamos a
heurística de formas que nos levam

00:10:18.000 --> 00:10:21.233
a nos comportarmos de
maneiras contraproducentes

00:10:21.233 --> 00:10:24.233
ao punir os indivíduos e coletivos.

00:10:24.233 --> 00:10:30.333
Em nossa hesitação para fazer
certos tipos de opções

00:10:30.333 --> 00:10:34.200
na área da medicina
reprodutiva, pensa Sunstein,

00:10:34.200 --> 00:10:38.467
corremos o risco de confundir as
heurísticas pelos alvos.

00:10:38.467 --> 00:10:43.800
E tomando a distinção
ato-omissão tão a sério,

00:10:43.800 --> 00:10:47.433
corremos o risco de confundir as
heurísticas pelos alvos.

00:10:47.433 --> 00:10:51.600
Portanto, vamos voltar à questão
da punição logo após

00:10:51.600 --> 00:10:56.367
o feriado e à questão do
ato-omissão na parte final da aula.

00:10:56.367 --> 00:10:59.733
O que eu quero fazer agora é
verificar 3 exemplos

00:10:59.733 --> 00:11:03.800
de regulação do risco a partir
da análise de Sunstein.

00:11:03.800 --> 00:11:06.667
E a 3ª delas... Na verdade,

00:11:06.667 --> 00:11:09.500
eu estou muito curiosa para ver os
números que vão sair daqui...

00:11:09.500 --> 00:11:13.667
Portanto, Sunstein aponta, e
isso me parece correto,

00:11:13.667 --> 00:11:18.833
que as pessoas são mais
suscetíveis a condenar uma empresa

00:11:18.833 --> 00:11:21.100
quando o comportamento descrito

00:11:21.100 --> 00:11:24.433
envolve a certeza de risco.

00:11:24.433 --> 00:11:26.233
Portanto, eis a empresa A,

00:11:26.233 --> 00:11:29.733
que oferece um produto usados por
10 milhões de pessoas,

00:11:29.733 --> 00:11:32.333
que matará 10 pessoas.

00:11:32.333 --> 00:11:36.000
Das 10 milhões de pessoas que
utilizam esse produto,

00:11:36.000 --> 00:11:40.833
10 vão apresentar uma reação
que as levará à morte.

00:11:40.833 --> 00:11:47.900
E o custo de eliminar totalmente esse
risco é de US$ 100 milhões.

00:11:47.900 --> 00:11:53.433
Há uma sensação, uma
inclinação por parte de muitos,

00:11:53.433 --> 00:11:58.133
em pensar que a empresa deve pagar
para se livrar desse risco;

00:11:58.133 --> 00:12:00.500
que isso é inaceitável na
produção de um produto

00:12:00.500 --> 00:12:02.633
em que 10 pessoas morrerão.

00:12:02.633 --> 00:12:09.433
Em contrapartida, se enquadramos o caso
em termos de probabilidades,

00:12:09.433 --> 00:12:11.533
que 10 milhões de pessoas
usam o produto,

00:12:11.533 --> 00:12:15.267
que ele produz um risco de
morte de 1 por milhão

00:12:15.267 --> 00:12:18.133
e eliminar o risco sai muito caro,

00:12:18.133 --> 00:12:22.867
este é o tipo de coisa que nós
permitimos o tempo todo.

00:12:22.867 --> 00:12:29.833
Sem esse tipo de tolerância ao risco não
haveria inovação tecnológica

00:12:29.833 --> 00:12:32.233
e a maioria dos bens e recursos

00:12:32.233 --> 00:12:36.933
que todos nós tomamos por certo
nunca teria existido.

00:12:36.933 --> 00:12:39.100
Assim, a alegação de
Sunstein aqui é que,

00:12:39.100 --> 00:12:43.667
embora os atributos-alvo sejam
idênticos nas 2 situações,

00:12:43.667 --> 00:12:46.033
neste caso, 10 pessoas morrerão

00:12:46.033 --> 00:12:48.367
e salvá-las teria um custo
de US$ 100 milhões.

00:12:48.367 --> 00:12:52.100
Neste caso, 10 pessoas morrerão e
salvá-las teria um custo

00:12:52.100 --> 00:12:55.767
de US$ 100 milhões; os
atributos-alvo são idênticos.

00:12:55.767 --> 00:12:58.333
Em ambos os casos, 10 pessoas morrem

00:12:58.333 --> 00:13:01.433
e salvá-las custa a
quantia especificada.

00:13:01.433 --> 00:13:04.167
Os atributos heurísticos diferem.

00:13:04.167 --> 00:13:06.400
Este está enquadrado em
termos de segurança;

00:13:06.400 --> 00:13:08.800
este em termos de risco.

00:13:08.800 --> 00:13:13.433
E nós temos uma ótima
heurística que é a seguinte:

00:13:13.433 --> 00:13:16.033
Se 10 pessoas morrerão por causa
do que você está fazendo,

00:13:16.033 --> 00:13:17.367
não faça isso.

00:13:17.367 --> 00:13:23.700
E o argumento de Sunstein é que
a assimetria na resposta

00:13:23.700 --> 00:13:27.233
para esses casos é irracional.

00:13:27.233 --> 00:13:29.867
De fato, se propusermos
este caso a um risco

00:13:29.867 --> 00:13:33.067
de 2 mortes por milhão

00:13:33.067 --> 00:13:37.000
e tivermos este com uma
certeza de 10 mortes,

00:13:37.000 --> 00:13:46.233
as pessoas ainda estariam inclinadas
a condenar a 1ª escolha,

00:13:46.233 --> 00:13:52.167
embora, nessa conjuntura, a 2ª
opção seja claramente a pior.

00:13:52.167 --> 00:13:57.133
Como resultado de interpretar mal os
atributos heurísticos pelos

00:13:57.133 --> 00:14:02.000
de alvo, cometemos erros em que tipos
de comportamentos permitimos.

00:14:02.000 --> 00:14:04.567
Sunstein acha que isso é o
que está acontecendo

00:14:04.567 --> 00:14:07.800
no caso do comércio de
emissões ("cap and trade")

00:14:07.800 --> 00:14:09.533
do qual ele foi um dos
primeiros defensores.

00:14:09.533 --> 00:14:12.067
No modelo do comércio de emissões,

00:14:12.067 --> 00:14:14.400
os poluidores recebem uma licença

00:14:14.400 --> 00:14:17.167
para poluir "n" unidades de
poluição no ar e,

00:14:17.167 --> 00:14:21.267
em seguida, negociam essas licenças
no mercado de tal forma que,

00:14:21.267 --> 00:14:25.167
sem dúvida, diminui a
poluição a um custo menor.

00:14:25.167 --> 00:14:28.267
Vamos deixar Sunstein com a economia...

00:14:28.267 --> 00:14:33.400
Ainda assim, existe uma resistência
quanto ao "cap and trade".

00:14:33.400 --> 00:14:36.033
Porque mesmo se estamos
dispostos a admitir

00:14:36.033 --> 00:14:37.867
a presença do atributo-alvo

00:14:37.867 --> 00:14:40.700
– isto é, que a quantidade de
poluição foi reduzida –,

00:14:40.700 --> 00:14:43.033
o atributo heurístico...

00:14:43.033 --> 00:14:45.800
"As pessoas estão pagando para poluir?

00:14:45.800 --> 00:14:49.633
Você não deve estar habilitado a
pagar por um delito grave"...

00:14:49.633 --> 00:14:53.000
nos parece problemático.

00:14:53.000 --> 00:14:55.433
Agora, é um fenômeno interessante

00:14:55.433 --> 00:14:59.400
que a resistência a esse tipo de
raciocínio ocorra segundo o contexto,

00:14:59.400 --> 00:15:05.200
tanto da direita quanto da esquerda.

00:15:05.200 --> 00:15:12.567
Portanto, há resistência à
mercantilização das coisas por parte

00:15:12.567 --> 00:15:19.333
da esquerda e há resistência por
parte da direita a outros tipos

00:15:19.333 --> 00:15:23.533
de enquadramentos que
sugerem que as respostas

00:15:23.533 --> 00:15:26.933
em casos de tecnologias
reprodutivas como a clonagem são,

00:15:26.933 --> 00:15:31.300
segundo Sunstein, devido à
heurística "não brinque de Deus".

00:15:31.300 --> 00:15:35.800
E quando confrontados com a sugestão

00:15:35.800 --> 00:15:43.500
"Você só está usando uma
heurística", ambos os lados

00:15:43.500 --> 00:15:47.900
respondem com hostilidade à
análise acadêmica.

00:15:47.900 --> 00:15:51.533
Nos anos 1970, era comum

00:15:51.533 --> 00:15:57.033
os defensores do acúmulo de
arsenais nucleares

00:15:57.033 --> 00:16:00.100
apelarem a uma noção chamada
"destruição mutuamente assegurada"

00:16:00.100 --> 00:16:03.733
que falaremos quando falarmos do
dilema dos prisioneiros.

00:16:03.733 --> 00:16:07.467
A ideia básica é que se ambos os
lados têm armas suficientes

00:16:07.467 --> 00:16:12.700
para destruir o outro, então
nenhum vai utilizá-las,

00:16:12.700 --> 00:16:16.500
porque a função de
dissuasão é muito grande.

00:16:16.500 --> 00:16:21.667
Houve resistência por
parte da esquerda,

00:16:21.667 --> 00:16:25.933
porque essa análise
parecia muito esperta.

00:16:25.933 --> 00:16:30.667
Há resistência por ambos os
lados ao tipo de análise

00:16:30.667 --> 00:16:35.367
exposto aqui por Sunstein, porque
ela vai contra a ideia

00:16:35.367 --> 00:16:39.767
de que somos
introspectivamente
transparentes

00:16:39.767 --> 00:16:42.600
de tal forma que os nossos
julgamentos são indicativos

00:16:42.600 --> 00:16:45.100
das coisas que nos preocupamos.

00:16:45.100 --> 00:16:49.367
Portanto, o último exemplo que
eu quero dar de Sunsteins

00:16:49.367 --> 00:16:52.233
é a nossa enquete.

00:16:52.233 --> 00:16:56.700
Sunstein formula... os seus
clickers estão funcionando?

00:16:56.700 --> 00:17:02.633
Sunstein diz que nos sentimos
mais desconfortáveis

00:17:02.633 --> 00:17:05.933
sendo prejudicados pelas coisas que
têm a função de nos proteger

00:17:05.933 --> 00:17:10.267
que pelas coisas que não
têm essa função.

00:17:10.267 --> 00:17:14.233
E ele sugere que existem dados mostrando

00:17:14.233 --> 00:17:16.733
que se as pessoas podem
escolher entre 2 carros...

00:17:16.733 --> 00:17:19.533
No 1º há uma chance de 2%

00:17:19.533 --> 00:17:23.200
de você ser morto pelo volante,
caso se envolva num acidente.

00:17:23.200 --> 00:17:26.267
E o 2º é um carro onde há
uma chance de 1%,

00:17:26.267 --> 00:17:28.967
caso se envolva num acidente, de você
ser morto pelo volante, mas,

00:17:28.967 --> 00:17:34.467
além disso, há um 1/10 de 1% de
chance do airbag matar você.

00:17:34.467 --> 00:17:37.867
E a pergunta é: qual
carro você escolhe?

00:17:37.867 --> 00:17:41.667
Aquele em que há uma chance de 2% de
você ser morto pelo volante

00:17:41.667 --> 00:17:46.133
ou aquele em que há uma chance de 1% de
você ser morto pelo volante,

00:17:46.133 --> 00:17:50.433
mas 1/10 de chance sobre 1% de
você ser morto pelo airbag,

00:17:50.433 --> 00:17:52.633
que deveria protegê-lo.

00:17:52.633 --> 00:17:55.000
E vamos ver os números que saem.

00:17:55.000 --> 00:17:56.233
Eu tenho de dizer, estou
fazendo esta votação

00:17:56.233 --> 00:17:59.533
porque as minhas intuições não
se alinham com Sunstein

00:17:59.533 --> 00:18:01.933
e estou curiosa para saber a de vocês.

00:18:01.933 --> 00:18:06.233
OK... Vamos ver como
saíram os números.

00:18:06.233 --> 00:18:12.133
Assim, 15% de vocês
comprariam o carro A

00:18:12.133 --> 00:18:15.900
e 85% de vocês comprariam o B.

00:18:15.900 --> 00:18:18.167
Portanto, 85% estão fazendo

00:18:18.167 --> 00:18:21.633
o que é a escolha
estatisticamente racional.

00:18:21.633 --> 00:18:28.400
Mas uma boa parte de vocês está
disposta a arriscar um dano maior,

00:18:28.400 --> 00:18:32.833
como para evitar este
sentimento de traição

00:18:32.833 --> 00:18:35.500
por aquilo que tem a
função de proteger.

00:18:35.500 --> 00:18:42.867
A sugestão de Sunstein, só para
resumir, é que no raciocínio moral

00:18:42.867 --> 00:18:46.367
frequentemente substituímos os
atributos heurísticos

00:18:46.367 --> 00:18:48.233
pelos alvos.

00:18:48.233 --> 00:18:51.700
E isso é um erro.

00:18:51.700 --> 00:18:55.133
Então, o que sugerem as 3
respostas ao problema do Trolley

00:18:55.133 --> 00:18:57.233
consideradas por nós?

00:18:57.233 --> 00:19:00.133
Bem, Thomson diz isto:

00:19:00.133 --> 00:19:03.833
Que reconsiderar as nossas intuições a
partir de casos alternativos,

00:19:03.833 --> 00:19:05.933
como é o caso
alternativo da
testemunha,

00:19:05.933 --> 00:19:09.900
onde você imagina ser uma
das pessoas na via...

00:19:09.900 --> 00:19:12.667
Que isso pode levar a mudanças

00:19:12.667 --> 00:19:17.000
na nossa avaliação dos casos.

00:19:17.000 --> 00:19:23.033
E essas mudanças nas nossas
respostas, ela pensa,

00:19:23.033 --> 00:19:27.567
revelam algo moralmente significativo.

00:19:27.567 --> 00:19:33.233
Podemos aprender com a
contemplação desses casos
específicos

00:19:33.233 --> 00:19:38.267
o que a moralidade exige de nós.

00:19:38.267 --> 00:19:40.700
Greene e Sunstein, por outro lado,

00:19:40.700 --> 00:19:43.833
afirmam que a nossa intuição
responde a casos seguindo

00:19:43.833 --> 00:19:47.900
frequentemente características que
são moralmente irrelevantes

00:19:47.900 --> 00:19:50.067
e que, como consequência,

00:19:50.067 --> 00:19:55.100
essas características não revelam
algo moralmente significativo.

00:19:55.100 --> 00:19:58.633
A pergunta é esta:

00:19:58.633 --> 00:20:05.400
Isso é um problema para Mill e Kant?

00:20:05.400 --> 00:20:14.300
Vamos rever as páginas iniciais do
tratado de Mill sobre o utilitarismo.

00:20:14.300 --> 00:20:17.400
Ele escreve lá... e eu não pedi
para ler esta passagem,

00:20:17.400 --> 00:20:19.633
por isso não há nenhuma razão para
que vocês saibam o que ele diz.

00:20:19.633 --> 00:20:21.433
"Embora na ciência

00:20:21.433 --> 00:20:24.200
as verdades particulares
precedam a teoria geral,

00:20:24.200 --> 00:20:28.700
pode-se esperar que aconteça o
contrário numa arte prática,

00:20:28.700 --> 00:20:31.033
como a moral ou a legislação."

00:20:31.033 --> 00:20:33.067
Portanto, na ciência,
analisamos casos particulares.

00:20:33.067 --> 00:20:37.067
Descobrimos que soltamos este objeto e
ele cai com aceleração A,

00:20:37.067 --> 00:20:40.033
soltamos este objeto e descobrimos que
ele cai com aceleração A,

00:20:40.033 --> 00:20:42.467
soltamos este objeto e que ele
cai com aceleração A.

00:20:42.467 --> 00:20:46.400
E, a partir daí, podemos concluir que a
lei que rege a queda dos corpos

00:20:46.400 --> 00:20:48.567
é a que eles caem com aceleração A.

00:20:48.567 --> 00:20:49.867
Portanto, "Embora na ciência

00:20:49.867 --> 00:20:51.567
as verdades particulares
precedam a teoria geral,

00:20:51.567 --> 00:20:54.233
pode-se esperar que aconteça o
contrário numa arte prática,

00:20:54.233 --> 00:20:55.800
como a moral…

00:20:55.800 --> 00:20:59.000
Um teste do certo e do errado tem
de ser o meio de determinar

00:20:59.000 --> 00:21:00.633
aquilo que está certo e
aquilo que está errado…

00:21:00.633 --> 00:21:05.133
e não uma consequência dos
termos já determinados".

00:21:05.133 --> 00:21:08.667
"Não se consegue evitar a dificuldade

00:21:08.667 --> 00:21:12.467
[de construir uma teoria de julgamentos]

00:21:12.467 --> 00:21:15.733
recorrendo a" – o que às vezes é
chamado de senso moral –

00:21:15.733 --> 00:21:18.233
"uma faculdade natural – diz
Mill – que discerne

00:21:18.233 --> 00:21:21.600
aquilo que está certo e errado no caso
particular que enfrentamos",

00:21:21.600 --> 00:21:23.067
assim como nossos outros sentidos

00:21:23.067 --> 00:21:26.000
discernem a visão e o som presentes.

00:21:26.000 --> 00:21:29.333
Portanto, como é possível ver se um
caso é moralmente errado.

00:21:29.333 --> 00:21:35.167
O raciocínio moral, a
percepção moral, segundo ele,

00:21:35.167 --> 00:21:40.133
"é um ramo da nossa razão, não da
nossa faculdade sensível".

00:21:40.133 --> 00:21:42.333
A moralidade da ação individual…

00:21:42.333 --> 00:21:48.000
é uma questão da aplicação da
lei a um caso particular…

00:21:48.000 --> 00:21:52.933
Assim sendo, qualquer que seja a
firmeza ou constância

00:21:52.933 --> 00:21:55.367
que a nossa crença moral
tenha alcançado,

00:21:55.367 --> 00:22:03.467
é devido à influência tácita dessa
norma reflexivamente disponível.

00:22:03.467 --> 00:22:10.600
Mill está construindo a teoria de
teorias, e não a teoria de casos.

00:22:10.600 --> 00:22:17.367
Kant. "Não se poderia prestar
pior serviço à moralidade

00:22:17.367 --> 00:22:21.100
do que querer extraí-la de exemplos.

00:22:21.100 --> 00:22:24.000
Pois cada exemplo [...] tem de
ser primeiro julgado

00:22:24.000 --> 00:22:27.300
segundo os princípios da
moralidade para se saber

00:22:27.300 --> 00:22:33.567
se é digno de servir de exemplo
original, isto é, de modelo."

00:22:33.567 --> 00:22:40.267
Temos aqui, de certa forma,
corporificado, o diálogo deste curso.

00:22:40.267 --> 00:22:45.367
Até que ponto a nossa
capacidade de reflexão racional

00:22:45.367 --> 00:22:50.600
é o melhor caminho para responder às
perguntas que nos preocupam?

00:22:50.600 --> 00:22:57.033
Até que ponto a nossa capacidade
para a resposta emocional,

00:22:57.033 --> 00:23:01.467
para a sensação, para o
julgamento instintivo

00:23:01.467 --> 00:23:05.567
com base na apresentação de
casos particulares

00:23:05.567 --> 00:23:10.267
é um indicativo das respostas às
questões que nos preocupam?

00:23:10.267 --> 00:23:14.867
Portanto, isso fecha a
discussão dos casos do Trolley.

00:23:14.867 --> 00:23:18.200
E o que eu quero fazer na
2ª metade da aula

00:23:18.200 --> 00:23:21.333
é percorrer 2 tipos de quebra-cabeças

00:23:21.333 --> 00:23:27.000
que persistem independentemente
de qual atitude tomamos.

00:23:27.000 --> 00:23:29.667
O 1º é algo apresentado a vocês

00:23:29.667 --> 00:23:33.500
como nota promissória na 1ª aula
expositiva, porque este é

00:23:33.500 --> 00:23:36.467
um dos trabalhos mais
divertidos que leremos.

00:23:36.467 --> 00:23:41.667
É o artigo de Roy Sorensen com
[Christopher] Boorse

00:23:41.667 --> 00:23:43.567
sobre esquiva e sacrifício.

00:23:43.567 --> 00:23:45.733
Vocês se lembram, esse
foi o fim de semana

00:23:45.733 --> 00:23:51.333
que o senador do Arizona foi baleado,

00:23:51.333 --> 00:23:52.767
então eu não podia fazer
isso com balas.

00:23:52.767 --> 00:23:57.100
Vocês se lembrem do caso...
você está numa fila.

00:23:57.100 --> 00:23:58.100
Você é o cara amarelo.

00:23:58.100 --> 00:24:00.867
E um urso está vindo na sua direção.

00:24:00.867 --> 00:24:02.800
E se você sair da fila,

00:24:02.800 --> 00:24:05.667
o urso come a pessoa atrás de você.

00:24:05.667 --> 00:24:09.200
Contraste isso com o caso em que
você está numa fila.

00:24:09.200 --> 00:24:10.533
Você ainda é o cara amarelo.

00:24:10.533 --> 00:24:12.300
Um urso está vindo na sua direção

00:24:12.300 --> 00:24:14.367
e você se vira, pega o
cara atrás de você

00:24:14.367 --> 00:24:18.100
e o coloca na sua frente; e
o urso come ele.

00:24:18.100 --> 00:24:22.933
O 1º é um caso clássico
chamado de esquiva.

00:24:22.933 --> 00:24:25.000
Ou seja, você está numa situação

00:24:25.000 --> 00:24:27.767
em que há um mal vindo
na sua direção.

00:24:27.767 --> 00:24:29.600
Você sai do caminho do mal

00:24:29.600 --> 00:24:32.100
e o mal atinge outra
pessoa no seu lugar.

00:24:32.100 --> 00:24:35.233
O 2º é um caso clássico
de sacrifício.

00:24:35.233 --> 00:24:36.800
Há um mal vindo na sua direção

00:24:36.800 --> 00:24:41.533
e você utiliza outra
pessoa como escudo.

00:24:41.533 --> 00:24:43.933
Então, esquivar-se é evitar danos,

00:24:43.933 --> 00:24:45.967
permitindo que ele recaia
em outra pessoa.

00:24:45.967 --> 00:24:48.967
Sacrificar é evitar danos
provocando que o mal vá para outro,

00:24:48.967 --> 00:24:54.067
se você usar essa pessoa como escudo.

00:24:54.067 --> 00:24:58.267
E isso é análogo à
distinção ato-omissão,

00:24:58.267 --> 00:24:59.933
já vista por nós,

00:24:59.933 --> 00:25:04.033
mas está totalmente dentro
do reino das ações.

00:25:04.033 --> 00:25:08.300
Agora, o que Sorensen e
Boorse trazem no artigo

00:25:08.300 --> 00:25:12.867
é o quão resiliente é este
fenômeno, independentemente

00:25:12.867 --> 00:25:17.033
de como você embaralha o
enquadramento dos casos.

00:25:17.033 --> 00:25:19.867
Eles dão o exemplo do
atirador no shopping.

00:25:19.867 --> 00:25:21.667
Há uma bala vindo na sua direção

00:25:21.667 --> 00:25:23.467
e a sua opção é pular para o lado

00:25:23.467 --> 00:25:27.033
ou puxar alguém para a sua frente,
evitando que a bala acerte você.

00:25:27.033 --> 00:25:28.900
É o caso do caminhão em
alta velocidade.

00:25:28.900 --> 00:25:31.267
Você está numa fila de carros.

00:25:31.267 --> 00:25:34.467
Um caminhão vem em alta
velocidade atrás de você;

00:25:34.467 --> 00:25:36.033
ele vai bater no seu carro.

00:25:36.033 --> 00:25:38.167
E você tem 2 coisas a fazer.

00:25:38.167 --> 00:25:39.667
Na 1ª, você muda de pista

00:25:39.667 --> 00:25:42.100
e o caminhão bate no carro que
estava na sua frente.

00:25:42.100 --> 00:25:46.367
Na 2ª, você indica para um carro
que está atrás de você

00:25:46.367 --> 00:25:47.667
que ele mude para a sua pista...

00:25:47.667 --> 00:25:50.000
e o caminhão bate nele.

00:25:50.000 --> 00:25:52.633
Há o caso dos terroristas.

00:25:52.633 --> 00:25:54.233
Você está num avião.

00:25:54.233 --> 00:25:55.533
Uns terroristas líbios

00:25:55.533 --> 00:25:58.233
– boa hora para falar
sobre a Líbia –,

00:25:58.233 --> 00:25:59.900
neste exemplo, tomam o seu avião

00:25:59.900 --> 00:26:04.967
e ameaçam matar todos os
norte-americanos.

00:26:04.967 --> 00:26:07.667
Você é um representante

00:26:07.667 --> 00:26:11.367
do Departamento de Estado dos EUA

00:26:11.367 --> 00:26:13.600
e a sua pasta tem um adesivo
da instituição,

00:26:13.600 --> 00:26:15.133
e os terroristas estão vindo no
corredor, prestes a atirar em você.

00:26:15.133 --> 00:26:16.267
2 possibilidades.

00:26:16.267 --> 00:26:21.233
Você cobre o adesivo com etiqueta
da companhia aérea líbia,

00:26:21.233 --> 00:26:25.400
então eles vão saltar você e atirar
na mulher sentada ao seu lado,

00:26:25.400 --> 00:26:26.300
a próximo da fila.

00:26:26.300 --> 00:26:30.533
Ou você troca a sua pasta com a
da pessoa ao seu lado e,

00:26:30.533 --> 00:26:35.000
portanto, eles vão matá-la em
vez de atirar em você.

00:26:35.000 --> 00:26:37.433
Ou o caso dos barcos afundando.

00:26:37.433 --> 00:26:39.200
Você está em alto mar.

00:26:39.200 --> 00:26:42.467
Você está tentando sinalizar que o
seu barco está afundando,

00:26:42.467 --> 00:26:44.167
o barco do cara ao lado está afundando.

00:26:44.167 --> 00:26:45.767
Você está tentando avisar

00:26:45.767 --> 00:26:48.233
um avião que sobrevoa vocês.

00:26:48.233 --> 00:26:50.900
E você pode fazer 1 de 2 coisas:

00:26:50.900 --> 00:26:53.400
Você pode aumentar o seu sinal, certo?

00:26:53.400 --> 00:26:58.133
Fazer uma luz muito forte e, em
seguida, o avião vai resgatá-lo.

00:26:58.133 --> 00:27:01.033
Ou você pode bloquear o
sinal do outro cara,

00:27:01.033 --> 00:27:03.667
sinalizando relativamente mais forte

00:27:03.667 --> 00:27:06.800
para que a companhia aérea
venha resgatar você.

00:27:06.800 --> 00:27:10.033
Sorensen fornece vários
casos sobre o tema.

00:27:10.033 --> 00:27:13.233
Se insetos estão comendo as suas rosas,

00:27:13.233 --> 00:27:17.333
está bem colocar
repelente nas suas rosas,

00:27:17.333 --> 00:27:19.900
o que fará com eles migrem
para o jardim do vizinho;

00:27:19.900 --> 00:27:25.633
mas não está bem colocar atrativo
de insetos nas rosas dele.

00:27:25.633 --> 00:27:31.533
Temos essa estranha tendência de pensar

00:27:31.533 --> 00:27:38.833
que está bem esquivar-se
mas não blindar-se.

00:27:38.833 --> 00:27:44.300
Agora, a perplexidade que
Sorensen e Boorse consideram

00:27:44.300 --> 00:27:48.800
é que parece que não há
nenhuma maneira sistemática

00:27:48.800 --> 00:27:53.900
para explicar esses tipos de
discrepâncias na intuição.

00:27:53.900 --> 00:27:55.100
Vocês podem pensar o seguinte:

00:27:55.100 --> 00:27:56.933
O problema nesses casos é que,

00:27:56.933 --> 00:28:01.733
quando você amarra os pés
do seu oponente,

00:28:01.733 --> 00:28:03.233
quando você está
tentando fugir do urso,

00:28:03.233 --> 00:28:05.867
ou quando você o empurra
no caso do disparo,

00:28:05.867 --> 00:28:08.867
você interfere na concorrência leal.

00:28:08.867 --> 00:28:15.167
E essa concorrência leal é o que
importa nesse tipo de circunstâncias.

00:28:15.167 --> 00:28:18.900
Mas, claro, há um monte
de circunstâncias

00:28:18.900 --> 00:28:23.467
em que a competição é
injusta já de início.

00:28:23.467 --> 00:28:27.700
E, no entanto, isso ainda
parece problemático.

00:28:27.700 --> 00:28:32.500
Mesmo que o cara que você esteja
tentando "oferecer" ao urso

00:28:32.500 --> 00:28:37.300
seja um corredor muito mais
lento que você,

00:28:37.300 --> 00:28:41.900
ainda não parece bem
amarrar os sapatos dele.

00:28:41.900 --> 00:28:44.200
A justiça da competição
não parece ser

00:28:44.200 --> 00:28:46.200
o que está conduzindo a intuição.

00:28:46.200 --> 00:28:51.200
Por isso, talvez, eles digam que em
cada um dos casos de "empurrão"

00:28:51.200 --> 00:28:54.433
o que você faz é, de alguma
forma, o errado contido.

00:28:54.433 --> 00:28:58.100
É errado pegar alguém e
colocar na sua frente.

00:28:58.100 --> 00:29:01.867
Ao passo que, está tudo bem se
abaixar para que algo o atinja.

00:29:01.867 --> 00:29:04.300
É errado roubar a pasta de alguém.

00:29:04.300 --> 00:29:06.567
É errado bloquear o sinal de alguém.

00:29:06.567 --> 00:29:12.167
Mas, eles ressaltam, parece tão ruim

00:29:12.167 --> 00:29:16.733
colocar a pessoa na sua frente de
forma amigável, dizendo

00:29:16.733 --> 00:29:19.900
"Você gostaria de ver uma bela vista?"

00:29:19.900 --> 00:29:23.633
como agarrá-la e
colocá-la na sua frente.

00:29:23.633 --> 00:29:28.600
Segundo eles, é tão
problemático forçar alguém

00:29:28.600 --> 00:29:34.533
a pular de um penhasco gritando "E=MC2!"

00:29:34.533 --> 00:29:37.267
como é induzi-la a saltar do penhasco

00:29:37.267 --> 00:29:40.300
gritando um epíteto racial.

00:29:40.300 --> 00:29:46.533
O errado contido não parece ser o
que explica a nossa resposta.

00:29:46.533 --> 00:29:50.933
Por isso, eu vou deixar vocês
lerem estas respostas

00:29:50.933 --> 00:29:53.633
se vocês ainda não tiveram a chance...

00:29:53.633 --> 00:29:56.367
No entanto, a distinção ato-omissão
ou a distinção feito-permitido

00:29:56.367 --> 00:30:00.500
não parece suficiente para
realizar o trabalho.

00:30:00.500 --> 00:30:03.467
Tampouco a ideia de que o que importa

00:30:03.467 --> 00:30:06.333
é se você fosse o locus de
uma cadeia causal,

00:30:06.333 --> 00:30:09.533
o criador de uma
sequência de causalidade.

00:30:09.533 --> 00:30:12.233
Tampouco a doutrina do efeito duplo

00:30:12.233 --> 00:30:15.833
parece dar conta de todos esses casos.

00:30:15.833 --> 00:30:20.433
Tampouco recorrer à noção
de direitos de Kant

00:30:20.433 --> 00:30:25.533
em contraste com os utilitários
parece explicar todos esses casos.

00:30:25.533 --> 00:30:31.667
Sorensen e Boorse, um pouco relutantes,

00:30:31.667 --> 00:30:33.367
consideram uma conclusão de ceticismo.

00:30:33.367 --> 00:30:36.967
Que, basicamente, é que isso é uma
característica hesitante

00:30:36.967 --> 00:30:39.300
da nossa psicologia.

00:30:39.300 --> 00:30:43.000
Mas nós, ao ter ouvido a
1ª metade da aula,

00:30:43.000 --> 00:30:45.533
temos outra explicação alternativa.

00:30:45.533 --> 00:30:47.967
E eu não prometo que
funcionará em todos os casos,

00:30:47.967 --> 00:30:49.900
embora pareça bastante promissora.

00:30:49.900 --> 00:30:52.967
O que está ocorrendo nos casos
de esquiva e blindagem

00:30:52.967 --> 00:30:56.167
é a sobreaplicação de
uma heurística.

00:30:56.167 --> 00:31:01.967
Em geral, parece que sair do
caminho de um dano

00:31:01.967 --> 00:31:03.867
não é uma coisa ruim a se fazer,

00:31:03.867 --> 00:31:08.000
enquanto que colocar alguém
no caminho de um dano

00:31:08.000 --> 00:31:10.767
é algo ruim a se fazer.

00:31:10.767 --> 00:31:14.300
Portanto, talvez esse 1º
conjunto de quebra-cabeças

00:31:14.300 --> 00:31:19.200
possa ser explicado por meio
das heurísticas.

00:31:19.200 --> 00:31:24.033
Eu quero focar os últimos
15 minutos da aula

00:31:24.033 --> 00:31:26.100
num conjunto de quebra-cabeças.

00:31:26.100 --> 00:31:28.867
E para isso, vocês vão
precisar dos seus clickers.

00:31:28.867 --> 00:31:33.233
Portanto, vamos começar
com 4 motoristas.

00:31:33.233 --> 00:31:36.300
O 1º deles, o Alberto Sortudo,

00:31:36.300 --> 00:31:38.400
faz o seguinte:

00:31:38.400 --> 00:31:39.700
Ele entra no seu carro.

00:31:39.700 --> 00:31:43.167
O seu carro está em
perfeitas condições.

00:31:43.167 --> 00:31:46.800
Ele está atento a cada semáforo.

00:31:46.800 --> 00:31:49.533
Ele dirige de uma forma
extremamente segura.

00:31:49.533 --> 00:31:54.800
E, no final do dia, chega em
casa do trabalho.

00:31:54.800 --> 00:31:57.100
É isso aí. Este é o Alberto Sortudo.

00:31:57.100 --> 00:32:01.000
Questão: Quando o Alberto Sortudo
dirige até a casa dele,

00:32:01.000 --> 00:32:07.233
deixando de lado se a amante
também está no carro,

00:32:07.233 --> 00:32:10.133
se ele comprou um carro

00:32:10.133 --> 00:32:16.433
que possui uma alta taxa de emissões
em vez de comprar um Prius...

00:32:16.433 --> 00:32:18.633
Enquanto ele dirige, deixando de lado

00:32:18.633 --> 00:32:22.133
todas as outras coisas moralmente
erradas que possa ter feito,

00:32:22.133 --> 00:32:26.000
será que ele fez algo
moralmente condenável

00:32:26.000 --> 00:32:28.067
ao dirigir do trabalho para casa, tendo
o seu carro perfeitamente bem,

00:32:28.067 --> 00:32:31.200
sem fazer mal a ninguém
durante o caminho?

00:32:31.200 --> 00:32:32.733
Essa não é uma pergunta capciosa.

00:32:32.733 --> 00:32:34.800
Portanto, se você acha o
Alberto Sortudo

00:32:34.800 --> 00:32:36.100
fez algo moralmente condenável,

00:32:36.100 --> 00:32:37.333
deixando de lado todas as coisas

00:32:37.333 --> 00:32:41.133
que são moralmente condenáveis
alheias à condução, aperte 1.

00:32:41.133 --> 00:32:44.300
E se você acha que ele não fez
nada moralmente condenável,

00:32:44.300 --> 00:32:45.600
aperte 2.

00:32:45.600 --> 00:32:48.700
Então o que vocês estão julgando é:

00:32:48.700 --> 00:32:51.433
é moralmente problemático
dirigir do trabalho para casa

00:32:51.433 --> 00:32:53.833
– considerando todas as coisas,
se nada de ruim acontece?

00:32:53.833 --> 00:32:55.833
E vamos esperar que... OK.

00:32:55.833 --> 00:32:57.600
Portanto, há sempre estes 5%.

00:32:57.600 --> 00:32:59.333
As multidões anticarro.

00:32:59.333 --> 00:33:00.500
São os mesmos que fazem Medicina

00:33:00.500 --> 00:33:03.800
para fatiar o pobre rapaz
saudável da sala de espera.

00:33:03.800 --> 00:33:06.267
95% de vocês pensam que o
Alberto Sortudo

00:33:06.267 --> 00:33:08.567
não fez nada moralmente condenável.

00:33:08.567 --> 00:33:14.233
Vamos conhecer o irmão gêmeo do
Alberto Sortudo: o Alberto Azarado.

00:33:14.233 --> 00:33:16.233
Aqui está o que o Alberto Azarado fez:

00:33:16.233 --> 00:33:19.333
Exatamente o mesmo que o
Alberto Sortudo.

00:33:19.333 --> 00:33:24.000
Exceto que quando ele chegava em casa,

00:33:24.000 --> 00:33:28.733
uma criança saltou na frente do
seu carro e ele a atingiu.

00:33:28.733 --> 00:33:31.067
OK? O Alberto Azarado

00:33:31.067 --> 00:33:34.567
fez exatamente o mesmo que o Sortudo.

00:33:34.567 --> 00:33:39.167
Saiu do trabalho, checou os pneus,
esteve atento o tempo todo,

00:33:39.167 --> 00:33:41.367
dirigiu a uma velocidade
segura e adequada.

00:33:41.367 --> 00:33:47.633
Mas, por azar, no caminho de
casa matou uma criança.

00:33:47.633 --> 00:33:53.333
Questão: Será que o Alberto Azarado
faz algo moralmente condenável?

00:33:53.333 --> 00:33:57.233
Para sim, pressione 1.

00:33:57.233 --> 00:33:59.033
Para não, pressione 2.

00:33:59.033 --> 00:34:02.933
E eu vou anotar aqui os números do
primeiro caso... 5 e 95.

00:34:02.933 --> 00:34:06.967
OK. Então vamos ver os
números que saem desta vez.

00:34:06.967 --> 00:34:11.933
Aqui, 81% de vocês acham que ele não
fez algo moralmente condenável,

00:34:11.933 --> 00:34:13.967
mas estamos acima de 5%...

00:34:13.967 --> 00:34:18.667
19% de pessoas pensam que ele fez
algo moralmente condenável.

00:34:18.667 --> 00:34:20.100
Vamos ao nosso 3º caso.

00:34:20.100 --> 00:34:22.133
Aqui está o sr. Celular Sortudo.

00:34:22.133 --> 00:34:23.967
Aqui está o feito do sr.
Celular Sortudo.

00:34:23.967 --> 00:34:26.800
Ele entra no carro e começa a
dirigir do trabalho para casa.

00:34:26.800 --> 00:34:29.067
E, durante o trajeto, ele fala
ao telefone celular...

00:34:29.067 --> 00:34:31.200
Mas sabem o quê? Não acontece nada.

00:34:31.200 --> 00:34:35.100
E ele chega em casa sem
prejudicar ninguém.

00:34:35.100 --> 00:34:39.567
Questão: Será que o Celular Sortudo
faz algo moralmente condenável

00:34:39.567 --> 00:34:44.433
na condução do trabalho para
casa falando ao celular?

00:34:44.433 --> 00:34:47.600
E vamos ver como estes números saem.

00:34:51.733 --> 00:34:55.233
OK. O veredicto de vocês é:

00:34:55.233 --> 00:34:58.133
78% acha que o Celular Sortudo
fez alguma coisa...

00:34:58.133 --> 00:35:00.100
gente, eu não acredito nisso.

00:35:00.100 --> 00:35:02.667
Quero dizer, vocês estão se
antecipando ao próximo caso!

00:35:02.667 --> 00:35:06.033
Todos vocês usam o celular
enquanto dirigem

00:35:06.033 --> 00:35:07.567
e vocês não pensam em vocês mesmos

00:35:07.567 --> 00:35:09.200
como fazendo algo
moralmente condenável!

00:35:09.200 --> 00:35:11.633
OK. Estes dados não são válidos.

00:35:11.633 --> 00:35:15.200
Isso tem a ver com o lugar onde eles
estão inseridos neste experimento.

00:35:15.200 --> 00:35:16.933
Tudo bem.

00:35:16.933 --> 00:35:19.400
Portanto, mesmo que vocês já
tenham respondido à pergunta 4,

00:35:19.400 --> 00:35:21.600
deixa eu fazê-la.

00:35:21.600 --> 00:35:23.600
O Celular Azarado vai do
trabalho para casa

00:35:23.600 --> 00:35:24.700
falando ao telefone celular.

00:35:24.700 --> 00:35:26.667
Uma criança salta na frente
do carro dele e...

00:35:26.667 --> 00:35:29.367
OK. Questão.

00:35:29.367 --> 00:35:32.933
Durante o trajeto até a
casa, o Celular Azarado

00:35:32.933 --> 00:35:34.933
faz algo moralmente condenável?

00:35:34.933 --> 00:35:38.633
E vamos ver os números que saem.

00:35:41.200 --> 00:35:42.000
Tudo bem.

00:35:42.000 --> 00:35:46.067
Vamos ver onde aparece a grande linha
vermelha do Celular Azarado.

00:35:46.067 --> 00:35:49.967
OK. Agora temos uma mudança
completa da original

00:35:49.967 --> 00:35:52.767
e, de fato, diferente do
nosso caso anterior.

00:35:52.767 --> 00:35:56.467
OK. Portanto, o que esses
exemplos demonstram

00:35:56.467 --> 00:36:00.467
é um fenômeno conhecido
como a sorte moral.

00:36:00.467 --> 00:36:04.767
Temos 2 pessoas aqui, o Alberto
Sortudo e o Alberto Azarado,

00:36:04.767 --> 00:36:07.600
que fazem exatamente a mesma coisa,

00:36:07.600 --> 00:36:09.700
mas as ações do Azarado

00:36:09.700 --> 00:36:12.300
causaram a morte de um inocente.

00:36:12.300 --> 00:36:14.933
E enquanto apenas 5% de vocês pensam

00:36:14.933 --> 00:36:16.733
que o Alberto Sortudo fez algo errado,

00:36:16.733 --> 00:36:21.367
19% acham que o Alberto
Azarado fez algo errado.

00:36:21.367 --> 00:36:24.000
Temos aqui, de forma similar,

00:36:24.000 --> 00:36:28.000
alguém que assumiu um risco ligeiro,

00:36:28.000 --> 00:36:31.267
que neste caso não teve
consequências ruins

00:36:31.267 --> 00:36:34.567
e, neste caso, teve
consequências ruins muito
graves.

00:36:34.567 --> 00:36:39.400
E 92% de vocês condenaram
o Celular Azarado.

00:36:39.400 --> 00:36:44.700
O fenômeno que ilustra isso é
conhecido como a sorte moral.

00:36:44.700 --> 00:36:48.400
Casos em que a culpa moral de uma
ação ou de suas consequências

00:36:48.400 --> 00:36:50.233
é atribuída a um agente,

00:36:50.233 --> 00:36:53.833
mesmo que o agente não
tenha controle total

00:36:53.833 --> 00:36:55.800
sobre tal ação ou suas consequências.

00:36:55.800 --> 00:37:01.400
Certo? Não é que o Alberto
Azarado ou o Celular Azarado

00:37:01.400 --> 00:37:04.900
queria que a criança saltasse
na frente do carro.

00:37:04.900 --> 00:37:08.167
Não é que o Alberto Azarado
ou o Celular Azarado

00:37:08.167 --> 00:37:12.800
pudesse fazer algo diferente
naquele instante.

00:37:12.800 --> 00:37:18.200
A criança estava na frente do carro e
o carro atingiu a criança.

00:37:18.200 --> 00:37:21.633
A sorte moral é desconcertante,

00:37:21.633 --> 00:37:26.933
porque parece que temos 2
compromissos concorrentes

00:37:26.933 --> 00:37:29.667
quando pensamos sobre a
responsabilidade moral.

00:37:29.667 --> 00:37:32.533
Por um lado, parece que
estamos aceitando algo

00:37:32.533 --> 00:37:34.800
que poderíamos chamar de
princípio do controle:

00:37:34.800 --> 00:37:39.133
que louvor e culpa morais não
devem ser atribuídos em casos

00:37:39.133 --> 00:37:44.400
em que a ação ou as consequências
estão além do controle do agente.

00:37:44.400 --> 00:37:49.267
E eu vejo que muitos de vocês
aderiram ao princípio de controle,

00:37:49.267 --> 00:37:55.167
porque 81% acharam que o
motorista azarado

00:37:55.167 --> 00:37:57.267
não fez nada moralmente errado,

00:37:57.267 --> 00:38:00.000
mesmo ao ter matado uma criança.

00:38:00.000 --> 00:38:02.133
E o motivo para que vocês
estejam inclinados a pensar

00:38:02.133 --> 00:38:04.767
que ele não fez nada
errado, eu suspeito,

00:38:04.767 --> 00:38:09.067
é porque o julgamento nesse
caso, como disse Mill,

00:38:09.067 --> 00:38:13.567
está regulado por um princípio
aderido tacitamente por vocês.

00:38:13.567 --> 00:38:17.500
Ou seja, algo como o
princípio do controle.

00:38:17.500 --> 00:38:20.167
É intuitivamente plausível, diz Nagel,

00:38:20.167 --> 00:38:22.633
que as pessoas não possam ser
moralmente avaliadas

00:38:22.633 --> 00:38:24.267
por aquilo que não é culpa delas

00:38:24.267 --> 00:38:26.800
ou devido a fatores fora
do controle delas.

00:38:26.800 --> 00:38:29.100
Se você esbarrar em mim,

00:38:29.100 --> 00:38:32.167
eu tropeçar e acidentalmente cair
no botão vermelho que faz

00:38:32.167 --> 00:38:36.333
com que comece uma guerra
nuclear, não é culpa minha.

00:38:36.333 --> 00:38:40.467
É uma coisa terrivelmente ruim que
o planeta seja destruído,

00:38:40.467 --> 00:38:43.533
mas eu só tropecei.

00:38:43.533 --> 00:38:49.100
Por outro lado, em oposição direta

00:38:49.100 --> 00:38:52.900
ao princípio do controle e segundo
atesta o princípio da sorte moral,

00:38:52.900 --> 00:38:55.133
parece que em alguns casos

00:38:55.133 --> 00:38:58.267
devem ser atribuídos
louvor e culpa morais,

00:38:58.267 --> 00:39:00.567
mesmo quando a ação ou
as consequências

00:39:00.567 --> 00:39:03.433
estão além do controle do agente.

00:39:03.433 --> 00:39:06.267
A diferença nas respostas

00:39:06.267 --> 00:39:08.933
entre os casos dos
sortudos e dos azarados

00:39:08.933 --> 00:39:14.133
indicam o grau em que
vocês aderiram a isso.

00:39:14.133 --> 00:39:21.100
Portanto, vocês foram de 95% a
81% de não culpabilidade.

00:39:21.100 --> 00:39:26.133
Vocês mudaram... 15%
mudaram de opinião

00:39:26.133 --> 00:39:29.700
como consequência de algo fora
do controle do agente.

00:39:29.700 --> 00:39:31.400
No caso do celular,

00:39:31.400 --> 00:39:35.733
de novo, cerca de 15% de vocês
mudaram de opinião.

00:39:35.733 --> 00:39:39.300
O problema é que esses 2 princípios

00:39:39.300 --> 00:39:42.300
são incrivelmente
difíceis de desprezar.

00:39:42.300 --> 00:39:47.967
O princípio do controle se baseia no
seguinte tipo de raciocínio:

00:39:47.967 --> 00:39:50.867
Em geral, temos um bom senso
de que tipo de fatores

00:39:50.867 --> 00:39:56.067
aumentam o merecimento ou a
culpabilidade de uma ação.

00:39:56.067 --> 00:39:59.100
Em geral, se um ato é voluntário,
ou seja, se você faz isso

00:39:59.100 --> 00:40:03.433
não por coerção ou por
engano... certo?

00:40:03.433 --> 00:40:07.867
Se você escolheu executar a
ação executada...

00:40:07.867 --> 00:40:11.533
então você recebe mais
elogios por tê-la executado

00:40:11.533 --> 00:40:12.933
se era uma boa ação

00:40:12.933 --> 00:40:16.033
e mais culpa se era uma má ação.

00:40:16.033 --> 00:40:18.933
Do mesmo jeito, se você tivesse
a informação completa,

00:40:18.933 --> 00:40:23.033
se estivesse ciente das
prováveis ??consequências,

00:40:23.033 --> 00:40:26.500
sabendo que era água ou cianeto o que
você deu para a pessoa beber,

00:40:26.500 --> 00:40:31.967
isso aumenta o grau de
merecimento ou culpabilidade.

00:40:31.967 --> 00:40:36.000
E essas são respostas bastante robustas

00:40:36.000 --> 00:40:39.200
que incidem não apenas na
análise dos casos,

00:40:39.200 --> 00:40:43.067
mas também fora da nossa
compreensão dos princípios

00:40:43.067 --> 00:40:46.367
que parecem fundamentar a
responsabilidade moral.

00:40:46.367 --> 00:40:50.133
Do mesmo modo, parece que a
ausência dessas características

00:40:50.133 --> 00:40:53.200
diminui o merecimento ou a
culpabilidade moral.

00:40:53.200 --> 00:40:57.533
Se você fizer algo sob coação, se você
fizer alguma coisa acidentalmente,

00:40:57.533 --> 00:41:01.167
a sua responsabilidade é menor.

00:41:01.167 --> 00:41:03.233
E se você fizer alguma coisa por
falta de informação, se eu,

00:41:03.233 --> 00:41:06.667
pensando que eu estou dando algo
saudável para você,

00:41:06.667 --> 00:41:09.133
acabar dando algo prejudicial,

00:41:09.133 --> 00:41:14.700
temos uma tendência a mitigar o
grau de culpabilidade.

00:41:14.700 --> 00:41:18.233
O princípio do controle diz que,

00:41:18.233 --> 00:41:20.533
sem uma diferença nesses fatores,

00:41:20.533 --> 00:41:24.400
como é possível ter uma diferença
entre merecimento e culpabilidade?

00:41:24.400 --> 00:41:27.067
Se mantivermos esses fatores constantes,

00:41:27.067 --> 00:41:30.867
devemos estar numa situação

00:41:30.867 --> 00:41:33.667
onde não existe diferença na
responsabilidade moral.

00:41:33.667 --> 00:41:38.133
Por outro lado, o princípio da sorte
moral também é muito forte.

00:41:38.133 --> 00:41:42.167
Parece inegável que haja
casos em que avaliamos

00:41:42.167 --> 00:41:44.733
o louvor e a culpa morais na
ausência de controle.

00:41:44.733 --> 00:41:46.233
O caso do motorista foi um deles.

00:41:46.233 --> 00:41:49.367
Um dos usuários do celular atinge
a criança; o outro não.

00:41:49.367 --> 00:41:51.167
O 1º é moralmente condenável.

00:41:51.167 --> 00:41:54.633
Eu deixo o fogão aceso na minha
casa... ou na sua casa.

00:41:54.633 --> 00:41:56.867
Eu vou visitar você e deixo o
fogão aceso na sua casa.

00:41:56.867 --> 00:41:57.833
Eu saio durante o dia.

00:41:57.833 --> 00:42:00.833
Se eu sou azarada, isso causa um
incêndio na sua casa.

00:42:00.833 --> 00:42:02.500
Se eu tiver sorte, isso não ocorre.

00:42:02.500 --> 00:42:05.333
Parece que, mesmo achando que
ambas coisas são ruins,

00:42:05.333 --> 00:42:09.500
é muito pior deixar o fogão
aceso e queimar a casa

00:42:09.500 --> 00:42:12.333
que deixar o fogão aceso, simplesmente.

00:42:12.333 --> 00:42:15.900
Nagel dá o exemplo de deixar
um bebê na banheira

00:42:15.900 --> 00:42:17.467
e sair um momento.

00:42:17.467 --> 00:42:23.500
É uma coisa irresponsável, mas
infinitamente mais problemática

00:42:23.500 --> 00:42:26.667
se o bebê se afogar.

00:42:26.667 --> 00:42:31.633
Ou o caso em que você e eu
temos caráteres parecidos.

00:42:31.633 --> 00:42:34.567
Eu fico na Alemanha; você não.

00:42:34.567 --> 00:42:37.633
Estamos na década de 1930. Eu
me torno um nazista;

00:42:37.633 --> 00:42:42.433
você vive a sua vida sem que haja
exigências morais para você.

00:42:42.433 --> 00:42:45.500
Portanto, há 3 tipos de
respostas que podemos dar

00:42:45.500 --> 00:42:46.567
aos casos de sorte moral.

00:42:46.567 --> 00:42:48.433
Podemos dar uma resposta racionalista.

00:42:48.433 --> 00:42:49.167
Podemos dizer:

00:42:49.167 --> 00:42:52.333
a sorte não pode desempenhar um
papel na avaliação moral.

00:42:52.333 --> 00:42:59.067
E podemos ir ao outro extremo, pensando
– tal como um kantiano puro

00:42:59.067 --> 00:43:03.333
– que todo agente é
responsável pela sua vontade

00:43:03.333 --> 00:43:06.133
e pelas coisas às quais ele
tem controle total.

00:43:06.133 --> 00:43:09.833
Ou podemos aceitar algo como uma
versão extrema de Mill.

00:43:09.833 --> 00:43:13.367
Que o agente é responsável por
todas as consequências

00:43:13.367 --> 00:43:16.567
de suas ações e que a atitude
não faz diferença.

00:43:16.567 --> 00:43:19.600
Você pode tomar uma atitude
irracionalista nesse sentido.

00:43:19.600 --> 00:43:19.967
Você pode dizer

00:43:19.967 --> 00:43:23.667
que a sorte pode desempenhar algum
papel na avaliação moral.

00:43:23.667 --> 00:43:27.867
Ou, ainda que isso seja muito
difícil de sustentar,

00:43:27.867 --> 00:43:34.033
você pode dizer que nunca sabemos a
responsabilidade de alguém

00:43:34.033 --> 00:43:38.867
numa ação até vermos quais são
as consequências dela.

00:43:38.867 --> 00:43:43.733
Que quando eu fundamentava que
esses casos eram idênticos,

00:43:43.733 --> 00:43:47.800
eu estava idealizando de
forma ilegítima.

00:43:47.800 --> 00:43:53.567
Parece que a 3ª resposta pode funcionar

00:43:53.567 --> 00:43:56.433
nos casos clássicos de sorte
moral descritos por mim,

00:43:56.433 --> 00:43:58.633
casos que poderíamos chamar
de sorte resultante,

00:43:58.633 --> 00:44:01.733
onde há sorte no resultado da ação.

00:44:01.733 --> 00:44:06.567
Eu executo uma ação e ela sai errado de
uma maneira que eu não esperava.

00:44:06.567 --> 00:44:09.533
Essa é uma categoria de
casos de sorte moral.

00:44:09.533 --> 00:44:13.133
Mas é mais difícil de ver como
podemos usar essa explicação

00:44:13.133 --> 00:44:15.667
em algumas instâncias profundas.

00:44:15.667 --> 00:44:19.000
Portanto, aceitem a sorte constitutiva.

00:44:19.000 --> 00:44:21.467
Alguns de vocês nasceram com genes

00:44:21.467 --> 00:44:24.633
que faz com que seja mais fácil se
comportar de maneira altruísta;

00:44:24.633 --> 00:44:26.200
e alguns de vocês não.

00:44:26.200 --> 00:44:28.067
Alguns de vocês foram
criados em famílias

00:44:28.067 --> 00:44:30.967
que favoreceram determinados tipos
de perspectivas morais;

00:44:30.967 --> 00:44:32.867
e alguns de vocês não.

00:44:32.867 --> 00:44:36.867
Será que o seu caráter resulta
dessas características

00:44:36.867 --> 00:44:39.033
sobre algo que você é responsável;

00:44:39.033 --> 00:44:43.100
e se não, como é que algo
relacionado com louvor e culpa moral

00:44:43.100 --> 00:44:45.000
pode ser avaliado?

00:44:45.000 --> 00:44:46.467
Aceitem a sorte circunstancial,

00:44:46.467 --> 00:44:50.433
que Jonathan Shay apresentou em
Achilles in Vietnam –

00:44:50.433 --> 00:44:53.200
a sorte no tocante ao entorno do agente.

00:44:53.200 --> 00:44:55.900
Às vezes, as circunstâncias
em que você está

00:44:55.900 --> 00:45:00.933
criam ou revelam características
ocultas do seu caráter.

00:45:00.933 --> 00:45:06.933
Isso significa que – já que elas são,
em parte, uma questão de sorte –

00:45:06.933 --> 00:45:11.200
você não é moralmente
responsável pelo que você fez?

00:45:11.200 --> 00:45:16.133
Finalmente, se começarmos a
pensar em nossas ações

00:45:16.133 --> 00:45:19.667
pela perspectiva do livre-arbítrio, é
difícil trinchar qualquer espaço

00:45:19.667 --> 00:45:24.233
em que nós somos responsáveis
??por aquilo que fazemos.

00:45:24.233 --> 00:45:30.067
É um fato geral sobre o mundo que as
ações e as consequências

00:45:30.067 --> 00:45:34.433
são, em geral, determinadas em parte
pelas características externas ou,

00:45:34.433 --> 00:45:38.467
pelo menos, fora do controle do agente.

00:45:38.467 --> 00:45:42.067
Então, se pensarmos por que é
que respondemos dessa forma

00:45:42.067 --> 00:45:42.967
ao caso do Trolley, veremos que é

00:45:42.967 --> 00:45:45.267
porque a parte emocional do
nosso cérebro está acesa.

00:45:45.267 --> 00:45:46.600
Mas por que isso está acontecendo?

00:45:46.600 --> 00:45:49.467
Bem, isso está acontecendo por
causa do fluxo de sangue,

00:45:49.467 --> 00:45:51.367
acontecendo em nosso cérebro.

00:45:51.367 --> 00:45:52.500
E por que isso está acontecendo?

00:45:52.500 --> 00:45:54.167
Bem, o sangue está
fluindo nessa direção

00:45:54.167 --> 00:45:56.767
por causa de certos tipos de moléculas.

00:45:56.767 --> 00:46:01.767
E ao pensar sobre isso, a área
da agência genuína,

00:46:01.767 --> 00:46:07.267
diz Nagel, parece encolher a um
ponto sem extensão.

00:46:07.267 --> 00:46:09.767
Então, antes da pausa de março,

00:46:09.767 --> 00:46:12.600
deixo vocês com a seguinte
não-solução intrigante

00:46:12.600 --> 00:46:15.667
de um problema moral muito profundo.

00:46:15.667 --> 00:46:19.167
Nagel sugere que o problema da
sorte não tem solução

00:46:19.167 --> 00:46:23.067
porque algo na ideia de perceber a
nós mesmos como agentes

00:46:23.067 --> 00:46:28.500
é incompatível com o fato inegável
de que as ações são eventos

00:46:28.500 --> 00:46:31.500
e as pessoas são coisas.

00:46:31.500 --> 00:46:35.000
"Como os determinantes externos
do que alguém fez estão

00:46:35.000 --> 00:46:38.300
gradualmente expostos no seu
efeito nas consequências,

00:46:38.300 --> 00:46:42.567
caráter e escolha própria, torna-se
gradualmente claro para nós

00:46:42.567 --> 00:46:48.133
que as ações são, de fato, eventos
e que as pessoas são coisas.

00:46:48.133 --> 00:46:49.667
Como resultado disso,

00:46:49.667 --> 00:46:53.300
não resta nada que possa ser
atribuído ao próprio responsável,

00:46:53.300 --> 00:46:56.833
e não nos sobra nada além de uma
porção da maior sequência

00:46:56.833 --> 00:47:00.133
de eventos que pode ser
lamentada ou celebrada,

00:47:00.133 --> 00:47:03.267
mas não louvada ou culpada."

00:47:03.267 --> 00:47:10.000
No entanto, abandonar a
linguagem de louvor e culpa

00:47:10.000 --> 00:47:13.100
é retirar do nosso
repertório conceitual

00:47:13.100 --> 00:47:17.267
o que talvez seja a ferramenta
mais importante que temos.

00:47:17.267 --> 00:47:20.500
E chegar a uma perspectiva
estável sobre essas questões

00:47:20.500 --> 00:47:23.933
parece extremamente difícil.

00:47:23.933 --> 00:47:27.267
Portanto, eu vejo todos
vocês após o feriado.

