WEBVTT

00:00:00.000 --> 00:00:03.952
[MUSIC PLAYING]

00:00:05.762 --> 00:00:07.220
MIKE TEODORESCU:
Hello, and welcome

00:00:07.220 --> 00:00:09.280
to this module on choices
of fairness criteria.

00:00:09.280 --> 00:00:10.210
My name is Mike Teodorescu.

00:00:10.210 --> 00:00:11.860
I'm an assistant professor
of information systems

00:00:11.860 --> 00:00:14.277
at Boston College, as was a
visiting scholar at MIT D-Lab.

00:00:14.277 --> 00:00:15.777
What we'll cover
in this module will

00:00:15.777 --> 00:00:17.290
be the concept of
confusion matrix,

00:00:17.290 --> 00:00:20.470
as well as three popular
examples of fairness criteria--

00:00:20.470 --> 00:00:22.600
demographic parity,
equalized odds,

00:00:22.600 --> 00:00:25.418
and equalized opportunity.

00:00:25.418 --> 00:00:26.960
Some countries have
laws that protect

00:00:26.960 --> 00:00:29.270
specific groups of people
from discrimination

00:00:29.270 --> 00:00:31.490
based on certain attributes.

00:00:31.490 --> 00:00:33.480
As we review in
the previous video,

00:00:33.480 --> 00:00:35.630
these are called
protected attributes.

00:00:35.630 --> 00:00:38.523
Some examples are on this slide.

00:00:38.523 --> 00:00:39.940
Regardless of the
legal landscape,

00:00:39.940 --> 00:00:41.357
machine learning
has the potential

00:00:41.357 --> 00:00:45.020
to produce unfair outcomes
for certain groups of people.

00:00:45.020 --> 00:00:47.540
As an algorithm designer,
one should make clear choices

00:00:47.540 --> 00:00:49.520
about fairness criteria.

00:00:49.520 --> 00:00:51.980
Some criteria will be reviewed
in the next few slides.

00:00:54.285 --> 00:00:56.910
[? In the ?] previous video, we
also discussed case of fairness

00:00:56.910 --> 00:00:58.910
through unawareness,
which refers

00:00:58.910 --> 00:01:02.190
to leaving out the protected
attributes out of your model.

00:01:02.190 --> 00:01:04.448
And we also explained why
this is not a good choice.

00:01:04.448 --> 00:01:06.740
Specifically, you may end up
with other attributes that

00:01:06.740 --> 00:01:08.630
correlate with
protected attributes,

00:01:08.630 --> 00:01:11.300
and you may end up
discriminating inadvertently

00:01:11.300 --> 00:01:13.928
nonetheless.

00:01:13.928 --> 00:01:15.970
In order to go into
additional fairness criteria,

00:01:15.970 --> 00:01:18.750
we need to discuss some
additional concepts.

00:01:18.750 --> 00:01:21.750
Consider you have a
binary classifier.

00:01:21.750 --> 00:01:24.630
For example, you're
looking at decision of hire

00:01:24.630 --> 00:01:28.380
or not hired or to lend
credit and not to lend credit.

00:01:28.380 --> 00:01:31.957
If we want to look at the
predictions for a model that

00:01:31.957 --> 00:01:33.540
would do such a
binary classification,

00:01:33.540 --> 00:01:35.420
we would look at the
predicted values.

00:01:35.420 --> 00:01:38.292
We could bucket them in four
categories-- true positives,

00:01:38.292 --> 00:01:39.750
which would be
correctly classified

00:01:39.750 --> 00:01:42.300
as positive, true negative,
correctly classified

00:01:42.300 --> 00:01:45.645
as negative, false positives,
which would be values that

00:01:45.645 --> 00:01:48.338
were incorrectly classified
as positive by the algorithm,

00:01:48.338 --> 00:01:50.880
false negatives, which would be
values incorrectly classified

00:01:50.880 --> 00:01:52.240
as negative.

00:01:52.240 --> 00:01:54.285
And, if we were to add
the the true positives

00:01:54.285 --> 00:01:58.270
to the true negatives and
divide that by all four,

00:01:58.270 --> 00:02:01.530
we would end up with the value
of the accuracy of the model.

00:02:01.530 --> 00:02:04.650
In this example where
accuracy is this fraction,

00:02:04.650 --> 00:02:08.970
an accuracy of 0.5 is the same
as a random classification.

00:02:08.970 --> 00:02:11.980
Now we should look
at accuracy carefully

00:02:11.980 --> 00:02:14.760
and see that it doesn't tell us
anything about the prediction

00:02:14.760 --> 00:02:15.360
of negatives.

00:02:15.360 --> 00:02:17.790
It could mislead us if two
classes were imbalanced,

00:02:17.790 --> 00:02:21.060
for example, if 90% of
the sample is positives,

00:02:21.060 --> 00:02:22.620
and 10% is negatives.

00:02:22.620 --> 00:02:24.990
For that, we have other
additional criteria

00:02:24.990 --> 00:02:29.130
we could go into deeper, such
as MCC score and AUC score.

00:02:31.940 --> 00:02:34.130
The true positives, true
negatives, false positives,

00:02:34.130 --> 00:02:37.410
and false negatives are,
oftentimes, represented in a 2

00:02:37.410 --> 00:02:41.160
by 2 matrix form called
a confusion matrix.

00:02:41.160 --> 00:02:43.560
This is simply an easier
presentation for us

00:02:43.560 --> 00:02:47.870
to see the behavior
of the classifier.

00:02:47.870 --> 00:02:49.670
The first additional
fairness criteria

00:02:49.670 --> 00:02:51.900
is called demographic parity.

00:02:51.900 --> 00:02:55.560
It's a criterion for what's
called group-level fairness.

00:02:55.560 --> 00:02:58.260
This criterion specify
that the outcome, which

00:02:58.260 --> 00:03:00.660
here is denoted by a
y hat, is independent

00:03:00.660 --> 00:03:05.310
of the protected attribute A.
For example, the probability

00:03:05.310 --> 00:03:09.610
of being hired is
independent of the gender.

00:03:09.610 --> 00:03:12.130
There are multiple problems
with demographic parity.

00:03:12.130 --> 00:03:13.810
One would be the
definition that we just

00:03:13.810 --> 00:03:16.030
discussed would
not hold if we had

00:03:16.030 --> 00:03:19.300
individuals who would be members
of multiple protected groups.

00:03:19.300 --> 00:03:21.642
By enforcing group-level
fairness for one attribute,

00:03:21.642 --> 00:03:23.350
we would still violate
the group fairness

00:03:23.350 --> 00:03:25.750
for other attributes or
combinations of attributes,

00:03:25.750 --> 00:03:29.310
such as subgroups
of the population.

00:03:29.310 --> 00:03:31.560
Furthermore, while enforcing
group-level fairness,

00:03:31.560 --> 00:03:34.620
for example, the same hiring
grade for females and males,

00:03:34.620 --> 00:03:36.737
that could still be
unfair to individuals.

00:03:36.737 --> 00:03:39.070
It could force the algorithm
to drop otherwise qualified

00:03:39.070 --> 00:03:40.800
individuals just to
achieve independence

00:03:40.800 --> 00:03:44.700
of outcome of the attribute.

00:03:44.700 --> 00:03:46.770
Fairness at the group
level could, potentially,

00:03:46.770 --> 00:03:49.300
be unfair at the
individual level.

00:03:49.300 --> 00:03:52.800
For example, if we have a
high rate of false positives,

00:03:52.800 --> 00:03:54.990
we could end up-- and a low
rate of false negatives,

00:03:54.990 --> 00:03:57.120
it could still end up
being unfair to individuals

00:03:57.120 --> 00:04:00.420
in that we could hire
people who are without merit

00:04:00.420 --> 00:04:03.030
at the disadvantage
of other individuals

00:04:03.030 --> 00:04:05.790
who could be qualified
and should be hired,

00:04:05.790 --> 00:04:08.550
but, due to the group-level
fairness criterion,

00:04:08.550 --> 00:04:11.160
we have to hire some
who are not qualified

00:04:11.160 --> 00:04:13.820
from one of the groups.

00:04:13.820 --> 00:04:15.710
The sweet spot would
be low false negatives

00:04:15.710 --> 00:04:18.440
and low false positives, which
would be fair, potentially,

00:04:18.440 --> 00:04:21.019
to both the individual
and the group level.

00:04:21.019 --> 00:04:23.240
We could also end up in
the top right corner, which

00:04:23.240 --> 00:04:26.060
would be the worst-case
scenario, low accuracy,

00:04:26.060 --> 00:04:28.550
unfair to individuals, but
potentially fair for the group

00:04:28.550 --> 00:04:30.410
where we have high
false negatives

00:04:30.410 --> 00:04:34.210
and high false positives.

00:04:34.210 --> 00:04:36.460
A stricter criterion
is equalized odds,

00:04:36.460 --> 00:04:38.770
which means matching both
of true positive rate

00:04:38.770 --> 00:04:41.200
and the false positive
rate for different values

00:04:41.200 --> 00:04:43.150
of the protected attribute.

00:04:43.150 --> 00:04:45.760
This is much harder to achieve
than demographic parity,

00:04:45.760 --> 00:04:49.150
but it is one of the higher
levels of algorithmic fairness.

00:04:49.150 --> 00:04:50.860
In this case, rather
than predicting

00:04:50.860 --> 00:04:52.690
the same average
risk across subgroups

00:04:52.690 --> 00:04:56.460
of protected social attributes,
like in demographic parity,

00:04:56.460 --> 00:04:59.080
equalized odds is stricter
in that it forces equality

00:04:59.080 --> 00:05:02.260
only among individuals who
reach similar outcomes.

00:05:02.260 --> 00:05:04.360
In the hiring example
that we've discussed

00:05:04.360 --> 00:05:06.640
in the previous
video, this implies

00:05:06.640 --> 00:05:09.120
that the probability of a
qualified applicant to be hired

00:05:09.120 --> 00:05:11.140
and the probability of
an unqualified applicant

00:05:11.140 --> 00:05:13.930
to be incorrectly hired should
be the same across genders.

00:05:17.030 --> 00:05:18.740
A milder version
of equalized odds

00:05:18.740 --> 00:05:20.510
is equalized opportunity
where we're only

00:05:20.510 --> 00:05:23.450
concerned with treating fairly
those who are determined

00:05:23.450 --> 00:05:26.540
to be worthy of acceptance,
i.e. dependent variable

00:05:26.540 --> 00:05:29.030
is equal to 1, or
they're worthy of being

00:05:29.030 --> 00:05:32.350
hired, worthy of being
admitted, and so on.

00:05:32.350 --> 00:05:34.850
Equalized opportunity is not
concerned with rejecting people

00:05:34.850 --> 00:05:37.140
fairly across protected groups.

00:05:37.140 --> 00:05:39.660
So to speak, the false positive
rates and the true positive

00:05:39.660 --> 00:05:41.952
rates do not both need to be
equal across the protected

00:05:41.952 --> 00:05:42.830
categories.

00:05:42.830 --> 00:05:44.300
We only need the
true positive rate

00:05:44.300 --> 00:05:47.410
to be equal across
protected categories.

00:05:47.410 --> 00:05:49.060
In a way, equalized
opportunity is

00:05:49.060 --> 00:05:51.280
less interventionist
than equalized odds,

00:05:51.280 --> 00:05:53.710
and it may be more
achievable, depending

00:05:53.710 --> 00:05:58.720
on your individual situation
and implementation challenges.

00:05:58.720 --> 00:06:01.330
In the example of
hiring, we only

00:06:01.330 --> 00:06:04.630
are concerned with individuals
who are worthy of being hired,

00:06:04.630 --> 00:06:07.000
i.e. the actual
qualified applicants.

00:06:07.000 --> 00:06:08.530
And, out of the
rejected applicants,

00:06:08.530 --> 00:06:10.155
we may be sometimes
rejecting unfairly.

00:06:12.610 --> 00:06:15.950
Here's some review questions
for the last two videos.

00:06:15.950 --> 00:06:17.960
What is demographic parity?

00:06:17.960 --> 00:06:20.580
What is fairness
through unawareness?

00:06:20.580 --> 00:06:23.490
Is fairness at the group
level always the best?

00:06:23.490 --> 00:06:25.915
What is a confusion matrix?

00:06:25.915 --> 00:06:27.540
What is the equality
of odds criterion?

00:06:27.540 --> 00:06:28.957
And when might you
want to use it?

00:06:31.150 --> 00:06:35.520
This course is sponsored by the
USAID grant through MIT D-Lab.

00:06:35.520 --> 00:06:38.160
And this is joint work with my
faculty colleagues Lily Morse

00:06:38.160 --> 00:06:41.400
and Gerald Kane from Boston
college and research assistant

00:06:41.400 --> 00:06:42.670
Yazeed Awwad from MIT D-Lab.

00:06:45.158 --> 00:06:46.950
If you would like to
learn more about this,

00:06:46.950 --> 00:06:50.040
please consult the
following references.

00:06:50.040 --> 00:06:51.960
Thank you so much for
watching this video.

00:06:51.960 --> 00:06:53.877
We hope you find it
useful and you'll continue

00:06:53.877 --> 00:06:55.880
watching the rest of the class.

00:06:55.880 --> 00:06:59.830
[MUSIC PLAYING]