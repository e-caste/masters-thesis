WEBVTT

00:00:00.000 --> 00:00:02.430
The following content is
provided under a Creative

00:00:02.430 --> 00:00:03.730
Commons license.

00:00:03.730 --> 00:00:06.030
Your support will help
MIT OpenCourseWare

00:00:06.030 --> 00:00:10.060
continue to offer high-quality
educational resources for free.

00:00:10.060 --> 00:00:12.660
To make a donation or to
view additional materials

00:00:12.660 --> 00:00:16.560
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:16.560 --> 00:00:17.874
at ocw.mit.edu.

00:00:21.810 --> 00:00:24.240
DUANE BONING: OK, so today
is a little bit different.

00:00:24.240 --> 00:00:26.220
Today we're going to talk
about yield modeling.

00:00:26.220 --> 00:00:29.490
And this is
unabashedly connected

00:00:29.490 --> 00:00:31.932
to semiconductor manufacturing--

00:00:31.932 --> 00:00:33.390
although I think
many of the things

00:00:33.390 --> 00:00:37.110
I talk about here are more
widely applicable, especially

00:00:37.110 --> 00:00:40.770
to anything that's large
area manufacturing.

00:00:40.770 --> 00:00:47.010
So for example, a few years ago,
I did a roll coding process,

00:00:47.010 --> 00:00:50.760
rolling out these giant
plastic sheets of film material

00:00:50.760 --> 00:00:51.810
at Kodak.

00:00:51.810 --> 00:00:55.290
And defect modeling,
and the yield of--

00:00:55.290 --> 00:00:59.530
on a per area basis was
a big deal there as well.

00:00:59.530 --> 00:01:01.750
Similarly, many of
the MEMS processes,

00:01:01.750 --> 00:01:06.120
thin film processes, yield
modeling associated especially

00:01:06.120 --> 00:01:08.310
with defects is very important.

00:01:08.310 --> 00:01:10.470
Many of the other ideas
I'll be talking about here

00:01:10.470 --> 00:01:14.160
in this lecture
are also connected

00:01:14.160 --> 00:01:18.450
to the idea of
assemblies or systems

00:01:18.450 --> 00:01:21.150
that consist of many,
many, many, many parts,

00:01:21.150 --> 00:01:26.190
whether it may be a failure
of probability or deviations

00:01:26.190 --> 00:01:27.270
in individual parts.

00:01:27.270 --> 00:01:29.400
And part of the question
is how those aggregate

00:01:29.400 --> 00:01:31.180
across the whole system.

00:01:31.180 --> 00:01:35.045
So all of the examples here
will be pretty much drawn

00:01:35.045 --> 00:01:36.420
from semiconductor
manufacturing,

00:01:36.420 --> 00:01:39.450
but I think they are
more broadly applicable.

00:01:39.450 --> 00:01:42.510
And perhaps many of the
tools actually developed here

00:01:42.510 --> 00:01:47.730
in semiconductor manufacturing
can be used and propagate

00:01:47.730 --> 00:01:50.290
to other processes.

00:01:50.290 --> 00:01:55.170
So the material will mostly
be drawn from chapter 5,

00:01:55.170 --> 00:01:58.900
so on-- need to add a note
as a reading assignment.

00:01:58.900 --> 00:02:02.700
This is drawn from
[INAUDIBLE] chapter 5.

00:02:02.700 --> 00:02:04.380
But I'm also showing
some examples

00:02:04.380 --> 00:02:08.340
from a couple of other papers,
and I'll put those papers

00:02:08.340 --> 00:02:09.990
on the website as well.

00:02:09.990 --> 00:02:14.070
I realized this morning
they're not up yet.

00:02:14.070 --> 00:02:15.420
One is a paper--

00:02:15.420 --> 00:02:17.850
sort of a classic
paper by Stapper

00:02:17.850 --> 00:02:22.320
on integrated circuit yield
management, yield analysis--

00:02:22.320 --> 00:02:24.570
and then a more recent one--

00:02:24.570 --> 00:02:27.210
I guess somewhat more recent--

00:02:27.210 --> 00:02:30.420
2000-- on predictive
yield modeling.

00:02:30.420 --> 00:02:32.730
So both of those will be
available on the website.

00:02:35.540 --> 00:02:37.060
So we've already
talked a little bit

00:02:37.060 --> 00:02:40.240
about some of the kinds of
variations that lead to--

00:02:40.240 --> 00:02:44.770
ultimately can lead to failures,
or failures in specification.

00:02:44.770 --> 00:02:48.550
When a deviation in some
continuous parameter

00:02:48.550 --> 00:02:53.380
exceeds some spec limit
for normal operation,

00:02:53.380 --> 00:02:56.300
we can think of those
as parametric failures.

00:02:56.300 --> 00:02:58.240
So we've talked about
things like line width

00:02:58.240 --> 00:03:03.160
or so on that might lead
to either direct functional

00:03:03.160 --> 00:03:03.760
failure--

00:03:03.760 --> 00:03:07.270
meaning it really won't work
because the parameter's too far

00:03:07.270 --> 00:03:08.290
away--

00:03:08.290 --> 00:03:12.160
or a more fuzzy failure--

00:03:12.160 --> 00:03:15.400
this continuous quality
loss in performance,

00:03:15.400 --> 00:03:20.050
meaning that I've
gotten such a deviation,

00:03:20.050 --> 00:03:22.810
the thing might still
turn on or might operate,

00:03:22.810 --> 00:03:28.360
but it does so with decayed
or degraded performance.

00:03:28.360 --> 00:03:30.040
In addition to
that, we also want

00:03:30.040 --> 00:03:33.760
to talk about random failures.

00:03:33.760 --> 00:03:37.150
And these are generally
thought of as more

00:03:37.150 --> 00:03:42.130
uncoordinated-- or uncorrelated
random failure in some element.

00:03:42.130 --> 00:03:46.960
They aren't necessarily due
to some continuous parameter,

00:03:46.960 --> 00:03:50.090
but maybe more of
a lumped failure.

00:03:50.090 --> 00:03:52.750
We'll talk in
particular about area

00:03:52.750 --> 00:03:54.820
dependent kinds of failures.

00:03:54.820 --> 00:03:57.520
In semiconductor manufacturing,
the main source of these

00:03:57.520 --> 00:04:02.800
are point defects associated
with very small particles,

00:04:02.800 --> 00:04:07.690
dust, or debris that
interfere with the operation

00:04:07.690 --> 00:04:11.440
of an electrical
element, generally.

00:04:11.440 --> 00:04:15.700
And we previewed some of those
in one of the first lectures.

00:04:15.700 --> 00:04:19.300
So we're going to talk about
these kinds of defects.

00:04:19.300 --> 00:04:26.290
So the key idea in these is many
of the area-dependent failures

00:04:26.290 --> 00:04:32.620
are ones where, depending
on the total square area

00:04:32.620 --> 00:04:36.100
of your circuit, you have
more or less opportunity

00:04:36.100 --> 00:04:38.390
for those kinds of failures.

00:04:38.390 --> 00:04:40.270
So it becomes an
interesting problem

00:04:40.270 --> 00:04:43.990
in terms of analyzing the
probabilities associated

00:04:43.990 --> 00:04:46.490
with failure for
different sized circuits.

00:04:46.490 --> 00:04:49.820
And we'll talk about those.

00:04:49.820 --> 00:04:53.560
So here's an example I pulled
out of the [INAUDIBLE] paper

00:04:53.560 --> 00:04:56.560
on an integrated
circuit yield tree--

00:04:56.560 --> 00:05:02.080
so looking at, say, 100 ASIC
chips that are manufactured,

00:05:02.080 --> 00:05:05.500
and what the breakdown
of those might

00:05:05.500 --> 00:05:08.620
be in terms of
their ultimate fate.

00:05:08.620 --> 00:05:13.930
And in this particular
process, out of the 100 chips,

00:05:13.930 --> 00:05:16.820
about 30 are ultimately
shipped to the customer.

00:05:16.820 --> 00:05:20.170
So in some sense, you've
got a yield of 30%--

00:05:20.170 --> 00:05:26.640
not great, but may not
be entirely unrealistic.

00:05:26.640 --> 00:05:31.470
And then, within that 30
chips shipped to the customer,

00:05:31.470 --> 00:05:35.430
there's already some
parametric variation

00:05:35.430 --> 00:05:41.130
going on that is
essentially a reflection

00:05:41.130 --> 00:05:45.630
of that kind of quality
loss degradation

00:05:45.630 --> 00:05:48.540
that we talked about before.

00:05:48.540 --> 00:05:56.340
Typically, chips are tested,
and we'll do a breakdown,

00:05:56.340 --> 00:05:58.960
give you a feel for the
kinds of tests that are done.

00:05:58.960 --> 00:06:01.740
But at the end,
they're often tested

00:06:01.740 --> 00:06:06.690
for a few key
performance parameters--

00:06:06.690 --> 00:06:08.640
in particular, speed.

00:06:08.640 --> 00:06:11.020
And then a thing called
speed binning is done,

00:06:11.020 --> 00:06:16.710
and you can see here, binning
down into three different speed

00:06:16.710 --> 00:06:20.490
categories, where you've
got a few that are operating

00:06:20.490 --> 00:06:22.950
at 400 megahertz that--
presumably, you can sell

00:06:22.950 --> 00:06:25.290
those chips a little bit more.

00:06:25.290 --> 00:06:28.230
350 megahertz you might not
have quite the same price

00:06:28.230 --> 00:06:32.580
premium on, and maybe you
have to sell with almost no--

00:06:32.580 --> 00:06:35.780
or very limited profit
the 300 megahertz chips,

00:06:35.780 --> 00:06:37.600
or something like that.

00:06:37.600 --> 00:06:40.860
So there's already still the
driver and process control

00:06:40.860 --> 00:06:43.470
to get as tight
control as you can

00:06:43.470 --> 00:06:48.180
and push the speed limit
as much as you can.

00:06:48.180 --> 00:06:51.300
But what we want to talk
about especially today

00:06:51.300 --> 00:06:55.980
are some of the sources for the
chips and sources of variation

00:06:55.980 --> 00:06:57.660
and kinds of failures
that are affecting

00:06:57.660 --> 00:06:58.950
the chips that are rejected.

00:06:58.950 --> 00:07:01.950
And we can break those
down here on this chart

00:07:01.950 --> 00:07:04.560
into these other categories.

00:07:04.560 --> 00:07:08.730
We've got growth functional
fail, unrepairable cache, speed

00:07:08.730 --> 00:07:11.550
less than 300 megahertz,
and all others.

00:07:11.550 --> 00:07:15.090
So out of these, first off, the
speed less than 300 megahertz--

00:07:15.090 --> 00:07:17.280
that's just our
cut-off on the speed,

00:07:17.280 --> 00:07:20.940
and that's probably more
of a parametric variation.

00:07:20.940 --> 00:07:25.530
We can break that down and
start to look at what devices,

00:07:25.530 --> 00:07:29.310
or what components, or what's
responsible for the slowness

00:07:29.310 --> 00:07:32.910
perhaps to improve either
design or manufacturing control.

00:07:32.910 --> 00:07:41.520
And here, for example, the clock
speed perhaps is a little bit

00:07:41.520 --> 00:07:45.330
too slow, either because
of the interconnect--

00:07:45.330 --> 00:07:49.170
the interconnect delay might
be a little bit too long--

00:07:49.170 --> 00:07:54.120
or because the transistors,
the active devices

00:07:54.120 --> 00:07:58.080
are not quite strong enough.

00:07:58.080 --> 00:08:00.900
Knowing those two
could tell you a lot

00:08:00.900 --> 00:08:02.505
about the sources of variation.

00:08:02.505 --> 00:08:04.620
For example, if
it's interconnect,

00:08:04.620 --> 00:08:07.500
it's probably something
with your back-end process.

00:08:07.500 --> 00:08:09.990
Some of your resistances
in the interconnect wires

00:08:09.990 --> 00:08:13.290
might be a little too high, or
some of those capacitances--

00:08:13.290 --> 00:08:18.090
whereas, if you've got an active
device strength failures--

00:08:18.090 --> 00:08:20.100
the devices are too slow--

00:08:20.100 --> 00:08:23.940
that's likely something to
do with, say, channel length,

00:08:23.940 --> 00:08:27.630
or perhaps gate oxide
thicknesses a little too thick.

00:08:27.630 --> 00:08:32.909
So it can start to lead to good
knowledge that can give rise

00:08:32.909 --> 00:08:35.640
to some improvement efforts.

00:08:35.640 --> 00:08:37.980
Now, a little bit more
interesting for today

00:08:37.980 --> 00:08:41.159
are these two other categories--
this gross functional

00:08:41.159 --> 00:08:45.540
fail and unrepairable cache.

00:08:45.540 --> 00:08:48.810
The unrepairable cache--
this might be an ASIC chip,

00:08:48.810 --> 00:08:51.870
and it might have different
components or different regions

00:08:51.870 --> 00:08:53.160
on it.

00:08:53.160 --> 00:08:55.980
Some of it may be
random logic performing

00:08:55.980 --> 00:09:04.410
particular combinatorial or
combination logic functions.

00:09:04.410 --> 00:09:08.610
But another component is
likely to be embedded memory.

00:09:08.610 --> 00:09:12.990
And in fact, as we get to
larger and larger chips

00:09:12.990 --> 00:09:15.930
and integration scale, most
of that additional area

00:09:15.930 --> 00:09:18.160
these days is going to memory.

00:09:18.160 --> 00:09:23.070
So I don't know what percentage
of the new 2 billion transistor

00:09:23.070 --> 00:09:25.920
Intel chip is cache, but it's--

00:09:25.920 --> 00:09:26.610
AUDIENCE: 90%--

00:09:26.610 --> 00:09:30.323
DUANE BONING: 90% is cache--

00:09:30.323 --> 00:09:31.740
something you can
do with the area

00:09:31.740 --> 00:09:33.600
that helps with performance.

00:09:33.600 --> 00:09:37.500
But very interesting
issue here is

00:09:37.500 --> 00:09:42.240
these are among the most
dense, most tightly packed

00:09:42.240 --> 00:09:45.630
and smallest scaled structures--

00:09:45.630 --> 00:09:53.520
highly repeated transistor SRAM
cells, little memory cells.

00:09:53.520 --> 00:09:57.480
But there you're
already expecting--

00:09:57.480 --> 00:09:59.820
and we'll talk about the
sources of some of these--

00:09:59.820 --> 00:10:02.550
you're already expecting some
number of those memory cells

00:10:02.550 --> 00:10:06.900
to fail, perhaps because of
particle-oriented defects.

00:10:06.900 --> 00:10:10.560
And so one builds in a
certain amount of redundancy

00:10:10.560 --> 00:10:13.620
into the cache so
that one can detect--

00:10:13.620 --> 00:10:18.180
or into the memory so one can
detect particular failed cells

00:10:18.180 --> 00:10:25.620
and program in or fold
in additional redundant

00:10:25.620 --> 00:10:27.010
capability.

00:10:27.010 --> 00:10:31.500
So that's the repair, the
direct repair of the cache.

00:10:31.500 --> 00:10:35.220
But at some point, depending
on where the failure is,

00:10:35.220 --> 00:10:37.620
you may not-- you may have
too many of those failures,

00:10:37.620 --> 00:10:42.405
or you may fail perhaps even in
some of the redundant switching

00:10:42.405 --> 00:10:43.920
in the circuitry.

00:10:43.920 --> 00:10:45.600
So you get to a
point where you can't

00:10:45.600 --> 00:10:48.510
repair all of those caches.

00:10:48.510 --> 00:10:51.690
And so you might start to
look then inside and start

00:10:51.690 --> 00:10:54.210
trying to say, OK, what
are the sources there?

00:10:54.210 --> 00:10:55.950
Some of those you
might not know--

00:10:55.950 --> 00:10:58.080
these unobservable root causes.

00:10:58.080 --> 00:11:02.130
Others-- a big chunk maybe due
to these points defects landing

00:11:02.130 --> 00:11:05.760
in particularly
damaging locations.

00:11:05.760 --> 00:11:08.740
And similarly, if you then
look at growth functional fail,

00:11:08.740 --> 00:11:11.550
you just try to-- you
can't even test the memory.

00:11:11.550 --> 00:11:15.330
The chip simply won't
power up or won't operate.

00:11:15.330 --> 00:11:19.230
One might then also do different
kinds of inspection approaches

00:11:19.230 --> 00:11:22.690
to try to detect what the
source of those errors are.

00:11:22.690 --> 00:11:25.080
And again, random
defects are typically

00:11:25.080 --> 00:11:29.160
a really large part of that.

00:11:29.160 --> 00:11:31.380
Systematic defect we'll talk
about a little bit later

00:11:31.380 --> 00:11:32.160
as well.

00:11:32.160 --> 00:11:35.730
Those might be not quite
random point defects,

00:11:35.730 --> 00:11:41.890
but some other failure that's
not quite a parametric failure,

00:11:41.890 --> 00:11:45.630
but it's something that's
affecting an awful lot

00:11:45.630 --> 00:11:47.190
of the components all together.

00:11:47.190 --> 00:11:49.350
And a typical
example here might be

00:11:49.350 --> 00:11:52.830
things like overlay
between different layers

00:11:52.830 --> 00:11:54.310
of the process.

00:11:54.310 --> 00:11:56.310
So everything's just
a little bit off

00:11:56.310 --> 00:12:00.450
set in terms of the alignment
from one layer to the next,

00:12:00.450 --> 00:12:08.790
causing a substantial yield loss
in lots and lots of components

00:12:08.790 --> 00:12:11.060
together.

00:12:11.060 --> 00:12:16.300
So what we've bolded here
in that big black rectangle

00:12:16.300 --> 00:12:17.860
are these random defects.

00:12:17.860 --> 00:12:20.350
And that's one of
the key elements

00:12:20.350 --> 00:12:23.530
I want to talk about here are
these point defects associated

00:12:23.530 --> 00:12:27.430
with particle-oriented
problems, and how

00:12:27.430 --> 00:12:32.360
to model the impact
of these basically

00:12:32.360 --> 00:12:34.280
from a statistical
point of view.

00:12:34.280 --> 00:12:38.330
But getting back to the
sources or the mental picture

00:12:38.330 --> 00:12:42.710
for these kinds of
defects, there's a--

00:12:42.710 --> 00:12:45.470
you can be a little bit more
careful with the terminology

00:12:45.470 --> 00:12:48.710
here and talk or differentiate--

00:12:48.710 --> 00:12:51.770
talk about the
difference between, say,

00:12:51.770 --> 00:12:54.720
particles and defects.

00:12:54.720 --> 00:12:57.950
So a particle-- you can think
of any kind of foreign matter

00:12:57.950 --> 00:12:59.840
that might be sitting
on the surface

00:12:59.840 --> 00:13:04.700
or might be embedded
in a layer on the chip.

00:13:04.700 --> 00:13:07.980
Now, some of these
might be benign,

00:13:07.980 --> 00:13:12.330
so a particle is not
necessarily a defect.

00:13:15.240 --> 00:13:17.250
A defect would be
when it affects

00:13:17.250 --> 00:13:20.520
some functionality of the chip.

00:13:20.520 --> 00:13:26.400
So a picture here, qualitatively
giving you this sense--

00:13:26.400 --> 00:13:29.040
we've got three
different dust particles

00:13:29.040 --> 00:13:33.870
on a particular feature on--

00:13:33.870 --> 00:13:35.880
lined up with particular
features on the mask.

00:13:35.880 --> 00:13:43.270
And one can easily imagine
that perhaps this particle,

00:13:43.270 --> 00:13:48.640
if it's conductive and those
crosshatch areas are also

00:13:48.640 --> 00:13:50.890
conductive-- or
say, metal lines--

00:13:50.890 --> 00:13:55.400
that's going to be a
problem, potentially.

00:13:55.400 --> 00:14:00.050
Actually, if someplace
else, these two things

00:14:00.050 --> 00:14:04.470
are the same wire, maybe
it's not a problem.

00:14:04.470 --> 00:14:06.920
So it's actually
kind of interesting.

00:14:06.920 --> 00:14:11.480
It depends on both
the particular layout

00:14:11.480 --> 00:14:15.650
and on the location
of a particle,

00:14:15.650 --> 00:14:19.280
whether it will lead to
degradation or functional

00:14:19.280 --> 00:14:20.840
failure.

00:14:20.840 --> 00:14:25.170
You can also imagine
perhaps this structure,

00:14:25.170 --> 00:14:30.770
this particle might be a defect.

00:14:30.770 --> 00:14:32.585
In what case would
this be a defect?

00:14:36.430 --> 00:14:38.260
Your initial
inclination might be,

00:14:38.260 --> 00:14:40.390
if I'm just looking
at this layer,

00:14:40.390 --> 00:14:43.910
it's not necessarily bridging
from this wire to that wire.

00:14:43.910 --> 00:14:46.482
So why would that be a problem?

00:14:46.482 --> 00:14:49.397
AUDIENCE: [INAUDIBLE]
capacitor then [INAUDIBLE]

00:14:49.397 --> 00:14:50.230
DUANE BONING: Right.

00:14:50.230 --> 00:14:52.960
So if it's conductive and
these two are conductive,

00:14:52.960 --> 00:14:56.230
I might have some additional
capacity of linking even

00:14:56.230 --> 00:14:58.720
within that one layer.

00:14:58.720 --> 00:15:02.450
But I'm trying to give
hints, saying one layer here.

00:15:02.450 --> 00:15:02.950
Yes?

00:15:02.950 --> 00:15:03.992
AUDIENCE: This direction.

00:15:03.992 --> 00:15:06.530
DUANE BONING: Yes,
in the z-direction--

00:15:06.530 --> 00:15:10.460
remember also, we've got
build up of many, many layers.

00:15:10.460 --> 00:15:12.670
Remember that stacking of
the interconnect layer.

00:15:12.670 --> 00:15:15.250
And even within the device
layer, there's many--

00:15:15.250 --> 00:15:19.570
so it can propagate
potentially bridge or short

00:15:19.570 --> 00:15:25.220
or cause deviations in the next
layer of processing as well.

00:15:25.220 --> 00:15:27.610
How about this last one here?

00:15:27.610 --> 00:15:28.390
Is that a problem?

00:15:32.120 --> 00:15:37.420
I'm seeing maybe,
yes, yes, probably.

00:15:37.420 --> 00:15:39.151
Why do you think
that's a problem?

00:15:39.151 --> 00:15:42.360
AUDIENCE: [INAUDIBLE] the
resistance on the [INAUDIBLE]??

00:15:42.360 --> 00:15:44.110
DUANE BONING: Well,
if it were conductive,

00:15:44.110 --> 00:15:47.590
it might not reduce
the resistance--

00:15:47.590 --> 00:15:50.070
or increase the resistance.

00:15:50.070 --> 00:15:57.120
AUDIENCE: [INAUDIBLE]

00:15:57.120 --> 00:15:58.330
DUANE BONING: Right.

00:15:58.330 --> 00:15:58.830
Yeah.

00:15:58.830 --> 00:16:01.050
So a couple of examples
there is certainly,

00:16:01.050 --> 00:16:03.540
if it's non-conductive,
what you've got now

00:16:03.540 --> 00:16:07.620
is increased resistance in
that segment right there,

00:16:07.620 --> 00:16:11.940
which can also cascade
to reliability problems.

00:16:11.940 --> 00:16:14.640
A well-known problem
is migration,

00:16:14.640 --> 00:16:19.710
where basically, the electron
flux of high current flowing

00:16:19.710 --> 00:16:22.920
very, very high current
density up in the 10

00:16:22.920 --> 00:16:26.160
to the fifth to 10 to the
sixth per centimeter squared

00:16:26.160 --> 00:16:29.550
kind of amps centimeter
squared can actually

00:16:29.550 --> 00:16:36.130
cause the metal atoms to move.

00:16:36.130 --> 00:16:38.910
So they will migrate
with the current flow--

00:16:38.910 --> 00:16:41.070
or with the electronic
flow, and you'll

00:16:41.070 --> 00:16:43.530
get-- you can very
often get voiding,

00:16:43.530 --> 00:16:46.530
especially in these
particular locations,

00:16:46.530 --> 00:16:49.020
where the wire gets
thinner and thinner,

00:16:49.020 --> 00:16:52.110
and ultimately, in
fact, may be an open.

00:16:54.750 --> 00:16:57.000
But again, maybe it's OK.

00:16:57.000 --> 00:16:59.550
Maybe you've got enough
latitude and you're not really

00:16:59.550 --> 00:17:01.990
pushing current through there.

00:17:01.990 --> 00:17:06.390
So one side message here is
you never want particles.

00:17:06.390 --> 00:17:08.250
You always want to
minimize the number

00:17:08.250 --> 00:17:12.480
of particles and the
opportunity for failure.

00:17:12.480 --> 00:17:13.960
But the other
message, of course,

00:17:13.960 --> 00:17:17.790
is how bad they
are kind of depend

00:17:17.790 --> 00:17:21.641
on particulars of your circuit
and your specifications.

00:17:21.641 --> 00:17:23.099
So we'll actually
talk a little bit

00:17:23.099 --> 00:17:24.690
about some of the
tools that have

00:17:24.690 --> 00:17:30.120
evolved to be able to analyze
some of those sorts of things.

00:17:30.120 --> 00:17:35.180
So I've talked a lot about opens
and-- or I guess short circuits

00:17:35.180 --> 00:17:35.870
here.

00:17:35.870 --> 00:17:38.990
You might lead to
an open circuit,

00:17:38.990 --> 00:17:42.300
in terms of losing
a conductive path,

00:17:42.300 --> 00:17:46.580
but there's also a lot of
other kinds of failures

00:17:46.580 --> 00:17:49.550
that-- where we're dust
particles or other defects

00:17:49.550 --> 00:17:52.670
might impact the
device operation so

00:17:52.670 --> 00:17:55.920
that you get failure.

00:17:55.920 --> 00:17:59.600
It could be, for example in
an active device, where you've

00:17:59.600 --> 00:18:03.530
got perturbation of
transistor parameters,

00:18:03.530 --> 00:18:05.840
not necessarily just
a short or an open.

00:18:08.910 --> 00:18:10.940
So what is yield?

00:18:13.830 --> 00:18:16.440
Very qualitatively,
yield is the percentage

00:18:16.440 --> 00:18:19.850
of parts meeting
some specification--

00:18:19.850 --> 00:18:21.560
set of specification.

00:18:21.560 --> 00:18:25.340
But what we often do
in IC yield terminology

00:18:25.340 --> 00:18:28.940
is look at different
points in the process--

00:18:28.940 --> 00:18:31.460
flow or different
points of testing--

00:18:31.460 --> 00:18:35.210
and we'll differentiate the
yield in some cases that way.

00:18:35.210 --> 00:18:39.020
And the other is we'll
sometimes break down

00:18:39.020 --> 00:18:45.950
what kind of yield losses
or what size of a bucket

00:18:45.950 --> 00:18:50.090
we're talking about for
thinking about yield.

00:18:50.090 --> 00:18:53.660
By size of a bucket, what
we've got in semiconductor

00:18:53.660 --> 00:18:56.600
manufacturing is a-- and many
other manufacturing process is

00:18:56.600 --> 00:18:58.970
a very hierarchical--

00:18:58.970 --> 00:19:02.660
spatially as well as
temperately-- structure.

00:19:02.660 --> 00:19:08.180
What I mean by that is we've got
within the fab many different

00:19:08.180 --> 00:19:09.740
lots of wafers.

00:19:09.740 --> 00:19:15.080
Each lot maybe is 25 wafers
being processed together.

00:19:15.080 --> 00:19:17.510
Within each lot, I've got--

00:19:17.510 --> 00:19:20.570
of those 25 wafers, I pull
out one wafer and I've got,

00:19:20.570 --> 00:19:24.480
what, 50 to thousands
of chips on it.

00:19:24.480 --> 00:19:27.845
So I can talk about
yield in terms of,

00:19:27.845 --> 00:19:32.190
well, what fraction of lots
make it through the line?

00:19:32.190 --> 00:19:36.120
It's possible I might scrap
a whole lot of wafers.

00:19:36.120 --> 00:19:40.560
Or what fraction of the wafers
within the lot make it through?

00:19:40.560 --> 00:19:44.460
I might have a
dropped wafer, and you

00:19:44.460 --> 00:19:49.560
might break it, or other kinds
of large-scale mechanical

00:19:49.560 --> 00:19:51.430
failure along the line.

00:19:51.430 --> 00:19:53.430
You're not going to
scrap the whole lot

00:19:53.430 --> 00:19:57.270
if one wafer breaks, but--

00:19:57.270 --> 00:20:01.320
so the actual percentage
of wafers that

00:20:01.320 --> 00:20:02.820
make it to the end of the line.

00:20:02.820 --> 00:20:05.520
And that's just,
mechanically, have

00:20:05.520 --> 00:20:07.410
the wafers made it to the end?

00:20:07.410 --> 00:20:09.130
Then you can start
looking and saying,

00:20:09.130 --> 00:20:10.920
OK, what fraction
of those wafers

00:20:10.920 --> 00:20:15.690
appear to be coarsely
are grossly within spec?

00:20:15.690 --> 00:20:17.400
And then similarly,
you start looking

00:20:17.400 --> 00:20:19.950
at the die and say,
which of the die

00:20:19.950 --> 00:20:26.290
are likely to be
able to function?

00:20:26.290 --> 00:20:29.650
What percentage of
die yield do I have?

00:20:29.650 --> 00:20:33.460
Before I go and I invest the
two hours of intense burn in

00:20:33.460 --> 00:20:35.590
and testing for
each of those chips,

00:20:35.590 --> 00:20:37.630
and packaging of
each of those chips,

00:20:37.630 --> 00:20:42.280
I might then also want to
do some on-chip electrical

00:20:42.280 --> 00:20:44.158
testing.

00:20:44.158 --> 00:20:45.700
And then, once it's
packaged, I might

00:20:45.700 --> 00:20:48.190
do a full functional
testing at speed

00:20:48.190 --> 00:20:52.130
to try to determine which
fraction of those are working.

00:20:52.130 --> 00:20:54.910
So we've got some of these
different terminologies here.

00:20:54.910 --> 00:20:57.670
So wafer yield is just--

00:20:57.670 --> 00:21:00.910
actually, let me go
to the next picture.

00:21:00.910 --> 00:21:06.460
This graphically defines some
of these different yields.

00:21:06.460 --> 00:21:10.510
We've drawn the
wafer fab itself,

00:21:10.510 --> 00:21:17.940
the sequence of the processing
steps shown up above.

00:21:17.940 --> 00:21:24.100
And very often, inline tests are
being performed all the time--

00:21:24.100 --> 00:21:28.740
not necessarily on every
piece of equipment or test

00:21:28.740 --> 00:21:32.550
of the wafer after
every individual fab

00:21:32.550 --> 00:21:36.090
step, but one will be making
measurements at various points

00:21:36.090 --> 00:21:40.140
along the flow to check on
the status of the wafer,

00:21:40.140 --> 00:21:42.720
as well as check on the
status of the equipment.

00:21:42.720 --> 00:21:46.080
One might also have some amount
of real-time measurements

00:21:46.080 --> 00:21:48.030
actually on the
equipment itself.

00:21:48.030 --> 00:21:50.760
And a whole
additional problem is

00:21:50.760 --> 00:21:52.650
correlating equipment
measurements

00:21:52.650 --> 00:21:55.020
to what's happening
on the wafer--

00:21:55.020 --> 00:21:57.420
that's especially
useful for debug,

00:21:57.420 --> 00:21:59.220
but it tends not
to be used directly

00:21:59.220 --> 00:22:01.450
for yield calculations.

00:22:01.450 --> 00:22:03.720
So it is possible that, as
you're coming along here,

00:22:03.720 --> 00:22:11.550
you may scrap out wafers based
on some of the inline tests.

00:22:11.550 --> 00:22:14.490
Then what's often referred
to as the back end--

00:22:14.490 --> 00:22:16.945
although it's kind of
confusing, because front end

00:22:16.945 --> 00:22:18.570
and back end can mean
different things,

00:22:18.570 --> 00:22:21.330
depending on who you're
talking to in an IC fab.

00:22:23.840 --> 00:22:26.570
If you're within the
IC fab, often they'll

00:22:26.570 --> 00:22:29.930
talk front end processing
as the transistor formation

00:22:29.930 --> 00:22:32.870
and back end as the
interconnect formation.

00:22:32.870 --> 00:22:39.080
But once you emerge out of the
fab itself into the testing,

00:22:39.080 --> 00:22:42.170
that's also referred
to as the back end.

00:22:42.170 --> 00:22:46.730
Once the wafer has
finished its fabrication,

00:22:46.730 --> 00:22:48.470
and is now going
both in the testing

00:22:48.470 --> 00:22:54.660
and then dicing up into
individual chips for packaging,

00:22:54.660 --> 00:22:57.200
that's also referred
to as the back end.

00:22:57.200 --> 00:22:58.670
So we've got some wafer fab.

00:22:58.670 --> 00:23:01.820
We've got wafer yield that
maybe those wafers that

00:23:01.820 --> 00:23:06.080
make it through the very
coarse electrical test.

00:23:06.080 --> 00:23:09.560
Then you do a more detailed
functional test on each die,

00:23:09.560 --> 00:23:12.770
and as pictured here, the
die yield or functional yield

00:23:12.770 --> 00:23:16.880
would be those fraction
that are making it through.

00:23:16.880 --> 00:23:20.600
Those set of a little bit more
elaborate functional tests--

00:23:20.600 --> 00:23:23.300
now you go ahead and
package up those--

00:23:23.300 --> 00:23:26.960
just those chips
that are successfully

00:23:26.960 --> 00:23:28.940
passing those
tests, and then you

00:23:28.940 --> 00:23:34.530
might do a binning or parametric
test on all of the chips.

00:23:34.530 --> 00:23:39.120
There is another
additional failure point

00:23:39.120 --> 00:23:40.840
that's really important.

00:23:40.840 --> 00:23:44.340
And this distinction between
yield and reliability

00:23:44.340 --> 00:23:46.910
is a little bit fuzzy.

00:23:46.910 --> 00:23:49.310
We've got die yield as being--

00:23:49.310 --> 00:23:53.990
the full parametric die
yield as those chips that

00:23:53.990 --> 00:23:55.430
meet all the specifications.

00:23:55.430 --> 00:23:56.900
You ship them to the customer.

00:23:56.900 --> 00:23:58.340
They go into parts.

00:23:58.340 --> 00:24:03.260
And three months later,
they fail in the field.

00:24:03.260 --> 00:24:06.710
That's also a yield
loss, in some sense.

00:24:06.710 --> 00:24:10.310
It's more described
as a reliability loss,

00:24:10.310 --> 00:24:15.840
but that's that field
loss can be really bad.

00:24:15.840 --> 00:24:19.920
You really want to avoid that,
because the costs associated

00:24:19.920 --> 00:24:25.230
typically with dealing with
in-field failures is very high.

00:24:25.230 --> 00:24:28.378
What's interesting
is, very often--

00:24:28.378 --> 00:24:30.420
and I'm not going to talk
too much about it here,

00:24:30.420 --> 00:24:32.850
but what's very
often the case is

00:24:32.850 --> 00:24:37.770
there is a relationship
between reliability failures

00:24:37.770 --> 00:24:42.900
and yield loss sources
back in the fab.

00:24:42.900 --> 00:24:45.570
The intuition is not
that hard to imagine.

00:24:45.570 --> 00:24:49.560
We even talked about it
back on in this picture.

00:24:49.560 --> 00:24:55.770
If I have a low yielding process
because, on a critical metal

00:24:55.770 --> 00:24:58.290
layer, I've got lots of
point defects leading

00:24:58.290 --> 00:25:03.840
to these kinds of problems, it
might survive through the test,

00:25:03.840 --> 00:25:06.090
but that problem
of, say, migration

00:25:06.090 --> 00:25:10.190
might be more prone
to occur in the field.

00:25:10.190 --> 00:25:14.530
So in general, actually,
yield problems in the fab

00:25:14.530 --> 00:25:17.530
can actually be a
great warning signal

00:25:17.530 --> 00:25:23.100
that you may have ultimate
reliability failures.

00:25:23.100 --> 00:25:25.550
So what we want to do
is try to get a handle

00:25:25.550 --> 00:25:33.240
on ways to model and
understand the yield loss,

00:25:33.240 --> 00:25:35.430
and be able to make
some predictions

00:25:35.430 --> 00:25:39.000
not just for-- based on
historical data for product A,

00:25:39.000 --> 00:25:41.340
but also make some
predictions for product B

00:25:41.340 --> 00:25:46.170
on your line-- what you might
expect the yield loss to be.

00:25:46.170 --> 00:25:49.890
What's interesting here is we've
done so much with the Gaussian

00:25:49.890 --> 00:25:52.230
distribution--

00:25:52.230 --> 00:25:56.070
this is a great case where
the normal distribution

00:25:56.070 --> 00:26:00.330
is generally not the
operative distribution--

00:26:00.330 --> 00:26:05.340
that the probability functions
of the binomial and Poisson

00:26:05.340 --> 00:26:08.700
statistics are
typically more at work

00:26:08.700 --> 00:26:11.320
with random kinds
of point failures.

00:26:11.320 --> 00:26:15.210
So we'll review that
just real briefly.

00:26:15.210 --> 00:26:17.460
And then what I think
is really interesting

00:26:17.460 --> 00:26:20.130
is the spatial nature
of some of these kinds

00:26:20.130 --> 00:26:23.090
of defect processes,
this area dependence.

00:26:23.090 --> 00:26:25.998
So we want to talk about how--

00:26:25.998 --> 00:26:28.020
what some of the basic
modeling approaches

00:26:28.020 --> 00:26:31.927
are for area-dependent failures.

00:26:31.927 --> 00:26:33.510
So earlier in the
semester, we already

00:26:33.510 --> 00:26:35.640
talked about the
binomial distribution.

00:26:35.640 --> 00:26:38.190
I think this may be the
same slide, or almost

00:26:38.190 --> 00:26:39.270
the same slide.

00:26:39.270 --> 00:26:42.540
Remember, the
binomial distribution

00:26:42.540 --> 00:26:45.750
is kind of a nice one
dealing with just this notion

00:26:45.750 --> 00:26:47.950
of success or failure.

00:26:47.950 --> 00:26:51.480
So if I have a point defect
that leads to success or failure

00:26:51.480 --> 00:26:54.300
with some probability
p, and I've

00:26:54.300 --> 00:26:57.510
got now lots of opportunities
for that failure--

00:26:57.510 --> 00:27:02.280
n trials for that particular
failure or success--

00:27:02.280 --> 00:27:06.990
then we can count up or
associate the probability

00:27:06.990 --> 00:27:14.940
of some number of successes x
using a binomial distribution.

00:27:14.940 --> 00:27:19.860
This very often is, in fact,
probably the most important

00:27:19.860 --> 00:27:22.170
underlying function--

00:27:22.170 --> 00:27:24.450
it and its approximation
on the next slide--

00:27:24.450 --> 00:27:27.600
for thinking about
ways to aggregate

00:27:27.600 --> 00:27:30.510
when I've got
multiple opportunities

00:27:30.510 --> 00:27:32.310
or multiple
structures, and I want

00:27:32.310 --> 00:27:36.270
to estimate, given
my probability,

00:27:36.270 --> 00:27:38.790
that any one-- say,
any one chip on a wafer

00:27:38.790 --> 00:27:41.560
is bad, assuming they
were uncorrelated,

00:27:41.560 --> 00:27:45.000
and just due to
random defects, what

00:27:45.000 --> 00:27:48.510
is the probability that, in
a wafer with 100 chips on it,

00:27:48.510 --> 00:27:55.380
I would have 95, 96, 9-- or
better number of chips actually

00:27:55.380 --> 00:27:56.340
coming out?

00:27:56.340 --> 00:28:00.930
What's my probability associated
with the yield of at least 95%

00:28:00.930 --> 00:28:02.010
on that?

00:28:02.010 --> 00:28:09.270
And that falls out directly
from a binomial distribution.

00:28:09.270 --> 00:28:13.050
You could add up then
the probabilities of F,

00:28:13.050 --> 00:28:21.520
with x being 95, 96, 97, 98,
99, 100, and there you go.

00:28:21.520 --> 00:28:24.070
Now, the Poisson distribution
we also talked about,

00:28:24.070 --> 00:28:29.320
and this is a good
one when we have

00:28:29.320 --> 00:28:35.260
in particular very large numbers
of opportunities for failure,

00:28:35.260 --> 00:28:40.090
but the failure probability
for any one of those occurring

00:28:40.090 --> 00:28:42.932
is exceptionally small.

00:28:42.932 --> 00:28:44.890
So we'll talk about this,
especially when we're

00:28:44.890 --> 00:28:48.580
talking about, say, the tens
of thousands or millions

00:28:48.580 --> 00:28:53.140
of devices or structures
within an individual chip.

00:28:53.140 --> 00:28:55.360
Typically, binomial,
the probabilities

00:28:55.360 --> 00:28:58.450
of failure for any one
chip are fairly large,

00:28:58.450 --> 00:29:00.760
and the numbers of
chips on the wafer

00:29:00.760 --> 00:29:05.800
are fairly moderate, so you
can directly use the binomial.

00:29:05.800 --> 00:29:10.810
But when we start to talk about
very, very small probabilities

00:29:10.810 --> 00:29:14.440
of failure, the Poisson ends up
being very, very interesting.

00:29:14.440 --> 00:29:19.360
And this is the case also where
you start to not just think

00:29:19.360 --> 00:29:22.780
about discrete
failure opportunities,

00:29:22.780 --> 00:29:29.140
but it's more useful to think
about a failure rate lambda.

00:29:29.140 --> 00:29:33.250
So for example, what-- if
you have a particular failure

00:29:33.250 --> 00:29:37.780
rate per unit area, just
as with queuing systems,

00:29:37.780 --> 00:29:43.170
you have an opportunity
for arrival per unit time.

00:29:43.170 --> 00:29:46.480
Now think, I've got the
opportunity for an arrival

00:29:46.480 --> 00:29:49.310
of a defect per unit area.

00:29:49.310 --> 00:29:53.450
How does the probability
then, for different areas,

00:29:53.450 --> 00:29:57.320
give rise to the
different probabilities

00:29:57.320 --> 00:30:01.200
of certain numbers of
successes or failures?

00:30:01.200 --> 00:30:03.470
So this ends up
being very useful

00:30:03.470 --> 00:30:08.780
for defect-oriented,
area-dependent oriented

00:30:08.780 --> 00:30:11.060
modeling, very often.

00:30:11.060 --> 00:30:12.830
It's also great
for any other case

00:30:12.830 --> 00:30:16.130
where you've got just a large
number of discrete failures.

00:30:16.130 --> 00:30:19.640
And a typical one
that we'll talk about

00:30:19.640 --> 00:30:23.360
are things like via yield
failures or contact yield

00:30:23.360 --> 00:30:24.330
failures.

00:30:24.330 --> 00:30:26.330
These are the little
electrical connections

00:30:26.330 --> 00:30:28.820
from one layer to the
next in the wiring,

00:30:28.820 --> 00:30:31.700
or from the wiring down
to the transistor level.

00:30:31.700 --> 00:30:36.650
You can imagine any one
metal layer may have millions

00:30:36.650 --> 00:30:41.720
to perhaps, in some
layers, billions of these.

00:30:41.720 --> 00:30:44.080
The opportunity for
failure of any one of those

00:30:44.080 --> 00:30:46.160
is exceptionally
small, but you've

00:30:46.160 --> 00:30:48.133
got a heck of a lot of them.

00:30:48.133 --> 00:30:49.550
And so then you
might want to ask,

00:30:49.550 --> 00:30:55.800
what's the probability of having
perfect operation within that?

00:30:55.800 --> 00:30:57.128
So I saw a hand somewhere.

00:30:57.128 --> 00:30:57.920
Was there question?

00:30:57.920 --> 00:30:59.503
AUDIENCE: No, you
answered [INAUDIBLE]

00:30:59.503 --> 00:31:01.740
DUANE BONING: Great, great--

00:31:01.740 --> 00:31:05.730
OK, so we're going
to be using those.

00:31:05.730 --> 00:31:07.245
Here is the via example.

00:31:10.480 --> 00:31:13.350
We could use the
binomial distribution.

00:31:13.350 --> 00:31:16.470
Well, first, let me give you
a couple of definitions here.

00:31:16.470 --> 00:31:20.100
So we're looking, say, at
one particular metal layer,

00:31:20.100 --> 00:31:24.720
and the probability of
failure for any one via

00:31:24.720 --> 00:31:26.100
is exceptionally small.

00:31:26.100 --> 00:31:29.940
We'll call that p sub v,
probability of failure

00:31:29.940 --> 00:31:31.380
for that.

00:31:31.380 --> 00:31:37.890
However, again, we have
n opportunities or n vias

00:31:37.890 --> 00:31:39.747
in each layer of the chip.

00:31:39.747 --> 00:31:41.580
So you might want to
ask the question, well,

00:31:41.580 --> 00:31:44.430
what's the probability that
I have one via failure,

00:31:44.430 --> 00:31:45.600
then I have 10--

00:31:45.600 --> 00:31:47.910
I have failures in some range?

00:31:47.910 --> 00:31:49.530
Or ultimately, you
want to really ask

00:31:49.530 --> 00:31:53.670
the question, what's the
probability I have zero

00:31:53.670 --> 00:31:56.610
via failures, so I
don't have any wiring

00:31:56.610 --> 00:31:59.750
problems on that chip?

00:31:59.750 --> 00:32:05.250
One could go ahead and directly
use the binomial distribution.

00:32:05.250 --> 00:32:08.550
Alternatively, what we
can start to think of as,

00:32:08.550 --> 00:32:11.730
what is a failure rate
or the average number

00:32:11.730 --> 00:32:19.230
of total via failures, lambda
v, for those vias on a layer?

00:32:19.230 --> 00:32:21.502
So here, we're essentially--

00:32:21.502 --> 00:32:22.460
what the heck happened?

00:32:22.460 --> 00:32:23.793
That's supposed to be an equals.

00:32:26.940 --> 00:32:30.180
So this failure rate
is simply the product

00:32:30.180 --> 00:32:33.060
of the opportunities in the
individual failure, which

00:32:33.060 --> 00:32:36.660
gives you per chip
now this failure rate.

00:32:36.660 --> 00:32:42.210
What is the average number
of failures-- via failures

00:32:42.210 --> 00:32:44.460
per chip for that layer?

00:32:44.460 --> 00:32:47.490
And now you can use the
Poisson distribution, again,

00:32:47.490 --> 00:32:53.142
because the conditions of
very small P, very large n.

00:32:53.142 --> 00:32:59.070
And just plugging in, we've
got this expression here.

00:32:59.070 --> 00:33:02.220
And again, what I said is what
we're really interested in

00:33:02.220 --> 00:33:05.130
is the probability that
the whole chip is good,

00:33:05.130 --> 00:33:08.940
that none of these via
failures are catastrophic.

00:33:08.940 --> 00:33:11.350
None of them occur.

00:33:11.350 --> 00:33:13.740
And so I'm really looking
for the probability

00:33:13.740 --> 00:33:15.270
that x equals 0--

00:33:15.270 --> 00:33:16.890
I have zero via failures.

00:33:16.890 --> 00:33:21.690
With an average number of
failures of three per chip,

00:33:21.690 --> 00:33:25.440
I'd like to know, well,
what's the likelihood then--

00:33:25.440 --> 00:33:30.940
what's my probability that
the whole chip is good?

00:33:30.940 --> 00:33:32.620
It's not zero.

00:33:32.620 --> 00:33:35.140
It's not 100%, because
on average, I've

00:33:35.140 --> 00:33:38.410
got three defects, or three
via failures per chip.

00:33:38.410 --> 00:33:42.370
But I've got some out
there in a tail that

00:33:42.370 --> 00:33:44.270
are still going to be good.

00:33:44.270 --> 00:33:49.945
And so even with
non-zero failure rates--

00:33:49.945 --> 00:33:53.740
fact, it's hard to imagine that
you have a full zero failure

00:33:53.740 --> 00:33:54.730
rate--

00:33:54.730 --> 00:33:56.145
you can still have good chips.

00:33:56.145 --> 00:33:57.520
Now, of course,
you'd like lambda

00:33:57.520 --> 00:34:01.780
to be perhaps less
than 1 on average.

00:34:01.780 --> 00:34:06.370
But now we can basically use
Poisson statistics to aggregate

00:34:06.370 --> 00:34:10.780
and calculate, given individual
failure likelihoods or failure

00:34:10.780 --> 00:34:12.580
rates, what the
probabilities are

00:34:12.580 --> 00:34:16.100
that the whole assembly works.

00:34:16.100 --> 00:34:17.889
Question here--

00:34:17.889 --> 00:34:25.010
AUDIENCE: [INAUDIBLE]

00:34:25.010 --> 00:34:25.900
DUANE BONING: Right.

00:34:25.900 --> 00:34:28.370
Right.

00:34:28.370 --> 00:34:32.179
In fact, what this has already
done is multiplied by the area,

00:34:32.179 --> 00:34:35.420
and so the area multiplication
here was per chip.

00:34:35.420 --> 00:34:41.460
So you ultimately get to
some per unit unitless.

00:34:41.460 --> 00:34:46.239
So lambda is unitless
in this case.

00:34:46.239 --> 00:34:48.780
We'll see other examples
when we do some other area

00:34:48.780 --> 00:34:53.520
dependencies, where you might
be looking within the chip,

00:34:53.520 --> 00:34:59.190
and actually explicitly adding
in or calculating some area,

00:34:59.190 --> 00:35:03.390
and then multiplying the failure
per unit area times the area

00:35:03.390 --> 00:35:09.460
that you're sensitive to to
get to a lambda-like parameter.

00:35:09.460 --> 00:35:11.960
OK?

00:35:11.960 --> 00:35:13.610
This is just a little example--

00:35:13.610 --> 00:35:15.440
I'm actually not going
to go through it--

00:35:15.440 --> 00:35:18.810
that's just working through
the two cases for the binomial

00:35:18.810 --> 00:35:20.630
and Poisson distributions--

00:35:20.630 --> 00:35:26.280
particularly in the case when
n is large and pv as small.

00:35:26.280 --> 00:35:29.030
So this is just looking
at the particular binomial

00:35:29.030 --> 00:35:33.170
distribution when x is 0, or
the Poisson distribution for x

00:35:33.170 --> 00:35:35.930
equals 0 for no failures
in the two cases,

00:35:35.930 --> 00:35:41.540
and just showing that, for
small lambda or for small pv,

00:35:41.540 --> 00:35:51.160
they both go to the
same approximate result.

00:35:51.160 --> 00:35:53.410
So we have our
simplest yield model.

00:35:53.410 --> 00:35:55.913
We have the binomial
distribution,

00:35:55.913 --> 00:35:57.330
which you might
use, for example--

00:35:57.330 --> 00:36:02.500
aggregating chip yield.

00:36:02.500 --> 00:36:07.990
We've got via kinds of
individual failure models--

00:36:07.990 --> 00:36:10.270
so per component
or a failure rate,

00:36:10.270 --> 00:36:14.500
and how to aggregate those on
a per unit or per area basis.

00:36:14.500 --> 00:36:18.460
But I do want to get, actually,
to exactly the question you

00:36:18.460 --> 00:36:21.550
just asked.

00:36:21.550 --> 00:36:24.280
How do we get our minds
around the situation

00:36:24.280 --> 00:36:28.570
when I've got those little dust
particles falling on some area,

00:36:28.570 --> 00:36:34.290
and I'm trying to understand the
area dependence of the circuit?

00:36:34.290 --> 00:36:36.220
And so what we're
going to do is actually

00:36:36.220 --> 00:36:39.130
want to build a yield model
that's a little bit more

00:36:39.130 --> 00:36:44.710
broken out, that explicitly
allows us to make predictions

00:36:44.710 --> 00:36:46.750
based on the area
of the circuit,

00:36:46.750 --> 00:36:50.530
the area of opportunity
for these failures,

00:36:50.530 --> 00:36:56.080
and a defect density or
knowledge about the number

00:36:56.080 --> 00:36:59.080
of defects on
average per unit area

00:36:59.080 --> 00:37:01.450
that we are likely to have.

00:37:01.450 --> 00:37:05.270
And the reason is, if
you think about it,

00:37:05.270 --> 00:37:06.910
the chip gets bigger and bigger.

00:37:06.910 --> 00:37:08.590
It's got larger area.

00:37:08.590 --> 00:37:14.200
And if it's-- only takes
one defect to fail,

00:37:14.200 --> 00:37:18.820
the larger it becomes, the more
likely that chip is to fail.

00:37:18.820 --> 00:37:22.630
So one key driver in this
that interacts a little bit

00:37:22.630 --> 00:37:27.160
with design is, how
big can I make the chip

00:37:27.160 --> 00:37:29.500
without incurring
undue yield loss,

00:37:29.500 --> 00:37:33.940
just because I'm going to have
some likelihood of defects

00:37:33.940 --> 00:37:36.050
per unit area?

00:37:36.050 --> 00:37:41.020
So we want to understand
that interplay.

00:37:41.020 --> 00:37:43.330
We'll start with
just overall area,

00:37:43.330 --> 00:37:46.540
but quickly get to this
notion of a critical area

00:37:46.540 --> 00:37:49.180
on the chip, which
is really just

00:37:49.180 --> 00:37:53.410
that area where the defect
has to fall or a particle

00:37:53.410 --> 00:37:56.410
has to fall in order for
it to actually be a defect,

00:37:56.410 --> 00:37:58.630
and cause an electrical
open, or a short,

00:37:58.630 --> 00:38:05.300
or some other fault, some
other failure in the circuit.

00:38:05.300 --> 00:38:08.240
So how might we go
about modeling these?

00:38:08.240 --> 00:38:11.500
Well, first, to help
with the mental model

00:38:11.500 --> 00:38:14.920
here, with spatial defects,
we're going to make,

00:38:14.920 --> 00:38:18.280
in the simplest yield
model, a few assumptions.

00:38:18.280 --> 00:38:20.890
And then I'll show you,
over the course of time,

00:38:20.890 --> 00:38:24.940
some of the improved versions
of these defect-oriented models

00:38:24.940 --> 00:38:28.870
that have arrived that account
for a little bit more--

00:38:28.870 --> 00:38:33.230
or additional effects or relax
a few of these assumptions.

00:38:33.230 --> 00:38:34.635
So what I've pictured here--

00:38:34.635 --> 00:38:36.040
[INAUDIBLE] it does show up--

00:38:36.040 --> 00:38:40.390
is a wafer with some
number of chips on it--

00:38:40.390 --> 00:38:43.360
I don't know-- 100,
150 different chips,

00:38:43.360 --> 00:38:49.480
and a splattering of a
few little red particles.

00:38:49.480 --> 00:38:51.020
These actually are defects.

00:38:51.020 --> 00:38:54.370
Each one of these
red particles falls

00:38:54.370 --> 00:38:57.040
into place that causes a
failure, some kind of a short.

00:38:57.040 --> 00:39:00.170
They're big enough that they
actually sort things out.

00:39:00.170 --> 00:39:06.740
And you can start to see,
essentially, an assumption here

00:39:06.740 --> 00:39:08.510
is that each one
of these defects

00:39:08.510 --> 00:39:16.850
corresponds to killing one
chip in this simple model.

00:39:16.850 --> 00:39:19.850
Some other assumptions
are they are, in fact,

00:39:19.850 --> 00:39:26.270
randomly distributed by
Poisson kinds of statistics.

00:39:26.270 --> 00:39:31.090
They're also randomly
spatially distributed.

00:39:31.090 --> 00:39:33.610
Knowing where one
defect is tells you

00:39:33.610 --> 00:39:37.000
nothing about where
another defect is.

00:39:37.000 --> 00:39:39.920
So they are spatially
uncorrelated.

00:39:39.920 --> 00:39:42.580
So those are some of
the initial assumptions,

00:39:42.580 --> 00:39:45.430
and under those
assumptions, what

00:39:45.430 --> 00:39:50.560
has been observed is a
very interesting or natural

00:39:50.560 --> 00:39:55.870
relationship between
the density, d0--

00:39:55.870 --> 00:40:01.120
the average number per
unit area of defects--

00:40:01.120 --> 00:40:04.000
in this case, I've
got, what, 1, 2, 3, 4--

00:40:04.000 --> 00:40:09.670
eight defects here per the
total unit area of the wafer,

00:40:09.670 --> 00:40:18.260
and the number of or percentage
of chips that fail, depending

00:40:18.260 --> 00:40:20.450
on the area of each chip.

00:40:24.670 --> 00:40:28.480
And it's pretty obvious,
especially if I go to extremes.

00:40:28.480 --> 00:40:33.250
What if the area of my chip were
the area of the entire wafer?

00:40:33.250 --> 00:40:37.110
I had one chip per wafer.

00:40:37.110 --> 00:40:40.350
If I had eight
defects on average

00:40:40.350 --> 00:40:45.240
per wafer, that means
pretty much every wafer,

00:40:45.240 --> 00:40:50.100
every time, I'm going to for
sure have most likely at least

00:40:50.100 --> 00:40:54.700
one defect, and my yield's
going to be extremely low.

00:40:54.700 --> 00:40:56.980
At some point,
though, my chip size

00:40:56.980 --> 00:41:00.430
gets small enough
that this assumption

00:41:00.430 --> 00:41:05.620
of every defect killing only
one chip is a very good one.

00:41:05.620 --> 00:41:10.270
And then I saturate out to
basically a relationship

00:41:10.270 --> 00:41:14.500
that's very close to
just being determined

00:41:14.500 --> 00:41:19.660
by counting the number of
defects I have per unit area.

00:41:19.660 --> 00:41:21.230
And what was done--

00:41:21.230 --> 00:41:23.350
and this is either
in the Stapper paper

00:41:23.350 --> 00:41:26.830
or referred to another
paper from Stapper--

00:41:26.830 --> 00:41:31.510
is very early on,
this dependence

00:41:31.510 --> 00:41:36.970
on the chip area and that
percentage functioning

00:41:36.970 --> 00:41:41.580
was observed, and it was
observed to be exponential.

00:41:41.580 --> 00:41:44.300
So this is empirical
observation that

00:41:44.300 --> 00:41:48.090
gives credence to this
notion of the Poisson

00:41:48.090 --> 00:41:49.800
statistics are really
what's at work,

00:41:49.800 --> 00:41:54.000
that exponential
dependence on area.

00:41:54.000 --> 00:41:55.980
And what he basically
found is that,

00:41:55.980 --> 00:42:00.450
as the chip area in square
millimeters went up,

00:42:00.450 --> 00:42:02.100
the yield went down.

00:42:02.100 --> 00:42:04.830
So when the chip area
was small enough--

00:42:04.830 --> 00:42:07.230
very close to 100% yield.

00:42:07.230 --> 00:42:10.160
And then-- this is
on a log scale--

00:42:10.160 --> 00:42:12.000
notice, this is on a long scale.

00:42:12.000 --> 00:42:15.726
So there appeared
to be roughly a--

00:42:15.726 --> 00:42:18.840
on the log scale,
a linear decrease

00:42:18.840 --> 00:42:21.270
in yield as the wafer area--

00:42:21.270 --> 00:42:25.470
or excuse me-- as the
chip area increased--

00:42:25.470 --> 00:42:28.510
so that kind of an
exponential dependence.

00:42:28.510 --> 00:42:32.280
And so very early on, the first
model that was really used

00:42:32.280 --> 00:42:36.120
was a Poisson defect model that
basically treated each defect

00:42:36.120 --> 00:42:37.140
as a point--

00:42:37.140 --> 00:42:38.760
said, again, these
same assumptions.

00:42:38.760 --> 00:42:41.040
Each defect results in a
fault, and these things

00:42:41.040 --> 00:42:43.210
are spatially uncorrelated.

00:42:43.210 --> 00:42:44.610
So then what you
can really do is

00:42:44.610 --> 00:42:49.080
start to say, for any
circuit, any chip,

00:42:49.080 --> 00:42:51.630
with some critical
area a sub c--

00:42:51.630 --> 00:42:53.220
so that's the area
within the chip

00:42:53.220 --> 00:42:56.490
that's sensitive to the
falling of these particles--

00:42:56.490 --> 00:43:00.060
maybe ac is equal to the
whole chip area, maybe not--

00:43:00.060 --> 00:43:02.940
and some defect
density, then the yield

00:43:02.940 --> 00:43:06.670
is simply exponential--

00:43:06.670 --> 00:43:09.250
e to the minus ac times d0.

00:43:09.250 --> 00:43:13.830
And recognize,
that ac times d0--

00:43:13.830 --> 00:43:18.150
that gives rise to something
like a lambda parameter,

00:43:18.150 --> 00:43:23.730
a failure rate
kind of parameter.

00:43:23.730 --> 00:43:29.490
Now, he did a little
bit of further breakdown

00:43:29.490 --> 00:43:32.070
here, which I'm not going
to go too much into.

00:43:32.070 --> 00:43:35.490
This actually distinguishes
between a given circuit

00:43:35.490 --> 00:43:39.290
and then the whole chip, which
might have n circuits on it.

00:43:39.290 --> 00:43:43.250
Looking individually at each
critical circuit on the chip,

00:43:43.250 --> 00:43:45.860
you could say for that
particular circuit--

00:43:45.860 --> 00:43:48.890
the wiring pattern, say,
of that particular circuit

00:43:48.890 --> 00:43:50.210
for a metal layer--

00:43:50.210 --> 00:43:53.970
what is the critical area
a sub c for that layer?

00:43:53.970 --> 00:43:57.980
And you can get the yield
statistics for metal layer 3

00:43:57.980 --> 00:43:59.990
for circuit--

00:43:59.990 --> 00:44:04.010
maybe it's the adder circuit
in the upper left corner

00:44:04.010 --> 00:44:05.780
of the device--

00:44:05.780 --> 00:44:07.760
or the chip.

00:44:07.760 --> 00:44:12.450
And then, if you
have n circuits, each

00:44:12.450 --> 00:44:17.670
with a critical area A sub
C, for all of them to work,

00:44:17.670 --> 00:44:20.250
you've just got a
multiplicative probability

00:44:20.250 --> 00:44:24.030
so that your yield is
a multiplicative yield

00:44:24.030 --> 00:44:29.160
factor for all of those
individual circuits.

00:44:29.160 --> 00:44:31.950
So you can read
this as your yield

00:44:31.950 --> 00:44:37.930
for an individual circuit,
just to the n-th power.

00:44:42.280 --> 00:44:46.120
And so what they're doing here
is just simply aggregating

00:44:46.120 --> 00:44:48.430
and saying the
total critical area

00:44:48.430 --> 00:44:51.190
might be across all
of your circuits.

00:44:51.190 --> 00:44:54.130
If each one of them
had equal area a sub c,

00:44:54.130 --> 00:44:56.950
the total area would be
just the product n times ac.

00:44:56.950 --> 00:44:59.410
Or you might do a
summation, might simply

00:44:59.410 --> 00:45:01.480
add up all of the
different critical areas.

00:45:05.280 --> 00:45:09.630
Now, an expansion on this
starts to pull in a little bit

00:45:09.630 --> 00:45:11.730
more statistics.

00:45:11.730 --> 00:45:14.760
And in particular, one of the
really interesting statistics

00:45:14.760 --> 00:45:21.480
is an observation that not every
wafer observes the same defect

00:45:21.480 --> 00:45:22.350
density--

00:45:22.350 --> 00:45:24.390
that there, in fact, is
a probability density

00:45:24.390 --> 00:45:28.380
function associated
with the defect density.

00:45:28.380 --> 00:45:31.320
Some wafers might
see larger numbers

00:45:31.320 --> 00:45:33.420
of defect per unit area.

00:45:33.420 --> 00:45:35.940
Other wafers may see fewer.

00:45:35.940 --> 00:45:37.680
In the natural
operation-- you've

00:45:37.680 --> 00:45:40.000
got the fab as clean
as you can make it--

00:45:40.000 --> 00:45:42.990
there's still a range of
different defect densities

00:45:42.990 --> 00:45:45.840
you expect on any one wafer.

00:45:45.840 --> 00:45:51.070
So the first extension
is to characterize

00:45:51.070 --> 00:45:54.090
the probability density
function associated

00:45:54.090 --> 00:45:56.730
with defect density--

00:45:56.730 --> 00:45:59.790
just number of
defects per unit area.

00:45:59.790 --> 00:46:06.545
And now you can integrate up
what your expected yield is,

00:46:06.545 --> 00:46:08.210
accounting for
the fact that I've

00:46:08.210 --> 00:46:11.810
got a whole range of, or a
whole PDF for different defect

00:46:11.810 --> 00:46:12.830
densities.

00:46:12.830 --> 00:46:17.990
And so the first extension
here is referred to

00:46:17.990 --> 00:46:21.080
as the Murphy yield
model, discussed, again,

00:46:21.080 --> 00:46:22.310
in [INAUDIBLE].

00:46:22.310 --> 00:46:26.420
And all we do is we
have, for any given d,

00:46:26.420 --> 00:46:30.080
we have our Poisson
yield model, and then

00:46:30.080 --> 00:46:35.990
I'm simply averaging
that over my PDF.

00:46:35.990 --> 00:46:40.770
So I'm integrating that over
all possible defect densities.

00:46:40.770 --> 00:46:43.020
Now, we can get back to
the Poisson yield model,

00:46:43.020 --> 00:46:44.880
and now we actually
recognize that that's

00:46:44.880 --> 00:46:48.210
the special case when we
assume there's only one defect

00:46:48.210 --> 00:46:50.400
density, and it
applies to every wafer.

00:46:50.400 --> 00:46:55.500
That is, our PDF, our f of
d, is just a delta function.

00:46:55.500 --> 00:46:58.950
All of the defects are at d0.

00:46:58.950 --> 00:47:03.270
So we can recover and get back
to our Poisson yield model.

00:47:03.270 --> 00:47:05.910
But what's interesting
now is, depending

00:47:05.910 --> 00:47:08.970
on the statistics associated
with defect densities,

00:47:08.970 --> 00:47:13.920
I might end up with different
final yield formulas.

00:47:13.920 --> 00:47:15.810
And so a number of
different-- whoops--

00:47:15.810 --> 00:47:20.940
yield distributions, PDF
associated with defect density

00:47:20.940 --> 00:47:24.180
have been explored, and
then some empirical fits

00:47:24.180 --> 00:47:27.090
done to data--

00:47:27.090 --> 00:47:31.410
yield data to try to see which
matched a little bit better.

00:47:31.410 --> 00:47:34.060
And what's nice is there
are at least a few PDFs

00:47:34.060 --> 00:47:35.970
that, if you plug them
into that integral,

00:47:35.970 --> 00:47:38.590
you're going to have a
closed form solution.

00:47:38.590 --> 00:47:43.410
So for example, if you have
a uniform probability density

00:47:43.410 --> 00:47:48.390
function associated
with defect density,

00:47:48.390 --> 00:47:52.990
that yields or gives rise to
this uniform yield formula,

00:47:52.990 --> 00:47:54.900
which is no longer exponential--

00:47:54.900 --> 00:47:56.460
or just an exponential.

00:47:56.460 --> 00:48:00.060
It's also got a 1 minus the--
this exponential in a scaling

00:48:00.060 --> 00:48:01.320
factor--

00:48:01.320 --> 00:48:04.470
can also do it for a
triangular distribution.

00:48:04.470 --> 00:48:09.700
You get a squared version.

00:48:09.700 --> 00:48:11.620
If I plug in a
Gaussian, we already

00:48:11.620 --> 00:48:15.490
know that an integral over a
Gaussian is kind of nasty--

00:48:15.490 --> 00:48:18.680
doesn't have a
closed form solution.

00:48:18.680 --> 00:48:20.830
So it's not directly integrable.

00:48:20.830 --> 00:48:22.900
One can certainly
do it numerically,

00:48:22.900 --> 00:48:27.920
and things like the
phi function does that.

00:48:27.920 --> 00:48:31.220
The Murphy yield
model was done back

00:48:31.220 --> 00:48:36.015
when people really wanted
closed form kinds of solutions.

00:48:36.015 --> 00:48:38.420
Oh, I thought I had a picture.

00:48:38.420 --> 00:48:40.080
Here we go.

00:48:40.080 --> 00:48:46.280
So here's a comparison of some
of these different PDFs that

00:48:46.280 --> 00:48:48.260
have been examined.

00:48:48.260 --> 00:48:51.380
Again, Poisson assumes
everything is at a d0--

00:48:51.380 --> 00:48:53.720
should be a d0 there--

00:48:53.720 --> 00:48:55.940
a uniform distribution.

00:48:55.940 --> 00:48:59.270
I might have defect densities
across that whole range,

00:48:59.270 --> 00:49:01.340
or some triangular
distribution where

00:49:01.340 --> 00:49:07.250
you might, in fact, restrict it
in some additional form related

00:49:07.250 --> 00:49:08.570
to some d0--

00:49:08.570 --> 00:49:16.070
or an exponentially decaying
defect density function.

00:49:16.070 --> 00:49:23.660
What was interesting is, if
you go back to the literature,

00:49:23.660 --> 00:49:27.320
Seeds proposed that--

00:49:27.320 --> 00:49:32.270
based on some experimental
data, that an exponential defect

00:49:32.270 --> 00:49:35.570
density distribution
appeared to make sense.

00:49:35.570 --> 00:49:37.850
First off, the
qualitative reason

00:49:37.850 --> 00:49:41.600
was its vanishingly
small likelihood

00:49:41.600 --> 00:49:46.880
that you've got lots and lots
of defects, because if you do,

00:49:46.880 --> 00:49:50.180
your overall process yield
is not going to be very good,

00:49:50.180 --> 00:49:54.230
and so you would have done
process correction or process

00:49:54.230 --> 00:49:56.390
development to remove that.

00:49:56.390 --> 00:50:00.380
But as the defect density
gets smaller and smaller,

00:50:00.380 --> 00:50:02.870
a good manufacturing process--

00:50:02.870 --> 00:50:04.040
that's where you want to be.

00:50:04.040 --> 00:50:05.915
You want to have much,
much higher likelihood

00:50:05.915 --> 00:50:11.270
of small numbers of defects
per unit area than high ones.

00:50:11.270 --> 00:50:15.290
So this is not really a
statement about physics.

00:50:15.290 --> 00:50:19.010
It's a statement about
manufacturing operation

00:50:19.010 --> 00:50:22.940
that drives a particular kind
of shape of defect density

00:50:22.940 --> 00:50:24.140
distributions.

00:50:24.140 --> 00:50:26.450
It says all of the--

00:50:26.450 --> 00:50:28.670
or a huge amount
of energy is put

00:50:28.670 --> 00:50:32.990
in to driving down the
defect density distribution.

00:50:32.990 --> 00:50:35.300
And what that should
lead to is something

00:50:35.300 --> 00:50:41.600
like an exponential
falloff, as pictured here.

00:50:41.600 --> 00:50:44.690
You would expect and hope that
your manufacturing process

00:50:44.690 --> 00:50:47.420
would have a much higher
likelihood of a small number

00:50:47.420 --> 00:50:49.340
of defects per unit area.

00:50:49.340 --> 00:50:52.340
And what's nice is, when you
plug that in, you can get

00:50:52.340 --> 00:50:54.920
a closed form
expression that's--

00:50:54.920 --> 00:50:58.250
that, for the exponential
defect density,

00:50:58.250 --> 00:51:04.500
is very nice and simple.

00:51:04.500 --> 00:51:05.010
Question--

00:51:05.010 --> 00:51:07.302
AUDIENCE: Does that really
make sense, though, compared

00:51:07.302 --> 00:51:11.250
to, say, using half a
Gaussian instead with, say,

00:51:11.250 --> 00:51:13.770
v centered at 0 and just
cropping half of it.

00:51:13.770 --> 00:51:16.290
It kind of seems
as you approach 0,

00:51:16.290 --> 00:51:18.790
it actually becomes
more difficult to remove

00:51:18.790 --> 00:51:22.530
those last couple of defects,
rather than going exponentially

00:51:22.530 --> 00:51:24.498
up that curve, that it
kind of flattens off.

00:51:24.498 --> 00:51:25.290
DUANE BONING: Yeah.

00:51:25.290 --> 00:51:28.950
So the question is, what
really is the defect density

00:51:28.950 --> 00:51:29.670
distribution?

00:51:29.670 --> 00:51:30.880
And does this make sense?

00:51:30.880 --> 00:51:35.580
Especially with the singularity,
as the defect density

00:51:35.580 --> 00:51:40.140
goes to 0, what's the
relative probability?

00:51:40.140 --> 00:51:45.630
Might you model this with a
Gaussian or a half Gaussian?

00:51:45.630 --> 00:51:49.020
There's all kinds of arguments.

00:51:49.020 --> 00:51:54.690
And it's actually difficult to
get enough data to really nail

00:51:54.690 --> 00:51:58.470
down the distribution.

00:51:58.470 --> 00:52:01.060
Think of how many
wafers, if you will--

00:52:01.060 --> 00:52:03.360
to get a very
careful description

00:52:03.360 --> 00:52:06.690
of defect density
per unit area--

00:52:06.690 --> 00:52:09.150
on average you might need.

00:52:09.150 --> 00:52:13.600
It's hard to fully get the
amount of data that you need.

00:52:13.600 --> 00:52:17.550
So you're really getting
a few data points in here

00:52:17.550 --> 00:52:20.460
that you're trying to get at
least the right trend with.

00:52:20.460 --> 00:52:24.360
And so it actually doesn't
matter too critically, as long

00:52:24.360 --> 00:52:27.330
as you've got the basic
essence of the shape.

00:52:29.910 --> 00:52:32.160
And I will show you
at the end a few

00:52:32.160 --> 00:52:33.660
of the kinds of
test structures that

00:52:33.660 --> 00:52:37.500
are used to try to approximate
or get at these defect density

00:52:37.500 --> 00:52:38.700
distributions.

00:52:38.700 --> 00:52:42.840
And in fact, what is very
often done, just to give you

00:52:42.840 --> 00:52:45.030
a little bit of a peek--

00:52:45.030 --> 00:52:51.330
people might use the
exponential with a fit

00:52:51.330 --> 00:52:55.380
to just a couple of points
or a couple of parameters.

00:53:02.520 --> 00:53:07.770
OK, so that's basically
the Seeds model,

00:53:07.770 --> 00:53:10.330
and that often is used.

00:53:10.330 --> 00:53:13.740
But I want to return to a
couple of other further extended

00:53:13.740 --> 00:53:16.500
models and give you a little
bit of a feel for them,

00:53:16.500 --> 00:53:22.450
because the arguments about
defect density distributions

00:53:22.450 --> 00:53:23.710
continue.

00:53:23.710 --> 00:53:26.650
But also, reassessing
or looking back

00:53:26.650 --> 00:53:28.960
at some of the other
assumptions that I've mentioned,

00:53:28.960 --> 00:53:31.490
arguments about
those also exist.

00:53:31.490 --> 00:53:33.580
And one of the
most important ones

00:53:33.580 --> 00:53:40.630
is this notion of no spatial
correlation in your defect

00:53:40.630 --> 00:53:42.260
locations.

00:53:42.260 --> 00:53:44.800
Rather than show this,
let me show this first.

00:53:44.800 --> 00:53:48.040
So we assumed the
picture over on the left,

00:53:48.040 --> 00:53:50.500
that all of your
defects were randomly

00:53:50.500 --> 00:53:52.030
distributed across the wafer.

00:53:52.030 --> 00:53:54.790
What's very often
observed in practice

00:53:54.790 --> 00:53:59.080
is that these defects tend
to cluster near each other.

00:53:59.080 --> 00:54:03.670
And maybe there's some process
going on in your chamber that

00:54:03.670 --> 00:54:07.720
occasionally splattering
particles, accelerating

00:54:07.720 --> 00:54:09.290
particles in some direction.

00:54:09.290 --> 00:54:12.850
And so those may naturally
send multiple particles

00:54:12.850 --> 00:54:23.760
all together, and they may very
often tend to cluster together.

00:54:23.760 --> 00:54:26.290
So that is very interesting.

00:54:26.290 --> 00:54:29.460
If, instead of each
and every particle

00:54:29.460 --> 00:54:34.410
being spatially distributed
and causing a fault

00:54:34.410 --> 00:54:38.730
on an individual
chip, now, well, I've

00:54:38.730 --> 00:54:42.510
got multiple particles
all falling and perhaps

00:54:42.510 --> 00:54:45.600
causing defects on
these two chips,

00:54:45.600 --> 00:54:48.780
but now that assumption
that every single defect is

00:54:48.780 --> 00:54:56.700
causing its own unique kill
event is no longer really true.

00:54:56.700 --> 00:54:59.730
You really can't keep
killing the same chip

00:54:59.730 --> 00:55:01.770
and causing
additional yield loss.

00:55:01.770 --> 00:55:04.150
So if you've got clustering
of your defects, in fact,

00:55:04.150 --> 00:55:06.090
you may be in better
shape than you

00:55:06.090 --> 00:55:10.950
would have assumed per the count
of defects over on the left.

00:55:10.950 --> 00:55:15.450
And a distribution that has an
additional parameter in it--

00:55:15.450 --> 00:55:17.340
this alpha parameter--

00:55:17.340 --> 00:55:24.410
that gives a defect
density distribution

00:55:24.410 --> 00:55:26.420
with an extra degree
of freedom that you

00:55:26.420 --> 00:55:28.010
can play with this shape--

00:55:28.010 --> 00:55:30.350
not necessarily even Gaussian.

00:55:30.350 --> 00:55:33.140
But some other
amounts of skewness

00:55:33.140 --> 00:55:41.480
away from that exponential is
a negative binomial or gamma

00:55:41.480 --> 00:55:44.150
probability distribution
that gives rise

00:55:44.150 --> 00:55:47.300
to this negative binomial model.

00:55:47.300 --> 00:55:50.810
And so empirically, there's
this additional alpha parameter

00:55:50.810 --> 00:55:55.130
that lets one tweak, or fit
your data to tweak the defect

00:55:55.130 --> 00:55:57.090
density distribution.

00:55:57.090 --> 00:55:59.540
So if you wanted something
that was a little bit more

00:55:59.540 --> 00:56:02.750
like a Gaussian or
a half Gaussian,

00:56:02.750 --> 00:56:05.420
but maybe behaved a
little bit differently

00:56:05.420 --> 00:56:08.300
right near your
low defect density,

00:56:08.300 --> 00:56:10.550
you've got that opportunity.

00:56:10.550 --> 00:56:14.340
And what's nice about
it is it actually

00:56:14.340 --> 00:56:18.150
correlates or relates to this
notion of spatial clustering

00:56:18.150 --> 00:56:19.120
of your defects.

00:56:19.120 --> 00:56:22.080
So there's a reasonable
physical explanation

00:56:22.080 --> 00:56:26.400
for these situations.

00:56:26.400 --> 00:56:27.570
Yes, question--

00:56:27.570 --> 00:56:32.870
AUDIENCE: [INAUDIBLE]

00:56:32.870 --> 00:56:33.870
DUANE BONING: I'm sorry.

00:56:33.870 --> 00:56:34.560
Say that again.

00:56:34.560 --> 00:56:39.330
AUDIENCE: [INAUDIBLE]

00:56:39.330 --> 00:56:41.610
DUANE BONING: Oh, well, if
they're doing the d0 Murphy

00:56:41.610 --> 00:56:43.290
model, it's probably--

00:56:48.140 --> 00:56:53.240
they might be just using
the delta function--

00:56:53.240 --> 00:56:56.030
simple kind of a model.

00:56:56.030 --> 00:56:58.190
But you'd actually have
to probe a little bit,

00:56:58.190 --> 00:57:01.130
because they might also
have a clustering parameter,

00:57:01.130 --> 00:57:04.470
and really, then what's going
on is something like this.

00:57:04.470 --> 00:57:07.670
So where your d0
is in here, there

00:57:07.670 --> 00:57:11.900
is still a scaling factor
to this distribution.

00:57:11.900 --> 00:57:14.030
You can see the d0 in here.

00:57:14.030 --> 00:57:19.700
So they might be using, in
fact, a negative binomial yield

00:57:19.700 --> 00:57:20.430
model.

00:57:20.430 --> 00:57:22.610
In fact, I think
that, right now, this

00:57:22.610 --> 00:57:24.560
is a dominant
model that is used,

00:57:24.560 --> 00:57:27.860
with clustering accounted for.

00:57:27.860 --> 00:57:30.800
And so the d0 is
still your average--

00:57:30.800 --> 00:57:34.850
it's your central scaling or
average on this distribution.

00:57:34.850 --> 00:57:40.480
AUDIENCE: [INAUDIBLE] 10
years or 5 years, then

00:57:40.480 --> 00:57:45.780
that [INAUDIBLE]
changing [INAUDIBLE]

00:57:45.780 --> 00:57:48.030
DUANE BONING: So the question
is, for a long lifetime,

00:57:48.030 --> 00:57:49.740
how do these parameters change?

00:57:49.740 --> 00:57:54.360
And generally, they do
change on your fab--

00:57:54.360 --> 00:57:57.660
not necessarily the
lifetime of your product

00:57:57.660 --> 00:58:00.240
so much, because I think of--

00:58:00.240 --> 00:58:04.020
your d0 tends to be more a
characteristic of your unit

00:58:04.020 --> 00:58:06.150
process or your
integrated process.

00:58:06.150 --> 00:58:10.080
But as you learn more, you
have this yield learning,

00:58:10.080 --> 00:58:15.390
where you hope you drive your
defect density down with time--

00:58:15.390 --> 00:58:17.310
drive your particle size down.

00:58:17.310 --> 00:58:21.810
So you do continue to
improve the process.

00:58:21.810 --> 00:58:25.320
In fact, in some of the
yield projections for product

00:58:25.320 --> 00:58:27.810
you might run in
your fab in a year,

00:58:27.810 --> 00:58:30.030
you might also include
some projections

00:58:30.030 --> 00:58:32.790
on what you think
d0 will improve to,

00:58:32.790 --> 00:58:35.550
based on historical
trends over time.

00:58:35.550 --> 00:58:42.010
AUDIENCE: [INAUDIBLE]

00:58:42.010 --> 00:58:44.650
DUANE BONING: Yes, yes--

00:58:44.650 --> 00:58:49.000
typically, perhaps
more with the d0--

00:58:49.000 --> 00:58:51.400
probably less
projections on alpha.

00:58:51.400 --> 00:58:54.130
Alpha tends to be
this clustering, which

00:58:54.130 --> 00:58:56.560
has two limit that
I'll talk about--

00:58:56.560 --> 00:59:01.780
low clustering and very
highly, tightly clustered.

00:59:01.780 --> 00:59:07.030
I don't think that's assumed
to change that much with time.

00:59:07.030 --> 00:59:10.060
But the d0 is the main
thing that goes down

00:59:10.060 --> 00:59:13.675
with improved processing.

00:59:13.675 --> 00:59:20.200
AUDIENCE: [INAUDIBLE]

00:59:20.200 --> 00:59:22.630
DUANE BONING: They
should, whether they're

00:59:22.630 --> 00:59:25.690
drawn to actually
integrate out to 1 or not.

00:59:25.690 --> 00:59:28.660
But they are all still defect
[INAUDIBLE] probability density

00:59:28.660 --> 00:59:31.600
functions.

00:59:31.600 --> 00:59:34.600
So we do have this alpha
clustering parameter.

00:59:34.600 --> 00:59:41.650
What's nice is, amazingly, you
plug that PDF into the integral

00:59:41.650 --> 00:59:52.960
with an exponential Poisson
kernel in e to the minus acd,

00:59:52.960 --> 00:59:55.750
you get a closed form
yield formula here,

00:59:55.750 --> 01:00:00.030
as shown at the bottom,
which has the alpha

01:00:00.030 --> 01:00:01.890
clustering parameter in it.

01:00:01.890 --> 01:00:04.290
And we can take two limits.

01:00:04.290 --> 01:00:06.990
One limit is the large
alpha limit, which

01:00:06.990 --> 01:00:09.970
is very little clustering.

01:00:09.970 --> 01:00:13.270
So think of maybe
alpha as the distance

01:00:13.270 --> 01:00:17.200
between individual defects,
and as that gets large,

01:00:17.200 --> 01:00:19.120
you don't have any clustering.

01:00:19.120 --> 01:00:22.660
And that limit converges
to the Poisson model.

01:00:22.660 --> 01:00:24.340
And in the very
small alpha limit,

01:00:24.340 --> 01:00:27.700
with very, very strong
clustering, that actually

01:00:27.700 --> 01:00:31.690
converges in the limit for alpha
going to 0 to the Seeds model

01:00:31.690 --> 01:00:36.650
that we saw earlier, which
was the pure exponential.

01:00:36.650 --> 01:00:41.660
So you see, as alpha
gets smaller and smaller,

01:00:41.660 --> 01:00:47.300
this approach is more and more
the exponential defect density

01:00:47.300 --> 01:00:49.710
model.

01:00:49.710 --> 01:00:53.070
Turns out that,
generally, people

01:00:53.070 --> 01:00:56.130
are fitting based on
experimental data, their D0.

01:00:56.130 --> 01:00:59.610
And they're also fitting,
empirically, alpha.

01:00:59.610 --> 01:01:04.290
And alpha tends to be related
both to the clustering,

01:01:04.290 --> 01:01:07.020
but also a little bit to
the sensitivity of your type

01:01:07.020 --> 01:01:10.990
of circuit to clustering.

01:01:10.990 --> 01:01:13.450
So it's not purely--

01:01:13.450 --> 01:01:15.550
if I did this just
on blanket wafers

01:01:15.550 --> 01:01:18.040
and looked at the clustering,
that may actually not

01:01:18.040 --> 01:01:21.610
tell me what is going to happen
for different kinds of product.

01:01:21.610 --> 01:01:27.550
So you actually might end up
with different components of

01:01:27.550 --> 01:01:29.110
or different
products, whether it

01:01:29.110 --> 01:01:32.830
be a memory product or a
microprocessor product,

01:01:32.830 --> 01:01:35.650
or different components
on a big multi-product

01:01:35.650 --> 01:01:37.600
that has a lot of
memory cache on it,

01:01:37.600 --> 01:01:41.380
and also has the
combinational logic on it.

01:01:41.380 --> 01:01:43.690
You might have slightly
different yield model

01:01:43.690 --> 01:01:46.690
components or slightly
different alphas for those two

01:01:46.690 --> 01:01:49.944
different cases, and
you would fit those.

01:01:49.944 --> 01:01:55.480
AUDIENCE: [INAUDIBLE] is
actually designed using

01:01:55.480 --> 01:01:56.795
different alpha parameters--

01:01:56.795 --> 01:01:57.920
DUANE BONING: Interesting--

01:01:57.920 --> 01:02:07.880
AUDIENCE: [INAUDIBLE]

01:02:07.880 --> 01:02:08.985
DUANE BONING: Yeah.

01:02:08.985 --> 01:02:11.360
So the observation, if you
didn't hear that in Singapore,

01:02:11.360 --> 01:02:15.170
was that, in practice, with
those memory redundancy

01:02:15.170 --> 01:02:18.470
schemes, those
also affect alpha,

01:02:18.470 --> 01:02:21.770
and so it's an empirical fitting
process with different kinds

01:02:21.770 --> 01:02:24.440
of redundancy to see
how that affects alpha--

01:02:24.440 --> 01:02:28.910
and what your ultimate yield
would be based on that.

01:02:28.910 --> 01:02:33.880
OK, so so far, we've talked
about a probability density

01:02:33.880 --> 01:02:38.750
associated with the number
of defects per unit area.

01:02:38.750 --> 01:02:43.180
We can also think about
another probability function,

01:02:43.180 --> 01:02:45.670
another statistical
relationship.

01:02:45.670 --> 01:02:48.550
So far, we've talked about
every defect abstractly

01:02:48.550 --> 01:02:53.120
as being infinitesimally small.

01:02:53.120 --> 01:02:55.970
So one defect doesn't cover
20 different chips, right?

01:02:55.970 --> 01:02:58.280
It's just infinitesimally small.

01:02:58.280 --> 01:03:02.480
But what if there is an area
dependence to the size--

01:03:02.480 --> 01:03:08.150
or a probability associated
with the size of those defects?

01:03:08.150 --> 01:03:10.430
That could interact
very importantly

01:03:10.430 --> 01:03:15.060
with some of those original
shorting and open physics

01:03:15.060 --> 01:03:17.110
that we talked about earlier.

01:03:17.110 --> 01:03:20.910
So for example, if I have
wiring lines like this,

01:03:20.910 --> 01:03:23.490
and I'm really worried
about either open or short,

01:03:23.490 --> 01:03:29.070
and my defect is substantially
smaller than either the spacing

01:03:29.070 --> 01:03:33.330
or the width of the line,
it can fall almost anywhere

01:03:33.330 --> 01:03:36.630
and not cause at least
an immediate failure--

01:03:36.630 --> 01:03:40.140
might still be a reliability
or a parametric resistance

01:03:40.140 --> 01:03:42.540
change that I'd
be worried about--

01:03:42.540 --> 01:03:45.660
whereas, if the defect
were much larger,

01:03:45.660 --> 01:03:48.480
it can fall almost
anywhere on my circuit

01:03:48.480 --> 01:03:51.030
and cause either
an open or a short.

01:03:51.030 --> 01:03:54.390
So the effect of particles
of different sizes

01:03:54.390 --> 01:03:57.420
can be very different,
and so I might also

01:03:57.420 --> 01:04:03.520
want to characterize the size
distribution of particles.

01:04:03.520 --> 01:04:07.120
And the interaction of
those science distributions

01:04:07.120 --> 01:04:09.940
with the particular
feature sizes on my circuit

01:04:09.940 --> 01:04:13.580
is going to be very important.

01:04:13.580 --> 01:04:19.990
So it interacts with this
notion then of a critical area.

01:04:19.990 --> 01:04:21.850
Let's see if I've
got a better picture.

01:04:21.850 --> 01:04:24.900
Nope, this is pretty much it.

01:04:24.900 --> 01:04:27.240
There is a formal notion
of critical error--

01:04:27.240 --> 01:04:32.490
area for any particular defect
size that you can actually

01:04:32.490 --> 01:04:38.610
analyze for your particular
layout and say, which area--

01:04:38.610 --> 01:04:43.120
which fraction of the
area on that layer

01:04:43.120 --> 01:04:47.260
does the center of the particle
have to fall in order for it

01:04:47.260 --> 01:04:49.030
to cause either an
open or a short?

01:04:51.827 --> 01:04:53.410
Let me try to erase
this a little bit.

01:04:57.360 --> 01:04:59.540
So for example,
the critical area

01:04:59.540 --> 01:05:05.630
perhaps associated with the
smallest particle may be 0.

01:05:05.630 --> 01:05:09.050
It can fall anywhere
and not cause a problem.

01:05:09.050 --> 01:05:15.330
This particle is perhaps
a more interesting one,

01:05:15.330 --> 01:05:19.185
in that maybe it's exactly
equal to the size of--

01:05:21.770 --> 01:05:29.790
actually, let's do an
example [INAUDIBLE]

01:05:29.790 --> 01:05:36.760
Let's do an example
where I've got something

01:05:36.760 --> 01:05:40.950
that's, say, equal to--

01:05:40.950 --> 01:05:45.840
or just slightly larger
than the spacing size.

01:05:45.840 --> 01:05:49.680
But in some places,
I've got wires where--

01:05:49.680 --> 01:05:53.640
and spaces that are
smaller than that,

01:05:53.640 --> 01:05:57.705
such as pictured here, but
I've also got other places--

01:05:57.705 --> 01:05:59.790
let's say, I have
now another wire

01:05:59.790 --> 01:06:06.300
up here, where the spacing is
larger than the particle size.

01:06:06.300 --> 01:06:09.600
Now this same particle
can fall right there

01:06:09.600 --> 01:06:12.340
and not cause a short.

01:06:12.340 --> 01:06:18.040
So you can calculate across your
entire particular layout what

01:06:18.040 --> 01:06:21.160
is the band where the
center of the particle

01:06:21.160 --> 01:06:25.480
has to fall to cause
either an open or a short

01:06:25.480 --> 01:06:29.800
and some up that
area of sensitivity

01:06:29.800 --> 01:06:34.060
for failure for
each of the layers.

01:06:34.060 --> 01:06:38.650
So there's this interaction
between a critical area

01:06:38.650 --> 01:06:41.410
per particle size, and
then a distribution

01:06:41.410 --> 01:06:43.840
associated with the
particle sizes that are very

01:06:43.840 --> 01:06:46.980
important to also characterize.

01:06:46.980 --> 01:06:48.960
And here are some examples--

01:06:48.960 --> 01:06:51.240
again, going way back--

01:06:51.240 --> 01:06:55.310
for defect size distributions.

01:06:55.310 --> 01:06:58.100
These are back
characterized in mils.

01:06:58.100 --> 01:07:01.590
Anybody know what a mil is?

01:07:01.590 --> 01:07:06.720
Thousandth of an inch,
or about 25 microns--

01:07:06.720 --> 01:07:09.870
so these are giant,
giant particles.

01:07:09.870 --> 01:07:12.990
We have driven down
defect sizes a bit.

01:07:12.990 --> 01:07:15.870
But what is very interesting
is the same trend

01:07:15.870 --> 01:07:17.880
has continued to be observed--

01:07:17.880 --> 01:07:20.550
that there is generally
believed to be something

01:07:20.550 --> 01:07:25.980
close to an exponential
dependence in defect size,

01:07:25.980 --> 01:07:29.800
not just in the number
of defects as well.

01:07:29.800 --> 01:07:34.960
And that exponential-- or
power law kind of dependence

01:07:34.960 --> 01:07:40.480
is very often used in
modeling the size distribution

01:07:40.480 --> 01:07:41.455
for defects.

01:07:45.040 --> 01:07:47.260
Now, there's a couple
of parameters-- this n

01:07:47.260 --> 01:07:49.240
and this p, which,
again, end up being

01:07:49.240 --> 01:07:51.610
technology dependent
and generally fitting

01:07:51.610 --> 01:07:54.680
parameters to the data.

01:07:54.680 --> 01:08:00.220
So now, if we take in that
notion of a distribution

01:08:00.220 --> 01:08:03.970
in defect sizes and the
probability associated

01:08:03.970 --> 01:08:07.630
with that, one can
form an aggregate sort

01:08:07.630 --> 01:08:09.530
of approximate parameter.

01:08:09.530 --> 01:08:12.880
And so sometimes that's
also the d0 that is quoted.

01:08:12.880 --> 01:08:18.490
So it's not only averaged in
terms of numbers per area,

01:08:18.490 --> 01:08:22.840
but it's also kind of a boiled
down approximate parameter

01:08:22.840 --> 01:08:27.609
giving you a sense of the basic
central moment of the size

01:08:27.609 --> 01:08:30.550
distribution as well.

01:08:30.550 --> 01:08:34.200
But if you really wanted to
do careful defect modeling,

01:08:34.200 --> 01:08:37.200
you actually need to know
the p parameters and the n

01:08:37.200 --> 01:08:37.890
parameters.

01:08:37.890 --> 01:08:40.170
You would like to have
that full defect size

01:08:40.170 --> 01:08:41.880
distribution at hand.

01:08:44.399 --> 01:08:45.819
Here are some examples.

01:08:45.819 --> 01:08:48.370
The typical ranges
of that exponent--

01:08:48.370 --> 01:08:51.180
I guess it's a p exponent
in the previous slide--

01:08:51.180 --> 01:08:54.300
is two, three, four.

01:08:54.300 --> 01:08:58.680
And it may depend on
the defect failure mode

01:08:58.680 --> 01:09:00.689
that you're looking
at-- so for example,

01:09:00.689 --> 01:09:04.740
extra metal or short versus
missing metal and opened.

01:09:04.740 --> 01:09:10.840
They may have slightly different
sensitivity to that defect size

01:09:10.840 --> 01:09:11.340
as well.

01:09:11.340 --> 01:09:17.302
AUDIENCE: [INAUDIBLE] more
distributions [INAUDIBLE]

01:09:17.302 --> 01:09:19.649
DUANE BONING: This is
basically this power law.

01:09:19.649 --> 01:09:23.500
1/x to the p is the assumed--

01:09:23.500 --> 01:09:24.870
AUDIENCE: That's [INAUDIBLE]

01:09:24.870 --> 01:09:25.890
DUANE BONING: Yes.

01:09:25.890 --> 01:09:28.400
AUDIENCE: But for [INAUDIBLE]

01:09:28.400 --> 01:09:30.500
DUANE BONING: For the d0,
the d0 is actually still

01:09:30.500 --> 01:09:34.189
not counting-- adding
in the defect density.

01:09:34.189 --> 01:09:36.710
That will come in in
the other distribution.

01:09:39.660 --> 01:09:42.260
Oh, OK, I did have a slide.

01:09:42.260 --> 01:09:43.970
I think I've already
explained this,

01:09:43.970 --> 01:09:46.430
but this is talking again
about the critical area

01:09:46.430 --> 01:09:49.430
for different size dependencies.

01:09:49.430 --> 01:09:51.350
And what's interesting
is then you can also

01:09:51.350 --> 01:09:56.580
start to produce a plot of this
critical area versus defect

01:09:56.580 --> 01:09:57.080
size.

01:10:00.170 --> 01:10:04.580
And what's really important
here is very intuitive trends

01:10:04.580 --> 01:10:08.930
that, for larger defects,
I've got larger critical area.

01:10:08.930 --> 01:10:11.810
More part of my chip
is sensitive to it.

01:10:11.810 --> 01:10:16.400
But once it gets smaller than
a certain dimension, when

01:10:16.400 --> 01:10:20.690
my defects tend to be smaller
than my minimum feature size

01:10:20.690 --> 01:10:23.060
on the device, I
start to be less

01:10:23.060 --> 01:10:25.260
sensitive to immediate failures.

01:10:25.260 --> 01:10:27.350
So if they're a fraction--

01:10:27.350 --> 01:10:31.330
your particles are a fraction
of your minimum dimension.

01:10:31.330 --> 01:10:33.080
And that's good, because
you probably have

01:10:33.080 --> 01:10:35.210
lots of defects of those sizes.

01:10:35.210 --> 01:10:38.590
It's very hard to
get rid of those.

01:10:38.590 --> 01:10:40.800
Now, what you can do is
start to put these together.

01:10:40.800 --> 01:10:42.240
You aggregate these.

01:10:42.240 --> 01:10:45.630
If you have a defect
size distribution that

01:10:45.630 --> 01:10:50.380
goes as one of these
1/x to the p's, you

01:10:50.380 --> 01:10:55.360
can start to try to empirically
fit that to your data.

01:10:55.360 --> 01:10:58.850
One thing that happens with
these distributions, of course,

01:10:58.850 --> 01:11:00.520
is the same thing
you were worried

01:11:00.520 --> 01:11:01.930
about with the exponential.

01:11:01.930 --> 01:11:07.470
When x gets really small,
that goes sky high--

01:11:07.470 --> 01:11:10.030
1 over a very small number.

01:11:10.030 --> 01:11:13.560
And so what people
basically do is

01:11:13.560 --> 01:11:18.310
they're mostly worried about
the defect sizes larger

01:11:18.310 --> 01:11:21.280
than their minimum feature size.

01:11:21.280 --> 01:11:24.520
And then they'll basically
truncate it either

01:11:24.520 --> 01:11:28.090
as a constant or, in fact,
as a linear drop-off,

01:11:28.090 --> 01:11:31.900
once you get down below
some minimum size.

01:11:31.900 --> 01:11:38.230
I should have drawn
that near x0 instead.

01:11:38.230 --> 01:11:42.280
So there is a size at which it's
hard to either detect these,

01:11:42.280 --> 01:11:44.890
and you don't care
about them, and so

01:11:44.890 --> 01:11:46.510
you're not really
trying to model

01:11:46.510 --> 01:11:48.220
that part of this distribution.

01:11:51.613 --> 01:11:53.280
In one of the first
lectures, I gave you

01:11:53.280 --> 01:11:56.010
a little bit of a preview
and an example here

01:11:56.010 --> 01:11:59.520
of how you might measure these
defect size distributions.

01:11:59.520 --> 01:12:06.120
Characterization test
vehicles, especially early

01:12:06.120 --> 01:12:08.280
in process development,
might be used

01:12:08.280 --> 01:12:11.160
to try to characterize the
capability of the process

01:12:11.160 --> 01:12:12.390
in terms of these--

01:12:12.390 --> 01:12:18.300
both your defect density,
but also your defect sizes.

01:12:18.300 --> 01:12:20.340
And imagine now that
I've got a whole array

01:12:20.340 --> 01:12:25.500
of these nested
metal lines that I

01:12:25.500 --> 01:12:28.440
can make electrical
measurements or connections to.

01:12:28.440 --> 01:12:31.770
And if I have now
a defect that's

01:12:31.770 --> 01:12:34.440
kind of small in general--

01:12:34.440 --> 01:12:39.630
or usually-- it might be even so
small that I rarely get short,

01:12:39.630 --> 01:12:42.330
but I might get some amount
of resistance change in these

01:12:42.330 --> 01:12:43.380
lines--

01:12:43.380 --> 01:12:47.820
versus other defects that are so
big that they start to bridge,

01:12:47.820 --> 01:12:51.780
on average, two or three lines.

01:12:51.780 --> 01:12:55.050
You can start to build up
some electrical measure

01:12:55.050 --> 01:12:59.010
of the likelihood of having--

01:12:59.010 --> 01:13:03.490
or the relative counts of
defects of different sizes.

01:13:03.490 --> 01:13:05.250
So here's an empirical--

01:13:05.250 --> 01:13:09.860
this is, again,
from a 2003 paper.

01:13:09.860 --> 01:13:16.090
You can start to see that 1/x
to the p empirical relationship,

01:13:16.090 --> 01:13:22.570
and it gives you a sense for how
you can actually measure those.

01:13:22.570 --> 01:13:26.920
So now you can have a
more careful definition

01:13:26.920 --> 01:13:31.210
of the critical area, where
you have your defect size

01:13:31.210 --> 01:13:32.080
distribution.

01:13:32.080 --> 01:13:36.930
I might use dsd
to indicate that.

01:13:36.930 --> 01:13:39.520
So there's our defect
size distribution,

01:13:39.520 --> 01:13:41.860
that 1/x to the p--

01:13:41.860 --> 01:13:46.450
again, really only worrying
about it above some x0.

01:13:46.450 --> 01:13:48.580
And then you also
have this probability

01:13:48.580 --> 01:13:53.880
of failure, which folds in
this notion of critical area.

01:13:53.880 --> 01:13:55.550
You can actually
look at your layout

01:13:55.550 --> 01:13:58.190
and say, if I've got
a defect of this size,

01:13:58.190 --> 01:14:01.370
this is my probability
of failure integrated

01:14:01.370 --> 01:14:03.590
across my whole chip.

01:14:03.590 --> 01:14:06.260
And now you can aggregate
the two of those

01:14:06.260 --> 01:14:09.320
into a net total critical area.

01:14:09.320 --> 01:14:12.730
So all I'm doing
is saying there is

01:14:12.730 --> 01:14:16.480
a probability of failure
associated with defects

01:14:16.480 --> 01:14:17.870
of different sizes.

01:14:17.870 --> 01:14:19.210
The defect is really small.

01:14:19.210 --> 01:14:25.760
Again, I have that critical area
plot, where I'm down in here,

01:14:25.760 --> 01:14:27.500
and it's very small.

01:14:27.500 --> 01:14:29.930
But empirically, for
my particular circuit,

01:14:29.930 --> 01:14:36.750
I may have different critical
area dependence curves, where

01:14:36.750 --> 01:14:39.870
I aggregate the total pof.

01:14:39.870 --> 01:14:42.330
And then I look at
the product of those

01:14:42.330 --> 01:14:44.940
two, integrate it
up, and that gives me

01:14:44.940 --> 01:14:46.890
a good aggregate sense--

01:14:46.890 --> 01:14:52.320
which really says where I'm
mostly worried is in this range

01:14:52.320 --> 01:14:54.510
here, where I've
got defects that

01:14:54.510 --> 01:14:58.350
are close to my feature size.

01:14:58.350 --> 01:15:01.640
The small ones aren't
going to kill me.

01:15:01.640 --> 01:15:03.658
The bigger ones are
going to kill me.

01:15:03.658 --> 01:15:06.200
Really big ones are not going
to kill me, because there's not

01:15:06.200 --> 01:15:08.420
that many big ones.

01:15:08.420 --> 01:15:14.510
So really, it tells you
the area to worry about.

01:15:14.510 --> 01:15:18.830
OK, I'm going to skip
over most of this.

01:15:18.830 --> 01:15:21.950
I just want you to get
a feel for this notion

01:15:21.950 --> 01:15:23.460
of critical area.

01:15:23.460 --> 01:15:24.860
And this is an
area where there's

01:15:24.860 --> 01:15:28.340
a lot of design automation
tools, where it can actually

01:15:28.340 --> 01:15:31.910
look at your particular
layout and start

01:15:31.910 --> 01:15:34.280
to do those kinds of
drawings that I showed you

01:15:34.280 --> 01:15:37.490
for critical area, if you
will, and shade in-- maybe

01:15:37.490 --> 01:15:40.400
a little hard to see, but
you can shade in and say,

01:15:40.400 --> 01:15:45.000
what is the gray critical area
for that particular circuit?

01:15:45.000 --> 01:15:47.060
Where am I sensitive
to the likelihood

01:15:47.060 --> 01:15:50.710
of a short for defects
of a particular size?

01:15:50.710 --> 01:15:53.120
Where am I sensitive?

01:15:53.120 --> 01:15:55.910
Where would a-- the
center of a particle

01:15:55.910 --> 01:16:02.080
have to fall in order
to cause an open?

01:16:02.080 --> 01:16:05.410
Or where might I have
to have a particle

01:16:05.410 --> 01:16:08.840
fall that would cause a short
between two different layers?

01:16:08.840 --> 01:16:11.980
And you can actually do these
calculations for your layout,

01:16:11.980 --> 01:16:14.560
and then do design
modifications that

01:16:14.560 --> 01:16:17.950
would come back in
and say, oh, if that's

01:16:17.950 --> 01:16:22.010
my critical area where I might
be susceptible to a break,

01:16:22.010 --> 01:16:24.880
let's make those lines
a little bit wider.

01:16:24.880 --> 01:16:26.770
If I make those
lines wider, I've

01:16:26.770 --> 01:16:29.830
improved my yield, because
now, if the particle falls

01:16:29.830 --> 01:16:33.460
in those lines, I'm not
as likely to actually have

01:16:33.460 --> 01:16:35.180
an open failure.

01:16:35.180 --> 01:16:37.750
So there is yield
improvement strategies

01:16:37.750 --> 01:16:41.830
that go with these
notions of critical area,

01:16:41.830 --> 01:16:46.480
and the probabilities
associated with them.

01:16:46.480 --> 01:16:48.190
The last notion
is simply you can

01:16:48.190 --> 01:16:50.740
integrate these up and
get aggregate notions

01:16:50.740 --> 01:16:54.040
of overall yield.

01:16:54.040 --> 01:16:57.070
This is also described in
[INAUDIBLE] Just reminding

01:16:57.070 --> 01:16:59.860
you-- there's also other
kinds of yield detractors,

01:16:59.860 --> 01:17:02.800
where you might have
gross yield losses,

01:17:02.800 --> 01:17:05.950
and so you might have a
global factor y0 that's

01:17:05.950 --> 01:17:09.190
associated with alignment
errors or other kinds

01:17:09.190 --> 01:17:12.070
of gross factors.

01:17:12.070 --> 01:17:13.810
And then this last
is a little example

01:17:13.810 --> 01:17:16.660
that you can read about in
[INAUDIBLE],, which is basically

01:17:16.660 --> 01:17:20.650
simply saying, in
practice, your chip yield

01:17:20.650 --> 01:17:23.110
is all so aggregated
that it doesn't really

01:17:23.110 --> 01:17:25.750
tell you what's gone wrong.

01:17:25.750 --> 01:17:28.630
So very often, you
want to slice yield

01:17:28.630 --> 01:17:31.120
into your different layers--

01:17:31.120 --> 01:17:33.850
your different process
layers, or slice them

01:17:33.850 --> 01:17:35.770
into different
functional blocks--

01:17:35.770 --> 01:17:40.540
maybe the memory or cache
block, a logic block--

01:17:40.540 --> 01:17:43.600
and basically look
at where you are

01:17:43.600 --> 01:17:45.700
most sensitive to yield loss.

01:17:45.700 --> 01:17:52.570
For example, that 95% yield
loss factor or via 2 inside

01:17:52.570 --> 01:17:54.730
of your SRAM might
be where you're

01:17:54.730 --> 01:17:56.220
losing most of your yield.

01:17:59.420 --> 01:18:04.150
There's quite a bit of
development of these test chips

01:18:04.150 --> 01:18:08.170
that have those things like
those nested via or nested

01:18:08.170 --> 01:18:13.900
snake structures in them so
that one can characterize

01:18:13.900 --> 01:18:17.950
defectivity distributions,
as well as sensitivity

01:18:17.950 --> 01:18:21.410
of different kinds of
circuits to those failures.

01:18:21.410 --> 01:18:25.840
So that's a whirlwind tour
there of semiconductor yield,

01:18:25.840 --> 01:18:28.090
these notions of not
just functional yield

01:18:28.090 --> 01:18:31.810
that we saw last
time, but also this--

01:18:31.810 --> 01:18:34.540
or parametric yield, but
defect yield as well.

01:18:34.540 --> 01:18:39.040
And you'll have a little
bit of fun playing around

01:18:39.040 --> 01:18:42.160
with some of these notions
of area-dependent yield,

01:18:42.160 --> 01:18:45.100
which I think is really
kind of the cool idea,

01:18:45.100 --> 01:18:49.480
the important idea in yield
modeling for semiconductors.

01:18:49.480 --> 01:18:51.760
OK, so we'll see you
again on Thursday.

01:18:51.760 --> 01:18:54.760
Thursday is the quiz.

01:18:54.760 --> 01:18:57.600
I think here, you had also
posted for office hours.

01:18:57.600 --> 01:18:58.600
I think you have those--

01:18:58.600 --> 01:18:58.900
AUDIENCE: Yes.

01:18:58.900 --> 01:19:00.010
DUANE BONING:
--tomorrow as well.

01:19:00.010 --> 01:19:01.000
AUDIENCE: [INAUDIBLE] to 6:00.

01:19:01.000 --> 01:19:03.125
DUANE BONING: Tomorrow,
5:00 to 6:00, if you want--

01:19:03.125 --> 01:19:05.590
any last questions
before the quiz--

01:19:05.590 --> 01:19:07.160
Hayden's available
for those as well.

01:19:07.160 --> 01:19:08.080
So thanks.

01:19:08.080 --> 01:19:09.744
We'll see you on Thursday.