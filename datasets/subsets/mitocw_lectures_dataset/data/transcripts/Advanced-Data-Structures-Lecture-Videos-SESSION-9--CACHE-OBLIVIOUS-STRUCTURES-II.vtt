WEBVTT

00:00:00.090 --> 00:00:02.490
The following content is
provided under a Creative

00:00:02.490 --> 00:00:04.030
Commons license.

00:00:04.030 --> 00:00:06.360
Your support will help
MIT OpenCourseWare

00:00:06.360 --> 00:00:10.720
continue to offer high quality
educational resources for free.

00:00:10.720 --> 00:00:13.320
To make a donation or
view additional materials

00:00:13.320 --> 00:00:17.280
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:17.280 --> 00:00:18.450
at ocw.mit.edu.

00:00:20.140 --> 00:00:21.640
ERIK DEMAINE: All
right, today we're

00:00:21.640 --> 00:00:25.420
going to do some crossover
between two kinds of data

00:00:25.420 --> 00:00:27.610
structures, memory
hierarchy data structures

00:00:27.610 --> 00:00:29.380
and geometric data structures.

00:00:29.380 --> 00:00:31.420
And this will be
the final lecture

00:00:31.420 --> 00:00:35.890
in the memory hierarchy series,
so the end of cache oblivious.

00:00:35.890 --> 00:00:38.860
So we're going to look at
two-dimensional geometric data

00:00:38.860 --> 00:00:43.120
structure problems,
both offline and online.

00:00:43.120 --> 00:00:45.800
So our good friend,
orthogonal 2D range searching,

00:00:45.800 --> 00:00:49.690
which we spent a lot of
time in a few years ago,

00:00:49.690 --> 00:00:52.930
we will come back to, and try
to get our bounds good, even

00:00:52.930 --> 00:00:56.790
cache obliviously.

00:00:56.790 --> 00:00:58.900
So instead of log n,
we want log base b of n

00:00:58.900 --> 00:01:00.269
to make things interesting.

00:01:00.269 --> 00:01:01.810
And the batch version
is where you're

00:01:01.810 --> 00:01:03.250
given a whole bunch
of rectangles,

00:01:03.250 --> 00:01:05.410
and a whole bunch
of points up front,

00:01:05.410 --> 00:01:08.020
and you want to find
all the points that

00:01:08.020 --> 00:01:09.890
live in all the rectangles.

00:01:09.890 --> 00:01:12.130
So that's an easier
version of the problem.

00:01:12.130 --> 00:01:13.630
We'll start with
that and then we'll

00:01:13.630 --> 00:01:16.690
go to the usual
online version, where

00:01:16.690 --> 00:01:20.410
you have queries coming one at
a time, rectangles coming one

00:01:20.410 --> 00:01:20.910
at a time.

00:01:20.910 --> 00:01:25.430
The points are pre-processed,
it will be static.

00:01:25.430 --> 00:01:26.920
And to do the
batched, we're going

00:01:26.920 --> 00:01:30.717
to introduce a new technique
called distribution sweep,

00:01:30.717 --> 00:01:32.800
which is a combination of
the sweep line technique

00:01:32.800 --> 00:01:35.440
we saw back as we
used persistence

00:01:35.440 --> 00:01:38.920
to make sweep line thing
into a data structure thing.

00:01:38.920 --> 00:01:41.440
But we're just going to use
the algorithmic version of that

00:01:41.440 --> 00:01:44.479
plus a cache oblivious
sorting algorithm.

00:01:44.479 --> 00:01:46.270
So we'll finally do
cache oblivious sorting

00:01:46.270 --> 00:01:50.260
and optimal N/B log
base M/B / of N/B

00:01:50.260 --> 00:01:54.719
using a particular algorithm
called lazy funnel sort, which

00:01:54.719 --> 00:01:57.010
you can actually also use to
make another kind of cache

00:01:57.010 --> 00:02:00.040
oblivious priority queue,
but we won't get into that.

00:02:00.040 --> 00:02:02.680
And so by combining those two
things, we'll get a divide

00:02:02.680 --> 00:02:05.440
and conquer technique for
geometric problems that

00:02:05.440 --> 00:02:07.702
lets us solve the batched
thing, and then we'll

00:02:07.702 --> 00:02:09.160
use completely
different techniques

00:02:09.160 --> 00:02:10.960
for the online thing.

00:02:10.960 --> 00:02:15.310
So for starters, let's finally
do cache oblivious optimal

00:02:15.310 --> 00:02:17.020
sorting.

00:02:17.020 --> 00:02:22.000
I'm not going to analyze this
algorithm because it's just

00:02:22.000 --> 00:02:24.730
an algorithm, not
a data structure,

00:02:24.730 --> 00:02:26.710
and also because the
analysis is pretty

00:02:26.710 --> 00:02:33.340
close to the analysis
for priority queues

00:02:33.340 --> 00:02:34.495
we did last class.

00:02:38.410 --> 00:02:43.810
So funnel sort is
basically a merge sort.

00:02:43.810 --> 00:02:46.720
I mentioned last time
that in external memory,

00:02:46.720 --> 00:02:50.890
the right way to do,
or a right way to do

00:02:50.890 --> 00:02:55.462
optimal external memory sorting
is an m over B-way merge sort.

00:02:55.462 --> 00:02:58.270
In cache obliviously, you
don't know what m and b are,

00:02:58.270 --> 00:03:00.460
so it's hard to do
m over B-way merge.

00:03:00.460 --> 00:03:03.820
So instead, you basically
do a N-way merge.

00:03:03.820 --> 00:03:05.530
Not quite N-way, I
can't afford that,

00:03:05.530 --> 00:03:10.090
but it's going to be n to
the 1/3 way merge sort.

00:03:10.090 --> 00:03:13.280
And the big question then
becomes, how do you do emerge?

00:03:13.280 --> 00:03:15.104
And the answer is with a funnel.

00:03:15.104 --> 00:03:17.020
And so the heart of the
algorithm is a funnel.

00:03:20.990 --> 00:03:25.270
So if you have K-sorted
lists that are big,

00:03:25.270 --> 00:03:34.150
sized K cubed, then you can
merge them in, basically,

00:03:34.150 --> 00:03:35.400
the optimal bound.

00:03:46.650 --> 00:03:53.490
So K-funnel, K-sorted
lists, total size K cubed.

00:03:53.490 --> 00:03:54.960
Number of memory
transfers to merge

00:03:54.960 --> 00:03:58.670
them is K cubed over B
times log base M/B of K

00:03:58.670 --> 00:04:02.680
cubed over B.
There's a plus K term

00:04:02.680 --> 00:04:05.880
and when you plug this into
an actual sorting algorithm,

00:04:05.880 --> 00:04:09.900
you need to think about that,
but that's not a big deal.

00:04:09.900 --> 00:04:11.502
Usually this term will dominate.

00:04:14.100 --> 00:04:17.894
OK, so let me show
you how funnel works.

00:04:17.894 --> 00:04:20.019
We're just going to go
through the algorithmic part

00:04:20.019 --> 00:04:22.650
and I won't analyze the
number of memory transfers.

00:04:32.330 --> 00:04:34.170
Maybe I'll draw this here.

00:04:39.397 --> 00:04:40.980
So we're going to
have the inputs down

00:04:40.980 --> 00:04:45.329
at the bottom of this funnel.

00:04:45.329 --> 00:04:46.870
It's going to have
some data in them.

00:04:52.580 --> 00:04:57.710
Those k inputs down
here, total size,

00:04:57.710 --> 00:05:00.385
all these is theta K cubed.

00:05:05.970 --> 00:05:13.320
And then at the top here,
we have our output buffer.

00:05:13.320 --> 00:05:17.340
This is where we're
going to put the results

00:05:17.340 --> 00:05:19.940
and this will have size K cubed.

00:05:22.990 --> 00:05:25.140
Maybe we've already
done some work

00:05:25.140 --> 00:05:27.470
and we've filled some of it.

00:05:27.470 --> 00:05:30.300
OK, the question is what
do you put in this triangle

00:05:30.300 --> 00:05:32.010
to do the merge?

00:05:32.010 --> 00:05:35.940
And the obvious thing
is recursive triangles.

00:05:35.940 --> 00:05:37.770
Recursion is like
the one technique

00:05:37.770 --> 00:05:41.370
we know in cache
oblivious data structures.

00:05:41.370 --> 00:05:48.660
So we're going to take
square root of K-funnels

00:05:48.660 --> 00:05:51.780
and just join them together
in the obvious way.

00:05:51.780 --> 00:06:11.370
So just like [INAUDIBLE]
layout, except--

00:06:11.370 --> 00:06:14.740
I didn't quite leave
enough room here--

00:06:14.740 --> 00:06:19.780
in between the
levels are buffers.

00:06:19.780 --> 00:06:25.530
There's a buffer here
two between the nodes

00:06:25.530 --> 00:06:27.910
of this funnel and the
nodes of this funnel.

00:06:33.130 --> 00:06:39.862
OK, these buffers may have some
stuff in them at any moment.

00:06:39.862 --> 00:06:42.320
OK, and the big question is
how do you set the buffer size?

00:06:42.320 --> 00:06:44.270
This is the key step.

00:06:44.270 --> 00:06:50.450
And the claim is each buffer,
we set to a size of K to the 3/2

00:06:50.450 --> 00:06:54.230
because the number of buffers
is about square root of K

00:06:54.230 --> 00:06:58.130
because there's one per
leaf of this funnel.

00:06:58.130 --> 00:07:00.760
And a K-funnel has K inputs,
so a root K funnel is going

00:07:00.760 --> 00:07:03.380
to have root K inputs here.

00:07:03.380 --> 00:07:07.640
And so the total size
of all the buffers

00:07:07.640 --> 00:07:15.199
is K squared, which
is not too big.

00:07:15.199 --> 00:07:16.990
I'm not going to go
through the recurrence,

00:07:16.990 --> 00:07:19.600
but if you add up the
total size of this thing,

00:07:19.600 --> 00:07:25.210
it is linear size in
the output, K cubed.

00:07:25.210 --> 00:07:28.000
I think also if you don't count
the output buffer, it's linear

00:07:28.000 --> 00:07:29.740
and K squared.

00:07:29.740 --> 00:07:32.860
If I recall correctly.

00:07:32.860 --> 00:07:35.725
We're not too concerned with
that here, just overall.

00:07:38.410 --> 00:07:44.920
Once we have
K-funnels, funnel sort

00:07:44.920 --> 00:08:00.170
is just going to be N to the
1/3 way merge sort with an N

00:08:00.170 --> 00:08:07.180
to the 1/3 funnel as the merger.

00:08:12.980 --> 00:08:16.790
We can only up to n the 1/3
because of this cubic thing.

00:08:16.790 --> 00:08:19.250
We can only merge--

00:08:19.250 --> 00:08:23.870
if we want the sorting bound
N/B log base M/B of N/B

00:08:23.870 --> 00:08:26.690
we can only afford K
being up to n to the 1/3.

00:08:26.690 --> 00:08:29.480
So that's the biggest we can do.

00:08:29.480 --> 00:08:33.620
So it's a recursive algorithm
where each of the merging steps

00:08:33.620 --> 00:08:36.470
is this recursive
data structure.

00:08:36.470 --> 00:08:38.539
Now, this is really
just about layout.

00:08:38.539 --> 00:08:42.210
I haven't told you what the
actual algorithm is yet,

00:08:42.210 --> 00:08:43.419
but it's a recursive layout.

00:08:43.419 --> 00:08:46.130
You store the entire
upper triangle,

00:08:46.130 --> 00:08:48.755
then each of the triangles,
somewhere you put the buffers.

00:08:48.755 --> 00:08:50.150
It doesn't really
matter where the buffers

00:08:50.150 --> 00:08:51.860
are as long as each
triangle is stored.

00:08:51.860 --> 00:08:57.230
As a consecutive array
of memory, we'll be OK.

00:08:57.230 --> 00:09:01.430
And now let me tell you
about the actual algorithm

00:09:01.430 --> 00:09:02.430
to do this.

00:09:02.430 --> 00:09:06.107
It's a very simple
lazy algorithm.

00:09:10.671 --> 00:09:12.170
So there's a whole
bunch of buffers.

00:09:12.170 --> 00:09:15.920
If you want to do this merge,
really what you'd like to do

00:09:15.920 --> 00:09:18.680
is fill this output buffer.

00:09:18.680 --> 00:09:22.310
So you call this subroutine
called fill on the output

00:09:22.310 --> 00:09:26.120
buffer and say, I would like
to fill this entire buffer

00:09:26.120 --> 00:09:27.940
with elements.

00:09:27.940 --> 00:09:30.890
Precondition, if you're going to
do a fill, right now the buffer

00:09:30.890 --> 00:09:33.800
is empty, and then at
the end of the fill

00:09:33.800 --> 00:09:37.080
you'd like this to
be completely full.

00:09:37.080 --> 00:09:38.640
And how do you do it?

00:09:38.640 --> 00:09:40.430
Well, if you look
at any buffer--

00:09:43.340 --> 00:09:46.010
partially filled, whatever--
and you look right below it,

00:09:46.010 --> 00:09:49.040
there's a node in this tree.

00:09:49.040 --> 00:09:50.570
You recurse all the way down.

00:09:50.570 --> 00:09:55.370
In the end, this is just a
binary tree with buffers in it.

00:09:55.370 --> 00:09:58.130
So it's going to be there's a
buffer, then there's a node,

00:09:58.130 --> 00:10:00.932
then there's two children,
each of which is a buffer,

00:10:00.932 --> 00:10:02.390
and then there's
a node below that.

00:10:06.920 --> 00:10:09.890
OK, so how do I fill this thing?

00:10:09.890 --> 00:10:15.090
I just read the first
item, the beginning,

00:10:15.090 --> 00:10:17.810
the smallest item for each
of these, compare them.

00:10:17.810 --> 00:10:20.600
Whichever smaller,
I stick at here.

00:10:20.600 --> 00:10:23.690
It's just a regular binary
merge which is kind of cool.

00:10:23.690 --> 00:10:24.982
You've got two arrays.

00:10:24.982 --> 00:10:25.940
You want to merge them.

00:10:25.940 --> 00:10:29.214
Stick the results here.

00:10:29.214 --> 00:10:30.255
So that's how we do fill.

00:10:38.250 --> 00:10:55.580
Binary merge of the two children
buffers until we're full.

00:10:55.580 --> 00:10:57.295
But there's one thing
that can happen,

00:10:57.295 --> 00:10:59.420
which is that one of the
child buffers might empty.

00:11:08.320 --> 00:11:10.390
What do we do then?

00:11:10.390 --> 00:11:11.320
Recursively fill it.

00:11:20.050 --> 00:11:21.400
That's the algorithm.

00:11:21.400 --> 00:11:22.540
Very simple.

00:11:22.540 --> 00:11:24.940
The obvious lazy thing to do.

00:11:24.940 --> 00:11:26.050
Do a binary merge.

00:11:26.050 --> 00:11:29.410
This is going to be nice
because it's like two scans,

00:11:29.410 --> 00:11:33.550
until one of these guys empties,
and then you pause this merge,

00:11:33.550 --> 00:11:36.700
and then say OK, I'm going to
fill this entire buffer, which

00:11:36.700 --> 00:11:41.260
will recursively do stuff
until it's completely full

00:11:41.260 --> 00:11:44.980
or I run out of input elements,
whichever comes first,

00:11:44.980 --> 00:11:47.390
and then resume this merge.

00:11:47.390 --> 00:11:47.890
Question?

00:11:47.890 --> 00:11:50.680
AUDIENCE: Aren't there more
than two child buffers?

00:11:50.680 --> 00:11:54.970
ERIK DEMAINE: Should only
be two children buffers.

00:11:54.970 --> 00:11:58.540
The question is, are
there more than two?

00:11:58.540 --> 00:12:03.220
This recursion of the root
k and root k child triangles

00:12:03.220 --> 00:12:05.510
of size root k is
exactly the recursion

00:12:05.510 --> 00:12:06.730
we did on a binary tree.

00:12:06.730 --> 00:12:09.006
I didn't say, but underlying
this is a binary tree.

00:12:09.006 --> 00:12:11.380
The only difference between
this and a [INAUDIBLE] layout

00:12:11.380 --> 00:12:13.990
is we're adding these buffers.

00:12:13.990 --> 00:12:15.936
I intended to draw
this as binary.

00:12:15.936 --> 00:12:18.310
It's a little hard to tell
because I didn't draw the base

00:12:18.310 --> 00:12:22.240
case, but it is indeed a
binary tree in the end.

00:12:24.980 --> 00:12:27.647
OK, other questions?

00:12:27.647 --> 00:12:29.230
So that's the algorithm
and as I said,

00:12:29.230 --> 00:12:33.370
I'm not going to analyze it, but
it's the same kind of analysis.

00:12:33.370 --> 00:12:36.310
You look at the threshold
where things fit in cache

00:12:36.310 --> 00:12:40.105
or don't and argue accordingly.

00:12:43.080 --> 00:12:45.880
It's pretty hand-wavy.

00:12:45.880 --> 00:12:47.800
What I want to get
to is how we use

00:12:47.800 --> 00:12:50.950
this to solve more interesting
problems than sorting.

00:12:50.950 --> 00:12:53.870
Sorting is a little bit boring.

00:12:53.870 --> 00:12:58.369
So let's go to batched
orthogonal range searching.

00:13:20.890 --> 00:13:25.120
And in general, this technique
called distribution sweep.

00:13:29.235 --> 00:13:34.570
The idea with distribution
sweep is that not only can we

00:13:34.570 --> 00:13:38.130
use this cool funnel
sort algorithm to sort,

00:13:38.130 --> 00:13:40.030
but we can think of
it as doing a divide

00:13:40.030 --> 00:13:42.550
and conquer on the key value.

00:14:13.010 --> 00:14:14.900
And in this case, we
have two coordinates.

00:14:14.900 --> 00:14:16.608
We're going to use
the divide and conquer

00:14:16.608 --> 00:14:19.360
on one of the coordinates.

00:14:19.360 --> 00:14:23.700
And where we have
some flexibility

00:14:23.700 --> 00:14:25.730
is in this binary merge step.

00:14:25.730 --> 00:14:27.730
We're doing this binary
merge, and normally it's

00:14:27.730 --> 00:14:30.313
just you take the min, you spit
it out here, you take the min,

00:14:30.313 --> 00:14:33.100
you spit it out here.

00:14:33.100 --> 00:14:35.231
That's the min of one
particular coordinate.

00:14:35.231 --> 00:14:37.480
Now you've got to deal with
some auxiliary information

00:14:37.480 --> 00:14:38.646
about the other coordinates.

00:14:38.646 --> 00:14:42.200
So in general, you're
merging two sorted things.

00:14:42.200 --> 00:14:43.990
If there's other
geometric information,

00:14:43.990 --> 00:14:46.060
you can try to preserve
it during the merge.

00:14:46.060 --> 00:14:49.630
As long as you can do that, this
is the conqueror part or that

00:14:49.630 --> 00:14:51.940
combine step of
divide and conquer.

00:14:51.940 --> 00:14:53.490
You can do a lot.

00:14:53.490 --> 00:14:57.610
There's a powerful
technique, it turns out.

00:14:57.610 --> 00:15:01.740
It's by Brodal and Fagerberg.

00:15:01.740 --> 00:15:04.750
It's in their early
days of cache oblivious.

00:15:04.750 --> 00:15:06.190
It was the first
geometric paper.

00:15:09.430 --> 00:15:22.630
Fine, so replace or say
augment the binary merge, which

00:15:22.630 --> 00:15:25.120
is, in the end, the only
part of the algorithm

00:15:25.120 --> 00:15:27.230
other than the recursion.

00:15:27.230 --> 00:15:34.600
So it's the only thing
you need to do to maintain

00:15:34.600 --> 00:15:36.970
auxiliary information.

00:15:36.970 --> 00:15:40.325
That's the generic idea
of distribution sweep.

00:15:40.325 --> 00:15:41.950
And distribution
sweep has been applied

00:15:41.950 --> 00:15:43.620
to solve lots of
different problems.

00:15:43.620 --> 00:15:47.620
Batched orthogonal range
queries is one of them.

00:15:47.620 --> 00:15:50.200
Generally, you've got a
bunch of orthogonal segments,

00:15:50.200 --> 00:15:53.330
rectangles, points, and you want
to compute how they intersect.

00:15:53.330 --> 00:15:57.160
Those sorts of problems
that can be solved here.

00:15:57.160 --> 00:15:59.320
Also weird things like I
give you a bunch of points

00:15:59.320 --> 00:16:01.028
and I want to know
for every point what's

00:16:01.028 --> 00:16:03.340
its nearest neighbor.

00:16:03.340 --> 00:16:05.920
In Euclidean sense,
that can be solved.

00:16:05.920 --> 00:16:08.020
But I like orthogonal
range searching

00:16:08.020 --> 00:16:10.360
because it's the closest to
our data structure problem

00:16:10.360 --> 00:16:12.850
and that's a problem we've seen.

00:16:12.850 --> 00:16:16.120
So the actual batched
orthogonal range searching

00:16:16.120 --> 00:16:27.470
is your given N points,
and N rectangles,

00:16:27.470 --> 00:16:32.025
and you want to know which
points are in which rectangles.

00:16:32.025 --> 00:16:33.150
That's the general problem.

00:16:33.150 --> 00:16:37.410
So normally, we're
given the points first,

00:16:37.410 --> 00:16:39.550
and then we're given the
rectangles one at a time.

00:16:39.550 --> 00:16:41.000
That's what we've
solved in the past.

00:16:41.000 --> 00:16:42.333
That's what we will solve later.

00:16:42.333 --> 00:16:43.446
That's the online version.

00:16:43.446 --> 00:16:44.820
The batched version
is I give you

00:16:44.820 --> 00:16:48.270
a whole bunch of queries
I want to simultaneously

00:16:48.270 --> 00:16:55.170
and we're going to achieve
the sorting bound N/B log base

00:16:55.170 --> 00:17:02.790
M/B of N/B plus the size
of the output over B.

00:17:02.790 --> 00:17:04.950
And this is
generally the optimal

00:17:04.950 --> 00:17:06.940
bound you could hope for.

00:17:06.940 --> 00:17:08.400
It's not obvious
you need the log,

00:17:08.400 --> 00:17:11.069
but I think for most
problems in external memory

00:17:11.069 --> 00:17:12.569
you need this log.

00:17:12.569 --> 00:17:14.490
It's hard to beat
the sorting bound,

00:17:14.490 --> 00:17:16.200
and then once you pay
the sorting bound,

00:17:16.200 --> 00:17:20.560
this is the optimal linear time
to just write down the output.

00:17:20.560 --> 00:17:23.069
Now, this problem can be solved.

00:17:23.069 --> 00:17:28.650
Give me all the point
rectangle pairs that result.

00:17:28.650 --> 00:17:32.505
I'm not going to
solve it here exactly.

00:17:32.505 --> 00:17:34.960
We're going to solve a
slightly different version,

00:17:34.960 --> 00:17:37.020
or in general--

00:17:37.020 --> 00:17:39.200
whatever.

00:17:39.200 --> 00:17:41.700
Let me tell you about another
version of this problem, which

00:17:41.700 --> 00:17:43.270
is a little bit easier.

00:17:43.270 --> 00:17:46.290
Then I'll sketch how
you solve that problem.

00:17:51.040 --> 00:17:53.070
So remember, we've talked
about range reporting

00:17:53.070 --> 00:17:58.260
and also range
counting, which is you

00:17:58.260 --> 00:18:00.910
just want to know the
number of answers.

00:18:00.910 --> 00:18:03.000
Here's something in between.

00:18:03.000 --> 00:18:06.300
You want to know
for every point,

00:18:06.300 --> 00:18:09.751
how many rectangles contain it?

00:18:09.751 --> 00:18:11.250
And particularly,
this will tell you

00:18:11.250 --> 00:18:12.791
for each point, does
it appear in any

00:18:12.791 --> 00:18:15.604
of the rectangles in the set?

00:18:15.604 --> 00:18:17.520
It will tell you how
many and this is actually

00:18:17.520 --> 00:18:19.230
necessary as a first
step because one

00:18:19.230 --> 00:18:22.760
of the hard parts in solving
these kinds of problems

00:18:22.760 --> 00:18:26.370
or reporting problems, is
that the output could be big.

00:18:26.370 --> 00:18:29.360
We know that's always an issue,
but with cache oblivious,

00:18:29.360 --> 00:18:35.370
it's a big issue, literally,
because space is important.

00:18:35.370 --> 00:18:38.430
You can't afford to
put space anywhere.

00:18:41.610 --> 00:18:44.130
If these buffers have to
get much bigger in order

00:18:44.130 --> 00:18:47.010
to store those
answers, then life

00:18:47.010 --> 00:18:50.040
is kind of tough because
then this data structure

00:18:50.040 --> 00:18:52.682
gets too big, and then my
analysis goes out the window

00:18:52.682 --> 00:18:54.390
because things that
used to fit in cache,

00:18:54.390 --> 00:18:56.580
no longer fit in cache.

00:18:56.580 --> 00:18:58.330
The analysis I didn't show you.

00:18:58.330 --> 00:19:02.040
So it's an issue.

00:19:02.040 --> 00:19:05.340
So the first step
of this algorithm

00:19:05.340 --> 00:19:07.620
is to first figure out
how big those buffers have

00:19:07.620 --> 00:19:11.060
to be so that we don't have
to allocate them too large.

00:19:11.060 --> 00:19:12.810
And to do that, we
need to basically count

00:19:12.810 --> 00:19:18.000
how many answers there are,
and this is what we'll do.

00:19:18.000 --> 00:19:20.610
To compute these values,
the answers aren't very big.

00:19:20.610 --> 00:19:23.220
These answers are just
single numbers per point,

00:19:23.220 --> 00:19:26.590
so it's no big deal.

00:19:26.590 --> 00:19:30.300
OK, so here's what we do.

00:19:30.300 --> 00:19:38.430
Sort the points and the
corners of the rectangles

00:19:38.430 --> 00:19:44.010
by x-coordinate using
lazy final sort.

00:19:44.010 --> 00:19:45.150
Nothing fancy here.

00:19:45.150 --> 00:19:48.570
No augmentation,
regular old sort.

00:19:48.570 --> 00:19:51.810
Then-- this will
be useful later--

00:19:51.810 --> 00:19:54.050
then we're going to
divide and conquer

00:19:54.050 --> 00:20:02.320
on y via a distribution sweep.

00:20:07.240 --> 00:20:17.320
And here, our binary merger
is going to be an upward sweep

00:20:17.320 --> 00:20:18.418
line algorithm.

00:20:29.880 --> 00:20:32.266
So let's talk about that
sweep line algorithm.

00:20:37.190 --> 00:20:41.310
We presorted our points by x.

00:20:41.310 --> 00:20:45.360
If you think about the merging
step, what this means--

00:20:50.020 --> 00:20:51.770
it's confusing.

00:20:51.770 --> 00:20:57.500
We're trying to sort by y,
we were in a certain sense,

00:20:57.500 --> 00:21:01.190
but we're always going to be
sorted by x because we did that

00:21:01.190 --> 00:21:02.730
up front.

00:21:02.730 --> 00:21:06.810
So the picture is going
to be something like this.

00:21:06.810 --> 00:21:08.090
We're in a slab.

00:21:08.090 --> 00:21:10.730
There's going to
be the left slab.

00:21:10.730 --> 00:21:13.880
So here's the binary merger.

00:21:17.440 --> 00:21:20.000
Here's the L points
and the R points.

00:21:20.000 --> 00:21:23.230
The L points are going to be
in a particular x interval.

00:21:23.230 --> 00:21:28.070
The R points are going to
be in an adjacent x interval

00:21:28.070 --> 00:21:31.910
corresponding to
this tree picture.

00:21:31.910 --> 00:21:39.680
And then we have these points,
which they overlap and why?

00:21:39.680 --> 00:21:44.120
Because the whole point is
we're trying to merge by y.

00:21:44.120 --> 00:21:49.360
OK, we also have
some rectangles,

00:21:49.360 --> 00:21:53.083
and their corners are
what we have represented.

00:21:58.544 --> 00:22:00.210
I probably should
have used colors here.

00:22:09.390 --> 00:22:11.025
Something like this.

00:22:17.790 --> 00:22:20.190
So we're given,
essentially-- we have

00:22:20.190 --> 00:22:23.197
whatever we want on the
points and corners in here.

00:22:23.197 --> 00:22:24.780
We have whatever we
want in the points

00:22:24.780 --> 00:22:26.700
and corners in this slab.

00:22:26.700 --> 00:22:30.280
Let me add a little
bit of color.

00:22:30.280 --> 00:22:30.960
These lines.

00:22:35.580 --> 00:22:40.950
And now we want to merge these
two things and merging here

00:22:40.950 --> 00:22:44.800
is all about counting how many
rectangles contain each point.

00:22:44.800 --> 00:22:48.240
Now, we already know how
many points over here

00:22:48.240 --> 00:22:51.310
are contained in rectangles
that are over here.

00:22:51.310 --> 00:22:53.000
So we've presumably
already found

00:22:53.000 --> 00:22:55.930
that this point lies
in this rectangle.

00:22:55.930 --> 00:22:57.270
We've already found--

00:22:57.270 --> 00:22:58.632
I guess there's no points here.

00:22:58.632 --> 00:23:00.090
We've already found
that this point

00:23:00.090 --> 00:23:02.200
is contained in this rectangle.

00:23:02.200 --> 00:23:06.150
OK, because these corners were
in this slab, and so let's say

00:23:06.150 --> 00:23:08.680
every corner knows
the entire rectangle.

00:23:08.680 --> 00:23:11.130
So when you were processing
R, you saw these corners,

00:23:11.130 --> 00:23:12.060
you saw this point.

00:23:12.060 --> 00:23:14.040
Somehow you figured that out.

00:23:14.040 --> 00:23:19.830
What we're missing are things
like this rectangle, where

00:23:19.830 --> 00:23:22.420
none of the corners
are inside R.

00:23:22.420 --> 00:23:24.450
So R knew nothing
about this rectangle,

00:23:24.450 --> 00:23:26.580
and yet it has points
that are contained in it.

00:23:26.580 --> 00:23:30.210
Similarly, there are these
rectangles that completely

00:23:30.210 --> 00:23:35.700
span L, and so therefore none
of the corners are inside L.

00:23:35.700 --> 00:23:38.730
But we need to know that
these points are in there.

00:23:38.730 --> 00:23:42.420
Those are the only things that
will be missing at this level.

00:23:44.782 --> 00:23:46.740
There might be other
rectangles that completely

00:23:46.740 --> 00:23:50.060
span L and R. Those will be
discovered at higher levels,

00:23:50.060 --> 00:23:52.090
now here.

00:23:52.090 --> 00:23:54.600
It's a little bit awkward to
check if this will actually

00:23:54.600 --> 00:23:58.200
find everything, but it will.

00:23:58.200 --> 00:24:03.725
So to figure this out, when
we're merging L and R--

00:24:03.725 --> 00:24:05.370
see, L knows about
this rectangle

00:24:05.370 --> 00:24:07.050
because it sees these points.

00:24:07.050 --> 00:24:09.840
We want to keep track
as we sweep upwards.

00:24:09.840 --> 00:24:12.390
We want to realize that these
points are in a big rectangle

00:24:12.390 --> 00:24:14.670
here, whereas they
weren't discovered in L,

00:24:14.670 --> 00:24:17.520
and they weren't
discovered in R.

00:24:17.520 --> 00:24:25.140
To do that, we maintain a number
as-- we have a horizontal line,

00:24:25.140 --> 00:24:28.020
we're sweeping up.

00:24:28.020 --> 00:24:34.170
We want to maintain the
number of active rectangles.

00:24:34.170 --> 00:24:39.120
Active means that it's currently
being sliced by the sweep line.

00:24:42.450 --> 00:24:53.130
That have left corners
in L and completely

00:24:53.130 --> 00:24:57.870
span R. So that's these guys.

00:24:57.870 --> 00:24:59.550
So that's easy to do.

00:24:59.550 --> 00:25:01.350
We're merging these points.

00:25:01.350 --> 00:25:04.470
So that each of them
has been sorted by y.

00:25:04.470 --> 00:25:06.870
Now we're doing
a merge, so we're

00:25:06.870 --> 00:25:09.540
considering all the
corners, and all the points,

00:25:09.540 --> 00:25:13.740
and increasing the y-coordinate
as we do that binary merge.

00:25:13.740 --> 00:25:17.610
So whenever we visit a left
corner of a rectangle--

00:25:17.610 --> 00:25:20.954
a lower left corner-- we
say oh, does this rectangle

00:25:20.954 --> 00:25:21.870
go all the way across?

00:25:21.870 --> 00:25:23.160
This one does not.

00:25:23.160 --> 00:25:26.820
By the time we get to here, this
one goes all the way cross R,

00:25:26.820 --> 00:25:28.620
and so we increment CL.

00:25:28.620 --> 00:25:32.670
And when we get to the upper
left corner, we decrement CL.

00:25:32.670 --> 00:25:34.690
Say oh, that rectangle's over.

00:25:34.690 --> 00:25:38.070
So it's very easy
to do constant time,

00:25:38.070 --> 00:25:40.590
but it's only going
to be 1/B memory

00:25:40.590 --> 00:25:42.570
transfers per one of
these because it's

00:25:42.570 --> 00:25:45.870
a nice, cheap merge.

00:25:45.870 --> 00:25:49.770
And then symmetrically,
we do CR.

00:25:49.770 --> 00:25:52.020
It's the number of
active rectangles

00:25:52.020 --> 00:25:54.750
with the right
corners in R that span

00:25:54.750 --> 00:26:05.440
L. So that's this guy, CR,
I guess, this guy is CL.

00:26:05.440 --> 00:26:09.030
In general, there might be a
lot of them, so you count them.

00:26:09.030 --> 00:26:22.271
And then the only
thing we need to do

00:26:22.271 --> 00:26:24.500
is whenever we
encounter a point as

00:26:24.500 --> 00:26:26.630
opposed to a corner,
because we're storing them

00:26:26.630 --> 00:26:30.440
all together, we add--

00:26:30.440 --> 00:26:31.470
I got this right--

00:26:31.470 --> 00:26:36.950
CR to it's counter.

00:26:36.950 --> 00:26:40.890
We want to know how many
rectangles contain that point.

00:26:40.890 --> 00:26:43.400
And so for example,
when we see this point,

00:26:43.400 --> 00:26:46.790
and CR is currently one, then
we know that this point appeared

00:26:46.790 --> 00:26:49.160
in some rectangle
that spanned L.

00:26:49.160 --> 00:26:51.050
So we increment
this points counter.

00:26:51.050 --> 00:26:53.810
Similarly, when we see these
points, CL is positive,

00:26:53.810 --> 00:26:57.500
so we increment these guys
counters by whatever CL is.

00:26:57.500 --> 00:27:05.329
So this is a symmetric
version in R when we add CL.

00:27:05.329 --> 00:27:07.370
Probably should have called
them the other names,

00:27:07.370 --> 00:27:10.350
but anyway, CL,
CR, doesn't matter.

00:27:10.350 --> 00:27:12.950
CLRS.

00:27:12.950 --> 00:27:13.796
Question?

00:27:13.796 --> 00:27:15.740
AUDIENCE: The bottom
is the x-axis, right?

00:27:15.740 --> 00:27:17.365
ERIK DEMAINE: This
is the x-axis, yeah.

00:27:17.365 --> 00:27:20.689
AUDIENCE: So are we dividing
and conquering on x?

00:27:20.689 --> 00:27:23.230
ERIK DEMAINE: It does look like
we're dividing and conquering

00:27:23.230 --> 00:27:25.020
on x, I think you're right.

00:27:25.020 --> 00:27:26.356
Sorry.

00:27:26.356 --> 00:27:28.682
For some reason I
thought it was y.

00:27:28.682 --> 00:27:30.610
You're right.

00:27:30.610 --> 00:27:31.750
So it's a funny thing.

00:27:31.750 --> 00:27:35.770
We're pre-sorting by x,
which is what's getting us--

00:27:35.770 --> 00:27:36.530
thank you.

00:27:36.530 --> 00:27:37.960
That's much clearer now.

00:27:37.960 --> 00:27:40.482
In my mind I was like
there's something weird here.

00:27:40.482 --> 00:27:42.190
We're presorting on
x and then we're just

00:27:42.190 --> 00:27:44.310
sticking these guys down here.

00:27:44.310 --> 00:27:48.760
So evenly dividing
them into lists.

00:27:48.760 --> 00:27:53.270
Or, I guess actually, we're
doing our funnel sort,

00:27:53.270 --> 00:27:54.389
the merge sort.

00:27:54.389 --> 00:27:55.930
Things have already
been sorted by x,

00:27:55.930 --> 00:27:59.740
but now we're merge
sorting again,

00:27:59.740 --> 00:28:04.190
and this time when we merge, we
carry along this information.

00:28:04.190 --> 00:28:07.330
So they're both in terms of
x, which is kind of funny.

00:28:07.330 --> 00:28:10.242
Is there another question?

00:28:10.242 --> 00:28:12.953
AUDIENCE: Sorry, is it
important that we do

00:28:12.953 --> 00:28:14.679
the upward sweep [INAUDIBLE]?

00:28:18.640 --> 00:28:21.750
ERIK DEMAINE: The upward sweep.

00:28:21.750 --> 00:28:24.145
Yeah, we have to do the
points in order by y.

00:28:24.145 --> 00:28:27.066
AUDIENCE: So do we
want to just sort

00:28:27.066 --> 00:28:29.900
by y, and then [INAUDIBLE].

00:28:29.900 --> 00:28:31.988
ERIK DEMAINE: Ah,
so confused now.

00:28:31.988 --> 00:28:35.707
AUDIENCE: Because in the
notes, it said x and then y.

00:28:35.707 --> 00:28:37.790
ERIK DEMAINE: Yeah, I know
in the notes it says y.

00:28:37.790 --> 00:28:40.010
It used to say x.

00:28:40.010 --> 00:28:44.480
I believe, we're dividing
and conquering on x,

00:28:44.480 --> 00:28:49.160
but we're sorting by y,
and that's the confusion.

00:28:49.160 --> 00:28:52.850
I'll double check
this, but in order

00:28:52.850 --> 00:28:57.290
for this sweep to work-- so
it's like you first sort by x.

00:28:57.290 --> 00:28:58.386
You

00:28:58.386 --> 00:29:00.260
We are in some sense
doing divide and conquer

00:29:00.260 --> 00:29:03.290
by x because we
did this sort by x.

00:29:03.290 --> 00:29:07.980
But the merge short is on y.

00:29:07.980 --> 00:29:08.930
It makes more sense.

00:29:08.930 --> 00:29:11.420
If you're already
in x order, sorting

00:29:11.420 --> 00:29:12.670
isn't going to learn you much.

00:29:12.670 --> 00:29:15.620
It isn't going to
teach you much.

00:29:15.620 --> 00:29:17.029
So first you sort by x.

00:29:17.029 --> 00:29:18.320
Things are nicely ordered by x.

00:29:18.320 --> 00:29:21.800
So we get these nice horizontal
slabs in the decomposition,

00:29:21.800 --> 00:29:23.450
but now when we merge--

00:29:23.450 --> 00:29:24.950
Now we're going to sort by y.

00:29:24.950 --> 00:29:26.908
So we're going to reorder
the points and that's

00:29:26.908 --> 00:29:28.680
what lets us do the sweep.

00:29:28.680 --> 00:29:31.100
And we are, in the end, merging
all these points together

00:29:31.100 --> 00:29:32.690
in y order.

00:29:32.690 --> 00:29:34.910
And as we do it, then
we get the information

00:29:34.910 --> 00:29:37.065
we want about
rectangles and points.

00:29:37.065 --> 00:29:41.270
OK, this is why I wanted
this to be both x and y.

00:29:41.270 --> 00:29:43.610
But really, the divide and
conquer is happening on x,

00:29:43.610 --> 00:29:48.170
but we are doing
a merge sort on y.

00:29:48.170 --> 00:29:49.950
Finally clear.

00:29:49.950 --> 00:29:51.500
Thanks for helping me.

00:29:51.500 --> 00:29:53.972
This is a new lecturers,
as you may have guessed,

00:29:53.972 --> 00:29:57.622
so still working out some kinks.

00:29:57.622 --> 00:29:59.330
I really wanted to
introduce this lecture

00:29:59.330 --> 00:30:03.100
because the next thing
we're going to cover,

00:30:03.100 --> 00:30:05.630
which is a way to do orthogonal
2D range search and cache

00:30:05.630 --> 00:30:07.990
obviously, is super cool.

00:30:07.990 --> 00:30:13.540
It's like one of the
craziest things there is.

00:30:13.540 --> 00:30:15.530
At least in the cache
oblivious world.

00:30:15.530 --> 00:30:17.650
Any other questions before--

00:30:17.650 --> 00:30:21.740
Oh, I should say a little
bit more about this.

00:30:21.740 --> 00:30:26.510
We've now solved this first
step, which is figuring out

00:30:26.510 --> 00:30:28.520
the output size.

00:30:28.520 --> 00:30:32.321
Counting for each point how
many rectangles contain it,

00:30:32.321 --> 00:30:34.070
which is an interesting
problem by itself.

00:30:34.070 --> 00:30:36.470
That's the range
counting problem.

00:30:36.470 --> 00:30:39.620
You can also use it to
figure out, at this level,

00:30:39.620 --> 00:30:43.760
at this merging step, how many
things will be output here?

00:30:43.760 --> 00:30:45.140
How many new outputs are there?

00:30:45.140 --> 00:30:46.920
How many points in
rectangles are there?

00:30:46.920 --> 00:30:49.003
It's essentially just the
sum of all those things.

00:30:52.650 --> 00:30:55.250
So you can count the
number of outputs per merge

00:30:55.250 --> 00:31:00.530
and so then there's a natural
strategy, which is you

00:31:00.530 --> 00:31:05.720
build a new funnel structure
where these buffers

00:31:05.720 --> 00:31:07.900
have the right size.

00:31:07.900 --> 00:31:10.220
You've pre-computed what
all sizes need to be.

00:31:10.220 --> 00:31:12.620
At every merge you
know how many things

00:31:12.620 --> 00:31:14.540
are going to get spit out here.

00:31:14.540 --> 00:31:17.780
So you could allocate that
much space and that will

00:31:17.780 --> 00:31:21.370
be a kind of decent merge sort.

00:31:21.370 --> 00:31:22.870
Because I haven't
done the analysis,

00:31:22.870 --> 00:31:25.370
it's hard to get into
detail about this.

00:31:25.370 --> 00:31:29.390
But it will not be
optimal, unfortunately.

00:31:29.390 --> 00:31:31.250
To actually make
it work, you end up

00:31:31.250 --> 00:31:33.620
having to take this
tree, carving it

00:31:33.620 --> 00:31:36.530
into subtrees of linear size.

00:31:36.530 --> 00:31:38.540
So normally, the whole
thing is linear size.

00:31:38.540 --> 00:31:40.019
Everything's fine.

00:31:40.019 --> 00:31:41.810
And where the analysis
breaks, essentially,

00:31:41.810 --> 00:31:45.470
is if you have a giant buffer
because one of the outputs--

00:31:45.470 --> 00:31:47.760
potentially, the output
size here is quadratic.

00:31:47.760 --> 00:31:51.050
And so the overall thing
might be super linear.

00:31:51.050 --> 00:31:53.930
And so when you have a super
linear buffer or a bunch

00:31:53.930 --> 00:31:56.004
of very large buffers
that sum to linear size,

00:31:56.004 --> 00:31:57.920
you essentially need to
carve that tree, which

00:31:57.920 --> 00:32:01.511
you do by recursive
carving of the tree.

00:32:01.511 --> 00:32:03.260
So that each of the
trees has linear size.

00:32:03.260 --> 00:32:05.759
Then you apply the analysis to
each of the trees separately.

00:32:05.759 --> 00:32:08.120
You store them
consecutively, separately.

00:32:08.120 --> 00:32:10.040
Each of them has good
optimal running time

00:32:10.040 --> 00:32:11.480
and then the combination does.

00:32:11.480 --> 00:32:13.640
That's the hand-wavy
version of how

00:32:13.640 --> 00:32:17.560
to do actual range reporting
with end points and end

00:32:17.560 --> 00:32:18.280
rectangles.

00:32:18.280 --> 00:32:20.460
If you're interested in the
details, read the paper.

00:32:20.460 --> 00:32:24.050
It's just a little bit
messy and especially when

00:32:24.050 --> 00:32:26.780
you don't know the analysis.

00:32:26.780 --> 00:32:30.220
I want to move on to
online orthogonal 2D

00:32:30.220 --> 00:32:38.270
range searching because it's
the hardest and coolest of them

00:32:38.270 --> 00:32:38.770
all.

00:32:38.770 --> 00:32:41.126
Unless there are more questions.

00:32:41.126 --> 00:32:42.090
All right.

00:32:44.827 --> 00:32:46.910
AUDIENCE: So you do the
range counting [INAUDIBLE]

00:32:46.910 --> 00:32:52.700
in detail, and [INAUDIBLE]
to the [INAUDIBLE]..

00:32:52.700 --> 00:32:53.780
ERIK DEMAINE: Exactly.

00:32:53.780 --> 00:32:56.090
At this point, if you
believe in funnel sort,

00:32:56.090 --> 00:33:00.260
you should believe that
range counting is easy to do,

00:33:00.260 --> 00:33:04.562
and I've just hand waved
the range reporting part.

00:33:04.562 --> 00:33:05.285
Are you scribing?

00:33:05.285 --> 00:33:06.890
Is that why you ask?

00:33:11.080 --> 00:33:14.460
That's where we stand.

00:33:14.460 --> 00:33:17.450
The next thing we're going to
do is regular range reporting,

00:33:17.450 --> 00:33:19.710
regular online stuff.

00:33:19.710 --> 00:33:24.565
So this is orthogonal
2D range search.

00:33:30.110 --> 00:33:31.970
And we spent a
couple of lectures

00:33:31.970 --> 00:33:34.520
on 2D and 3D range search.

00:33:34.520 --> 00:33:38.450
All this crazy stuff with
fractional cascading,

00:33:38.450 --> 00:33:41.720
and so on, and the
layered range trees.

00:33:41.720 --> 00:33:44.450
We're going to use some of those
techniques that we built there,

00:33:44.450 --> 00:33:47.060
and in particular,
you may recall

00:33:47.060 --> 00:33:51.890
there was this idea that if
we have a bunch of points,

00:33:51.890 --> 00:33:55.010
regular 2D range searching
is I give you a rectangle,

00:33:55.010 --> 00:33:57.240
give me all the points
in the rectangle.

00:33:57.240 --> 00:33:57.740
Fine.

00:33:57.740 --> 00:34:06.710
Our goal is to achieve log base
B of N plus output size over B.

00:34:06.710 --> 00:34:08.960
That's the new optimal bound.

00:34:08.960 --> 00:34:11.330
This is how long it takes to
do a regular search in one

00:34:11.330 --> 00:34:13.550
dimension.

00:34:13.550 --> 00:34:16.659
So if you have output
size whatever--

00:34:16.659 --> 00:34:18.877
and we'll probably be
able to do range counting,

00:34:18.877 --> 00:34:20.210
but I won't worry about it here.

00:34:20.210 --> 00:34:21.835
We'll just think
about range reporting.

00:34:21.835 --> 00:34:23.449
If there's this
many points, we'll

00:34:23.449 --> 00:34:26.330
output them all in
that much over B.

00:34:26.330 --> 00:34:29.929
This is what we call a
regular range search,

00:34:29.929 --> 00:34:32.719
but I'm going to distinguish it
and call it a four sided range

00:34:32.719 --> 00:34:37.489
search because a
rectangle has four sides.

00:34:37.489 --> 00:34:39.670
But you could think
of the other versions

00:34:39.670 --> 00:34:43.760
and we actually did this when
we were doing the 3-D problem.

00:34:43.760 --> 00:34:47.770
So if these are two
rays and an edge,

00:34:47.770 --> 00:34:51.530
this you might call a
three sided rectangle,

00:34:51.530 --> 00:34:54.159
and you can go all the
way down to two sides.

00:34:54.159 --> 00:34:58.100
Hard to go down to one side.

00:34:58.100 --> 00:35:03.150
Here's a two sided rectangle,
it just has two rays.

00:35:03.150 --> 00:35:07.530
OK, as you might expect,
this is easier than that.

00:35:07.530 --> 00:35:11.760
And if I recall, in 3-D we ended
up doing this thing in linear

00:35:11.760 --> 00:35:14.550
space with this fancy--

00:35:14.550 --> 00:35:16.470
first you do a search
on the left coordinate

00:35:16.470 --> 00:35:17.970
and then you just walk.

00:35:17.970 --> 00:35:20.220
We'd subdivided with
fractional cascading

00:35:20.220 --> 00:35:23.040
so that every face
had constant size,

00:35:23.040 --> 00:35:24.540
and so you could
just walk, and each

00:35:24.540 --> 00:35:26.400
step you'd report a new point.

00:35:26.400 --> 00:35:28.666
If you may recall for this
kind of two sided thing.

00:35:28.666 --> 00:35:30.040
First, you would
search for this,

00:35:30.040 --> 00:35:32.700
and then you would basically
just follow this line until you

00:35:32.700 --> 00:35:37.110
found this point, this corner.

00:35:37.110 --> 00:35:42.750
This we could achieve in a
linear space, logarithmic time.

00:35:42.750 --> 00:35:46.200
This one we needed
N log N space.

00:35:46.200 --> 00:35:51.660
Actually, the best known is
N log N divided by log log N.

00:35:51.660 --> 00:35:54.150
But we could N log
N using range trees.

00:35:54.150 --> 00:35:58.260
And we got down to
log N time using--

00:35:58.260 --> 00:36:03.390
log N query time and log N
space using layered range trees.

00:36:03.390 --> 00:36:05.910
That was the internal
memory regular algorithms.

00:36:05.910 --> 00:36:09.044
AUDIENCE: Aren't you
missing an M/B though?

00:36:09.044 --> 00:36:10.460
ERIK DEMAINE: Am
I missing an M/B?

00:36:10.460 --> 00:36:14.920
No, this is log base B of N,
not log base M/B of N. Yeah,

00:36:14.920 --> 00:36:15.870
it's good to ask.

00:36:15.870 --> 00:36:21.634
When we're sorting this kind
of thing, we get log base M/B,

00:36:21.634 --> 00:36:23.550
but when you're searching,
the best you can do

00:36:23.550 --> 00:36:25.050
is log base B. We
actually proved

00:36:25.050 --> 00:36:27.780
a lower bound about this in the
first memory hierarchy lecture.

00:36:31.740 --> 00:36:33.900
Because this is online,
you read it in a block.

00:36:33.900 --> 00:36:36.115
You can only learn where
you fit among B items.

00:36:36.115 --> 00:36:37.740
And so the best you
can hope to achieve

00:36:37.740 --> 00:36:40.680
is log base B of N for
search in one dimension.

00:36:40.680 --> 00:36:43.032
So this is a lower
bound for search.

00:36:43.032 --> 00:36:44.490
When you're doing
batch operations,

00:36:44.490 --> 00:36:47.100
then you can hope to
achieve this stuff, which

00:36:47.100 --> 00:36:48.350
is a lot faster.

00:36:48.350 --> 00:36:52.770
Then it's like 1/B times
log base M/B of M/B.

00:36:52.770 --> 00:36:54.300
OK, so in a certain
sense, this is

00:36:54.300 --> 00:36:55.810
slower than the
batched operations,

00:36:55.810 --> 00:36:56.684
but it's more online.

00:36:56.684 --> 00:36:57.872
So it's a trade-off.

00:37:06.080 --> 00:37:09.790
So for all these problems we
can achieve log base B of N

00:37:09.790 --> 00:37:12.040
plus [? out ?] over B.
The issue is with space.

00:37:17.980 --> 00:37:25.240
Maybe I'll do sort of regular
RAM algorithms versus cache

00:37:25.240 --> 00:37:25.870
oblivious.

00:37:30.040 --> 00:37:34.330
So we've got two sided,
three sided, four sided.

00:37:34.330 --> 00:37:44.440
And for two sided, I believe
these are the right answers.

00:37:44.440 --> 00:37:50.260
Log N over log log N. But we
haven't actually seen this one.

00:37:55.900 --> 00:38:00.310
And cache oblivious,
here's what we can do.

00:38:00.310 --> 00:38:04.750
This is with optimal query
times and this is all static.

00:38:12.270 --> 00:38:15.450
OK, and if there's time,
I'll cover all of these.

00:38:15.450 --> 00:38:17.130
So they're not perfect.

00:38:17.130 --> 00:38:21.960
These two were off by a
log factor, but not bad.

00:38:21.960 --> 00:38:24.780
Pretty good orthogonal
2D range queries.

00:38:24.780 --> 00:38:27.930
And really, the coolest
one is this one.

00:38:27.930 --> 00:38:31.696
This one blows my mind
every time I see it.

00:38:31.696 --> 00:38:32.320
So let's do it.

00:38:36.600 --> 00:38:38.700
We'll start with two
sided and then we

00:38:38.700 --> 00:38:40.080
have existing
techniques once you

00:38:40.080 --> 00:38:43.800
have two sided to
add on more sides,

00:38:43.800 --> 00:38:46.770
you may recall from the 3D
range searching lecture.

00:38:46.770 --> 00:38:48.840
So we're going to
use those techniques

00:38:48.840 --> 00:38:55.110
and refine them a little bit
to get that log log factor.

00:38:55.110 --> 00:39:00.780
But you may recall way back
when, at lecture six or so,

00:39:00.780 --> 00:39:02.070
that we had a technique.

00:39:02.070 --> 00:39:03.720
Once it was two
sided, every time

00:39:03.720 --> 00:39:07.650
we added a log factor in space,
we could add another side.

00:39:07.650 --> 00:39:09.900
The hard part was getting
up the number of dimensions.

00:39:09.900 --> 00:39:14.820
Then the easy part was turning
half infinite intervals

00:39:14.820 --> 00:39:16.990
into regular intervals.

00:39:16.990 --> 00:39:21.300
So once we have this, it's easy
to add a log, add another log.

00:39:21.300 --> 00:39:25.896
With a bit of sophistication,
we can save a log log factor.

00:39:25.896 --> 00:39:28.930
OK, but let's do two sided.

00:39:28.930 --> 00:39:31.410
This will be the
bulk of the lecture.

00:39:36.630 --> 00:39:44.420
This is a paper by [? Harga ?]
and [? Zey ?] in 2006.

00:39:44.420 --> 00:39:46.170
All right, so we want to do--

00:39:46.170 --> 00:39:49.050
I'm going to assume that
they are this kind of quarter

00:39:49.050 --> 00:39:49.770
plain query.

00:39:49.770 --> 00:39:53.760
So less than or
equal to x, less than

00:39:53.760 --> 00:39:58.210
or equal to some y-coordinate.

00:39:58.210 --> 00:40:03.390
We want to know all the
points in that quarter plane.

00:40:03.390 --> 00:40:07.800
So here's what
we're going to do.

00:40:07.800 --> 00:40:09.960
It's all static.

00:40:09.960 --> 00:40:12.540
We're going to have a
Van Emde Boas layout.

00:40:12.540 --> 00:40:21.100
So a binary tree on
the y-coordinate.

00:40:21.100 --> 00:40:23.760
So this just stores all
the points sorted by y.

00:40:26.280 --> 00:40:32.750
So if you want to do this query,
use search for that value of y,

00:40:32.750 --> 00:40:39.520
then each of these positions
in between two keys in here

00:40:39.520 --> 00:40:42.490
has a pointer to an array.

00:40:45.370 --> 00:40:49.570
The array is not sorted by x
or y, it's a very weird thing.

00:40:52.450 --> 00:40:55.330
And then here's the
algorithm you follow.

00:40:55.330 --> 00:40:59.920
You follow this pointer, you
go here, you walk to the right

00:40:59.920 --> 00:41:07.285
until you find a point whose
x-coordinate is too big.

00:41:07.285 --> 00:41:09.080
It's bigger than x.

00:41:09.080 --> 00:41:12.870
I should probably
call this x2, y2.

00:41:12.870 --> 00:41:19.390
So first you search for a y2
here, in this thing keyed by y.

00:41:19.390 --> 00:41:20.430
Follow the pointer.

00:41:20.430 --> 00:41:24.005
You look at all the points that
have x-coordinate less than

00:41:24.005 --> 00:41:25.270
or equal to x2.

00:41:25.270 --> 00:41:26.600
Those are the ones you want.

00:41:26.600 --> 00:41:29.290
Once you find a point whose
x-coordinate is bigger than x2,

00:41:29.290 --> 00:41:33.410
you stop, and then you
report these points.

00:41:33.410 --> 00:41:35.830
It's not quite so simple
because some of these points

00:41:35.830 --> 00:41:37.630
might be duplicates.

00:41:37.630 --> 00:41:39.400
You have to remove duplicates.

00:41:39.400 --> 00:41:40.420
That is your answer.

00:41:46.170 --> 00:41:49.410
To me, this is an insane idea.

00:41:49.410 --> 00:41:51.930
I would never
imagine this to work.

00:41:51.930 --> 00:41:56.730
But the claim is you can make
this array have linear size.

00:41:56.730 --> 00:41:59.520
That's the hard part.

00:41:59.520 --> 00:42:03.210
Make this, the amount of stuff
that you have to traverse here,

00:42:03.210 --> 00:42:08.970
be linear in out in the number
of points that are actually

00:42:08.970 --> 00:42:12.337
in this range.

00:42:12.337 --> 00:42:14.670
You are going to do a little
bit more work because there

00:42:14.670 --> 00:42:18.780
are duplicates in here, but only
a constant factor of more work.

00:42:18.780 --> 00:42:21.360
And yet somehow, you've taken
this two dimensional problem

00:42:21.360 --> 00:42:23.540
and squashed it onto a line.

00:42:23.540 --> 00:42:25.290
You did one search at
the beginning, which

00:42:25.290 --> 00:42:28.650
costs you log base B of N,
then you do this linear scan,

00:42:28.650 --> 00:42:33.300
and you get the right
answer, magically.

00:42:33.300 --> 00:42:36.120
I don't know how they thought
this would be possible,

00:42:36.120 --> 00:42:38.850
but magically, it turns
out it is possible.

00:42:38.850 --> 00:42:42.750
It was kind of a breakthrough
in cache oblivious

00:42:42.750 --> 00:42:43.890
range searching.

00:42:43.890 --> 00:42:47.580
It was known how to do this for
external memory a lot easier.

00:42:50.680 --> 00:42:54.950
For example, you can
do it with persistence,

00:42:54.950 --> 00:43:00.050
but this is a much cooler way
to do two sided range queries.

00:43:00.050 --> 00:43:02.920
All right, so I've explained
the query algorithm.

00:43:07.126 --> 00:43:08.500
The big thing I
haven't explained

00:43:08.500 --> 00:43:09.640
is how to build this array.

00:43:15.192 --> 00:43:16.650
Maybe I'll write
down the things we

00:43:16.650 --> 00:43:19.500
need to prove as well
before we get there,

00:43:19.500 --> 00:43:21.000
so you can think
about them as we're

00:43:21.000 --> 00:43:23.460
writing down the algorithm.

00:43:23.460 --> 00:43:26.460
First claim is that this
algorithm, which just decides

00:43:26.460 --> 00:43:29.250
to stop whenever it gets an
x-coordinate that is too big,

00:43:29.250 --> 00:43:31.230
actually finds the right answer.

00:43:31.230 --> 00:43:37.110
It Finds all points in the
range that we care about.

00:43:40.710 --> 00:43:45.210
The second thing is that the
number of scanned points,

00:43:45.210 --> 00:43:56.830
the length of that step here,
is order the size of the output.

00:43:56.830 --> 00:43:59.500
The number of actual
output points.

00:43:59.500 --> 00:44:02.830
We don't waste time
doing the scan.

00:44:02.830 --> 00:44:09.490
And the other thing is that the
array has size order N. That's

00:44:09.490 --> 00:44:13.889
the biggest surprise to me.

00:44:13.889 --> 00:44:15.430
So those are the
three things we need

00:44:15.430 --> 00:44:19.690
to prove about the algorithm,
which I will now tell you.

00:44:36.270 --> 00:44:39.040
OK, before I can define
how this array works,

00:44:39.040 --> 00:44:40.825
I need to define a
concept called density.

00:44:49.450 --> 00:44:59.760
If we look at a query, there's
two things that could happen.

00:44:59.760 --> 00:45:05.240
The good thing for
us would be if--

00:45:05.240 --> 00:45:05.960
get this right.

00:45:10.070 --> 00:45:20.570
The number of points in
lesser or equal to x star

00:45:20.570 --> 00:45:27.780
is at most, alpha times the
number of points in the answer.

00:45:34.530 --> 00:45:37.680
OK, star means no
restriction on y.

00:45:37.680 --> 00:45:39.674
Minus infinity to infinity.

00:45:45.110 --> 00:45:48.560
This would be good for
us because it says--

00:45:48.560 --> 00:45:52.085
ultimately what we're trying
to do here is do a scan in x.

00:45:57.050 --> 00:46:00.050
It's the right thing to do here.

00:46:00.050 --> 00:46:02.460
Then for this
particular y-coordinate,

00:46:02.460 --> 00:46:05.240
we could just basically start
at the beginning of the array,

00:46:05.240 --> 00:46:08.300
start scanning, and just
report all the points

00:46:08.300 --> 00:46:10.410
that are actually in our range.

00:46:10.410 --> 00:46:14.300
Sorry, I need to also
potentially throw away

00:46:14.300 --> 00:46:18.560
points that are not low enough.

00:46:18.560 --> 00:46:22.300
So the answer is
contained in here.

00:46:22.300 --> 00:46:24.350
I should say to throw
away duplicates,

00:46:24.350 --> 00:46:26.689
you have to throw away points
that are not in the range

00:46:26.689 --> 00:46:28.730
lesser or equal to x,
comma lesser or equal to y.

00:46:28.730 --> 00:46:30.500
Still, we claim the
number of scan points

00:46:30.500 --> 00:46:33.950
is proportional to
the output size.

00:46:33.950 --> 00:46:35.420
That's what we need.

00:46:35.420 --> 00:46:40.120
So if this held for every
query, we'd be happy.

00:46:40.120 --> 00:46:42.710
Just start at the
beginning, scan,

00:46:42.710 --> 00:46:44.600
and as long as this
alpha is some constant--

00:46:44.600 --> 00:46:48.380
it's going to be a
constant bigger than 1,

00:46:48.380 --> 00:46:52.450
then the number of points in
the answer is proportional--

00:46:52.450 --> 00:46:54.200
sorry, the number of
points we had to scan

00:46:54.200 --> 00:46:57.175
through is proportional to the
number of points in the answer,

00:46:57.175 --> 00:46:58.799
and so we're done.

00:46:58.799 --> 00:46:59.840
So this is the easy case.

00:46:59.840 --> 00:47:03.830
We need to distinguish
it, otherwise we

00:47:03.830 --> 00:47:09.677
call this range query
sparse, and those

00:47:09.677 --> 00:47:10.760
are the interesting cases.

00:47:13.700 --> 00:47:16.250
So nothing deep
here, but we're going

00:47:16.250 --> 00:47:17.360
to use this concept a lot.

00:47:32.650 --> 00:47:35.260
OK, so we're going
to actually try

00:47:35.260 --> 00:47:37.840
to solve this problem twice.

00:47:37.840 --> 00:47:42.010
The first try isn't going
to be quite successful,

00:47:42.010 --> 00:47:45.160
but it gets a lot
of the right ideas.

00:47:45.160 --> 00:47:55.470
So I'm going to let S0 be
all the points sorted by x.

00:47:55.470 --> 00:47:57.940
It's going to be sorted by x.

00:47:57.940 --> 00:48:00.340
I put things down here.

00:48:00.340 --> 00:48:02.830
And just to give you an
idea of where we're going,

00:48:02.830 --> 00:48:06.490
the array we're
imagining here is first

00:48:06.490 --> 00:48:08.680
we write down all
the points, then

00:48:08.680 --> 00:48:11.380
we'll write down some subset
of the points, S1, then

00:48:11.380 --> 00:48:16.030
some subset of that subset,
and so on until we get down

00:48:16.030 --> 00:48:18.590
to a constant size structure.

00:48:18.590 --> 00:48:20.630
OK, first we write
down all the points.

00:48:20.630 --> 00:48:21.130
Why?

00:48:21.130 --> 00:48:23.590
Because for dense queries,
that's what we want.

00:48:23.590 --> 00:48:25.610
We want all the points
just sitting there.

00:48:25.610 --> 00:48:28.660
So then you can just read
through all the points

00:48:28.660 --> 00:48:30.680
and dense queries will be happy.

00:48:30.680 --> 00:48:35.380
So if we detect a y-coordinate
where the queries going to be

00:48:35.380 --> 00:48:36.477
dense--

00:48:36.477 --> 00:48:37.810
I don't know how we detect that.

00:48:37.810 --> 00:48:39.340
Let's not worry
about it right now--

00:48:39.340 --> 00:48:41.620
then you could just
look through S0.

00:48:41.620 --> 00:48:42.654
That's fine.

00:48:42.654 --> 00:48:44.320
But some queries are
going to be sparse,

00:48:44.320 --> 00:48:47.420
and for that we're going
to use S1, S2, and so on.

00:48:47.420 --> 00:48:50.920
The intuition is the following.

00:48:50.920 --> 00:48:53.620
If in your query,
the y-coordinate

00:48:53.620 --> 00:48:57.130
is very large,
like say infinity,

00:48:57.130 --> 00:48:59.060
then your query is
guaranteed to be dense.

00:48:59.060 --> 00:49:01.930
It doesn't matter what x is.

00:49:01.930 --> 00:49:04.600
And in general, if
y is near the top,

00:49:04.600 --> 00:49:07.692
like it's at the top most
point, or maybe the next of top

00:49:07.692 --> 00:49:09.650
most point, or maybe a
little bit farther down,

00:49:09.650 --> 00:49:12.700
it depends on the point
set, then a lot of queries

00:49:12.700 --> 00:49:14.380
are going to be dense.

00:49:14.380 --> 00:49:16.540
So that's good news.

00:49:16.540 --> 00:49:21.620
Let's consider the first time
when there's a sparse query.

00:49:21.620 --> 00:49:32.980
So we're going to let yi be the
largest y-coordinate where some

00:49:32.980 --> 00:49:37.420
query, some x-coordinate--

00:49:37.420 --> 00:49:38.626
that y-coordinate.

00:49:38.626 --> 00:49:41.125
This is going to be less than
or equal to x, comma less than

00:49:41.125 --> 00:49:42.340
or equal to yi--

00:49:44.920 --> 00:49:53.326
is sparse in Si minus 1.

00:49:53.326 --> 00:49:56.530
OK, so initially we
have S0, all points.

00:49:56.530 --> 00:49:59.079
y1 is the largest y
co-ordinate where there's--

00:49:59.079 --> 00:50:00.620
so we work our way
down until there's

00:50:00.620 --> 00:50:03.556
some sparse query in S0.

00:50:03.556 --> 00:50:06.065
That's yi.

00:50:06.065 --> 00:50:11.400
So then we just
filter, based on that.

00:50:11.400 --> 00:50:15.280
So throw away all
the points above yi.

00:50:15.280 --> 00:50:17.860
So we're going to
say take Si minus 1,

00:50:17.860 --> 00:50:22.610
intersect it with the
range query, star less than

00:50:22.610 --> 00:50:24.950
or equal to yi.

00:50:24.950 --> 00:50:28.175
OK, so the picture is
we have some point set.

00:50:35.930 --> 00:50:40.370
Up here, every possible
query along this line

00:50:40.370 --> 00:50:42.110
is going to be dense
because everything

00:50:42.110 --> 00:50:45.260
to the left of the x-coordinate
will be in the output.

00:50:45.260 --> 00:50:48.600
At some point, we're going
to decide this is too scary.

00:50:48.600 --> 00:50:50.510
There's a query
here, maybe this one,

00:50:50.510 --> 00:50:53.630
or maybe it's this
query that's sparse.

00:50:53.630 --> 00:50:56.960
And so we say OK, throw
away these points.

00:50:56.960 --> 00:50:59.330
Redo the data structure
from here down,

00:50:59.330 --> 00:51:03.710
ignoring all these
points, repeat,

00:51:03.710 --> 00:51:06.960
and write down these things.

00:51:06.960 --> 00:51:09.620
So the idea is that if you
look at a particular query,

00:51:09.620 --> 00:51:14.950
it will be dense in
one of these Si's.

00:51:14.950 --> 00:51:18.289
And you can tell that just
according to your y-coordinate.

00:51:18.289 --> 00:51:20.830
Because you said oh, well, if
you're up here in y-coordinate,

00:51:20.830 --> 00:51:23.060
you're guaranteed safe.

00:51:23.060 --> 00:51:27.520
So just do that
search and you're OK.

00:51:27.520 --> 00:51:32.320
In general, we
continue this process

00:51:32.320 --> 00:51:37.150
until we get to some Si
that has constant size.

00:51:37.150 --> 00:51:38.950
At that point, we're
done, and then we

00:51:38.950 --> 00:51:42.250
can afford to look
through all the points.

00:51:42.250 --> 00:51:44.800
Unfortunately, this is
not a very good strategy,

00:51:44.800 --> 00:51:50.300
but it's the first cut, and
it's close to what works.

00:51:50.300 --> 00:51:52.540
Here's a problem with it.

00:51:52.540 --> 00:51:54.775
Suppose you have this point set.

00:51:58.960 --> 00:52:02.770
OK, what happens is you start at
the top, everything looks fine.

00:52:02.770 --> 00:52:06.510
At some point you decide
there's a query here, namely

00:52:06.510 --> 00:52:09.420
this one, which has
an empty answer,

00:52:09.420 --> 00:52:13.020
and yet there are points to
the left of this x-coordinate.

00:52:13.020 --> 00:52:16.440
So that's bad because
it's very hard

00:52:16.440 --> 00:52:18.540
to get within a
constant factor of zero.

00:52:18.540 --> 00:52:22.290
So pretty much immediately
you've got to draw a line here

00:52:22.290 --> 00:52:30.280
and say OK, S0 is all
points, S1 is these points,

00:52:30.280 --> 00:52:33.240
S2 is going to be these points.

00:52:33.240 --> 00:52:36.270
In general, there's
suffixes of the points,

00:52:36.270 --> 00:52:39.240
and so the total space
will be quadratic.

00:52:39.240 --> 00:52:41.990
So the first two
properties will be correct

00:52:41.990 --> 00:52:45.780
because you're just looking
in S0, or S1, or whatever.

00:52:45.780 --> 00:52:48.030
Everything looks
fine, but your right

00:52:48.030 --> 00:52:50.400
does not have linear size.

00:52:50.400 --> 00:52:52.650
So no good.

00:52:52.650 --> 00:52:54.090
First try, failed.

00:52:59.101 --> 00:53:00.100
Second time's the charm.

00:53:11.840 --> 00:53:15.840
You need a little
more sophistication

00:53:15.840 --> 00:53:20.712
in how we do this partitioning,
how we build our array,

00:53:20.712 --> 00:53:21.420
and we'll get it.

00:53:31.810 --> 00:53:33.450
I didn't read this before.

00:53:33.450 --> 00:53:40.135
This one line that says
maximize common suffix.

00:53:40.135 --> 00:53:42.179
I have no idea what
this means, but maybe it

00:53:42.179 --> 00:53:43.470
will mean something by the end.

00:53:43.470 --> 00:53:45.120
Let's see.

00:53:45.120 --> 00:53:49.530
OK, this is the part I read.

00:53:49.530 --> 00:53:53.790
So xi is going to be--

00:53:53.790 --> 00:53:56.701
so we had a yi That's going
to be the same as before.

00:53:56.701 --> 00:53:58.200
This is why I did
the first attempt.

00:53:58.200 --> 00:54:01.070
This definition
remains the same.

00:54:01.070 --> 00:54:06.930
So largest y where we have some
sparse query in Si minus 1.

00:54:06.930 --> 00:54:11.014
I want to look at what
that x-coordinate is.

00:54:11.014 --> 00:54:12.805
It's just that here it
says there's some x.

00:54:15.420 --> 00:54:18.620
What is that x?

00:54:18.620 --> 00:54:22.260
Let's just look at the maximum
possible x that it could be.

00:54:22.260 --> 00:54:25.150
This will turn out
to be really useful.

00:54:25.150 --> 00:54:30.700
The maximum x-coordinate where
less than or equal to xi,

00:54:30.700 --> 00:54:33.645
comma less than or
equal to yi is sparse--

00:54:37.020 --> 00:54:39.974
and Si minus 1.

00:54:39.974 --> 00:54:41.640
OK, we know there's
something we can put

00:54:41.640 --> 00:54:43.530
in here that makes yi sparse.

00:54:43.530 --> 00:54:47.190
So look at the largest
possible such x.

00:54:47.190 --> 00:54:50.100
So that means any query--

00:54:50.100 --> 00:54:52.750
so we have this new point.

00:54:52.750 --> 00:54:55.060
It's not an actual
point in our problem,

00:54:55.060 --> 00:54:59.280
but it's a query, xi, yi.

00:54:59.280 --> 00:55:01.740
And it's dense, oh
sorry, it's sparse.

00:55:01.740 --> 00:55:03.430
It's bad.

00:55:03.430 --> 00:55:11.700
We know that any query
up here is dense.

00:55:11.700 --> 00:55:14.280
That was the definition of yi.

00:55:14.280 --> 00:55:20.130
And now we also know that
any query over here, I guess,

00:55:20.130 --> 00:55:21.990
that's saying a lot.

00:55:21.990 --> 00:55:23.969
But these queries
are also dense.

00:55:23.969 --> 00:55:26.010
Because again, if you're
far enough to the right,

00:55:26.010 --> 00:55:28.020
that's going to be
basically everything.

00:55:28.020 --> 00:55:30.570
So let's get rid
of that as well.

00:55:30.570 --> 00:55:33.100
And this is a problem,
queries over here

00:55:33.100 --> 00:55:34.640
are also potentially a problem.

00:55:34.640 --> 00:55:36.600
We don't know.

00:55:36.600 --> 00:55:40.830
It doesn't seem like much,
but it will be enough.

00:55:40.830 --> 00:55:43.360
We're going to
redefine Si as well.

00:55:43.360 --> 00:55:45.210
So here's the fun part.

00:55:45.210 --> 00:55:52.134
If we have some
Si minus 1, we're

00:55:52.134 --> 00:55:53.550
going to define a
new thing, which

00:55:53.550 --> 00:56:03.672
is Pi minus 1, which is this.

00:56:03.672 --> 00:56:13.960
This is a funny thing, but it
is this part of the point set.

00:56:13.960 --> 00:56:17.990
This is Pi minus 1.

00:56:17.990 --> 00:56:20.500
So the points we care
about are kind of here,

00:56:20.500 --> 00:56:22.990
but let's just take
everything to the left

00:56:22.990 --> 00:56:24.200
of this x-coordinate.

00:56:24.200 --> 00:56:24.700
Why not?

00:56:24.700 --> 00:56:26.530
It's a thing.

00:56:26.530 --> 00:56:28.880
That is Pi minus 1.

00:56:28.880 --> 00:56:32.590
So Si minus 1 is
everything in this picture.

00:56:32.590 --> 00:56:35.020
First, let's restrict
to x, then the next step

00:56:35.020 --> 00:56:37.600
is we're going to restrict to y.

00:56:37.600 --> 00:56:39.940
But it's in a funny way.

00:56:39.940 --> 00:56:44.200
This is the Si, the next s set.

00:56:44.200 --> 00:56:47.110
Take the previous set
and we intersect it

00:56:47.110 --> 00:56:49.000
with a funny thing.

00:56:52.984 --> 00:56:54.400
It's harder to
write algebraically

00:56:54.400 --> 00:56:55.720
than it is to draw the picture.

00:57:01.160 --> 00:57:05.800
So it's intersected with a
union, which is basically--

00:57:05.800 --> 00:57:08.020
dare I draw it on
the same picture?

00:57:08.020 --> 00:57:08.790
Where's my red?

00:57:15.960 --> 00:57:18.620
It's going to be less
than or equal to y.

00:57:28.640 --> 00:57:30.398
This thing is going to be Si.

00:57:39.250 --> 00:57:42.212
We'll see why,
eventually, this works.

00:57:42.212 --> 00:57:44.420
I still don't know what
maximize common suffix means,

00:57:44.420 --> 00:57:47.450
but we'll get there.

00:57:47.450 --> 00:57:50.770
So we're looking at the
points below the line.

00:57:50.770 --> 00:57:52.180
That's what we did before.

00:57:52.180 --> 00:57:54.910
We used to say Si is just the
intersection with less than

00:57:54.910 --> 00:57:56.380
or equal to yi.

00:57:56.380 --> 00:57:59.560
But things are just
a little bit messier

00:57:59.560 --> 00:58:03.270
because of this restriction.

00:58:03.270 --> 00:58:06.400
Do I really not have a P here?

00:58:06.400 --> 00:58:08.762
OK, here's the difference.

00:58:08.762 --> 00:58:10.720
The reason we have to go
through this business.

00:58:10.720 --> 00:58:14.740
The array that we're going
to store is not the Si's.

00:58:14.740 --> 00:58:17.061
Si's are still too
big, potentially.

00:58:17.061 --> 00:58:18.685
What we're going to
store are the Pi's.

00:58:27.664 --> 00:58:29.557
Pi minus 1.

00:58:29.557 --> 00:58:31.265
And then in the end,
we're in a store Si.

00:58:31.265 --> 00:58:34.250
Si, again, has constant size.

00:58:34.250 --> 00:58:35.870
The final Si has constants size.

00:58:35.870 --> 00:58:37.953
I probably should have
used a different letter, Sk

00:58:37.953 --> 00:58:39.490
or whatever.

00:58:39.490 --> 00:58:41.274
We keep doing this
until we get down

00:58:41.274 --> 00:58:43.190
to something constant
sized, then we store it.

00:58:43.190 --> 00:58:46.460
That's the easy case.

00:58:46.460 --> 00:58:50.060
Until then, we just store
the Pi's, because really, we

00:58:50.060 --> 00:58:55.800
know that all the queries up
here and over here are OK.

00:58:55.800 --> 00:58:57.200
They're nice and dense.

00:58:57.200 --> 00:59:02.210
We sort of only care about the
points to the left of the line.

00:59:02.210 --> 00:59:07.310
OK, but essentially, the
Si has to pick up the slack

00:59:07.310 --> 00:59:11.390
and we have to include
these points in the next Si.

00:59:11.390 --> 00:59:12.950
Whereas, before, we did not.

00:59:12.950 --> 00:59:14.947
Before we just took
things below the line.

00:59:14.947 --> 00:59:17.030
Now we have to take things
that are below the line

00:59:17.030 --> 00:59:20.180
or to the right of
the vertical line.

00:59:23.390 --> 00:59:26.231
This is essentially
necessary for correctness.

00:59:31.640 --> 00:59:34.480
So we kind of win
some, we lose some.

00:59:34.480 --> 00:59:39.730
But it turns out all is well.

00:59:39.730 --> 00:59:46.840
So I know this is weird, but
let's jump to the analysis.

00:59:46.840 --> 00:59:51.370
These claims, in particular,
that the array has linear size.

00:59:51.370 --> 00:59:54.394
Let's think about that and it
will become clear why the heck

00:59:54.394 --> 00:59:55.435
we've made these choices.

00:59:57.836 --> 00:59:59.210
Unless you have
a question first.

00:59:59.210 --> 01:00:00.194
AUDIENCE: Is there
any relationship

01:00:00.194 --> 01:00:02.660
between the Si here and
the Si on the first try?

01:00:02.660 --> 01:00:04.480
ERIK DEMAINE: No,
this definition of Si

01:00:04.480 --> 01:00:06.340
is no longer in effect.

01:00:06.340 --> 01:00:12.250
S0 is correct, and all the
Si's are still sorted by x.

01:00:12.250 --> 01:00:13.900
We're no longer doing this.

01:00:13.900 --> 01:00:18.010
Instead of this rule,
we're doing this rule.

01:00:18.010 --> 01:00:20.950
This part is the same, but we
have this extra union, which

01:00:20.950 --> 01:00:23.320
contradicts the previous rule.

01:00:23.320 --> 01:00:25.480
So the yi definition
is the same.

01:00:25.480 --> 01:00:26.710
Sorry, it's a little weird.

01:00:26.710 --> 01:00:30.520
xi is new, Pi is
new, and Si is new.

01:00:36.900 --> 01:00:39.940
At this point, it's this
algebraic weird thing.

01:00:39.940 --> 01:00:43.080
Here's the cool thing.

01:00:43.080 --> 01:00:50.930
For the space
bound, the claim is

01:00:50.930 --> 01:00:57.710
Pi minus 1 intersect Si
is less than or equal to 1

01:00:57.710 --> 01:01:04.370
over alpha times Pi minus 1.

01:01:04.370 --> 01:01:06.590
This is hard to even
interpret what it means,

01:01:06.590 --> 01:01:09.450
but it's good news.

01:01:09.450 --> 01:01:12.920
So remember, alpha is
a number bigger than 1.

01:01:12.920 --> 01:01:15.059
It's what we use in the
definition of density,

01:01:15.059 --> 01:01:17.600
and you could set this parameter
to whatever you want, say 2.

01:01:20.420 --> 01:01:23.360
So then we're going to get that
this thing, whatever it is,

01:01:23.360 --> 01:01:25.730
is at most half the size
of the previous one.

01:01:28.640 --> 01:01:30.680
I claim this is good news.

01:01:30.680 --> 01:01:36.080
I claim it means that these Pi's
essentially are geometrically

01:01:36.080 --> 01:01:40.550
decreasing in size,
which is how we get--

01:01:40.550 --> 01:01:45.050
that's not quite right, but this
will give us a charging scheme.

01:01:45.050 --> 01:01:47.530
which will prove that the
whole thing has linear size.

01:01:47.530 --> 01:01:50.830
First, why is this true?

01:01:50.830 --> 01:01:55.010
It could really only be true
for sparsity from the alpha.

01:01:55.010 --> 01:01:57.550
Right, so we said
oh, density is good.

01:01:57.550 --> 01:01:59.840
If we have dense,
there's nothing to do.

01:01:59.840 --> 01:02:03.650
Just put the points in
x order, we're done.

01:02:03.650 --> 01:02:04.670
Sparse is bad.

01:02:04.670 --> 01:02:07.470
But actually, sparse
tells us something.

01:02:07.470 --> 01:02:09.680
It tells us there
are a lot of points

01:02:09.680 --> 01:02:11.550
that are not in the answer.

01:02:11.550 --> 01:02:14.250
So we're looking at
this query, xi yi.

01:02:14.250 --> 01:02:17.750
And we'd like to just say oh,
start at negative infinity,

01:02:17.750 --> 01:02:21.350
and just take all the
points up to here.

01:02:21.350 --> 01:02:24.320
If we're dense, that is
within a constant factor

01:02:24.320 --> 01:02:27.320
of the number of points that are
actually in the answer, which

01:02:27.320 --> 01:02:29.400
is down here.

01:02:29.400 --> 01:02:31.815
If we're sparse, that means
there are a lot of points

01:02:31.815 --> 01:02:34.730
up here.

01:02:34.730 --> 01:02:39.710
Most of the points have to be
up here in order to be sparse.

01:02:39.710 --> 01:02:41.480
And that's actually
what this is saying

01:02:41.480 --> 01:02:43.870
if you expand the definitions.

01:02:43.870 --> 01:02:47.820
So Pi minus 1, that was
all the stuff to the left.

01:02:47.820 --> 01:02:48.980
So that's this thing.

01:02:48.980 --> 01:02:51.410
This is what we would get
if we just did a linear scan

01:02:51.410 --> 01:02:53.810
from left to right.

01:02:53.810 --> 01:02:56.870
Versus we're
considering the points

01:02:56.870 --> 01:03:00.980
in Pi minus 1, which
just restricts to x,

01:03:00.980 --> 01:03:04.090
and then we're looking at Si.

01:03:04.090 --> 01:03:06.890
Si does this business.

01:03:06.890 --> 01:03:09.320
But if we restrict
to the Si points that

01:03:09.320 --> 01:03:11.660
are to the left of the line--

01:03:11.660 --> 01:03:13.460
so we're looking
at, basically, this

01:03:13.460 --> 01:03:17.150
left portion, which was this
white rectangle, intersected

01:03:17.150 --> 01:03:20.750
with this funny red rectangle,
which was kind of awkward--

01:03:20.750 --> 01:03:22.310
the intersection is just this.

01:03:22.310 --> 01:03:26.120
That's the answer for
this query, xi yi.

01:03:26.120 --> 01:03:38.540
OK, so this is the size
of the answer for xi yi.

01:03:38.540 --> 01:03:47.270
And this was the
number of points

01:03:47.270 --> 01:03:50.910
in less than or
equal to xi star.

01:03:53.660 --> 01:03:56.330
We wanted to just do a
linear scan like this.

01:03:56.330 --> 01:03:59.300
But this is the correct
answer and because we

01:03:59.300 --> 01:04:01.190
know that this point
is sparse-- that was

01:04:01.190 --> 01:04:04.280
the definition of xi and yi, it
was the maximum sparse point.

01:04:04.280 --> 01:04:05.930
So it's a sparse
point, therefore

01:04:05.930 --> 01:04:08.584
we know that this does not hold.

01:04:08.584 --> 01:04:11.000
So the number of points less
than or equal to x comma star

01:04:11.000 --> 01:04:13.730
is greater than alpha
times the number

01:04:13.730 --> 01:04:16.190
of points in the correct range.

01:04:16.190 --> 01:04:18.700
And if I got it right,
that should be this.

01:04:18.700 --> 01:04:21.240
You could put alpha over
here without the one over

01:04:21.240 --> 01:04:23.840
and I guess this is
strictly greater.

01:04:23.840 --> 01:04:26.000
No big deal.

01:04:26.000 --> 01:04:28.820
So that's the
definition of sparsity.

01:04:28.820 --> 01:04:30.690
So this is the
cool thing we know.

01:04:30.690 --> 01:04:33.930
Now, we're going to use--

01:04:33.930 --> 01:04:36.581
this is now a
numbered less than 1.

01:04:36.581 --> 01:04:37.080
Question?

01:04:37.080 --> 01:04:39.520
AUDIENCE: So for Pi
minus 1, we add them

01:04:39.520 --> 01:04:41.472
as the number of points
less than xi star.

01:04:41.472 --> 01:04:42.450
But for example--

01:04:42.450 --> 01:04:43.860
ERIK DEMAINE: Yes, that's
the definition here.

01:04:43.860 --> 01:04:45.276
AUDIENCE: [INAUDIBLE]
like Pi, you

01:04:45.276 --> 01:04:48.980
don't have that block in
the top left corner, right?

01:04:48.980 --> 01:04:50.250
ERIK DEMAINE: Right.

01:04:50.250 --> 01:04:52.080
After we restrict
to Si, yeah, we've

01:04:52.080 --> 01:04:53.490
thrown away all of these points.

01:04:53.490 --> 01:04:53.990
AUDIENCE: Right.

01:04:53.990 --> 01:04:55.990
So if you take the next
Pi, it's not necessarily

01:04:55.990 --> 01:04:58.031
going to be the points
less than or equal to xi--

01:04:58.031 --> 01:04:59.280
ERIK DEMAINE: It's true.

01:04:59.280 --> 01:05:01.650
When I say points, I
don't mean all points.

01:05:01.650 --> 01:05:04.480
I mean points in Si minus 1.

01:05:04.480 --> 01:05:05.880
I'm dropping that
because it gets

01:05:05.880 --> 01:05:08.610
awkward to keep talking about.

01:05:08.610 --> 01:05:10.530
So that's a correctness
issue, essentially.

01:05:10.530 --> 01:05:12.738
You have to argue that we
can throw away these points

01:05:12.738 --> 01:05:14.070
and it's safe.

01:05:14.070 --> 01:05:18.940
Once we do, then you could
just ignore their existence.

01:05:18.940 --> 01:05:21.942
You can ignore their
existence because you already

01:05:21.942 --> 01:05:23.400
solved all the
dense queries, which

01:05:23.400 --> 01:05:26.700
are over here, or over here,
which involve those points.

01:05:26.700 --> 01:05:28.290
And so we now know
that we're only

01:05:28.290 --> 01:05:31.950
going to be doing
queries from here down.

01:05:31.950 --> 01:05:34.140
Otherwise, you look at P0.

01:05:34.140 --> 01:05:35.250
So forget about those.

01:05:35.250 --> 01:05:36.970
Forget about those points.

01:05:36.970 --> 01:05:39.790
Now you're going to be searching
in one of these structures.

01:05:39.790 --> 01:05:42.040
So you can forget about
all the points over here.

01:05:42.040 --> 01:05:44.385
So that's that argument.

01:05:44.385 --> 01:05:46.132
Once you've restricted
to Si minus 1

01:05:46.132 --> 01:05:48.090
and you don't have to
look at any other points,

01:05:48.090 --> 01:05:49.810
among those points,
this is going

01:05:49.810 --> 01:05:52.800
to be all the points
less than or equal to xi.

01:05:52.800 --> 01:05:54.600
But that's how we were
defining bar sparse.

01:05:54.600 --> 01:05:56.920
We said sparse in Si minus 1.

01:05:56.920 --> 01:05:59.520
So it's among those
points we have sparsity.

01:05:59.520 --> 01:06:03.800
So this is the definition
of what we have.

01:06:03.800 --> 01:06:05.305
OK, the claim is
it's a good thing.

01:06:05.305 --> 01:06:06.430
Here's the charging scheme.

01:06:09.790 --> 01:06:11.010
So this is by sparsity.

01:06:17.910 --> 01:06:29.070
So I'm going to charge storing
Pi minus 1 to Pi minus 1

01:06:29.070 --> 01:06:32.290
minus Si.

01:06:32.290 --> 01:06:35.760
This algebra, I have to
interpret every single time,

01:06:35.760 --> 01:06:37.410
but that's fine.

01:06:37.410 --> 01:06:38.700
Let's look at the picture.

01:06:38.700 --> 01:06:43.692
OK, Pi minus 1 remember, was
this white rectangle over here.

01:06:43.692 --> 01:06:45.150
Everything to the
left of the line.

01:06:47.690 --> 01:06:49.950
We have to store Pi.

01:06:49.950 --> 01:06:52.950
We want that the sum of the
sizes of the Pi's is good.

01:06:52.950 --> 01:06:54.510
And so here's my
charging scheme.

01:06:54.510 --> 01:06:56.590
We have to store Pi minus 1.

01:06:56.590 --> 01:06:59.200
I'm going to charge
it to these points.

01:06:59.200 --> 01:07:00.985
What are those points?

01:07:00.985 --> 01:07:03.360
Those are the points that are
inside the white rectangle,

01:07:03.360 --> 01:07:08.220
but outside the red L-shape.

01:07:08.220 --> 01:07:09.900
So that's these points.

01:07:09.900 --> 01:07:14.730
This is Pi minus 1 minus Si.

01:07:14.730 --> 01:07:16.830
Those are the points
that I'm throwing away.

01:07:16.830 --> 01:07:17.730
That's good.

01:07:17.730 --> 01:07:21.810
So if I charge them now, I will
never charge them in the future

01:07:21.810 --> 01:07:23.670
because I just threw them away.

01:07:23.670 --> 01:07:25.020
They are not in the next Si.

01:07:28.800 --> 01:07:32.490
Each point overall in the point
set only gets charged once.

01:07:41.080 --> 01:07:43.960
OK, how much does
it get charged?

01:07:43.960 --> 01:07:47.380
How do these things relate
to each other in size?

01:07:47.380 --> 01:07:49.660
That's where we use this thing.

01:07:49.660 --> 01:07:53.170
It gets confusing to think about
intersection versus difference,

01:07:53.170 --> 01:07:56.150
but the point is if we look
at the Pi minus ones that are

01:07:56.150 --> 01:08:00.070
in Si, that's a small fraction.

01:08:00.070 --> 01:08:01.750
Think of alpha as 100.

01:08:01.750 --> 01:08:06.880
So then the Pi minus 1-- so this
part down here that's in Si,

01:08:06.880 --> 01:08:11.030
this is only 1/100 of the
whole white rectangle.

01:08:11.030 --> 01:08:17.800
So that means this part
is 99/100 of the Pi.

01:08:17.800 --> 01:08:20.950
So if we charged the storing
of the entire rectangle

01:08:20.950 --> 01:08:24.279
to these guys, we're only
losing a very small factor

01:08:24.279 --> 01:08:26.840
like 100/99 or something.

01:08:26.840 --> 01:08:30.460
It isn't actually exactly
100/99, I believe.

01:08:30.460 --> 01:08:34.689
I worked it out and
the factor of charging,

01:08:34.689 --> 01:08:37.120
assuming I did it
correctly, is 1 over 1

01:08:37.120 --> 01:08:41.020
minus 1 over alpha,
which works out

01:08:41.020 --> 01:08:43.420
to alpha over alpha minus 1.

01:08:43.420 --> 01:08:46.279
It doesn't really matter, but
the point is it's constant.

01:08:46.279 --> 01:08:48.622
I think that's easy to believe.

01:08:48.622 --> 01:08:51.080
Maybe it's actually easiest to
think about when alpha is 2.

01:08:54.939 --> 01:08:56.920
At most, half the
points are here.

01:08:56.920 --> 01:08:58.710
At least, half the
points are here.

01:08:58.710 --> 01:09:01.210
And so we're charging
storing the entire point set

01:09:01.210 --> 01:09:03.850
to these points, which will
never get charged again.

01:09:03.850 --> 01:09:06.220
So we're only charging
with a factor of two.

01:09:06.220 --> 01:09:09.000
That's all we need,
a constant factor.

01:09:09.000 --> 01:09:12.580
OK, therefore, this
thing has linear size.

01:09:12.580 --> 01:09:13.870
That's the cool thing.

01:09:13.870 --> 01:09:15.250
We get more though.

01:09:15.250 --> 01:09:16.840
We also get the
query bound we want.

01:09:20.394 --> 01:09:21.810
Let's think about
the query bound.

01:09:31.590 --> 01:09:32.818
This is fun.

01:09:32.818 --> 01:09:34.109
Think about where the query is.

01:09:34.109 --> 01:09:36.420
It used to be over here.

01:09:36.420 --> 01:09:41.220
We do a search in S0,
or we do a search in S1,

01:09:41.220 --> 01:09:43.950
or we do a search in S2.

01:09:43.950 --> 01:09:47.819
We'd never look at multiple Si's
because there'd be no point.

01:09:47.819 --> 01:09:50.420
Either S0 was dense, and
we're fine, just do it.

01:09:50.420 --> 01:09:53.702
Or you have to jump to
S1, skip some guys up top,

01:09:53.702 --> 01:09:54.660
do the search in there.

01:09:54.660 --> 01:09:55.600
Fine.

01:09:55.600 --> 01:09:58.080
We no longer have that luxury
over here because we're using

01:09:58.080 --> 01:09:59.970
Pi's instead of Si's.

01:09:59.970 --> 01:10:02.730
So it actually may be
the search starts in P1,

01:10:02.730 --> 01:10:06.630
but then has to go through
P2, and has to go through P3.

01:10:06.630 --> 01:10:09.720
But it's OK because
the farther we

01:10:09.720 --> 01:10:14.040
go right, we have this sparsity
condition that tells us

01:10:14.040 --> 01:10:17.689
basically the points
we're looking at are--

01:10:17.689 --> 01:10:19.230
the number of points
we're looking at

01:10:19.230 --> 01:10:20.563
are getting smaller and smaller.

01:10:23.560 --> 01:10:26.070
So I'll wave my hands
a little bit here,

01:10:26.070 --> 01:10:30.390
but the claim is it's
a geometric series.

01:10:33.130 --> 01:10:37.860
This needs a formal proof, but
we won't go through it here.

01:10:37.860 --> 01:10:43.680
Decreasing-- so this
is the query bound.

01:10:43.680 --> 01:10:47.670
The number of scanned
points is order output size.

01:10:47.670 --> 01:10:50.514
So you have to check that no
matter where you start in Pi--

01:10:50.514 --> 01:10:51.930
that's the little
bit tricky part.

01:10:51.930 --> 01:10:53.760
We're not looking at all of Pi.

01:10:53.760 --> 01:10:56.760
We're looking at
some of Pi and then

01:10:56.760 --> 01:10:59.222
we're going to the
right from there.

01:10:59.222 --> 01:11:00.180
Actually, is that true?

01:11:00.180 --> 01:11:01.596
Maybe we always
look at all of Pi.

01:11:04.010 --> 01:11:05.010
Let me think about this.

01:11:08.410 --> 01:11:10.170
I think we do, actually.

01:11:10.170 --> 01:11:12.477
Sorry.

01:11:12.477 --> 01:11:13.560
That's what we did before.

01:11:18.360 --> 01:11:20.789
We basically figure out
where we are in y-coordinate.

01:11:20.789 --> 01:11:22.080
That was the overall structure.

01:11:22.080 --> 01:11:25.290
We had a Van Emde
Boas search tree on y.

01:11:25.290 --> 01:11:29.400
So all we know at this point is
the y-coordinate of our search.

01:11:29.400 --> 01:11:31.830
And so we use that to
determine which of the Pi's we

01:11:31.830 --> 01:11:37.120
go to, based on where the
yi becomes no longer dense.

01:11:39.279 --> 01:11:41.820
And then we're going to have to
search through that entire Pi

01:11:41.820 --> 01:11:47.100
and potentially more
of them because this

01:11:47.100 --> 01:11:48.390
is no longer an Si.

01:11:48.390 --> 01:11:51.150
It's just doing the
things to the left.

01:11:51.150 --> 01:11:56.250
And so if we're lucky,
the Pi we're looking at,

01:11:56.250 --> 01:12:01.647
or the query we're doing, is
not to the right of this point.

01:12:01.647 --> 01:12:02.730
OK, maybe it's right here.

01:12:02.730 --> 01:12:03.563
That would be great.

01:12:03.563 --> 01:12:06.809
Then all our answers are done.

01:12:06.809 --> 01:12:08.850
If our query is here, that
would have been dense,

01:12:08.850 --> 01:12:11.400
so we would have done
it at an earlier stage.

01:12:11.400 --> 01:12:13.860
Our query might be
down here though.

01:12:13.860 --> 01:12:19.182
When the query's down here, we
need to report on these points.

01:12:19.182 --> 01:12:20.640
Then we're going
to have to do more

01:12:20.640 --> 01:12:22.860
and that's going
to be Pi plus 1.

01:12:22.860 --> 01:12:25.950
So we'll do more and
more Pi's until we

01:12:25.950 --> 01:12:30.930
get to our actual query here.

01:12:30.930 --> 01:12:33.270
But in any case, the claim
is that this is geometrically

01:12:33.270 --> 01:12:35.110
decreasing by the
same charging scheme.

01:12:38.150 --> 01:12:40.220
OK, that's two out
of the three claims.

01:12:40.220 --> 01:12:46.530
There's one more, which
is closely related.

01:12:46.530 --> 01:12:48.520
It's still about
the query problem.

01:12:48.520 --> 01:12:51.490
What we haven't shown is that
we actually find all the points.

01:12:51.490 --> 01:12:53.760
This is what you might
call correctness.

01:12:57.640 --> 01:13:01.360
To prove this, what
we need to say--

01:13:01.360 --> 01:13:03.890
what we claim is that
after you do the P1's--

01:13:07.280 --> 01:13:08.430
and now you do the P2's.

01:13:12.910 --> 01:13:13.900
Well, I'll tell you.

01:13:13.900 --> 01:13:18.790
The claim is that you visited
some x-coordinates here.

01:13:18.790 --> 01:13:21.380
The Pi's were all the things
up to some x-coordinate.

01:13:21.380 --> 01:13:23.530
Claim that the very
next point in here,

01:13:23.530 --> 01:13:27.231
in P2, has a smaller
x-coordinate than what you just

01:13:27.231 --> 01:13:27.730
did.

01:13:30.600 --> 01:13:34.110
I think that should be clear
because presumably there

01:13:34.110 --> 01:13:38.610
are some points in here, and
so the very next Pi, it's

01:13:38.610 --> 01:13:40.050
restricted within
this red thing,

01:13:40.050 --> 01:13:41.700
but it's going to be up
to some x-coordinate.

01:13:41.700 --> 01:13:43.116
So you're basically
starting over.

01:13:43.116 --> 01:13:47.310
Every time you go to the Pi's,
you're starting over in x.

01:13:47.310 --> 01:13:50.010
Go back to minus infinity in x.

01:13:50.010 --> 01:13:53.130
So the idea is the picture
will look something like this.

01:13:53.130 --> 01:13:55.266
You start at minus infinity,
you read some points.

01:13:55.266 --> 01:13:56.890
At some point, you
run out of the Pi's.

01:13:56.890 --> 01:13:59.597
Then you start over again,
you read some smaller set

01:13:59.597 --> 01:14:00.180
of the points.

01:14:00.180 --> 01:14:01.471
Maybe you get a little farther.

01:14:01.471 --> 01:14:04.140
You start over again,
read a little farther.

01:14:04.140 --> 01:14:07.050
At some point, you're going
to reach your threshold x.

01:14:07.050 --> 01:14:08.990
That's when you stop.

01:14:08.990 --> 01:14:10.726
So that's correctness.

01:14:13.510 --> 01:14:15.260
I feel like I need
another sentence there.

01:14:19.310 --> 01:14:23.180
Once your Pi encompasses
your x range,

01:14:23.180 --> 01:14:24.620
that's going to
have your answer.

01:14:24.620 --> 01:14:25.590
Then you're done.

01:14:25.590 --> 01:14:27.260
So that's this moment.

01:14:27.260 --> 01:14:32.690
And so the only worry is that
an early Pi, basically, or maybe

01:14:32.690 --> 01:14:35.450
the next Pi does
this, and then we

01:14:35.450 --> 01:14:36.757
do this or something like this.

01:14:36.757 --> 01:14:38.840
That never happens basically
because you're always

01:14:38.840 --> 01:14:39.770
resetting x range.

01:14:39.770 --> 01:14:42.320
And so your x will always
start over to something

01:14:42.320 --> 01:14:44.060
less than what you had.

01:14:44.060 --> 01:14:46.760
And so the
termination condition,

01:14:46.760 --> 01:14:51.090
which I probably didn't
write down here, but which is

01:14:51.090 --> 01:14:54.140
stop when your x-coordinate
is bigger than what you want.

01:14:54.140 --> 01:14:56.210
Never terminates early.

01:14:56.210 --> 01:14:59.480
Therefore we get all the
points we care about.

01:14:59.480 --> 01:15:01.250
OK, a little bit
hand-wavy, but that

01:15:01.250 --> 01:15:04.520
is why this structure works.

01:15:04.520 --> 01:15:10.740
It's a very weird set up, but
linear sized, and you just

01:15:10.740 --> 01:15:12.510
jump into the right
point in the array,

01:15:12.510 --> 01:15:15.300
start reading, throw
away the points that

01:15:15.300 --> 01:15:17.790
aren't in your range because
they just happen to be there.

01:15:17.790 --> 01:15:20.270
Those would be these
points up here.

01:15:20.270 --> 01:15:24.560
Throw away duplicates.

01:15:24.560 --> 01:15:28.010
Just output the points in
your range and it gives you,

01:15:28.010 --> 01:15:31.790
magically, all the points
in here by a linear scan.

01:15:31.790 --> 01:15:35.310
I still find this so
weird, but it's true.

01:15:38.050 --> 01:15:40.740
Truth is stranger
than fiction, I guess.

01:15:40.740 --> 01:15:41.620
They're fun facts.

01:15:41.620 --> 01:15:45.220
You can actually compute this
thing in the sorting bound.

01:15:45.220 --> 01:15:48.700
So pre-processing is just sort.

01:15:48.700 --> 01:15:52.125
I won't prove that here.

01:15:52.125 --> 01:15:53.440
So this was two sided.

01:15:53.440 --> 01:15:56.080
Let me briefly tell
you how to solve

01:15:56.080 --> 01:15:58.300
three sided and four sided.

01:15:58.300 --> 01:16:02.830
We basically already did
this one, which was--

01:16:02.830 --> 01:16:05.110
I'll remind you
what it looks like.

01:16:10.550 --> 01:16:15.160
So you have a binary
tree, and in each node

01:16:15.160 --> 01:16:18.400
you store two
augmented structures.

01:16:18.400 --> 01:16:20.910
One which can do ranged
queries like this,

01:16:20.910 --> 01:16:23.707
and one which can do inverted
range queries like this.

01:16:23.707 --> 01:16:24.790
This should look familiar.

01:16:27.900 --> 01:16:31.930
And so you do a search on--

01:16:31.930 --> 01:16:34.420
let's say we want
to do this thing.

01:16:34.420 --> 01:16:39.490
So we have x1, x2, y2.

01:16:39.490 --> 01:16:41.740
You search for x1,
you search for x2.

01:16:41.740 --> 01:16:50.140
You find the LCA and then in
this subtree, you do a search.

01:16:50.140 --> 01:16:52.900
In this subtree, you already
know that you're less than x2,

01:16:52.900 --> 01:17:02.650
and so you do the x1,
y2 search in this node.

01:17:02.650 --> 01:17:09.460
And then in the right subtree,
you do the x2, y2 search.

01:17:09.460 --> 01:17:15.170
You take the union of those two
results and that is this query.

01:17:15.170 --> 01:17:18.370
That's how we did it before.

01:17:18.370 --> 01:17:19.560
No difficulty here.

01:17:19.560 --> 01:17:22.240
And the point is,
you can build this,

01:17:22.240 --> 01:17:24.040
put it in a Van
Emde Boas layout.

01:17:24.040 --> 01:17:25.960
You do this search,
you do this search,

01:17:25.960 --> 01:17:28.300
you find the LCA in
log base B of N--

01:17:28.300 --> 01:17:30.760
to check that everything
works, cache obviously.

01:17:30.760 --> 01:17:33.700
Then these structures are just
structures which we already

01:17:33.700 --> 01:17:37.120
built, and so yes,
we lose a lag factor

01:17:37.120 --> 01:17:42.190
because every point appears
in log data structures,

01:17:42.190 --> 01:17:44.419
but that's it.

01:17:44.419 --> 01:17:45.710
Everything else works the same.

01:17:45.710 --> 01:17:48.430
So we get N log N
space log base B of N

01:17:48.430 --> 01:17:51.144
plus output over B query.

01:17:51.144 --> 01:17:53.560
Because now we just have to
do two queries instead of one.

01:17:53.560 --> 01:17:56.908
We don't there's a log factor.

01:17:56.908 --> 01:18:01.810
That's the trick we did
before OK, that was easy.

01:18:05.510 --> 01:18:08.071
One more.

01:18:08.071 --> 01:18:09.070
So that was three sided.

01:18:13.580 --> 01:18:15.450
Next is four sided.

01:18:20.850 --> 01:18:23.520
Four sided, of course, we could
do exactly the same thing.

01:18:23.520 --> 01:18:25.035
Lose another log
factor in space.

01:18:29.040 --> 01:18:33.300
Maintain log base B of N plus
output over B query time.

01:18:33.300 --> 01:18:36.120
But I want to do
slightly better and this

01:18:36.120 --> 01:18:39.150
is a trick we could have done
in internal memory as well.

01:18:39.150 --> 01:18:42.270
But I have two minutes
to show it to you.

01:18:42.270 --> 01:18:44.360
So here's a bonus.

01:18:46.980 --> 01:18:49.230
Didn't have to do this in
external memory context,

01:18:49.230 --> 01:18:49.800
but we can.

01:18:52.500 --> 01:18:54.690
Four sided.

01:18:54.690 --> 01:18:58.200
So we're going to do
the same thing, but not

01:18:58.200 --> 01:19:00.150
on a binary tree.

01:19:00.150 --> 01:19:05.120
Take this binary tree, this
is sorted by x, I suppose.

01:19:05.120 --> 01:19:06.910
This is key on x.

01:19:09.540 --> 01:19:16.200
Instead of making it binary,
make it root log [? N ary. ?]

01:19:16.200 --> 01:19:18.300
So imagine taking
the binary tree,

01:19:18.300 --> 01:19:22.130
taking little chunks, which
have size square root log

01:19:22.130 --> 01:19:27.630
N. Its capital N. And
imagine contracting

01:19:27.630 --> 01:19:29.220
those chunks into single nodes.

01:19:29.220 --> 01:19:32.280
So we have a single note
which has square root

01:19:32.280 --> 01:19:38.290
log N. Children [INAUDIBLE]
has square root log N children.

01:19:38.290 --> 01:19:41.470
This is all static.

01:19:41.470 --> 01:19:42.190
And so on.

01:19:42.190 --> 01:19:45.060
Otherwise, the same.

01:19:45.060 --> 01:19:47.890
The augmentation is going to
be a little bit different.

01:19:47.890 --> 01:19:50.490
If we look at a
node, we're going

01:19:50.490 --> 01:19:52.950
to store the same
things we had before,

01:19:52.950 --> 01:19:57.270
which was this kind of query,
and this kind of query.

01:19:57.270 --> 01:20:00.120
We're going to store
a little bit more.

01:20:00.120 --> 01:20:04.230
Namely, for any
interval of children,

01:20:04.230 --> 01:20:08.010
like here you have some start
child and some end child.

01:20:08.010 --> 01:20:12.060
I want to store for all the
points that are down there.

01:20:12.060 --> 01:20:16.290
For this thing, store a
regular binary search tree

01:20:16.290 --> 01:20:20.650
on y for those points.

01:20:20.650 --> 01:20:21.510
Why?

01:20:21.510 --> 01:20:24.090
Because if we do a search--

01:20:24.090 --> 01:20:32.610
OK, same deal-- we
find the LCA of x1, x1?

01:20:32.610 --> 01:20:33.920
I don't know.

01:20:33.920 --> 01:20:35.760
Let's say it's on x.

01:20:35.760 --> 01:20:39.940
We'll have to do it
again on y whatever.

01:20:39.940 --> 01:20:41.820
So here's the LCA.

01:20:41.820 --> 01:20:43.730
Let's say there's
a lot of children.

01:20:43.730 --> 01:20:52.410
OK, maybe here is
x1 and here is x2.

01:20:52.410 --> 01:20:56.490
So in this subtree, we do this--

01:20:56.490 --> 01:21:00.810
sorry, we do this
range query because we

01:21:00.810 --> 01:21:03.330
want to go from x1 to infinity.

01:21:03.330 --> 01:21:07.440
Over in this subtree, we
want to do this range query

01:21:07.440 --> 01:21:11.430
because we want to go from
negative infinity to x2.

01:21:11.430 --> 01:21:13.620
But then there's all
this stuff in the middle.

01:21:13.620 --> 01:21:17.310
I don't want to have to do a
query for every single tree.

01:21:17.310 --> 01:21:19.410
Instead, I have this
augmentation that

01:21:19.410 --> 01:21:22.260
says for this interval,
here are all the points

01:21:22.260 --> 01:21:24.870
sorted by x-coordinate.

01:21:24.870 --> 01:21:27.030
I guess we're doing it this way.

01:21:30.090 --> 01:21:33.780
Fine, so then it
is a range query.

01:21:33.780 --> 01:21:37.170
I want to know what
are all the points.

01:21:37.170 --> 01:21:39.390
Whoa, this is confusing.

01:21:39.390 --> 01:21:41.190
I feel like I've
missed something here.

01:21:41.190 --> 01:21:41.780
No, this on y.

01:21:41.780 --> 01:21:42.280
Sorry.

01:21:45.360 --> 01:21:47.460
These points I've
got sorted by y.

01:21:47.460 --> 01:21:50.250
So I should draw
it the other way.

01:21:50.250 --> 01:21:54.240
These points we already know
are in-between x1 and x2 in x.

01:21:54.240 --> 01:21:56.320
We've already solved
the x problem here.

01:21:56.320 --> 01:22:03.510
So now I just need to restrict
to the y range from y1 to y2.

01:22:03.510 --> 01:22:06.330
In these trees, these
already match in x.

01:22:06.330 --> 01:22:08.610
I just need to make
sure they match in y.

01:22:08.610 --> 01:22:10.910
So I do a regular 1D range tree.

01:22:10.910 --> 01:22:12.570
I search for y1,
I search for y2,

01:22:12.570 --> 01:22:14.490
take all the points in between.

01:22:14.490 --> 01:22:18.510
This is cheap if I just have a
regular old binary search tree.

01:22:18.510 --> 01:22:22.380
Now, this thing has linear size.

01:22:22.380 --> 01:22:29.340
This thing has-- sorry,
I think I actually need--

01:22:29.340 --> 01:22:31.440
I should have a three
sided range query.

01:22:31.440 --> 01:22:33.780
Thanks.

01:22:33.780 --> 01:22:37.080
These should be three
sided because here I

01:22:37.080 --> 01:22:40.044
know that I've got the
right side covered already

01:22:40.044 --> 01:22:42.210
in this tree, I've got the
left side covered already

01:22:42.210 --> 01:22:44.970
in this tree, but I still need
the remaining three sides.

01:22:44.970 --> 01:22:46.980
In here, I only
need these two sides

01:22:46.980 --> 01:22:50.050
because I've already
got x1 and x2 covered.

01:22:50.050 --> 01:22:51.471
OK, so this is cheap.

01:22:51.471 --> 01:22:53.220
I only need a linear
space data structure.

01:22:53.220 --> 01:22:54.570
This thing is not so cheap.

01:22:54.570 --> 01:22:56.940
I'm using the previous
data structure.

01:22:56.940 --> 01:22:58.950
This thing, which
has N log N size,

01:22:58.950 --> 01:23:01.330
these are three
sided range queries.

01:23:01.330 --> 01:23:03.000
Sorry for drawing it wrong.

01:23:05.550 --> 01:23:08.760
So I need two three
sided structures.

01:23:08.760 --> 01:23:10.290
Then I need actually
a whole bunch

01:23:10.290 --> 01:23:13.440
of these structures because
this was for every interval.

01:23:13.440 --> 01:23:15.660
But conveniently, they're
only log N intervals

01:23:15.660 --> 01:23:17.500
because there's
root log N children.

01:23:17.500 --> 01:23:21.410
So root log N squared is
log N. So there's root N,

01:23:21.410 --> 01:23:24.460
but then we need log N of them.

01:23:24.460 --> 01:23:27.260
And so that's why these
things balance out.

01:23:27.260 --> 01:23:28.820
See?

01:23:28.820 --> 01:23:34.790
So normally, this would be N log
squared N because every point

01:23:34.790 --> 01:23:37.040
would appear in log N trees.

01:23:37.040 --> 01:23:40.220
But now the height
of my tree is merely

01:23:40.220 --> 01:23:44.550
log N over log log
N with a factor

01:23:44.550 --> 01:23:49.170
2 out here because I
have a square root here.

01:23:49.170 --> 01:23:51.950
OK, so the tree has height
log N over log log N.

01:23:51.950 --> 01:23:54.770
So each point only appears
in log N over log log N

01:23:54.770 --> 01:23:55.760
structures.

01:23:55.760 --> 01:23:58.580
Each of them needs a
structure size N log N.

01:23:58.580 --> 01:24:03.470
So we end up with N log
squared N over log log N space.

01:24:03.470 --> 01:24:05.150
Kind of crazy, but
this is how you

01:24:05.150 --> 01:24:07.850
get that last little bit of
log log N space improvement

01:24:07.850 --> 01:24:10.400
by contracting nodes,
doing a simpler data

01:24:10.400 --> 01:24:14.780
structure for these
middle children,

01:24:14.780 --> 01:24:16.290
and just focusing on--

01:24:16.290 --> 01:24:19.280
The left child and the right
child you have to do one three

01:24:19.280 --> 01:24:20.930
sided call, but
then the middle is

01:24:20.930 --> 01:24:22.140
a very simple two sided call.

01:24:22.140 --> 01:24:26.960
It's just a 1D structure
and so it's really cheap.

01:24:26.960 --> 01:24:28.690
That's it.