WEBVTT

00:00:01.550 --> 00:00:03.920
The following content is
provided under a Creative

00:00:03.920 --> 00:00:05.310
Commons license.

00:00:05.310 --> 00:00:07.520
Your support will help
MIT OpenCourseWare

00:00:07.520 --> 00:00:11.610
continue to offer high quality
educational resources for free.

00:00:11.610 --> 00:00:14.180
To make a donation, or to
view additional materials

00:00:14.180 --> 00:00:18.140
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:18.140 --> 00:00:19.026
at ocw.mit.edu.

00:00:23.018 --> 00:00:23.810
GILBERT STRANG: OK.

00:00:23.810 --> 00:00:27.070
So what I promised, and
now I'm going to do it,

00:00:27.070 --> 00:00:34.010
to talk about gradient
descent and its descendants.

00:00:34.010 --> 00:00:37.190
So from the basic
gradient descent formula,

00:00:37.190 --> 00:00:38.390
which we all know--

00:00:38.390 --> 00:00:40.640
let me just write that down--

00:00:40.640 --> 00:00:47.150
the new point is the old point.

00:00:47.150 --> 00:00:49.280
We're going downwards,
so with a minus

00:00:49.280 --> 00:00:52.520
sign that's the step size.

00:00:52.520 --> 00:00:57.260
And we compute the
gradient at XK.

00:00:57.260 --> 00:01:00.110
So we're descending
in the direction

00:01:00.110 --> 00:01:02.810
of the negative gradient.

00:01:02.810 --> 00:01:09.470
And that's the basic formula,
and is in every book studied.

00:01:09.470 --> 00:01:18.980
So my main reference for
some of these lectures

00:01:18.980 --> 00:01:24.910
is the book by Stephen Boyd
and Lieven Vandenberghe.

00:01:24.910 --> 00:01:30.350
And I mention again, Professor
Boyd is talking, in this room,

00:01:30.350 --> 00:01:35.360
next week Wednesday,
Thursday and he's speaking

00:01:35.360 --> 00:01:38.870
somewhere on Friday at 4:30--

00:01:38.870 --> 00:01:41.310
and of course,
about optimization.

00:01:41.310 --> 00:01:43.950
And he's a good lecturer,
yeah, very good.

00:01:43.950 --> 00:01:45.200
OK.

00:01:45.200 --> 00:01:49.790
So there's steepest descent,
and I've redrawn my picture

00:01:49.790 --> 00:01:50.780
from last time.

00:01:50.780 --> 00:01:53.840
Now I'll go over there
and look at that picture.

00:01:53.840 --> 00:01:56.390
But let me say what's coming.

00:01:56.390 --> 00:01:59.350
So that's pretty standard--

00:01:59.350 --> 00:02:02.800
very standard, you could say.

00:02:02.800 --> 00:02:09.889
Then this is the improvement
that is widely used.

00:02:09.889 --> 00:02:11.980
Adding in something
called momentum

00:02:11.980 --> 00:02:15.920
to avoid the zigzag that
we're going to see over there.

00:02:15.920 --> 00:02:17.500
And there's another
way to do it.

00:02:17.500 --> 00:02:20.640
There's a Russian
guy named Nesterov.

00:02:20.640 --> 00:02:22.920
His papers are not easy
to read, but they've

00:02:22.920 --> 00:02:25.330
got serious content.

00:02:25.330 --> 00:02:29.370
And one thing he did
was find an alternative

00:02:29.370 --> 00:02:37.110
to momentum that also
accelerated the descent.

00:02:37.110 --> 00:02:42.870
So this produces-- these
both produce faster descent

00:02:42.870 --> 00:02:45.600
than the ordinary one.

00:02:45.600 --> 00:02:46.410
OK.

00:02:46.410 --> 00:02:48.930
And then you know,
looking ahead,

00:02:48.930 --> 00:02:52.410
that for problems
of machine learning,

00:02:52.410 --> 00:02:56.750
they're so large
that the gradient--

00:02:56.750 --> 00:02:58.440
we have so many variables--

00:02:58.440 --> 00:03:01.740
all those weights are variables.

00:03:01.740 --> 00:03:07.420
And that could-- hundreds of
thousands is not uncommon.

00:03:07.420 --> 00:03:12.690
So then the gradient becomes
a pretty big calculation,

00:03:12.690 --> 00:03:15.070
and we just don't have
to do it all at once.

00:03:15.070 --> 00:03:16.440
We don't have to change--

00:03:16.440 --> 00:03:22.900
so XK is a vector of all the
weights, or-- and using--

00:03:22.900 --> 00:03:29.740
and our equations are
matching the training data.

00:03:29.740 --> 00:03:33.030
So we don't have to use all
the training data at once,

00:03:33.030 --> 00:03:34.290
and we don't.

00:03:34.290 --> 00:03:38.970
We take a batch of
training data, like one.

00:03:38.970 --> 00:03:42.240
But that's sort of inefficient
in the opposite direction,

00:03:42.240 --> 00:03:43.920
to do them one at a time.

00:03:43.920 --> 00:03:46.210
So we don't know want to
do them one at a time,

00:03:46.210 --> 00:03:49.710
but we don't want to do
all million at a time.

00:03:49.710 --> 00:03:53.580
So the compromise
is a mini batch.

00:03:53.580 --> 00:03:59.490
So stochastic gradient descent
does a mini batch at a time--

00:03:59.490 --> 00:04:04.710
a mini batch of
training, of samples

00:04:04.710 --> 00:04:08.730
training data each step.

00:04:12.620 --> 00:04:15.330
And it can choose
them stochastically--

00:04:15.330 --> 00:04:19.560
meaning randomly, or
more systematically--

00:04:19.560 --> 00:04:23.190
but we do a batch at a time.

00:04:23.190 --> 00:04:26.266
And that will come after the--

00:04:26.266 --> 00:04:32.340
it'll come next week after a
marathon, of course, on Monday.

00:04:32.340 --> 00:04:33.510
OK.

00:04:33.510 --> 00:04:36.810
So let me just go back to
that picture for a moment,

00:04:36.810 --> 00:04:39.420
but then the real
content of today

00:04:39.420 --> 00:04:43.970
is this one with momentum added.

00:04:43.970 --> 00:04:44.550
OK.

00:04:44.550 --> 00:04:48.720
I just-- I probably haven't
got the picture perfect yet.

00:04:48.720 --> 00:04:56.620
I'm just not an artist,
but I think I'm closer.

00:04:56.620 --> 00:05:01.120
So this is-- those
are the level sets.

00:05:01.120 --> 00:05:06.550
Those are the sets f
of x equal constant.

00:05:06.550 --> 00:05:11.590
And in our model problem,
f of x is x1 squared--

00:05:11.590 --> 00:05:16.750
or let's say x squared plus
b-y squared equal constant with

00:05:16.750 --> 00:05:18.280
small b--

00:05:18.280 --> 00:05:22.240
b below 1 and maybe far below 1.

00:05:22.240 --> 00:05:23.260
So those are ellipses.

00:05:26.277 --> 00:05:27.860
Those are the equations
of an ellipse,

00:05:27.860 --> 00:05:30.050
and that's what I tried to draw.

00:05:30.050 --> 00:05:32.240
And if b is small,
then the ellipses

00:05:32.240 --> 00:05:35.300
are long and thin like that.

00:05:35.300 --> 00:05:38.510
And now, what's the picture?

00:05:38.510 --> 00:05:42.470
You start with a point x
nought, and you descend

00:05:42.470 --> 00:05:44.400
in the steepest direction.

00:05:44.400 --> 00:05:47.740
So the steepest direction is
perpendicular to the level set,

00:05:47.740 --> 00:05:48.240
right?

00:05:48.240 --> 00:05:49.650
Perpendicular to the ellipse.

00:05:49.650 --> 00:05:51.270
So you're down, down, down.

00:05:51.270 --> 00:05:53.100
You're passing
through more ellipses,

00:05:53.100 --> 00:05:55.470
more ellipses, more ellipses.

00:05:55.470 --> 00:05:58.620
Eventually, your tangent to a--

00:05:58.620 --> 00:06:00.840
it seems to me it
has to be tangent.

00:06:00.840 --> 00:06:05.180
I didn't read this, but
looks reasonable to me that

00:06:05.180 --> 00:06:11.760
the farthest in level
set-- farthest in ellipse--

00:06:11.760 --> 00:06:15.220
you're tangent to, and then
you start going up again.

00:06:15.220 --> 00:06:19.440
So that's the optimal point
to stop to end that step.

00:06:19.440 --> 00:06:21.810
And then where does
the next step go?

00:06:21.810 --> 00:06:23.420
Well, you're here.

00:06:23.420 --> 00:06:25.080
You're on an ellipse.

00:06:25.080 --> 00:06:26.520
That's a level set.

00:06:26.520 --> 00:06:28.980
You want to move in
the gradient direction.

00:06:28.980 --> 00:06:31.350
That's perpendicular
to the level set.

00:06:31.350 --> 00:06:33.870
So you're going
down somewhere here,

00:06:33.870 --> 00:06:37.620
and you're passing again
through more and more ellipses,

00:06:37.620 --> 00:06:44.760
until you're tangent to
a smaller ellipse here.

00:06:44.760 --> 00:06:48.280
And you see the zigzag pattern.

00:06:48.280 --> 00:06:50.460
And that zigzag
pattern is what we

00:06:50.460 --> 00:07:00.240
see, by formula, in Boyd's book,
and many other places, too.

00:07:00.240 --> 00:07:06.990
The formula has those
powers of the magic number.

00:07:06.990 --> 00:07:08.850
So we start at the--

00:07:08.850 --> 00:07:21.300
start at the point b1,
and follow this path.

00:07:21.300 --> 00:07:27.120
Then the X's are the same
b times this quantity

00:07:27.120 --> 00:07:29.670
to the kth power.

00:07:29.670 --> 00:07:34.180
And here is that quantity,
b minus 1 over b plus 1.

00:07:34.180 --> 00:07:38.410
So you see, for a small b,
that's a negative number.

00:07:38.410 --> 00:07:42.720
So it's flipping sine in the
X's, as we saw in the picture.

00:07:42.720 --> 00:07:45.690
At least that part of
the picture is correct.

00:07:45.690 --> 00:07:48.390
The y's don't flip sine.

00:07:48.390 --> 00:07:55.380
So this was XK, and
when k is 0, we got b.

00:07:55.380 --> 00:08:03.600
YK is, I think, is
not flipping sine.

00:08:03.600 --> 00:08:05.400
So that looks good.

00:08:05.400 --> 00:08:09.780
And then FK-- the value of f--

00:08:09.780 --> 00:08:12.690
also was the same quantity.

00:08:12.690 --> 00:08:20.910
FK is that same quantity
to the kth times f0.

00:08:20.910 --> 00:08:24.810
So that quantity's
all important.

00:08:24.810 --> 00:08:27.930
And so the purpose
of today's lecture,

00:08:27.930 --> 00:08:33.159
is to tell you what
the momentum term--

00:08:33.159 --> 00:08:35.090
what improvement--
what change that

00:08:35.090 --> 00:08:39.350
brings in the basic
steepest descent formula.

00:08:39.350 --> 00:08:41.870
I'm going to add on
another term, which

00:08:41.870 --> 00:08:43.940
is going to have some--

00:08:43.940 --> 00:08:48.120
give us some memory
of the previous step.

00:08:48.120 --> 00:08:58.080
And so when I do that, I want
to track that kind of descent

00:08:58.080 --> 00:09:00.830
for the new--

00:09:00.830 --> 00:09:06.410
for the accelerated descent,
and track it and see

00:09:06.410 --> 00:09:11.190
what improvement the
momentum term brings.

00:09:11.190 --> 00:09:14.450
And so the final result
will be to tell you

00:09:14.450 --> 00:09:15.860
the improvement in the--

00:09:15.860 --> 00:09:18.380
produced by the momentum term.

00:09:18.380 --> 00:09:21.740
Maybe while I have
your attention,

00:09:21.740 --> 00:09:24.140
I'll tell you what it is now.

00:09:24.140 --> 00:09:29.430
And then will come the
details, the algebra.

00:09:29.430 --> 00:09:33.610
And to me-- so this
is as my own thought--

00:09:36.360 --> 00:09:38.940
it's a miracle that
the algebra, which

00:09:38.940 --> 00:09:43.740
is straightforward-- you really
see the value of eigenvectors.

00:09:43.740 --> 00:09:49.410
We explained eigenvectors in
class, but here you see why--

00:09:49.410 --> 00:09:51.310
how to use them.

00:09:51.310 --> 00:09:54.180
That is really a good exercise.

00:09:54.180 --> 00:10:03.940
But to me it's a miracle that
the expression with momentum

00:10:03.940 --> 00:10:07.450
is very much like that
expression, but different,

00:10:07.450 --> 00:10:08.770
of course.

00:10:08.770 --> 00:10:18.100
The decay-- the term that tells
you how fast the decay is--

00:10:18.100 --> 00:10:19.900
is smaller.

00:10:19.900 --> 00:10:21.520
So you're taking kth power.

00:10:21.520 --> 00:10:25.070
So let me-- I'll write that
down, if that's all right.

00:10:25.070 --> 00:10:26.560
I didn't plan to do--

00:10:26.560 --> 00:10:34.010
to reveal the final result at
the beginning of the lecture.

00:10:34.010 --> 00:10:37.560
But I think you want to
see where we're going.

00:10:37.560 --> 00:10:48.500
So with momentum-- and we
have to see what that means--

00:10:48.500 --> 00:10:56.810
this term of 1 minus b
over 1 plus b becomes--

00:10:56.810 --> 00:11:06.090
changes to-- 1 minus
square root of b over 1

00:11:06.090 --> 00:11:08.820
plus square root of b.

00:11:08.820 --> 00:11:12.930
So I mentioned that
before, but I don't think

00:11:12.930 --> 00:11:16.950
I wrote it down as clearly.

00:11:16.950 --> 00:11:20.940
So the miracle to me is to
get such a nice expression

00:11:20.940 --> 00:11:22.080
for the--

00:11:22.080 --> 00:11:31.030
because you'll see the algebra
is-- it works, but it involves

00:11:31.030 --> 00:11:34.150
more terms because
of momentum, involves

00:11:34.150 --> 00:11:37.420
doing a minimization
of eigenvalues,

00:11:37.420 --> 00:11:39.460
and yet it comes out nicely.

00:11:39.460 --> 00:11:42.860
And then you have to see
the importance of that.

00:11:42.860 --> 00:11:45.340
So let me-- I will just
take the same example

00:11:45.340 --> 00:11:47.290
that I mentioned before.

00:11:47.290 --> 00:11:55.750
If b is 1 over 100, then
this is 0.99 over 1.01.

00:11:55.750 --> 00:11:58.490
I think that these--

00:11:58.490 --> 00:12:05.470
there's a square here, 2k.

00:12:05.470 --> 00:12:10.830
So if we're-- so I'll just
keep the square there,

00:12:10.830 --> 00:12:14.460
no big change, but I'm looking
at-- now here-- at the square.

00:12:17.450 --> 00:12:21.700
Maybe squares are everywhere.

00:12:21.700 --> 00:12:22.200
OK.

00:12:22.200 --> 00:12:26.730
So that's close to 1.

00:12:26.730 --> 00:12:29.540
And now let's compare
that with what we have.

00:12:29.540 --> 00:12:32.510
So if b is 1 over 100--

00:12:32.510 --> 00:12:35.120
so I'm taking b
to be 1 over 100--

00:12:35.120 --> 00:12:37.460
and square root
of b is 1 over 10.

00:12:37.460 --> 00:12:45.090
So this is 0.9 over 1.1 squared.

00:12:45.090 --> 00:12:46.790
And there's a
tremendous-- that's

00:12:46.790 --> 00:12:49.950
a lot smaller than that is.

00:12:49.950 --> 00:12:51.180
Right.

00:12:51.180 --> 00:12:57.120
9/10-- 9 over 11,
compared to 99 over 101.

00:12:57.120 --> 00:13:03.690
This one is
definitely-- oh, sorry.

00:13:03.690 --> 00:13:10.560
Yeah, this reduction factor
is well below that one.

00:13:10.560 --> 00:13:11.520
So it's a good thing.

00:13:11.520 --> 00:13:14.040
It's worth doing.

00:13:14.040 --> 00:13:17.020
And now what does it involve?

00:13:17.020 --> 00:13:25.240
So I'll write down the
expression for the stochastic--

00:13:25.240 --> 00:13:26.200
here we go.

00:13:26.200 --> 00:13:27.070
OK.

00:13:27.070 --> 00:13:29.530
So here's one way to see it.

00:13:29.530 --> 00:13:36.130
The new X is the old
X minus the gradient.

00:13:40.330 --> 00:13:47.300
And now comes an extra term,
which gives us a little memory.

00:13:47.300 --> 00:13:50.640
Well, sorry.

00:13:50.640 --> 00:13:54.700
The algebra is slightly nicer
if I write it a little bit

00:13:54.700 --> 00:13:56.100
differently.

00:13:56.100 --> 00:14:06.480
I'll create a new quantity,
ZK, with a step size.

00:14:06.480 --> 00:14:06.980
OK.

00:14:15.800 --> 00:14:19.100
So if I took ZK to
be just the gradient,

00:14:19.100 --> 00:14:20.540
that would be steepest descent.

00:14:20.540 --> 00:14:22.370
Nothing has changed.

00:14:22.370 --> 00:14:26.240
But instead, I'm
going to take ZK--

00:14:26.240 --> 00:14:28.400
well, it's leading term
will be the gradient.

00:14:31.530 --> 00:14:34.080
But here comes
the momentum term.

00:14:34.080 --> 00:14:36.930
I add on a multiple beta.

00:14:36.930 --> 00:14:40.050
One way to do it is
of the previous Z.

00:14:40.050 --> 00:14:42.600
So the Z is the
search direction.

00:14:42.600 --> 00:14:44.305
Z is the gradient
you're traveling.

00:14:44.305 --> 00:14:47.730
It is the direction
you're moving.

00:14:47.730 --> 00:14:51.630
So it's different from
that direction there.

00:14:51.630 --> 00:14:56.430
That direction was the gradient.

00:14:56.430 --> 00:15:02.310
This direction is the
gradient corrected by a memory

00:15:02.310 --> 00:15:05.020
term, a momentum term.

00:15:05.020 --> 00:15:09.780
And one way to interpret that
is to say that that ball--

00:15:09.780 --> 00:15:14.730
is to think of a heavy ball,
instead of just a point.

00:15:14.730 --> 00:15:17.610
I think of a heavy ball.

00:15:17.610 --> 00:15:27.060
It, instead of bouncing back and
forth as uselessly as this one,

00:15:27.060 --> 00:15:28.500
it tends to--

00:15:28.500 --> 00:15:32.640
it still bounces, of course,
on the sides of the level set--

00:15:32.640 --> 00:15:35.820
but it comes down
the valley faster.

00:15:35.820 --> 00:15:37.840
And that's the effect of this.

00:15:37.840 --> 00:15:43.710
So you could play with
different adjustment

00:15:43.710 --> 00:15:45.210
terms, different corrections.

00:15:45.210 --> 00:15:48.780
So I'll follow through this one.

00:15:48.780 --> 00:15:53.760
Nesterov had another way to
make a change in the formula,

00:15:53.760 --> 00:15:57.140
and there are certainly
others beyond that.

00:15:57.140 --> 00:16:01.260
OK, so how do we
analyze that one?

00:16:01.260 --> 00:16:07.610
Well, the real point is,
we've sort of, by taking--

00:16:07.610 --> 00:16:11.900
by involving the
previous step, we now

00:16:11.900 --> 00:16:16.620
have a three level method
instead of a two level method,

00:16:16.620 --> 00:16:18.000
you could say.

00:16:18.000 --> 00:16:21.360
This involves only
level K plus 1

00:16:21.360 --> 00:16:28.320
and level K. The formulas
now involve K plus 1K,

00:16:28.320 --> 00:16:30.240
and K minus 1.

00:16:30.240 --> 00:16:35.490
It's just like going from
a first order differential

00:16:35.490 --> 00:16:38.680
equation to a second order
differential equation.

00:16:42.540 --> 00:16:46.300
I'm not really thinking
that K is a time variable.

00:16:46.300 --> 00:16:50.290
But in the analogy, K
could be a time variable.

00:16:50.290 --> 00:16:54.480
So that here we had a
first order equation.

00:16:54.480 --> 00:16:57.150
If I wanted to model
that, it's sort

00:16:57.150 --> 00:17:01.840
of a DXDT coming in
there, equal gradient.

00:17:01.840 --> 00:17:04.260
And these models
are highly useful

00:17:04.260 --> 00:17:09.390
and developed for sort
of a continuous model

00:17:09.390 --> 00:17:12.369
of steepest descent--

00:17:12.369 --> 00:17:19.740
a continuous motion instead
of the discrete motion.

00:17:19.740 --> 00:17:20.520
OK.

00:17:20.520 --> 00:17:25.140
So that would-- that
continuous model for that guy

00:17:25.140 --> 00:17:27.540
would be a first order in time.

00:17:27.540 --> 00:17:30.590
For this one, it'll be
second order in time.

00:17:30.590 --> 00:17:33.990
And second order
equations, of course,

00:17:33.990 --> 00:17:35.730
and there'd be
constant coefficients

00:17:35.730 --> 00:17:37.560
in our model problem.

00:17:37.560 --> 00:17:41.730
And the thing about a second
order equation that we all know

00:17:41.730 --> 00:17:45.420
is, there is a momentum term--

00:17:45.420 --> 00:17:49.440
a damping term, you could say--

00:17:49.440 --> 00:17:53.960
in multiplying the
first derivative.

00:17:53.960 --> 00:17:59.400
So that's what a second
order equation offers--

00:17:59.400 --> 00:18:02.670
is the inclusion
of a damping term

00:18:02.670 --> 00:18:07.800
which isn't present in
the original first order.

00:18:07.800 --> 00:18:09.300
OK.

00:18:09.300 --> 00:18:11.010
So how do we analyze this?

00:18:13.590 --> 00:18:17.010
I have to-- so how do
you analyze second order

00:18:17.010 --> 00:18:19.080
differential equations?

00:18:19.080 --> 00:18:22.860
You write them as a system
of two first order equations.

00:18:22.860 --> 00:18:25.170
So that's exactly what
we're going to do here,

00:18:25.170 --> 00:18:27.000
in the discrete case.

00:18:27.000 --> 00:18:29.880
We're going to see--

00:18:29.880 --> 00:18:31.890
because we have two equations.

00:18:31.890 --> 00:18:34.590
And they're first
order, and we can--

00:18:34.590 --> 00:18:38.940
let me play with them for
a moment to make them good.

00:18:38.940 --> 00:18:39.690
OK.

00:18:39.690 --> 00:18:40.920
So I'm going to have--

00:18:40.920 --> 00:18:46.680
so this will go to two
first order equations,

00:18:46.680 --> 00:18:48.180
in which the first one--

00:18:48.180 --> 00:18:51.720
I'm just going to
copy, XK plus 1

00:18:51.720 --> 00:18:56.620
is XK minus that step size ZK.

00:19:00.330 --> 00:19:00.880
Yeah.

00:19:00.880 --> 00:19:02.220
OK.

00:19:02.220 --> 00:19:03.050
Yeah.

00:19:03.050 --> 00:19:03.550
OK.

00:19:03.550 --> 00:19:07.150
Time the previous
times step here--

00:19:07.150 --> 00:19:09.320
the next time step on the left.

00:19:09.320 --> 00:19:09.820
OK.

00:19:09.820 --> 00:19:11.500
So I just copied that.

00:19:11.500 --> 00:19:17.090
Now this one I'm going
to increase K by 1.

00:19:17.090 --> 00:19:21.190
So in order to have that
looking to match this,

00:19:21.190 --> 00:19:27.160
I'll write that as ZK plus 1,
and I'll bring the K, saying,

00:19:27.160 --> 00:19:35.920
grad FK plus 1 equal beta ZK.

00:19:35.920 --> 00:19:37.610
That work with you?

00:19:37.610 --> 00:19:42.850
I just, in this thing,
instead of looking at it at K,

00:19:42.850 --> 00:19:47.260
I went to K plus 1.

00:19:47.260 --> 00:19:50.080
And I put the K plus
1 terms on one side.

00:19:50.080 --> 00:19:50.920
OK.

00:19:50.920 --> 00:19:52.440
So now I have a--

00:19:56.600 --> 00:19:57.100
let's see.

00:19:57.100 --> 00:19:58.680
Let's remember, we're doing--

00:19:58.680 --> 00:20:04.060
the model we're doing is F
equal a half X transpose SX.

00:20:04.060 --> 00:20:09.250
So the gradient of F is SX.

00:20:09.250 --> 00:20:13.450
So what I've written there,
for gradient, is really--

00:20:13.450 --> 00:20:15.980
I know what that gradient is.

00:20:15.980 --> 00:20:20.350
So that's really SX K plus 1.

00:20:23.190 --> 00:20:23.690
OK.

00:20:27.700 --> 00:20:30.040
How to analyze that.

00:20:30.040 --> 00:20:36.940
What happens as K travels
forward 1, 2, 3, 4, 5?

00:20:36.940 --> 00:20:40.870
We have a constant coefficient
problem at every step.

00:20:40.870 --> 00:20:46.190
The XZ variable is getting
multiplied by a matrix.

00:20:46.190 --> 00:20:52.480
So here's XZ at K plus 1.

00:20:52.480 --> 00:20:58.750
And over here will
be XZ at step K.

00:20:58.750 --> 00:21:02.620
And I just have to
figure out what matrix

00:21:02.620 --> 00:21:05.580
is multiplying here and here.

00:21:05.580 --> 00:21:07.220
OK.

00:21:07.220 --> 00:21:09.310
And I guess here I see it.

00:21:09.310 --> 00:21:12.730
For the first equation
has a 1 and a minus S,

00:21:12.730 --> 00:21:14.740
looks like, in the first row.

00:21:14.740 --> 00:21:17.590
And it has a beta
in the second row.

00:21:17.590 --> 00:21:24.100
And here the first equation
has a 1, 0 in that row.

00:21:24.100 --> 00:21:27.070
And then a minus
S. So I'll put in

00:21:27.070 --> 00:21:31.840
minus S, multiplying
XK plus 1, and then

00:21:31.840 --> 00:21:34.902
the 1 that multiplies ZK plus 1.

00:21:34.902 --> 00:21:35.700
Is that all right?

00:21:39.400 --> 00:21:39.900
Sorry.

00:21:39.900 --> 00:21:42.412
I've got two S's, and I
didn't draw that one--

00:21:42.412 --> 00:21:43.995
didn't write that
one in large enough,

00:21:43.995 --> 00:21:47.370
and I'd planned to
erase it anyway.

00:21:47.370 --> 00:21:49.800
This is the step sizes.

00:21:49.800 --> 00:21:51.620
This is the matrix.

00:21:51.620 --> 00:21:57.050
But it's not quite
fitting its place.

00:21:57.050 --> 00:22:01.730
This is the point where I'm
going to use eigenvalues.

00:22:01.730 --> 00:22:05.450
I'm going to follow
each eigenvalue.

00:22:05.450 --> 00:22:06.800
That's the whole point.

00:22:06.800 --> 00:22:10.070
When I follow each
eigenvalue-- each eigenvector,

00:22:10.070 --> 00:22:11.000
I should say--

00:22:11.000 --> 00:22:17.510
I'll follow each eigenvector
of S. So let's do that.

00:22:17.510 --> 00:22:23.450
So eigenvectors of S-- what
are we going to call those?

00:22:23.450 --> 00:22:26.000
Lambda, probably.

00:22:26.000 --> 00:22:30.850
So SX equal lambda X. I
think that's what's coming.

00:22:34.930 --> 00:22:41.560
Or Q. To do things
right, I want to remember

00:22:41.560 --> 00:22:45.350
that S is a positive,
definite symmetric matrix.

00:22:45.350 --> 00:22:47.620
That's why I call
it S, instead of A.

00:22:47.620 --> 00:22:49.240
So I really should
call the eigen--

00:22:49.240 --> 00:22:54.940
it doesn't matter,
but to be on the ball,

00:22:54.940 --> 00:22:58.780
let me call the eigenvector
Q, and the eigenvalue lambda.

00:23:02.224 --> 00:23:02.724
OK.

00:23:06.280 --> 00:23:10.720
So now I want to follow
this eigenvector.

00:23:10.720 --> 00:23:17.930
So I'm supposing that
XK is sum CK times Q.

00:23:17.930 --> 00:23:21.200
I'm assuming that X is in the--

00:23:21.200 --> 00:23:23.390
tracking this eigenvector.

00:23:23.390 --> 00:23:30.350
And I'm going to assume that ZK
is some other constant times Q.

00:23:30.350 --> 00:23:31.670
Everybody, do you see?

00:23:31.670 --> 00:23:33.390
That's a vector and
that's a vector.

00:23:33.390 --> 00:23:35.330
And I want scalars.

00:23:35.330 --> 00:23:40.580
I want to attract
just scalar CK and DK.

00:23:40.580 --> 00:23:43.010
So that's really
what I have here.

00:23:43.010 --> 00:23:47.630
This was a little tricky,
because X here is a vector,

00:23:47.630 --> 00:23:50.180
and two components
are N components.

00:23:50.180 --> 00:23:51.630
I didn't want that.

00:23:51.630 --> 00:23:56.030
I really wanted just to
track an eigenvector.

00:23:56.030 --> 00:23:59.210
Once I've settled
on the direction Q,

00:23:59.210 --> 00:24:02.690
everything is-- all vectors
are in the direction of Q.

00:24:02.690 --> 00:24:06.840
So we just have numbers
C and D to track.

00:24:06.840 --> 00:24:07.340
OK.

00:24:07.340 --> 00:24:15.660
So I'm going to rewrite
this correctly, as, yeah.

00:24:15.660 --> 00:24:19.310
Well, let me keep going
with this little formula.

00:24:19.310 --> 00:24:20.650
Then what will--

00:24:20.650 --> 00:24:22.320
I needed an SX.

00:24:22.320 --> 00:24:26.020
What will SXK be?

00:24:26.020 --> 00:24:33.000
If XK is in the direction of
the eigenvector Q, and it's CK--

00:24:33.000 --> 00:24:34.890
what happens when
I multiply by S?

00:24:37.740 --> 00:24:39.170
Q was an eigenvector.

00:24:39.170 --> 00:24:42.035
So the multiplying
by S gives me a--

00:24:42.035 --> 00:24:42.910
AUDIENCE: Eigenvalue.

00:24:42.910 --> 00:24:44.327
GILBERT STRANG:
Eigenvalue, right?

00:24:44.327 --> 00:24:50.810
So it's CK lambda Q.
Everything is a multiple of Q.

00:24:50.810 --> 00:24:53.810
And it's only those
multiples I'm looking

00:24:53.810 --> 00:24:56.000
for, the C's and the D's.

00:24:56.000 --> 00:24:59.340
And then the lambda
comes into the S term.

00:24:59.340 --> 00:24:59.840
Yeah.

00:24:59.840 --> 00:25:05.970
I think that's probably
all I need to do this.

00:25:05.970 --> 00:25:07.350
And then the gradient-- yeah.

00:25:07.350 --> 00:25:09.840
So that's the
gradient, of course.

00:25:09.840 --> 00:25:15.170
This is the gradient of F at K--

00:25:15.170 --> 00:25:15.820
is that one.

00:25:15.820 --> 00:25:16.830
OK.

00:25:16.830 --> 00:25:19.520
So instead of this,
let me just write

00:25:19.520 --> 00:25:26.400
what's happening if I'm tracking
the coefficients CK plus 1

00:25:26.400 --> 00:25:29.400
and DK plus 1.

00:25:29.400 --> 00:25:34.200
Then what I really meant
to have there is 1, 0.

00:25:34.200 --> 00:25:39.060
And minus S is a minus lambda.

00:25:43.450 --> 00:25:44.810
Is that right?

00:25:44.810 --> 00:25:45.310
Yeah.

00:25:45.310 --> 00:25:50.150
When I multiply the eigenvector
by S, I'm just getting--

00:25:50.150 --> 00:25:54.040
oh, it's a lambda times a CK.

00:25:54.040 --> 00:25:54.970
Yeah.

00:25:54.970 --> 00:25:57.280
Lambda times the
CK-- that's good.

00:25:57.280 --> 00:26:02.160
I think that that's the left
hand side of my equation.

00:26:02.160 --> 00:26:10.540
And on the right hand
side, I have here.

00:26:10.540 --> 00:26:11.260
That's 1.

00:26:11.260 --> 00:26:14.320
And this was the
scalar, the step size.

00:26:14.320 --> 00:26:16.300
And this was the
other coefficient.

00:26:16.300 --> 00:26:17.710
It's the beta.

00:26:17.710 --> 00:26:18.900
So I want to choose--

00:26:18.900 --> 00:26:22.550
what's my purpose now?

00:26:22.550 --> 00:26:26.330
That gives me the--

00:26:26.330 --> 00:26:30.770
what happens at every
step to the C and D.

00:26:30.770 --> 00:26:34.820
So I want to choose the
two things that I have--

00:26:34.820 --> 00:26:36.740
I'm free to choose
are S and beta.

00:26:36.740 --> 00:26:39.770
So that's my big job--

00:26:39.770 --> 00:26:41.510
choose S and beta.

00:26:44.140 --> 00:26:45.180
OK.

00:26:45.180 --> 00:26:46.980
Now I-- to make--

00:26:46.980 --> 00:26:52.350
oh, let me just shape this
by multiplying the inverse

00:26:52.350 --> 00:26:54.090
of that, and get it over here.

00:26:54.090 --> 00:26:55.410
So that will really--

00:26:55.410 --> 00:26:57.060
you'll see everything.

00:26:57.060 --> 00:27:02.520
So CK plus 1, DK plus 1.

00:27:02.520 --> 00:27:05.190
What's the inverse of 1, 0?

00:27:05.190 --> 00:27:07.410
Oh, I don't think
I want to-- that

00:27:07.410 --> 00:27:12.030
would have a tough time
finding an inverse.

00:27:12.030 --> 00:27:13.390
It was a 1, wasn't it?

00:27:19.180 --> 00:27:20.050
Yeah.

00:27:20.050 --> 00:27:20.660
OK.

00:27:20.660 --> 00:27:23.630
So I'm going to multiply by
the inverse of that matrix

00:27:23.630 --> 00:27:26.330
to get it over here.

00:27:26.330 --> 00:27:29.320
And what's the inverse
of 1, 1 minus lambda?

00:27:29.320 --> 00:27:32.420
It's 1, 1 plus lambda.

00:27:32.420 --> 00:27:34.640
So that the inverse
brought it over here,

00:27:34.640 --> 00:27:41.040
times this matrix, 1, 0 beta,
and minus the step size.

00:27:41.040 --> 00:27:42.520
That's what multiply CK DK.

00:27:45.770 --> 00:27:49.310
So we have these
simple, beautiful steps

00:27:49.310 --> 00:27:53.950
which come from tracking
one eigenvector--

00:27:53.950 --> 00:27:55.790
makes the whole problem scalar.

00:27:55.790 --> 00:27:58.880
So I multiply those two
matrices and I finally

00:27:58.880 --> 00:28:01.610
get the matrix that I
really have to think about.

00:28:01.610 --> 00:28:06.110
1, 0 times that'll be 1 minus
S. Lambda 1 times that'll

00:28:06.110 --> 00:28:08.030
be a lambda there.

00:28:08.030 --> 00:28:11.840
And minus lambda S plus beta.

00:28:11.840 --> 00:28:15.950
Beta minus lambda
S. That's the matrix

00:28:15.950 --> 00:28:18.970
that we see at every step.

00:28:18.970 --> 00:28:27.422
Let me call that matrix R.

00:28:27.422 --> 00:28:32.320
So I've done some algebra--
more than I would always do

00:28:32.320 --> 00:28:33.440
in a lecture--

00:28:33.440 --> 00:28:35.300
but it's really my--

00:28:35.300 --> 00:28:37.670
I wouldn't do it if it
wasn't nice algebra.

00:28:37.670 --> 00:28:39.680
What's the conclusion?

00:28:39.680 --> 00:28:44.060
That conclusion is that
with the momentum term--

00:28:44.060 --> 00:28:49.610
with this number beta available
to choose, as well as S,

00:28:49.610 --> 00:28:51.200
the step--

00:28:51.200 --> 00:28:57.080
the coefficient
of the eigenvector

00:28:57.080 --> 00:29:00.665
is multiplied at every
step by that matrix R.

00:29:00.665 --> 00:29:04.200
R is that matrix.

00:29:04.200 --> 00:29:06.910
And of course, that matrix
involves the eigenvalue.

00:29:10.120 --> 00:29:14.610
So we have to think about--

00:29:14.610 --> 00:29:17.110
what do we want to do now?

00:29:17.110 --> 00:29:23.780
We want to choose
beta and S to make

00:29:23.780 --> 00:29:26.740
R as small as possible, right?

00:29:26.740 --> 00:29:29.350
We want to make R as
small as possible.

00:29:29.350 --> 00:29:34.055
And we are free to choose beta
and S, but R depends on lambda.

00:29:36.780 --> 00:29:39.360
So I'm going to make
it as small as possible

00:29:39.360 --> 00:29:42.240
over the whole range
of possible lambdas.

00:29:42.240 --> 00:29:45.840
So let me-- so now
here we really go.

00:29:49.410 --> 00:29:55.740
So we have lambda between sum.

00:29:55.740 --> 00:30:04.520
These are the eigenvalue
of S. And what we know--

00:30:04.520 --> 00:30:09.100
what's reasonable to
know-- is a lower bound.

00:30:09.100 --> 00:30:10.160
It's a positive.

00:30:10.160 --> 00:30:13.250
This is a symmetric
positive definite matrix.

00:30:13.250 --> 00:30:20.880
A lower bound and an upper
bound, for example, m was B,

00:30:20.880 --> 00:30:25.880
and M was 1, in
that 2 by 2 problem.

00:30:25.880 --> 00:30:28.310
And this is what we know,
that the eigenvalues

00:30:28.310 --> 00:30:38.850
are between m and M. And
the ratio of m to M--

00:30:38.850 --> 00:30:42.000
well, if I write--

00:30:45.380 --> 00:30:50.880
this is the key quantity.

00:30:50.880 --> 00:30:53.020
And what's it called?

00:30:53.020 --> 00:30:55.675
Lambda max divided by
lambda min is the--

00:30:55.675 --> 00:30:56.800
AUDIENCE: Condition number.

00:30:56.800 --> 00:30:57.730
GILBERT STRANG:
Condition number.

00:30:57.730 --> 00:30:58.230
Right.

00:30:58.230 --> 00:31:00.910
This is all sometimes
written kappa--

00:31:00.910 --> 00:31:10.420
Greek letter kappa-- the
condition number of S.

00:31:10.420 --> 00:31:14.830
And when that's big, then the
problem is going to be harder.

00:31:14.830 --> 00:31:19.780
When that's 1, then my
matrix is just a multiple

00:31:19.780 --> 00:31:21.260
of the identity matrix.

00:31:21.260 --> 00:31:22.480
And the problem is trivial.

00:31:22.480 --> 00:31:27.710
When capital M and
small m are the same,

00:31:27.710 --> 00:31:31.810
then that's saying that
the largest and smallest

00:31:31.810 --> 00:31:34.840
eigenvalues are identical,
that the matrix is

00:31:34.840 --> 00:31:36.730
a multiple of the identity.

00:31:36.730 --> 00:31:39.310
That's the condition number one.

00:31:39.310 --> 00:31:47.980
But the bad one is when it's
1 over b, in our example,

00:31:47.980 --> 00:31:51.790
and that could be very large.

00:31:51.790 --> 00:31:52.540
OK.

00:31:52.540 --> 00:31:56.680
That's where we
have our problem.

00:31:56.680 --> 00:32:05.830
Let me just insert about the
ordinary gradient descent.

00:32:05.830 --> 00:32:11.470
Of course, the textbooks find a
estimate for how fast that is.

00:32:11.470 --> 00:32:15.590
And of course, it
depends on that number.

00:32:15.590 --> 00:32:16.090
Yeah.

00:32:16.090 --> 00:32:19.810
So it depends on that
number, and you exactly

00:32:19.810 --> 00:32:23.070
saw how it depended
on that number.

00:32:23.070 --> 00:32:25.210
Right.

00:32:25.210 --> 00:32:27.070
But now we have a
different problem.

00:32:27.070 --> 00:32:29.570
And we're going to finish it.

00:32:29.570 --> 00:32:30.070
OK.

00:32:30.070 --> 00:32:31.000
So what's my job?

00:32:31.000 --> 00:32:38.650
I'm going to choose S and beta
to keep the eigenvalues of R.

00:32:38.650 --> 00:32:42.450
So let's give the
eigenvalues of R a name.

00:32:42.450 --> 00:32:50.490
So R-- let's say R has
eigenvalues e1, that

00:32:50.490 --> 00:32:56.840
depends on the lambda and
the S and the beta and e2.

00:33:00.400 --> 00:33:03.700
So those are the
eigenvalues of R--

00:33:03.700 --> 00:33:07.210
just giving a letter to them.

00:33:07.210 --> 00:33:09.430
So what's our job?

00:33:09.430 --> 00:33:14.680
We want to choose S and beta
to make those eigenvalues as

00:33:14.680 --> 00:33:16.900
small as possible.

00:33:16.900 --> 00:33:17.680
Right?

00:33:17.680 --> 00:33:24.770
Small eigenvalues-- if R has
small eigenvalues, its powers--

00:33:24.770 --> 00:33:29.930
every step multiplies by
R. So the convergence rate

00:33:29.930 --> 00:33:32.450
with momentum is--

00:33:32.450 --> 00:33:36.410
depends on the powers
of R getting small fast.

00:33:36.410 --> 00:33:39.350
It depends on the
eigenvalues being small.

00:33:39.350 --> 00:33:48.500
We want to minimize
the largest eigenvalue.

00:33:48.500 --> 00:33:56.000
So I'll say the
maximum of e1 and e2--

00:33:56.000 --> 00:33:57.650
that's our job.

00:33:57.650 --> 00:34:01.430
Minimize-- we want to choose
S and beta to minimize

00:34:01.430 --> 00:34:03.550
the largest eigenvalue.

00:34:03.550 --> 00:34:05.560
Because if there's
one small eigenvalue,

00:34:05.560 --> 00:34:08.679
but the other is big, then the
other one is going to kill us.

00:34:08.679 --> 00:34:12.670
So we have to get
both eigenvalues down.

00:34:12.670 --> 00:34:16.239
And of course, those
depend on lambda.

00:34:16.239 --> 00:34:18.050
E1 depends on lambda.

00:34:18.050 --> 00:34:20.620
So we have a little
algebra problem.

00:34:20.620 --> 00:34:23.679
And this is what I
described as a miracle--

00:34:23.679 --> 00:34:26.770
the fact that this
little algebra problem--

00:34:26.770 --> 00:34:30.969
the eigenvalues of that
matrix, e1 and e2, which

00:34:30.969 --> 00:34:35.080
depend on lambda in some way.

00:34:35.080 --> 00:34:39.159
And we want to make
both e1 and e2 small--

00:34:39.159 --> 00:34:42.040
the maximum of those-- of them.

00:34:42.040 --> 00:34:47.050
And we have to do it for
all the eigenvalues lambda,

00:34:47.050 --> 00:34:48.639
because we have to--

00:34:48.639 --> 00:34:54.370
we're now thinking-- we've
been tracking each eigenvector.

00:34:54.370 --> 00:34:56.020
So that gave us 1--

00:34:56.020 --> 00:34:59.930
so this is for all
possible lambda.

00:34:59.930 --> 00:35:03.350
So we have to decide, what do
I mean by all possible lambda?

00:35:03.350 --> 00:35:12.910
And I mean all lambda that
are between some m and M.

00:35:12.910 --> 00:35:17.200
There is a beautiful problem.

00:35:17.200 --> 00:35:18.790
You have a 2 by 2 matrix.

00:35:18.790 --> 00:35:22.960
You can find its eigenvalues.

00:35:22.960 --> 00:35:24.610
They depend on lambda.

00:35:24.610 --> 00:35:27.815
And what we-- all we know
about lambda is it's between m

00:35:27.815 --> 00:35:32.920
and cap M. And also, they
also depend on S and beta--

00:35:32.920 --> 00:35:35.380
the two parameters
we can choose.

00:35:35.380 --> 00:35:37.780
And we want to choose
those parameters,

00:35:37.780 --> 00:35:43.060
so that for all the
possible eigenvalues,

00:35:43.060 --> 00:35:45.910
the larger of the
two eigenvalues

00:35:45.910 --> 00:35:47.490
will be as small as possible.

00:35:47.490 --> 00:35:51.040
That's-- it's a
little bit of algebra,

00:35:51.040 --> 00:35:54.730
but do you see that
that's the tricky--

00:35:54.730 --> 00:35:59.680
that-- I shouldn't say
tricky, because it comes out--

00:35:59.680 --> 00:36:03.760
this is the one that is a
miracle in the simplicity

00:36:03.760 --> 00:36:05.270
of the solution.

00:36:05.270 --> 00:36:05.930
OK.

00:36:05.930 --> 00:36:07.150
And I'm going to--

00:36:07.150 --> 00:36:10.120
in fact, maybe I'll move over
here to write the answer.

00:36:13.930 --> 00:36:16.570
OK.

00:36:16.570 --> 00:36:19.690
And I just want to
say that miracles

00:36:19.690 --> 00:36:22.440
don't happen so often in math.

00:36:22.440 --> 00:36:26.470
There is-- all of mathematics--
the whole point of math

00:36:26.470 --> 00:36:28.810
is to explain miracles.

00:36:28.810 --> 00:36:33.850
So there is something
to explain here,

00:36:33.850 --> 00:36:37.390
and I don't have my
finger on it yet.

00:36:37.390 --> 00:36:41.230
Because-- anyway, it happens.

00:36:41.230 --> 00:36:45.550
So let me tell you what the
right S, and the right beta,

00:36:45.550 --> 00:36:53.500
and the resulting
minimum eigenvalue are.

00:36:53.500 --> 00:37:00.300
So again, they depend
on little m and big M.

00:37:00.300 --> 00:37:05.230
That's a very nice
feature, which we expect.

00:37:05.230 --> 00:37:07.680
And they depend on the ratio.

00:37:07.680 --> 00:37:08.190
OK.

00:37:08.190 --> 00:37:09.540
So that ratio-- all right.

00:37:09.540 --> 00:37:11.340
Let's see it.

00:37:11.340 --> 00:37:12.300
OK.

00:37:12.300 --> 00:37:13.275
So the best S--

00:37:18.750 --> 00:37:29.470
the S optimal has the formula 2
over square root of lambda max.

00:37:29.470 --> 00:37:37.290
That's the square root of M
and the squared of m squared.

00:37:37.290 --> 00:37:38.730
Amazing OK.

00:37:38.730 --> 00:37:49.020
And beta optimal turns out
to be the square root of M

00:37:49.020 --> 00:37:53.760
minus the square of little
m, over the square root of M

00:37:53.760 --> 00:37:57.592
plus the square root of
little m, all squared.

00:37:57.592 --> 00:37:59.550
And of course, we know
what these numbers are--

00:37:59.550 --> 00:38:02.430
1 and beta, in
our model problem.

00:38:02.430 --> 00:38:06.720
That's where I'm going to
get this square root of--

00:38:06.720 --> 00:38:09.660
this is 1 minus the
square root-- oh sorry, b.

00:38:09.660 --> 00:38:13.050
This is 1 minus the
square root of b.

00:38:13.050 --> 00:38:17.520
In fact, for our example--

00:38:17.520 --> 00:38:19.670
well, let me just write
what they would be.

00:38:19.670 --> 00:38:25.080
2 over 1 plus square
root of b squared,

00:38:25.080 --> 00:38:29.700
and 1 minus square root
of b over 1 plus square--

00:38:29.700 --> 00:38:33.530
you see where this is--

00:38:33.530 --> 00:38:36.510
1 minus square root of b is
beginning to appear in that.

00:38:36.510 --> 00:38:38.910
It appears in this
solution to this problem.

00:38:38.910 --> 00:38:41.775
And then I have to
tell you what the--

00:38:45.090 --> 00:38:49.700
how small do these
optimal choices

00:38:49.700 --> 00:38:52.520
make the eigenvalues
of R, right?

00:38:52.520 --> 00:38:57.600
This is what we're really
paying attention to, because

00:38:57.600 --> 00:38:59.210
if the eigenvalues--

00:38:59.210 --> 00:39:02.600
that matrix tells us what
happens at every step.

00:39:02.600 --> 00:39:06.860
And its eigenvalues have to be
small to get fast convergence.

00:39:06.860 --> 00:39:08.570
So how small are they?

00:39:08.570 --> 00:39:09.830
Well they involve this--

00:39:13.480 --> 00:39:13.980
yeah.

00:39:13.980 --> 00:39:17.150
So it's the number
that I've seen.

00:39:17.150 --> 00:39:21.630
So in this case, the e's--

00:39:21.630 --> 00:39:29.300
the eigenvalues of R--

00:39:29.300 --> 00:39:32.090
that's the iterating matrix--

00:39:32.090 --> 00:39:36.560
are below-- now you're going
to see the 1 minus square root

00:39:36.560 --> 00:39:41.060
of b over 1 plus
square root of b--

00:39:41.060 --> 00:39:43.220
I think, maybe, squared.

00:39:43.220 --> 00:39:44.480
Let me just see.

00:39:44.480 --> 00:39:45.590
Yeah.

00:39:45.590 --> 00:39:50.450
It happens to come
out that number again.

00:39:50.450 --> 00:39:53.470
So that's the conclusion.

00:39:53.470 --> 00:39:57.790
That with the right
choice of S and beta,

00:39:57.790 --> 00:40:03.490
by adding this look back
term-- look back one step--

00:40:03.490 --> 00:40:05.920
you get this improvement.

00:40:05.920 --> 00:40:13.490
And it happens, and you see
it in practice, of course.

00:40:13.490 --> 00:40:15.650
You'll see it exactly.

00:40:15.650 --> 00:40:26.310
And so you do the
job to use momentum.

00:40:26.310 --> 00:40:30.290
Now I'm going to mention
what the Nesterov--

00:40:30.290 --> 00:40:33.600
Nesterov had a slightly
different way to do it,

00:40:33.600 --> 00:40:37.170
and I'll tell you what that is.

00:40:37.170 --> 00:40:40.320
But it's the same idea--
get a second thing.

00:40:40.320 --> 00:40:42.540
So let's see if I can find that.

00:40:42.540 --> 00:40:44.300
Yeah, Nesterov.

00:40:44.300 --> 00:40:44.800
OK.

00:40:51.250 --> 00:40:53.040
Here we go.

00:40:53.040 --> 00:40:55.770
So let me bring
Nesterov's name down.

00:41:01.740 --> 00:41:07.320
So that's basically what I
wanted to say about number 1.

00:41:07.320 --> 00:41:09.300
And when you see
Nesterov, you'll

00:41:09.300 --> 00:41:14.910
see that it's a similar idea
of involving the previous time

00:41:14.910 --> 00:41:16.140
value.

00:41:16.140 --> 00:41:17.550
OK.

00:41:17.550 --> 00:41:24.720
There are very popular
methods in use now

00:41:24.720 --> 00:41:28.500
for machine learning
that involve--

00:41:28.500 --> 00:41:29.940
by a simple formula--

00:41:29.940 --> 00:41:34.020
all the previous
values, by sort of a--

00:41:34.020 --> 00:41:36.970
just by an addition
of a bunch of terms.

00:41:36.970 --> 00:41:44.160
So it's really-- so it
goes under the names

00:41:44.160 --> 00:41:50.970
adagrad, or others.

00:41:50.970 --> 00:41:54.510
Those of you who already
know about machine learning

00:41:54.510 --> 00:41:55.980
will know what I'm
speaking about.

00:41:55.980 --> 00:41:58.020
And I'll say more about those.

00:41:58.020 --> 00:41:59.910
Yeah.

00:41:59.910 --> 00:42:02.790
But it doesn't involve
a separate coefficient

00:42:02.790 --> 00:42:05.490
for each previous
value, or that would

00:42:05.490 --> 00:42:08.880
be a momentous amount of work.

00:42:08.880 --> 00:42:12.120
So now I just want to tell
you what Nesterov is, and then

00:42:12.120 --> 00:42:13.240
we're good.

00:42:13.240 --> 00:42:13.740
OK.

00:42:13.740 --> 00:42:14.880
Nesterov's idea.

00:42:18.366 --> 00:42:20.820
Let me bring that down.

00:42:20.820 --> 00:42:22.660
Shoot this up.

00:42:22.660 --> 00:42:23.972
Bring down Nesterov.

00:42:31.060 --> 00:42:35.170
Because he had an idea that
you might not have thought of.

00:42:35.170 --> 00:42:38.790
Somehow the momentum
idea was pretty natural--

00:42:38.790 --> 00:42:41.770
to use that previous value.

00:42:41.770 --> 00:42:43.780
And actually, I
would like to know

00:42:43.780 --> 00:42:46.810
what happens if you use two
previous values, or three

00:42:46.810 --> 00:42:47.890
previous values.

00:42:47.890 --> 00:42:57.310
Can you then get improvements
on this convergence rate

00:42:57.310 --> 00:43:00.550
by going back two
steps or three steps?

00:43:00.550 --> 00:43:05.170
If I'd use the analogy
with ordinary differential

00:43:05.170 --> 00:43:07.870
equations, maybe you know.

00:43:07.870 --> 00:43:12.720
So there are backward
difference formulas.

00:43:12.720 --> 00:43:14.800
Do you know about those for--

00:43:14.800 --> 00:43:18.380
those would be in
MATLAB software,

00:43:18.380 --> 00:43:20.440
and all other software.

00:43:20.440 --> 00:43:22.750
Backward differences--
so maybe you

00:43:22.750 --> 00:43:27.040
go back two steps or four steps.

00:43:27.040 --> 00:43:29.800
If you're doing
planetary calculations,

00:43:29.800 --> 00:43:33.460
if you're an astronomer, you go
back maybe seven or eight steps

00:43:33.460 --> 00:43:35.950
to get super high accuracy.

00:43:35.950 --> 00:43:40.050
So that doesn't seem
to have happened yet,

00:43:40.050 --> 00:43:42.110
but it's should happen here--

00:43:42.110 --> 00:43:43.150
to go back more.

00:43:43.150 --> 00:43:48.010
But Nesterov has this
different way to go back.

00:43:48.010 --> 00:43:52.870
So his formula is XK
plus 1-- the new X--

00:43:52.870 --> 00:43:58.360
is YK-- so he's introducing
something a little different--

00:43:58.360 --> 00:44:03.790
minus S gradient f at YK.

00:44:09.100 --> 00:44:10.930
I'm a little surprised
about that YK,

00:44:10.930 --> 00:44:13.330
but this is the point, here--

00:44:13.330 --> 00:44:15.940
that the gradient
is being evaluated

00:44:15.940 --> 00:44:18.010
at some different point.

00:44:18.010 --> 00:44:22.750
And then he has to give a
formula for that to track those

00:44:22.750 --> 00:44:23.950
Y's.

00:44:23.950 --> 00:44:27.760
So the Y's are like
the X's, but they

00:44:27.760 --> 00:44:33.230
are shifted a little bit by some
term-- and beta would be fine.

00:44:33.230 --> 00:44:35.830
Oh no.

00:44:35.830 --> 00:44:39.830
Yeah-- beta-- have
we got Nesterov here?

00:44:39.830 --> 00:44:40.330
Yes.

00:44:40.330 --> 00:44:45.150
Nesterov has a factor gamma in.

00:44:45.150 --> 00:44:45.650
Yeah.

00:44:45.650 --> 00:44:47.240
So all right.

00:44:47.240 --> 00:44:50.170
Let me try to get this right.

00:44:50.170 --> 00:44:52.870
OK.

00:44:52.870 --> 00:44:53.540
All right.

00:44:53.540 --> 00:44:56.890
On a previous line, I've written
the whole Nesterov thing.

00:44:56.890 --> 00:44:59.240
Here, let's see a
Nesterov completely.

00:44:59.240 --> 00:45:00.230
And then it'll break--

00:45:00.230 --> 00:45:04.010
then this is the step that
breaks it into two first order.

00:45:04.010 --> 00:45:06.780
But you'll see the
main formula here.

00:45:06.780 --> 00:45:08.230
XK plus 1 is XK.

00:45:10.750 --> 00:45:19.600
And then a beta times
XK minus XK minus 1.

00:45:19.600 --> 00:45:22.570
So that's a momentum term.

00:45:22.570 --> 00:45:26.560
And then a typical gradient.

00:45:26.560 --> 00:45:29.950
But now here is
Nesterov speaking up.

00:45:29.950 --> 00:45:35.710
Nesterov evaluates the gradient
not at XK, not at XK minus 1.

00:45:35.710 --> 00:45:38.650
But it his own, Nesterov point.

00:45:38.650 --> 00:45:41.950
So this is Nesterov's
favorite point.

00:45:41.950 --> 00:45:46.210
Gamma XK minus XK minus 1.

00:45:46.210 --> 00:45:54.950
Some point, part
way along that step.

00:45:54.950 --> 00:46:01.190
So this point-- because gamma is
going to be some non-integer--

00:46:01.190 --> 00:46:04.900
this evaluation point
for the gradient of f

00:46:04.900 --> 00:46:07.570
is a little
unexpected and weird,

00:46:07.570 --> 00:46:09.970
because it's not a mesh point.

00:46:09.970 --> 00:46:13.470
It's somewhere between.

00:46:13.470 --> 00:46:15.190
OK.

00:46:15.190 --> 00:46:17.170
Yeah.

00:46:17.170 --> 00:46:29.410
And then that-- so that involves
XK plus 1, XK, and XK minus 1.

00:46:29.410 --> 00:46:33.260
So it's a second order--

00:46:33.260 --> 00:46:35.580
there's a second
order method here.

00:46:35.580 --> 00:46:39.350
We're going to-- to analyze it,
we're going to go through this

00:46:39.350 --> 00:46:45.260
same process of writing it
as two first order steps--

00:46:45.260 --> 00:46:48.590
two first-- two single step--

00:46:48.590 --> 00:46:58.460
two one step from K to K plus
1 coupled with one step thing.

00:46:58.460 --> 00:47:03.230
Follow that same thing
through, and then the result

00:47:03.230 --> 00:47:08.280
is, the same factor
appears for him.

00:47:08.280 --> 00:47:11.810
The same factor-- this is also--

00:47:11.810 --> 00:47:24.140
so the point is, this is
for momentum and Nesterov,

00:47:24.140 --> 00:47:33.530
with some constant--
different by some constant.

00:47:33.530 --> 00:47:41.840
But the key quantity is that
one and that appears in both.

00:47:41.840 --> 00:47:49.550
So I don't propose, of
course, to repeat these steps

00:47:49.550 --> 00:47:50.660
for Nesterov.

00:47:50.660 --> 00:47:54.770
But you see what you could do.

00:47:54.770 --> 00:47:59.720
You see that it involves
K minus 1, KNK plus 1.

00:47:59.720 --> 00:48:01.550
You write it as--

00:48:01.550 --> 00:48:03.890
you follow an eigenvector.

00:48:03.890 --> 00:48:08.900
You write it as a coupled
system of-- that's a one step.

00:48:08.900 --> 00:48:10.570
That has a matrix.

00:48:10.570 --> 00:48:12.320
You find the matrix.

00:48:12.320 --> 00:48:14.840
You find the eigenvalues
of the matrix.

00:48:14.840 --> 00:48:17.210
You make those eigenvalues
as small as possible.

00:48:17.210 --> 00:48:22.320
And you have optimized the
coefficients in Nesterov.

00:48:22.320 --> 00:48:22.820
OK.

00:48:22.820 --> 00:48:27.800
That's sort of a lot
of algebra that's

00:48:27.800 --> 00:48:32.840
at the heart of accelerated
gradient descent.

00:48:32.840 --> 00:48:37.670
And of course, it's
worth doing because it's

00:48:37.670 --> 00:48:42.590
a tremendous saving in
the convergence rate.

00:48:42.590 --> 00:48:44.630
OK.

00:48:44.630 --> 00:48:49.640
Anybody running in the
marathon or just watching?

00:48:49.640 --> 00:48:53.480
It's possible to run, you know.

00:48:53.480 --> 00:48:57.350
Anyway, I'll see you after
the marathon, next Wednesday.

00:48:57.350 --> 00:49:01.300
And Professor Boyd
will also see you.