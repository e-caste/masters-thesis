WEBVTT

00:00:00.000 --> 00:00:01.964
[SQUEAKING]

00:00:01.964 --> 00:00:04.419
[RUSTLING]

00:00:04.419 --> 00:00:07.856
[CLICKING]

00:00:10.983 --> 00:00:12.900
NANCY KANWISHER: All
right, let's get started.

00:00:12.900 --> 00:00:16.340
So today, we're going to talk
at some length about what

00:00:16.340 --> 00:00:20.060
I mean by this idea of
Marr's computational theory

00:00:20.060 --> 00:00:21.200
level of analysis.

00:00:21.200 --> 00:00:23.773
It's a way of asking questions
about mind and brain,

00:00:23.773 --> 00:00:25.190
and we're going
to talk about that

00:00:25.190 --> 00:00:26.995
in the case of color vision.

00:00:26.995 --> 00:00:28.370
And that's going
to take a while.

00:00:28.370 --> 00:00:29.720
We'll go down and do the demo.

00:00:29.720 --> 00:00:31.670
We'll come back and
talk about color vision

00:00:31.670 --> 00:00:34.820
and how we think about it at the
level of computational theory

00:00:34.820 --> 00:00:37.057
and why that matters
for mind and brain.

00:00:37.057 --> 00:00:39.140
And then we're going to
start, in the second half,

00:00:39.140 --> 00:00:42.770
a whole session, which is
going to roll into next class,

00:00:42.770 --> 00:00:45.860
on the methods we can use
in cognitive neuroscience

00:00:45.860 --> 00:00:49.190
to understand the human brain,
and we'll illustrate those

00:00:49.190 --> 00:00:50.510
with a case of face perception.

00:00:50.510 --> 00:00:52.910
And we'll talk about
computational theory,

00:00:52.910 --> 00:00:55.678
light very briefly,
a face perception,

00:00:55.678 --> 00:00:57.470
what you can learn from
behavioral studies,

00:00:57.470 --> 00:00:59.262
and what you can learn
from functional MRI.

00:00:59.262 --> 00:01:02.000
And then we'll go on and
do other methods next time.

00:01:02.000 --> 00:01:04.129
Everybody with the program?

00:01:04.129 --> 00:01:05.510
All right.

00:01:05.510 --> 00:01:08.787
So to back up a little,
the biggest theme addressed

00:01:08.787 --> 00:01:10.370
in this course, the
big question we're

00:01:10.370 --> 00:01:12.470
trying to understand
in this field is,

00:01:12.470 --> 00:01:14.822
how does the brain
give rise to the mind?

00:01:14.822 --> 00:01:16.280
That's really what
we're in it for.

00:01:16.280 --> 00:01:17.930
That's why there's lots
of cognitive science.

00:01:17.930 --> 00:01:19.940
We're trying to understand
how the mind emerges

00:01:19.940 --> 00:01:21.470
from this physical object.

00:01:21.470 --> 00:01:23.420
And so for the
last few lectures,

00:01:23.420 --> 00:01:26.780
you've been learning some
stuff about the physical basis

00:01:26.780 --> 00:01:28.705
of the brain, what it
actually looks like.

00:01:28.705 --> 00:01:30.080
Some of you guys
got to touch it.

00:01:30.080 --> 00:01:33.630
I hope you thought that was
half as awesome as I did.

00:01:33.630 --> 00:01:36.980
And we got a sense of the
basic physicality of the brain

00:01:36.980 --> 00:01:38.960
and some of its major parts.

00:01:38.960 --> 00:01:40.580
But now the agenda
is, how are we

00:01:40.580 --> 00:01:43.250
going to explain how this
physical object gives rise

00:01:43.250 --> 00:01:45.110
to something like the mind?

00:01:45.110 --> 00:01:46.730
And the first
problem you encounter

00:01:46.730 --> 00:01:48.380
is, what is the mind anyway?

00:01:48.380 --> 00:01:50.457
I drew it as a weird,
big, amorphous cloud

00:01:50.457 --> 00:01:52.790
because it's just not obvious
how you think about minds,

00:01:52.790 --> 00:01:52.970
right?

00:01:52.970 --> 00:01:54.080
It feels like one
of those things

00:01:54.080 --> 00:01:56.240
like, we could even have
a science of the mind.

00:01:56.240 --> 00:01:57.590
What is mind?

00:01:57.590 --> 00:02:00.110
All kind of nervous
making, right?

00:02:00.110 --> 00:02:02.960
And so our field of
cognitive science,

00:02:02.960 --> 00:02:04.640
over the last few
decades, has come up

00:02:04.640 --> 00:02:07.220
with this framework for how
we can think about minds,

00:02:07.220 --> 00:02:08.539
and this isn't even a theory.

00:02:08.539 --> 00:02:10.370
It's more meta than that.

00:02:10.370 --> 00:02:13.070
It's a framework for thinking
about what a mind is,

00:02:13.070 --> 00:02:15.770
and the framework is
the idea that the mind

00:02:15.770 --> 00:02:20.150
is a set of computations
that extract representations.

00:02:20.150 --> 00:02:22.280
OK, now that's pretty abstract.

00:02:22.280 --> 00:02:23.780
You can think of
a representation

00:02:23.780 --> 00:02:26.480
in your mind as
anything from a percept,

00:02:26.480 --> 00:02:30.330
like I see motion right
now, or I see color.

00:02:30.330 --> 00:02:32.390
And as you learned before,
you might see motion

00:02:32.390 --> 00:02:35.670
even if there isn't actually
motion in the stimulus.

00:02:35.670 --> 00:02:37.940
But that representation
of motion in your head,

00:02:37.940 --> 00:02:41.090
that percept, that's a kind
of mental representation.

00:02:41.090 --> 00:02:45.170
Or if you're thinking, why is
Nancy going through this really

00:02:45.170 --> 00:02:45.680
basic stuff?

00:02:45.680 --> 00:02:46.940
She's insulting
our intelligence.

00:02:46.940 --> 00:02:48.920
If something like that is
going on in the background

00:02:48.920 --> 00:02:50.390
as I'm lecturing,
that's a thought.

00:02:50.390 --> 00:02:52.640
That's a mental
representation of a sort.

00:02:52.640 --> 00:02:55.577
Or if you're thinking, oh
my god, it's after 11:00,

00:02:55.577 --> 00:02:57.410
and I'm not going to
get to eat until 12:30.

00:02:57.410 --> 00:02:58.733
I'm going to starve.

00:02:58.733 --> 00:03:00.650
Whatever thoughts are
going through your head,

00:03:00.650 --> 00:03:03.800
those are mental
representations, too, right?

00:03:03.800 --> 00:03:07.470
And so the question is, how
do we think about those?

00:03:07.470 --> 00:03:14.060
And so this idea that mental
processes are computations

00:03:14.060 --> 00:03:16.940
and mental contents
are representations

00:03:16.940 --> 00:03:20.510
implies that ideally, in
the long run, if we really

00:03:20.510 --> 00:03:23.120
understood minds, we'd
be able to write the code

00:03:23.120 --> 00:03:25.250
to do everything
that minds do, right?

00:03:25.250 --> 00:03:28.890
And that code would work, in
some sense, in the same way.

00:03:28.890 --> 00:03:31.070
Now, that's a tall order.

00:03:31.070 --> 00:03:34.160
Mostly, we can't do that
yet, like not even close,

00:03:34.160 --> 00:03:36.380
a few little cases
in perception,

00:03:36.380 --> 00:03:38.930
kind of sort of maybe, but
mostly, we can't do that yet.

00:03:38.930 --> 00:03:40.290
But that's the goal.

00:03:40.290 --> 00:03:42.567
That's the aspiration.

00:03:42.567 --> 00:03:44.150
And so the question
is, how do we even

00:03:44.150 --> 00:03:46.400
get off the ground
trying to launch

00:03:46.400 --> 00:03:47.870
this enterprise
of coming up with

00:03:47.870 --> 00:03:52.910
an actual precise computational
theory of what minds do?

00:03:52.910 --> 00:03:56.600
And the first step to that is by
thinking about what is computed

00:03:56.600 --> 00:04:01.860
and why, and so that is the
crux of David Marr's big idea,

00:04:01.860 --> 00:04:06.205
the brief reading assignment
that I gave you guys from Marr.

00:04:06.205 --> 00:04:07.580
And he's talking
about, how do we

00:04:07.580 --> 00:04:08.900
think about minds and brains?

00:04:08.900 --> 00:04:11.630
Step number one, what
is computed and why?

00:04:11.630 --> 00:04:14.630
So we're going to focus
on that for a bit here.

00:04:14.630 --> 00:04:16.740
And let's take
vision, for example.

00:04:16.740 --> 00:04:18.980
You start with a
world out there that

00:04:18.980 --> 00:04:20.959
sends light into your eyes.

00:04:20.959 --> 00:04:23.870
That's my icon of a retina,
that blue thing in the back,

00:04:23.870 --> 00:04:24.830
the back of your eyes--

00:04:24.830 --> 00:04:28.910
sends an image onto your eye,
and then some magic happens.

00:04:28.910 --> 00:04:31.310
And then you know what
you're looking at, OK?

00:04:31.310 --> 00:04:33.200
So that's what we're
trying to understand.

00:04:33.200 --> 00:04:34.220
What goes on in there?

00:04:34.220 --> 00:04:35.660
In a sense, what
is the code that

00:04:35.660 --> 00:04:38.120
goes on in here that
takes this as an input

00:04:38.120 --> 00:04:41.570
and delivers that
as an output, OK?

00:04:41.570 --> 00:04:44.210
More specifically,
we can ask, as we

00:04:44.210 --> 00:04:46.070
did in the last
couple of lectures--

00:04:46.070 --> 00:04:47.960
let's take the case
of visual motion.

00:04:47.960 --> 00:04:50.060
So suppose you're seeing
a display like this,

00:04:50.060 --> 00:04:51.352
like something in front of you.

00:04:51.352 --> 00:04:53.870
Somebody jumps on
a beach like that,

00:04:53.870 --> 00:04:55.760
and there's visual
motion information.

00:04:55.760 --> 00:04:57.315
What are the kinds of things--

00:04:57.315 --> 00:04:58.190
so that's your input.

00:04:58.190 --> 00:05:01.110
What are the kinds of outputs
you might get from that?

00:05:01.110 --> 00:05:04.370
Well, to understand that, we
need to know what is computed

00:05:04.370 --> 00:05:05.600
and why.

00:05:05.600 --> 00:05:06.915
So what is computed?

00:05:06.915 --> 00:05:07.790
Well, lots of things.

00:05:07.790 --> 00:05:10.570
You might see the
presence of motion.

00:05:10.570 --> 00:05:13.180
You might see the
presence of a person.

00:05:13.180 --> 00:05:15.130
Actually, you can
detect people just

00:05:15.130 --> 00:05:17.050
from their pattern of motion.

00:05:17.050 --> 00:05:18.730
We should have done
this at the demo.

00:05:18.730 --> 00:05:20.830
Write me a note to think
about that next time.

00:05:20.830 --> 00:05:25.312
If we stuck a little tiny
LEDs on each of my joints

00:05:25.312 --> 00:05:27.520
and we're in a totally black
room and I jumped around

00:05:27.520 --> 00:05:29.452
and all you could see
was those dots moving,

00:05:29.452 --> 00:05:30.910
you would see that
it was a person.

00:05:30.910 --> 00:05:32.980
It would be trivially obvious.

00:05:32.980 --> 00:05:35.385
So motion can give you
lots of information

00:05:35.385 --> 00:05:37.510
aside from "something's
moving" and "what direction

00:05:37.510 --> 00:05:40.390
is it moving?"

00:05:40.390 --> 00:05:42.280
You can see someone's jumping.

00:05:42.280 --> 00:05:45.130
That also comes from the
information about motion.

00:05:45.130 --> 00:05:47.230
You can infer something
about the health

00:05:47.230 --> 00:05:49.630
of this person or
even their mood,

00:05:49.630 --> 00:05:53.110
so there's a huge range
of kinds of information

00:05:53.110 --> 00:05:56.920
we glean from even a pretty
simple stimulus attribute

00:05:56.920 --> 00:05:57.682
like motion.

00:05:57.682 --> 00:05:59.140
And so if we're
going to understand

00:05:59.140 --> 00:06:00.610
how do we perceive
motion, we first

00:06:00.610 --> 00:06:03.010
need to get organized
about, what's the input,

00:06:03.010 --> 00:06:05.350
and which of those outputs
are we talking about?

00:06:05.350 --> 00:06:09.430
And probably, the code that
goes on in between in your head

00:06:09.430 --> 00:06:11.680
or in a computer program,
if you ever figured out

00:06:11.680 --> 00:06:13.360
how to do that, will
be quite different

00:06:13.360 --> 00:06:14.860
for each of those
things, but that's

00:06:14.860 --> 00:06:18.610
the way you need to be
thinking about minds.

00:06:18.610 --> 00:06:19.700
OK, what are the inputs?

00:06:19.700 --> 00:06:20.590
What are the outputs?

00:06:20.590 --> 00:06:23.410
And then as soon as you
pose that challenge--

00:06:23.410 --> 00:06:24.910
OK, let's say it's
just moving dots,

00:06:24.910 --> 00:06:26.920
and you're trying to
tell if that's a person.

00:06:26.920 --> 00:06:28.990
Think about, what is
the code you'd write?

00:06:28.990 --> 00:06:30.820
Just these moving dots.

00:06:30.820 --> 00:06:32.830
How the hell are you
going to go from that

00:06:32.830 --> 00:06:36.190
to detecting if those dots are
on the joints of a person who's

00:06:36.190 --> 00:06:38.260
moving around versus
on something else?

00:06:38.260 --> 00:06:41.080
That's how you think, what are
the computational challenges

00:06:41.080 --> 00:06:42.473
involved, OK?

00:06:42.473 --> 00:06:44.890
And I'm not going to ever ask
you guys to write that code.

00:06:44.890 --> 00:06:49.120
We're just going to consider
it as a thought enterprise

00:06:49.120 --> 00:06:52.570
to kind of see what the problem
is that the brain is facing,

00:06:52.570 --> 00:06:54.580
that it's solving.

00:06:54.580 --> 00:06:57.220
OK, and so Marr's big idea
is this whole business

00:06:57.220 --> 00:06:59.112
of thinking about what
is computed and why,

00:06:59.112 --> 00:07:00.820
what the inputs and
outputs are, and what

00:07:00.820 --> 00:07:03.970
the computational challenges
are getting from those inputs

00:07:03.970 --> 00:07:06.190
to those outputs,
that all of that

00:07:06.190 --> 00:07:10.360
is a prerequisite for thinking
about minds or brains, OK?

00:07:10.360 --> 00:07:13.330
So we can't understand what
brains are doing until we first

00:07:13.330 --> 00:07:14.200
think about this.

00:07:14.200 --> 00:07:16.657
That's why I'm carrying on
about this at some length.

00:07:16.657 --> 00:07:18.490
And Marr writes so
beautifully that I'm just

00:07:18.490 --> 00:07:21.130
going to read some of
my favorite paragraphs

00:07:21.130 --> 00:07:24.550
because paraphrasing
beautiful prose is a sin.

00:07:24.550 --> 00:07:26.980
So Marr says, "Trying
to understand perception

00:07:26.980 --> 00:07:28.960
by studying only
neurons is like trying

00:07:28.960 --> 00:07:32.020
to understand bird flight
by studying only feathers.

00:07:32.020 --> 00:07:33.790
It just can't be done.

00:07:33.790 --> 00:07:35.320
To understand bird
flight, you need

00:07:35.320 --> 00:07:37.330
to understand aerodynamics.

00:07:37.330 --> 00:07:40.450
Only then can one make sense
of the structure of feathers

00:07:40.450 --> 00:07:42.190
and the shape of wings.

00:07:42.190 --> 00:07:44.440
Similarly, you can't
reach an understanding

00:07:44.440 --> 00:07:46.030
of why neurons in
the visual system

00:07:46.030 --> 00:07:48.970
behave the way they do just
by studying their anatomy

00:07:48.970 --> 00:07:50.740
and physiology," OK?

00:07:50.740 --> 00:07:53.200
You have to understand the
problem that's being solved,

00:07:53.200 --> 00:07:54.220
OK?

00:07:54.220 --> 00:07:57.370
Further, he says, "The
nature of the computations

00:07:57.370 --> 00:07:59.710
that underlie
perception depends more

00:07:59.710 --> 00:08:01.150
on the computational
problems that

00:08:01.150 --> 00:08:04.630
have to be solved than on the
particular hardware in which

00:08:04.630 --> 00:08:06.705
their solutions
are implemented."

00:08:06.705 --> 00:08:08.080
So he's basically
saying we could

00:08:08.080 --> 00:08:11.020
have a theory of any
aspect of perception

00:08:11.020 --> 00:08:13.422
that would be essentially
the same theory whether you

00:08:13.422 --> 00:08:15.130
write it in code and
put it in a computer

00:08:15.130 --> 00:08:17.240
or whether it's being
implemented in a brain.

00:08:17.240 --> 00:08:17.740
Yeah.

00:08:17.740 --> 00:08:20.507
AUDIENCE: Was Marr an engineer?

00:08:20.507 --> 00:08:22.090
NANCY KANWISHER:
Marr was many things.

00:08:22.090 --> 00:08:24.100
He was a visionary,
a visionary who

00:08:24.100 --> 00:08:27.190
studied vision, a
truly brilliant guy

00:08:27.190 --> 00:08:29.230
with a very strong
engineering background.

00:08:29.230 --> 00:08:32.260
And this is now
pervading the whole field

00:08:32.260 --> 00:08:34.630
of cognitive science, that
people take an engineering

00:08:34.630 --> 00:08:36.940
approach to understanding
minds and brains,

00:08:36.940 --> 00:08:40.070
to try to really
understand how they work.

00:08:40.070 --> 00:08:42.250
OK, so to better
understand this,

00:08:42.250 --> 00:08:45.070
we're going to now consider
the case of color vision.

00:08:45.070 --> 00:08:47.470
And so in this case,
we start with color

00:08:47.470 --> 00:08:51.110
in the world that sends images
onto the back of your retina,

00:08:51.110 --> 00:08:52.960
so magic happens.

00:08:52.960 --> 00:08:55.295
And we get a bunch
of information out.

00:08:55.295 --> 00:08:56.920
So the question we're
going to consider

00:08:56.920 --> 00:09:00.220
is, what do we
use color for, OK?

00:09:00.220 --> 00:09:01.960
And we're going to
use the same strategy

00:09:01.960 --> 00:09:03.565
we used in the Edgerton Center.

00:09:03.565 --> 00:09:05.440
We're trying to understand
some of the things

00:09:05.440 --> 00:09:09.730
that we use color for by
experiencing perception

00:09:09.730 --> 00:09:11.320
without color, OK?

00:09:11.320 --> 00:09:12.550
What are the outputs?

00:09:12.550 --> 00:09:15.310
OK, so to do that, we're
going to head over right now

00:09:15.310 --> 00:09:17.050
to the imaging center,
and we're going

00:09:17.050 --> 00:09:19.600
to have a cool demo
by Rosa Lafer-Sousa.

00:09:19.600 --> 00:09:23.328
So if it's going to be faster
to leave your stuff here--

00:09:23.328 --> 00:09:23.870
I don't know.

00:09:23.870 --> 00:09:24.460
Maybe we should-- yeah?

00:09:24.460 --> 00:09:24.940
AUDIENCE: [INAUDIBLE]

00:09:24.940 --> 00:09:26.970
NANCY KANWISHER: Yeah,
we'll lock the room, OK?

00:09:26.970 --> 00:09:27.470
Yeah?

00:09:27.470 --> 00:09:29.560
AUDIENCE: How long are
we going to be there?

00:09:29.560 --> 00:09:30.950
NANCY KANWISHER: 10 minutes,
something like that,

00:09:30.950 --> 00:09:32.860
and I need everyone to boogie
because there's a lot of stuff

00:09:32.860 --> 00:09:34.027
I want to get through today.

00:09:34.027 --> 00:09:37.080
So let's go.

00:09:37.080 --> 00:09:42.510
All right, so what do we use
color for when we have it?

00:09:42.510 --> 00:09:43.690
It's not a trick question.

00:09:43.690 --> 00:09:47.500
It's supposed to be
really obvious now.

00:09:47.500 --> 00:09:49.170
Yeah, what's your name?

00:09:49.170 --> 00:09:49.930
AUDIENCE: Chardon.

00:09:49.930 --> 00:09:51.138
NANCY KANWISHER: Chardon, hi.

00:09:51.138 --> 00:09:53.170
AUDIENCE: Like choosing
which food to eat.

00:09:53.170 --> 00:09:54.700
NANCY KANWISHER: Yeah, yeah.

00:09:54.700 --> 00:09:56.590
Choosing which.

00:09:56.590 --> 00:09:59.830
What else related to
that but different?

00:09:59.830 --> 00:10:00.655
Yeah.

00:10:00.655 --> 00:10:01.955
AUDIENCE: Check-in procedure.

00:10:01.955 --> 00:10:03.580
NANCY KANWISHER:
Yeah, yeah, like what?

00:10:03.580 --> 00:10:07.135
What did you notice that
you could identify better?

00:10:07.135 --> 00:10:09.360
AUDIENCE: Different
types of things.

00:10:09.360 --> 00:10:12.210
NANCY KANWISHER: Mm-hmm,
but besides identifying

00:10:12.210 --> 00:10:13.510
and choosing, what else?

00:10:13.510 --> 00:10:15.218
AUDIENCE: More generally,
bringing things

00:10:15.218 --> 00:10:17.175
into our awareness with
the reds in particular

00:10:17.175 --> 00:10:18.480
with the strawberries.

00:10:18.480 --> 00:10:20.780
NANCY KANWISHER: Yeah, do
you find them easier to find?

00:10:20.780 --> 00:10:22.183
AUDIENCE: No, much harder.

00:10:22.183 --> 00:10:23.850
NANCY KANWISHER: Oh,
yeah, right, harder

00:10:23.850 --> 00:10:25.920
without the light, right.

00:10:25.920 --> 00:10:26.580
Exactly.

00:10:26.580 --> 00:10:27.240
What else.

00:10:29.750 --> 00:10:30.260
Yeah.

00:10:30.260 --> 00:10:31.250
AUDIENCE: Like driving.

00:10:31.250 --> 00:10:33.350
You need to have color to
know the traffic lights.

00:10:33.350 --> 00:10:34.767
NANCY KANWISHER:
Totally, totally.

00:10:34.767 --> 00:10:37.820
That's a modern invention
but a really important one.

00:10:37.820 --> 00:10:40.350
What else?

00:10:40.350 --> 00:10:41.740
AUDIENCE: Are we general?

00:10:41.740 --> 00:10:43.290
Are we very general or like--

00:10:43.290 --> 00:10:43.950
NANCY KANWISHER: Whatever.

00:10:43.950 --> 00:10:45.030
What do we use color for?

00:10:45.030 --> 00:10:48.240
AUDIENCE: I mean, we
used to figure out

00:10:48.240 --> 00:10:51.270
what to eat because
one of the strawberries

00:10:51.270 --> 00:10:53.310
wasn't actually a strawberry.

00:10:53.310 --> 00:10:55.180
So yeah, I used
color to [INAUDIBLE]..

00:10:55.180 --> 00:10:56.025
NANCY KANWISHER:
Uh-huh, and the bananas.

00:10:56.025 --> 00:10:56.932
Did anybody notice?

00:10:56.932 --> 00:10:58.140
Sometimes, it's hard to tell.

00:10:58.140 --> 00:11:00.006
Yeah, boy in the back?

00:11:00.006 --> 00:11:03.180
AUDIENCE: For
assessing health risks.

00:11:03.180 --> 00:11:04.526
NANCY KANWISHER: Say more.

00:11:04.526 --> 00:11:07.040
AUDIENCE: If someone's face
doesn't have color in it,

00:11:07.040 --> 00:11:09.438
you tend to assume
that they're sickly.

00:11:09.438 --> 00:11:10.480
NANCY KANWISHER: Totally.

00:11:10.480 --> 00:11:13.510
Did you feel like people's
faces looked a little sickly?

00:11:13.510 --> 00:11:15.820
Absolutely, absolutely.

00:11:15.820 --> 00:11:20.560
OK, so this is just to show
you that a lot of computation

00:11:20.560 --> 00:11:23.478
theory starts with common
sense of just reasoning, what

00:11:23.478 --> 00:11:24.520
do we use this stuff for?

00:11:24.520 --> 00:11:28.070
It helps to not have it to
reveal what we use it for,

00:11:28.070 --> 00:11:30.910
but you guys have just
reinvented the key insights

00:11:30.910 --> 00:11:33.490
in early field of color vision.

00:11:33.490 --> 00:11:36.280
OK, so standard story
is to find fruit.

00:11:36.280 --> 00:11:38.350
If you ask yourself how
many berries are here,

00:11:38.350 --> 00:11:39.260
take a moment.

00:11:39.260 --> 00:11:40.780
Get a mental tally.

00:11:40.780 --> 00:11:42.100
How many berries?

00:11:42.100 --> 00:11:43.030
OK, ready?

00:11:43.030 --> 00:11:44.830
Now how many berries, OK?

00:11:44.830 --> 00:11:46.150
You see more.

00:11:46.150 --> 00:11:49.060
And in fact, there's a long
literature showing that

00:11:49.060 --> 00:11:51.815
primates who have
three cone colors--

00:11:51.815 --> 00:11:54.190
we're not going to go through
all the physiological basis

00:11:54.190 --> 00:11:56.830
of cones and stuff like that,
but they have a richer color

00:11:56.830 --> 00:12:00.010
vision because the number
of different color receptors

00:12:00.010 --> 00:12:01.240
in their retina--

00:12:01.240 --> 00:12:02.740
they're better at
finding berries.

00:12:02.740 --> 00:12:04.750
And in fact, a paper came
out a couple of years

00:12:04.750 --> 00:12:07.750
ago where they studied wild
macaques on an island off

00:12:07.750 --> 00:12:10.270
of Puerto Rico
called Cayo Santiago,

00:12:10.270 --> 00:12:14.110
and the macaques there have a
natural variation genetically

00:12:14.110 --> 00:12:18.130
where some of them have two
color photoreceptors instead

00:12:18.130 --> 00:12:19.480
of three, OK?

00:12:19.480 --> 00:12:21.650
And in fact, they
followed them around,

00:12:21.650 --> 00:12:26.650
and the monkeys that have
three photoreceptor types

00:12:26.650 --> 00:12:29.380
are better at finding fruit than
the ones that have only two,

00:12:29.380 --> 00:12:29.650
OK?

00:12:29.650 --> 00:12:31.900
So that story that's just
been a story for a long time

00:12:31.900 --> 00:12:34.413
turns out it's true.

00:12:34.413 --> 00:12:35.830
And also, as you
guys have already

00:12:35.830 --> 00:12:40.300
said, to not just find things
but identify properties--

00:12:40.300 --> 00:12:41.920
you can probably
tell whether you'd

00:12:41.920 --> 00:12:43.587
want to eat those
bananas on the bottom.

00:12:43.587 --> 00:12:46.510
Maybe not, but it's hard to tell
on the top which ones you like.

00:12:46.510 --> 00:12:49.270
And yet that's all
you need to know, OK?

00:12:49.270 --> 00:12:52.840
So these are just a few of
the ways that we use color

00:12:52.840 --> 00:12:54.820
and why it's important.

00:12:54.820 --> 00:12:58.670
But there is a very big problem
now that we try to figure out,

00:12:58.670 --> 00:13:01.810
OK, what is the code that goes
between the wavelength of light

00:13:01.810 --> 00:13:03.730
hitting your retina and
trying to figure out,

00:13:03.730 --> 00:13:05.240
what color is that thing?

00:13:05.240 --> 00:13:06.920
So here's the problem.

00:13:06.920 --> 00:13:09.310
We want to determine a
property of the object,

00:13:09.310 --> 00:13:11.860
of its surface properties,
its color, right?

00:13:11.860 --> 00:13:14.230
That's a material
property of that thing.

00:13:14.230 --> 00:13:15.940
But all we have--

00:13:15.940 --> 00:13:17.050
so here's a thing.

00:13:17.050 --> 00:13:18.640
We'll call that reflectance.

00:13:18.640 --> 00:13:21.070
It's a property of wavelength,
but you can think of it as,

00:13:21.070 --> 00:13:22.690
for now, just a single number.

00:13:22.690 --> 00:13:25.330
It's a property of that
surface, but all we

00:13:25.330 --> 00:13:30.190
have is the light coming
from that object to our eyes.

00:13:30.190 --> 00:13:31.190
That's called luminance.

00:13:31.190 --> 00:13:33.357
I'm not going to test you
on these particular words,

00:13:33.357 --> 00:13:34.780
but you should get the idea, OK?

00:13:34.780 --> 00:13:36.280
So that's what we have.

00:13:36.280 --> 00:13:37.390
That's our input.

00:13:37.390 --> 00:13:39.520
But here's the problem.

00:13:39.520 --> 00:13:41.560
The light that's
coming off the object

00:13:41.560 --> 00:13:44.020
is a function not
just of the object

00:13:44.020 --> 00:13:46.930
but of the nature of the light
that's shining on the object.

00:13:46.930 --> 00:13:48.880
That's called the illuminant.

00:13:48.880 --> 00:13:52.210
So the problem is we
have this equation.

00:13:52.210 --> 00:13:54.820
This light coming from
the object to our eyes

00:13:54.820 --> 00:13:58.930
is the product of the properties
of the surface and the incident

00:13:58.930 --> 00:14:00.760
light, OK?

00:14:00.760 --> 00:14:04.365
And our problem is, we have
to solve for L. I'm sorry.

00:14:04.365 --> 00:14:06.490
We have to solve for R,
the property of the object.

00:14:06.490 --> 00:14:09.760
Given L, what is R?

00:14:09.760 --> 00:14:11.950
That's a problem, OK?

00:14:11.950 --> 00:14:15.260
That's kind of like if
I said, A times B is 48.

00:14:15.260 --> 00:14:18.760
Please solve for A and B, OK?

00:14:18.760 --> 00:14:21.940
That's known in the field as
an ill-posed or underdetermined

00:14:21.940 --> 00:14:22.610
problem.

00:14:22.610 --> 00:14:26.170
We don't have enough information
to uniquely solve this, OK?

00:14:26.170 --> 00:14:29.530
That's a very, very deep
problem in perception

00:14:29.530 --> 00:14:30.550
and a lot of cognition.

00:14:30.550 --> 00:14:35.110
We are often, in fact, most
of the time in this boat, OK?

00:14:35.110 --> 00:14:37.120
So the implications
are, when we want

00:14:37.120 --> 00:14:40.450
to infer reflectance of the
property of the object from L,

00:14:40.450 --> 00:14:45.400
we must bring in other
information, right?

00:14:45.400 --> 00:14:47.200
We must have some
way to make guesses

00:14:47.200 --> 00:14:51.190
about I, about the light
shining on that object, OK?

00:14:51.190 --> 00:14:53.530
So the big point is
many, many inferences

00:14:53.530 --> 00:14:57.700
in perception and cognition are
ill posed in exactly this way,

00:14:57.700 --> 00:14:58.510
all right?

00:14:58.510 --> 00:15:01.450
And so here are two other
examples of ill-posed problems

00:15:01.450 --> 00:15:02.740
in perception.

00:15:02.740 --> 00:15:06.040
In shape perception, you
have a similar situation.

00:15:06.040 --> 00:15:07.660
You have stuff in
the world that's

00:15:07.660 --> 00:15:10.990
making an image on the
back of your eyes, OK?

00:15:10.990 --> 00:15:13.630
That's optics.

00:15:13.630 --> 00:15:15.610
What we're trying
to do as perceivers

00:15:15.610 --> 00:15:17.540
is reason backwards
from that image.

00:15:17.540 --> 00:15:20.620
What object in the world
caused that image on my retina?

00:15:20.620 --> 00:15:22.358
That's sometimes
called inverse optics

00:15:22.358 --> 00:15:24.400
because you're trying to
reason the opposite way.

00:15:24.400 --> 00:15:26.770
That's basically what
we're doing in vision.

00:15:26.770 --> 00:15:28.960
So here's a problem.

00:15:28.960 --> 00:15:31.550
It's a crappy diagram,
but if you can see here,

00:15:31.550 --> 00:15:36.760
there's three very different
surface shapes here

00:15:36.760 --> 00:15:40.420
that are all casting the
same image, for example,

00:15:40.420 --> 00:15:41.650
on a retina.

00:15:41.650 --> 00:15:44.290
You could do this with cardboard
and cast it with a shadow.

00:15:44.290 --> 00:15:46.150
So everybody get
what this shows here?

00:15:46.150 --> 00:15:48.400
What that means is if
you start with this

00:15:48.400 --> 00:15:51.440
and you have to reason backwards
to the shape that caused it,

00:15:51.440 --> 00:15:53.620
that's an ill posed
problem, big time.

00:15:53.620 --> 00:15:55.480
It could be any of those things.

00:15:55.480 --> 00:15:57.490
This information
doesn't constrain it.

00:15:57.490 --> 00:15:59.320
Does everybody see that problem?

00:15:59.320 --> 00:16:03.730
OK, so that's another
ill-posed problem.

00:16:03.730 --> 00:16:06.450
Here's a totally different
example of an ill-posed problem

00:16:06.450 --> 00:16:08.070
that's big in cognition.

00:16:08.070 --> 00:16:10.620
When you learn the meaning
of a word, especially

00:16:10.620 --> 00:16:14.370
as an infant trying to learn
language, the classic example

00:16:14.370 --> 00:16:15.540
the philosophers like--

00:16:15.540 --> 00:16:16.230
God knows why.

00:16:16.230 --> 00:16:19.620
Philosophers like weird
stuff, but never mind.

00:16:19.620 --> 00:16:24.390
Somebody points to that and
says "gavagai," and your job

00:16:24.390 --> 00:16:27.810
is to figure out, what
does "gavagai" mean, OK?

00:16:27.810 --> 00:16:31.150
So "gavagai" could mean all
kinds of different things.

00:16:31.150 --> 00:16:33.300
It could just mean
rabbit if you already

00:16:33.300 --> 00:16:34.890
have a concept of a rabbit.

00:16:34.890 --> 00:16:36.630
It could mean fur.

00:16:36.630 --> 00:16:38.970
It could mean ears.

00:16:38.970 --> 00:16:42.000
It could mean motion if the
rabbit is jumping around,

00:16:42.000 --> 00:16:44.550
or in the example of
the philosophers love,

00:16:44.550 --> 00:16:47.640
it could mean
undetached rabbit parts.

00:16:47.640 --> 00:16:50.230
Weird, but anyway, philosophers
like that kind of thing.

00:16:50.230 --> 00:16:52.050
Anyway, the point
is, it's ill posed.

00:16:52.050 --> 00:16:54.460
We don't know from this
what is the correct meaning

00:16:54.460 --> 00:16:54.960
of the word.

00:16:54.960 --> 00:16:57.930
Does everybody see how
this underdetermines

00:16:57.930 --> 00:16:59.340
the correct meaning of the word?

00:16:59.340 --> 00:17:02.820
We don't have enough
information to solve it, OK?

00:17:02.820 --> 00:17:07.530
So yeah-- blah, blah.

00:17:07.530 --> 00:17:11.640
So there's a whole literature
on the extra assumptions

00:17:11.640 --> 00:17:14.573
that infants bring to bear
to constrain that problem,

00:17:14.573 --> 00:17:15.990
so they can make
a damn good guess

00:17:15.990 --> 00:17:18.270
about what the actual
meaning of the word is, OK?

00:17:18.270 --> 00:17:20.763
It's a whole big literature,
quite fascinating.

00:17:20.763 --> 00:17:22.680
OK, but for now, I just
want you to understand

00:17:22.680 --> 00:17:24.810
what an ill-posed
problem is and why

00:17:24.810 --> 00:17:27.900
it's central to understanding
perception and cognition.

00:17:27.900 --> 00:17:31.620
OK, so back to
the case of color.

00:17:31.620 --> 00:17:35.340
As I said, the big point is that
lots of inferences, including

00:17:35.340 --> 00:17:38.170
determining the reflectance
of an object, are ill posed,

00:17:38.170 --> 00:17:41.340
and so we have to bring in
assumptions and knowledge

00:17:41.340 --> 00:17:44.108
from other places, from our
knowledge of the statistics

00:17:44.108 --> 00:17:45.900
and the physics of the
world, our knowledge

00:17:45.900 --> 00:17:47.100
of particular objects.

00:17:47.100 --> 00:17:51.210
All kinds of other things
must be brought to bear, OK?

00:17:51.210 --> 00:17:54.810
So all of that, again, is
considering the problem

00:17:54.810 --> 00:17:57.660
of color vision at the level
of Marr's computational theory.

00:17:57.660 --> 00:17:59.880
Notice we haven't made
any measurements yet.

00:17:59.880 --> 00:18:01.950
We've just thought
about light and optics

00:18:01.950 --> 00:18:05.160
and what the problem is
and what we use it for, OK?

00:18:05.160 --> 00:18:06.470
All this stuff.

00:18:06.470 --> 00:18:08.190
What is extracted and why?

00:18:08.190 --> 00:18:11.130
R, the reflectance of an object,
useful for characterizing

00:18:11.130 --> 00:18:12.600
objects and finding them.

00:18:12.600 --> 00:18:14.160
What cues are available?

00:18:14.160 --> 00:18:19.040
Only L, and that's a problem
because it's ill posed.

00:18:19.040 --> 00:18:23.127
OK, next question-- so
obviously, we get around,

00:18:23.127 --> 00:18:24.960
and we can figure out
what colors are which.

00:18:24.960 --> 00:18:27.510
What are the other
sources of information

00:18:27.510 --> 00:18:30.570
that we might use in principle
and that humans do use

00:18:30.570 --> 00:18:33.990
in practice, OK?

00:18:33.990 --> 00:18:36.210
And so all of that
kind of stuff has

00:18:36.210 --> 00:18:38.130
been done without
making any measurements.

00:18:38.130 --> 00:18:41.850
We're just thinking about
the problem itself, OK?

00:18:41.850 --> 00:18:48.060
All right, so next, Marr's other
levels of analysis, algorithm

00:18:48.060 --> 00:18:50.310
and representation in hardware,
are more standard ones

00:18:50.310 --> 00:18:51.768
you will have
encountered, which is

00:18:51.768 --> 00:18:54.060
why I'm making a big deal
of computational theory.

00:18:54.060 --> 00:18:57.340
It's really his major
novel contribution,

00:18:57.340 --> 00:18:59.910
but it's better understood
by contrast with these.

00:18:59.910 --> 00:19:02.280
So at the level of algorithm
and representation,

00:19:02.280 --> 00:19:04.405
this is like, what is the
code that you would write

00:19:04.405 --> 00:19:05.970
to solve that problem, right?

00:19:05.970 --> 00:19:09.960
And so we could ask, how does
the system do what it does?

00:19:09.960 --> 00:19:11.880
Can we write the code
to do it, and what

00:19:11.880 --> 00:19:15.450
assumptions and computations
and representations

00:19:15.450 --> 00:19:17.290
would be entailed?

00:19:17.290 --> 00:19:21.550
So how would we find
out how humans do this?

00:19:21.550 --> 00:19:24.600
Well, one of the ways is a
slightly more organized version

00:19:24.600 --> 00:19:28.560
of what you guys just did, and
that's called psychophysics.

00:19:28.560 --> 00:19:31.080
Psychophysics just means
showing people stuff

00:19:31.080 --> 00:19:33.600
and asking them what they
see or playing them sounds

00:19:33.600 --> 00:19:35.190
and asking them what they hear.

00:19:35.190 --> 00:19:37.703
And you can do it in very
sophisticated, formalized ways,

00:19:37.703 --> 00:19:39.120
or you can do it
like we just did.

00:19:39.120 --> 00:19:41.850
Talk to us about what
the world looks like, OK?

00:19:41.850 --> 00:19:44.340
Usually, psychophysics means
a slightly more organized

00:19:44.340 --> 00:19:45.960
version.

00:19:45.960 --> 00:19:48.490
OK, so here's an example.

00:19:48.490 --> 00:19:51.503
In fact, it's a cool
demo, also from Rosa.

00:19:51.503 --> 00:19:52.920
And so what I'm
going to do is I'm

00:19:52.920 --> 00:19:55.350
going to show you a bunch
of pictures of cars,

00:19:55.350 --> 00:19:58.620
and your task is going to be to
shout out loud as fast as you

00:19:58.620 --> 00:20:00.457
can the color of the car, OK?

00:20:00.457 --> 00:20:02.040
They're going to
appear on the screen.

00:20:02.040 --> 00:20:03.697
Everyone ready?

00:20:03.697 --> 00:20:05.280
As fast as you can,
shout it out loud.

00:20:05.280 --> 00:20:05.670
Here we go.

00:20:05.670 --> 00:20:05.970
What color?

00:20:05.970 --> 00:20:06.300
AUDIENCE: Red.

00:20:06.300 --> 00:20:06.716
AUDIENCE: Green.

00:20:06.716 --> 00:20:07.299
AUDIENCE: Red.

00:20:07.299 --> 00:20:07.964
AUDIENCE: Red.

00:20:07.964 --> 00:20:08.547
AUDIENCE: Red.

00:20:08.547 --> 00:20:09.422
AUDIENCE: It's green.

00:20:09.422 --> 00:20:10.630
NANCY KANWISHER: Interesting.

00:20:10.630 --> 00:20:11.675
OK, here's another one.

00:20:11.675 --> 00:20:12.300
AUDIENCE: Blue.

00:20:12.300 --> 00:20:12.735
AUDIENCE: Yellow.

00:20:12.735 --> 00:20:13.443
AUDIENCE: Yellow.

00:20:13.443 --> 00:20:15.030
NANCY KANWISHER:
Uh-huh, interesting.

00:20:15.030 --> 00:20:15.390
Ready?

00:20:15.390 --> 00:20:15.690
Here we go.

00:20:15.690 --> 00:20:16.482
Here's another one.

00:20:16.482 --> 00:20:17.670
AUDIENCE: Green.

00:20:17.670 --> 00:20:19.353
NANCY KANWISHER: OK,
here's another one.

00:20:19.353 --> 00:20:19.978
AUDIENCE: Blue.

00:20:19.978 --> 00:20:21.561
NANCY KANWISHER: Ah,
you guys cottoned

00:20:21.561 --> 00:20:22.740
on to that pretty fast.

00:20:22.740 --> 00:20:26.760
OK, so good job.

00:20:26.760 --> 00:20:28.515
Nice consensus,
although I noticed

00:20:28.515 --> 00:20:29.890
a little bit of
transition there,

00:20:29.890 --> 00:20:32.140
which is very interesting.

00:20:32.140 --> 00:20:34.680
But here's the thing.

00:20:34.680 --> 00:20:37.530
All of those cars are
the exact same color.

00:20:37.530 --> 00:20:40.330
The body of the car is the
exact same in all of them,

00:20:40.330 --> 00:20:43.560
and if you don't believe it,
I'm going to include everything

00:20:43.560 --> 00:20:45.210
except for a patch, OK?

00:20:45.210 --> 00:20:47.970
Here we go.

00:20:47.970 --> 00:20:50.580
Boom, they're all gray.

00:20:50.580 --> 00:20:51.090
I know.

00:20:51.090 --> 00:20:52.260
It's awesome.

00:20:52.260 --> 00:20:54.120
It's Rosa that's
awesome, not me.

00:20:54.120 --> 00:20:57.390
I just had to bum this
because it's so awesome.

00:20:57.390 --> 00:21:01.380
OK, so Rosa spent months
designing these stimuli to test

00:21:01.380 --> 00:21:05.380
particular ideas about
vision, but the basic demo

00:21:05.380 --> 00:21:06.880
is simple and straightforward.

00:21:06.880 --> 00:21:09.160
You can get the point here, OK?

00:21:09.160 --> 00:21:11.810
So what's going on here?

00:21:11.810 --> 00:21:14.783
What's going on here is that
you guys, the algorithm running

00:21:14.783 --> 00:21:16.450
in your head that's
trying to figure out

00:21:16.450 --> 00:21:18.310
what is the color
of that car, is

00:21:18.310 --> 00:21:21.100
trying to solve the
ill-posed problem,

00:21:21.100 --> 00:21:24.820
and it's using other information
than just the luminance

00:21:24.820 --> 00:21:26.710
of light coming from the object.

00:21:26.710 --> 00:21:29.830
It's using information from
the rest of the object.

00:21:29.830 --> 00:21:33.580
It's making inferences about
L, the luminance, the light

00:21:33.580 --> 00:21:35.560
hitting the object, OK?

00:21:35.560 --> 00:21:38.950
And in particular, when you
look at that picture up there,

00:21:38.950 --> 00:21:41.170
what is the color of
light shining on that car?

00:21:41.170 --> 00:21:41.920
AUDIENCE: Green.

00:21:41.920 --> 00:21:44.740
NANCY KANWISHER: Yeah,
right, officially

00:21:44.740 --> 00:21:46.900
known as teal in the
field, but some of you

00:21:46.900 --> 00:21:48.567
shouted out "green"
first because that's

00:21:48.567 --> 00:21:51.310
what you saw first, is
the color of light, OK?

00:21:51.310 --> 00:21:54.550
What's the color of
light hitting that car?

00:21:54.550 --> 00:21:56.230
Yeah, purple, magenta.

00:21:56.230 --> 00:21:57.615
Here?

00:21:57.615 --> 00:21:58.240
AUDIENCE: Blue.

00:21:58.240 --> 00:21:59.980
NANCY KANWISHER:
Yeah, and over there?

00:21:59.980 --> 00:22:00.820
AUDIENCE: Yellow.

00:22:00.820 --> 00:22:02.362
NANCY KANWISHER:
Yeah, yellow-orange.

00:22:02.362 --> 00:22:06.220
Yeah, OK, so basically
what your visual system did

00:22:06.220 --> 00:22:11.230
is look quickly, figure out the
color of the incident light, L,

00:22:11.230 --> 00:22:14.980
and use that to solve the
otherwise ill-posed problem

00:22:14.980 --> 00:22:17.770
of solving for R,
the color of the car.

00:22:17.770 --> 00:22:19.960
And in this case,
this demo shows

00:22:19.960 --> 00:22:22.450
that if you just change the
color of the illuminant light

00:22:22.450 --> 00:22:25.948
and hold constant the
actual wavelengths coming

00:22:25.948 --> 00:22:28.240
from that patch, you can
radically change the perceived

00:22:28.240 --> 00:22:29.530
color of the car.

00:22:29.530 --> 00:22:30.820
Everyone got that?

00:22:30.820 --> 00:22:31.320
OK.

00:22:31.320 --> 00:22:31.900
AUDIENCE: So wait, wait, wait--

00:22:31.900 --> 00:22:32.270
NANCY KANWISHER: Yeah.

00:22:32.270 --> 00:22:34.525
AUDIENCE: If you ran
this through a computer,

00:22:34.525 --> 00:22:38.260
asked it to get the
intensity of the pixel

00:22:38.260 --> 00:22:41.200
on the hood of the car there, it
would not correspond to yellow,

00:22:41.200 --> 00:22:42.445
it would correspond to green?

00:22:42.445 --> 00:22:44.320
NANCY KANWISHER: Well,
it depends what you're

00:22:44.320 --> 00:22:46.030
asking the computer exactly.

00:22:46.030 --> 00:22:47.923
If you hold up a
spectrophotometer that's

00:22:47.923 --> 00:22:49.840
just going to measure
the wavelength of light,

00:22:49.840 --> 00:22:51.520
they're all gray.

00:22:51.520 --> 00:22:52.910
Right there on
top of those cars,

00:22:52.910 --> 00:22:55.270
they're all the exact
same neutral gray.

00:22:55.270 --> 00:22:59.620
That's just the raw physical
light coming from that patch,

00:22:59.620 --> 00:23:00.610
OK?

00:23:00.610 --> 00:23:03.640
But if you coded up the
computer to do something smart

00:23:03.640 --> 00:23:06.400
and you coded it up to take
other cues from the image,

00:23:06.400 --> 00:23:09.687
try to figure out what L is,
and therefore solve for R,

00:23:09.687 --> 00:23:11.770
you might be able to get
it to do the right thing.

00:23:11.770 --> 00:23:14.470
AUDIENCE: Yeah, I mean just
like you looked at the pixels,

00:23:14.470 --> 00:23:18.310
like in the matrix, I
mean, the color on the car,

00:23:18.310 --> 00:23:21.953
would it be what yellow is,
or would it be more grays?

00:23:21.953 --> 00:23:22.870
NANCY KANWISHER: Gray.

00:23:22.870 --> 00:23:23.470
They're all gray.

00:23:23.470 --> 00:23:24.928
So that's what I
was trying to show

00:23:24.928 --> 00:23:28.810
you here is that, in fact,
they are actually gray, right?

00:23:28.810 --> 00:23:31.060
The cars are underneath
there, and you can see

00:23:31.060 --> 00:23:32.800
they're all exactly the same.

00:23:32.800 --> 00:23:35.650
And they're gray, and
there's no color in it, OK?

00:23:35.650 --> 00:23:37.420
Everyone got that?

00:23:37.420 --> 00:23:42.250
All right, OK, so all
of that is a little baby

00:23:42.250 --> 00:23:46.270
example of
psychophysics, what we

00:23:46.270 --> 00:23:48.520
do at the level of trying
to understand the algorithms

00:23:48.520 --> 00:23:52.360
and representations extracted by
the mind to try to figure out,

00:23:52.360 --> 00:23:56.320
what are the strategies that
we use to solve problems

00:23:56.320 --> 00:23:58.210
about the visual world?

00:23:58.210 --> 00:24:01.960
OK, and so behavior or
psychophysics or seeing,

00:24:01.960 --> 00:24:04.763
as you just did, can
reveal those assumptions

00:24:04.763 --> 00:24:06.430
and reveal some of
the tricks that we're

00:24:06.430 --> 00:24:09.040
using in the human
visual system to solve

00:24:09.040 --> 00:24:10.912
those ill-posed problems, OK?

00:24:10.912 --> 00:24:12.370
So in this case,
it was assumptions

00:24:12.370 --> 00:24:15.850
about the illuminant
that enabled

00:24:15.850 --> 00:24:20.140
us to infer the reflectance
from the luminance.

00:24:20.140 --> 00:24:23.290
OK, the third level
Marr talks about

00:24:23.290 --> 00:24:25.270
is the level of
hardware implementation.

00:24:25.270 --> 00:24:28.930
In the case of brains,
that's neurons and brains,

00:24:28.930 --> 00:24:31.663
and so we won't cover
this in any detail here.

00:24:31.663 --> 00:24:33.580
But there's lots and
lots of work on the brain

00:24:33.580 --> 00:24:34.600
basis of color vision.

00:24:34.600 --> 00:24:36.560
We'll mention it
briefly next time.

00:24:36.560 --> 00:24:38.440
So this is some of
Rosa's work showing

00:24:38.440 --> 00:24:41.530
those little blue patches on
the side of the monkey brain

00:24:41.530 --> 00:24:43.030
that are involved
in color vision,

00:24:43.030 --> 00:24:44.620
and some work that
Rosa did in my lab

00:24:44.620 --> 00:24:46.600
showing the bottom
surface of the human brain

00:24:46.600 --> 00:24:49.300
with a very similar organization
with those little blue patches

00:24:49.300 --> 00:24:52.540
in there that are particularly
sensitive to color.

00:24:52.540 --> 00:24:55.120
So you can study brain
regions that do that.

00:24:55.120 --> 00:24:57.280
If it's a monkey, you can
stick electrodes in there

00:24:57.280 --> 00:24:58.960
and record from
individual neurons

00:24:58.960 --> 00:25:01.030
and see what they code
for, and you can really

00:25:01.030 --> 00:25:05.860
tackle at multiple levels the
hardware neural basis of color

00:25:05.860 --> 00:25:08.260
vision in brains as well.

00:25:08.260 --> 00:25:11.020
OK, so the big
general point is we

00:25:11.020 --> 00:25:14.650
need lots of levels of analysis
to understand a problem

00:25:14.650 --> 00:25:16.960
like color vision, OK?

00:25:16.960 --> 00:25:19.750
And so accordingly, we
need lots of methods

00:25:19.750 --> 00:25:22.090
to understand those
things, all right?

00:25:22.090 --> 00:25:24.370
So what I want to do
next is now launch

00:25:24.370 --> 00:25:27.503
into this whole thing
about the different methods

00:25:27.503 --> 00:25:29.920
that we can use in the field,
and this part of the lecture

00:25:29.920 --> 00:25:31.190
will go on to next time.

00:25:31.190 --> 00:25:32.170
But let's get going.

00:25:32.170 --> 00:25:34.000
Everybody good with this so far?

00:25:34.000 --> 00:25:38.560
All right, so we're going to
use the case of face perception

00:25:38.560 --> 00:25:41.890
to think about the
different kinds of questions

00:25:41.890 --> 00:25:44.650
and different levels of
analysis in face perception.

00:25:44.650 --> 00:25:46.960
So let me start by saying,
why face perception?

00:25:46.960 --> 00:25:48.850
Not just that I've worked
on it for 20 years,

00:25:48.850 --> 00:25:50.710
although I'll admit
that's relevant.

00:25:50.710 --> 00:25:52.180
There's lots of
other good reasons

00:25:52.180 --> 00:25:56.410
beyond that, why we should
care about face perception.

00:25:56.410 --> 00:26:00.490
So I don't have a demo that
enables me to kind of put you

00:26:00.490 --> 00:26:03.523
in a situation where you can
see everything but faces.

00:26:03.523 --> 00:26:04.940
That would be cool
and informative

00:26:04.940 --> 00:26:07.450
if we could do that,
but failing that,

00:26:07.450 --> 00:26:10.810
I can tell you about somebody
who's in that situation.

00:26:10.810 --> 00:26:13.870
And this is a guy
named Jacob Hodes.

00:26:13.870 --> 00:26:15.820
So this is a picture
of him recently.

00:26:15.820 --> 00:26:17.800
I met him around a
decade ago when he

00:26:17.800 --> 00:26:19.870
was a freshman at Swarthmore.

00:26:19.870 --> 00:26:23.060
And he sent me an
email, and he said,

00:26:23.060 --> 00:26:26.920
I've just learned about face
perception and the phenomenon

00:26:26.920 --> 00:26:29.320
of prosopagnosia, the
fact that some people

00:26:29.320 --> 00:26:33.130
have a specific deficit
in face recognition.

00:26:33.130 --> 00:26:37.000
And it explains everything in
my life, and I want to meet you.

00:26:37.000 --> 00:26:39.580
And I said-- because he knew
I worked on face perception.

00:26:39.580 --> 00:26:40.720
I said, that's awesome.

00:26:40.720 --> 00:26:42.340
I would love to meet you,
but I got to tell you,

00:26:42.340 --> 00:26:43.700
I'm not going to
be able to help.

00:26:43.700 --> 00:26:45.533
So if you're interested
in chatting, please,

00:26:45.533 --> 00:26:48.077
please come by, but
I don't want you

00:26:48.077 --> 00:26:50.410
to feel like I'm going to be
able to do anything useful.

00:26:50.410 --> 00:26:51.493
He said, no, I don't care.

00:26:51.493 --> 00:26:53.330
I want to understand
the science.

00:26:53.330 --> 00:26:56.380
So he comes by, and by
the way, one of the things

00:26:56.380 --> 00:26:58.360
that people have
wondered for a while is,

00:26:58.360 --> 00:27:00.580
are people who have
particular problems with face

00:27:00.580 --> 00:27:02.770
recognition-- are they
just socially weird?

00:27:02.770 --> 00:27:05.553
Are they just bizarre, maybe
a little bit on the spectrum?

00:27:05.553 --> 00:27:06.970
They don't pay
attention to faces,

00:27:06.970 --> 00:27:10.780
and so they don't get them
very well and so forth.

00:27:10.780 --> 00:27:14.770
Or can they be totally
normal in every other respect

00:27:14.770 --> 00:27:16.960
except for just face perception?

00:27:16.960 --> 00:27:18.250
And so I was very interested.

00:27:18.250 --> 00:27:21.310
I'd only emailed with this
guy, and when he showed up

00:27:21.310 --> 00:27:24.620
in my office, within
about 15 seconds,

00:27:24.620 --> 00:27:26.950
it's like, this is like the
nicest, normalest kid you

00:27:26.950 --> 00:27:27.880
could ever meet.

00:27:27.880 --> 00:27:32.210
Such a nice guy, so normal,
socially adept, smart,

00:27:32.210 --> 00:27:34.967
thoughtful-- lovely,
lovely person.

00:27:34.967 --> 00:27:36.550
So I chatted with
him for a long time,

00:27:36.550 --> 00:27:38.260
and he told me he
was then halfway

00:27:38.260 --> 00:27:40.610
through-- he grew up
in Lynn, Massachusetts,

00:27:40.610 --> 00:27:43.420
and he went off to
Swarthmore his freshman year.

00:27:43.420 --> 00:27:46.870
And he had been having a
really rough time of it

00:27:46.870 --> 00:27:50.230
because in his hometown, he
was with the same group of kids

00:27:50.230 --> 00:27:52.840
all the way from first
grade through high school.

00:27:52.840 --> 00:27:56.380
And so in fact, he just
can't recognize faces at all,

00:27:56.380 --> 00:27:57.370
never could.

00:27:57.370 --> 00:27:59.650
When he was a little kid
his mom used to drive him

00:27:59.650 --> 00:28:02.710
to the practice field, and they
would sit there and come up

00:28:02.710 --> 00:28:05.030
with cues about, this is
how you tell that's Johnny.

00:28:05.030 --> 00:28:06.400
He's got this weird
thing about his hair,

00:28:06.400 --> 00:28:07.983
and this is how you
tell that's Bobby.

00:28:07.983 --> 00:28:10.520
And they would
practice and practice.

00:28:10.520 --> 00:28:12.850
And so he developed
these clues to be

00:28:12.850 --> 00:28:16.360
able to figure out who was
who in his small little cohort

00:28:16.360 --> 00:28:21.160
of kids that he knew all
the way through high school.

00:28:21.160 --> 00:28:24.580
Then he goes off to college,
and it's all these new people.

00:28:24.580 --> 00:28:26.100
And he's screwed.

00:28:26.100 --> 00:28:27.730
And he said to me
that he was just

00:28:27.730 --> 00:28:29.722
devastated because he
would go to a party,

00:28:29.722 --> 00:28:31.430
and he would meet
someone and think, wow,

00:28:31.430 --> 00:28:33.362
this is a really nice person.

00:28:33.362 --> 00:28:35.320
I would really like to
be this person's friend,

00:28:35.320 --> 00:28:37.028
but he would realize
he would have no way

00:28:37.028 --> 00:28:40.180
to find that person again.

00:28:40.180 --> 00:28:42.912
And he's like, it's
kind of oversharing

00:28:42.912 --> 00:28:45.370
to say when you've met somebody
for 10 minutes, by the way,

00:28:45.370 --> 00:28:46.330
I'm not going be
able to find you.

00:28:46.330 --> 00:28:47.163
You have to find me.

00:28:47.163 --> 00:28:49.870
It's like, you just don't want
to have to go there yet, right?

00:28:49.870 --> 00:28:53.500
So there's all kinds of things
that would make it a real drag

00:28:53.500 --> 00:28:55.720
to not be able to
recognize other faces,

00:28:55.720 --> 00:28:59.140
and now having said all of that,
I'll say that a surprisingly

00:28:59.140 --> 00:29:03.880
large of the population is in
Jacob's situation, about 2%

00:29:03.880 --> 00:29:04.930
of the population.

00:29:04.930 --> 00:29:06.460
It would be
unsurprising if there

00:29:06.460 --> 00:29:08.590
were one or two of you
in here, and if there is,

00:29:08.590 --> 00:29:09.507
you can tell me later.

00:29:09.507 --> 00:29:11.590
I'd love to scan you.

00:29:11.590 --> 00:29:16.510
But about 2% of the
population routinely

00:29:16.510 --> 00:29:19.000
fails to recognize
family members, people

00:29:19.000 --> 00:29:22.450
they know really well, right?

00:29:22.450 --> 00:29:26.890
And interestingly, this is
completely uncorrelated with IQ

00:29:26.890 --> 00:29:29.380
or with any other perceptual
ability or ability

00:29:29.380 --> 00:29:31.270
to read or recognize
scenes or anything else.

00:29:31.270 --> 00:29:31.770
Yeah.

00:29:31.770 --> 00:29:32.500
AUDIENCE: Is this
the kind of thing

00:29:32.500 --> 00:29:34.270
where you either have
it or don't have it?

00:29:34.270 --> 00:29:35.728
NANCY KANWISHER:
Oh, good question.

00:29:35.728 --> 00:29:36.760
No, it's a gradation.

00:29:36.760 --> 00:29:39.692
So the 2% at the bottom are not
this 2% who are really screwed,

00:29:39.692 --> 00:29:40.900
and everyone else is up here.

00:29:40.900 --> 00:29:44.290
It's a hugely wide
distribution, and the point

00:29:44.290 --> 00:29:46.240
is that the bottom end
of that distribution

00:29:46.240 --> 00:29:47.950
is really, really bad.

00:29:47.950 --> 00:29:49.880
They just can't do it at all.

00:29:49.880 --> 00:29:56.230
Similarly, the top end of that
distribution is weirdly good.

00:29:56.230 --> 00:29:58.270
They are so good
at face recognition

00:29:58.270 --> 00:30:00.760
that they have to hide it
socially because otherwise

00:30:00.760 --> 00:30:02.350
people feel creeped out.

00:30:02.350 --> 00:30:04.060
For example, as one
of those people--

00:30:04.060 --> 00:30:06.160
they're called
super recognizers.

00:30:06.160 --> 00:30:11.320
A bunch of them have been
hired by investigation services

00:30:11.320 --> 00:30:13.210
in London recently
as part of their kind

00:30:13.210 --> 00:30:14.530
of crime-solving unit.

00:30:14.530 --> 00:30:18.053
Those people are so
good that one of them

00:30:18.053 --> 00:30:19.970
said to me-- we scanned
a few of these people.

00:30:19.970 --> 00:30:24.040
One of them said, if I--

00:30:24.040 --> 00:30:26.800
she recounted this
event where she's

00:30:26.800 --> 00:30:29.050
standing in line waiting
for movie tickets,

00:30:29.050 --> 00:30:31.780
and she realizes that the
person in front of her in line

00:30:31.780 --> 00:30:34.660
was sitting at the next
table over at a cafe four

00:30:34.660 --> 00:30:35.980
years before.

00:30:35.980 --> 00:30:38.680
She says, if I share this
information with that person,

00:30:38.680 --> 00:30:40.240
they'll be creeped
out, so I've just

00:30:40.240 --> 00:30:41.450
learned to keep it to myself.

00:30:41.450 --> 00:30:43.990
But I know that was
the same person, right?

00:30:43.990 --> 00:30:46.090
So there's a huge spread.

00:30:46.090 --> 00:30:47.610
You had a question a while back.

00:30:47.610 --> 00:30:50.110
AUDIENCE: It's not a problem
with recognizing facial images,

00:30:50.110 --> 00:30:50.655
is it?

00:30:50.655 --> 00:30:53.320
So for example, Jacob,
looking at a person,

00:30:53.320 --> 00:30:54.700
could he describe them?

00:30:54.700 --> 00:30:56.260
NANCY KANWISHER: Absolutely.

00:30:56.260 --> 00:30:57.700
He knows that it's a face.

00:30:57.700 --> 00:30:59.440
He can tell if they're
male or female.

00:30:59.440 --> 00:31:02.420
He can tell if
they're happy or sad.

00:31:02.420 --> 00:31:04.530
It looks like a face to him.

00:31:04.530 --> 00:31:06.940
It just doesn't look
different than anyone else.

00:31:06.940 --> 00:31:07.950
Yeah.

00:31:07.950 --> 00:31:09.700
AUDIENCE: Is there
any difference--

00:31:09.700 --> 00:31:14.050
OK, for example, my father-- he
can tell faces in person just

00:31:14.050 --> 00:31:19.210
fine, but when he watches videos
of people, he just cannot.

00:31:19.210 --> 00:31:20.930
He cannot recognize
faces at all,

00:31:20.930 --> 00:31:22.583
so is there any difference?

00:31:22.583 --> 00:31:24.250
NANCY KANWISHER: There
are lots of cues.

00:31:24.250 --> 00:31:25.900
I mean, that's a very
interesting exercise

00:31:25.900 --> 00:31:26.525
to think about.

00:31:26.525 --> 00:31:29.290
What are the cues that
you have in person, right?

00:31:29.290 --> 00:31:30.770
You have all kinds
of other things.

00:31:30.770 --> 00:31:32.350
So first of all, there's lots
of constraining information.

00:31:32.350 --> 00:31:33.250
The person you're
looking at-- there

00:31:33.250 --> 00:31:35.417
are all kinds of things you
know about where you are

00:31:35.417 --> 00:31:39.040
and who that might
be that help, right?

00:31:39.040 --> 00:31:40.630
So yeah, there's
many different cues

00:31:40.630 --> 00:31:43.400
to face recognition that
might be engaged here.

00:31:43.400 --> 00:31:48.400
So my point is just that
face recognition matters.

00:31:48.400 --> 00:31:51.560
You can get by if you
can't do it, but it sucks.

00:31:51.560 --> 00:31:53.710
It's really hard, OK?

00:31:53.710 --> 00:31:55.560
OK, so yes, question?

00:31:55.560 --> 00:31:57.310
AUDIENCE: Do you have
any idea what people

00:31:57.310 --> 00:32:00.352
at [INAUDIBLE] university?

00:32:00.352 --> 00:32:02.550
Are there any sort
of models that

00:32:02.550 --> 00:32:05.220
suggest that when there are--

00:32:05.220 --> 00:32:07.060
like, when they see
faces, do they just

00:32:07.060 --> 00:32:09.327
see a sort of
conglomerate of shapes?

00:32:09.327 --> 00:32:11.660
NANCY KANWISHER: No, they see
the structure of the face.

00:32:11.660 --> 00:32:13.060
They see a proper face.

00:32:13.060 --> 00:32:15.460
If the eye was in the wrong
place, they would know.

00:32:15.460 --> 00:32:18.220
They absolutely know the
structure of the face.

00:32:18.220 --> 00:32:19.690
They all look kind of the same.

00:32:22.430 --> 00:32:24.398
By the way, we won't
have time to talk

00:32:24.398 --> 00:32:25.940
about this in any
detail, but there's

00:32:25.940 --> 00:32:28.160
a well-known fact that
probably many of you guys

00:32:28.160 --> 00:32:30.860
have experienced, which is
called the other-race effect.

00:32:30.860 --> 00:32:33.530
And that is the fact that
they all look the same.

00:32:33.530 --> 00:32:36.830
Whoever they are, if you
have less experience looking

00:32:36.830 --> 00:32:39.140
at that group of
people, you're less well

00:32:39.140 --> 00:32:41.360
able to tell them apart, OK?

00:32:41.360 --> 00:32:43.490
I have this problem
teaching all the time.

00:32:43.490 --> 00:32:46.278
I grew up in a rural
lily-white community.

00:32:46.278 --> 00:32:48.320
My face recognition is
not so good to begin with,

00:32:48.320 --> 00:32:51.080
and it's really not good
for non-Caucasian faces.

00:32:51.080 --> 00:32:52.340
It's embarrassing as hell.

00:32:52.340 --> 00:32:53.450
It feels disrespectful.

00:32:53.450 --> 00:32:55.310
I hate it.

00:32:55.310 --> 00:32:56.750
I fault myself,
but actually, it's

00:32:56.750 --> 00:32:58.430
just a fact of the
perceptual system.

00:32:58.430 --> 00:33:01.970
Your perceptual system is tuned
to the statistics of its input,

00:33:01.970 --> 00:33:06.540
and it's not so
plastic later in life.

00:33:06.540 --> 00:33:10.850
And so a way to simulate
a version that some of you

00:33:10.850 --> 00:33:13.670
may have experienced is
whatever race of faces

00:33:13.670 --> 00:33:15.777
you have less
experience with, if you

00:33:15.777 --> 00:33:17.360
find those people
hard to distinguish,

00:33:17.360 --> 00:33:19.100
it's not that you
can't tell it's a face.

00:33:19.100 --> 00:33:22.188
It's not that you wouldn't be
able to tell if the nose was

00:33:22.188 --> 00:33:22.980
in the wrong place.

00:33:22.980 --> 00:33:25.290
It's just hard to tell
one person from another,

00:33:25.290 --> 00:33:26.510
so it's a lot like that.

00:33:26.510 --> 00:33:28.430
I really need to get going,
so I'll take one more question

00:33:28.430 --> 00:33:28.930
and go.

00:33:28.930 --> 00:33:31.820
AUDIENCE: Wait, could you
kind of use an analogy?

00:33:31.820 --> 00:33:34.370
It's like being able
to tell people apart

00:33:34.370 --> 00:33:39.830
by their hands or something
to the point that you just--

00:33:39.830 --> 00:33:42.680
you can't really tell
people apart by their hands,

00:33:42.680 --> 00:33:45.600
usually, so is that kind of
how people with face blindness

00:33:45.600 --> 00:33:46.100
feel?

00:33:46.100 --> 00:33:47.150
It's just looking
at [INAUDIBLE]..

00:33:47.150 --> 00:33:48.900
NANCY KANWISHER: That's
all you had, yeah.

00:33:48.900 --> 00:33:50.013
Yeah, probably, probably.

00:33:50.013 --> 00:33:52.430
Yeah, and there is, by the
way, an interesting literature.

00:33:52.430 --> 00:33:54.500
You show people photographs
of their own hand

00:33:54.500 --> 00:33:55.880
and a bunch of other hands.

00:33:55.880 --> 00:33:58.503
People can't pick out
their own hand from--

00:33:58.503 --> 00:33:59.420
so yeah, you're right.

00:33:59.420 --> 00:34:01.970
We're not so good at that.

00:34:01.970 --> 00:34:03.053
OK, I'm going to go ahead.

00:34:03.053 --> 00:34:04.512
If you guys are
interested, I could

00:34:04.512 --> 00:34:06.680
post-- there's a whole
fascinating literature here,

00:34:06.680 --> 00:34:08.090
but actually, I got
dinged last year

00:34:08.090 --> 00:34:09.673
for talking about
face recognition too

00:34:09.673 --> 00:34:10.730
much and prosopagnosia.

00:34:10.730 --> 00:34:13.132
We all heard about it in 900.

00:34:13.132 --> 00:34:15.590
So I took most of that out,
and now you guys are asking me.

00:34:15.590 --> 00:34:16.820
So I don't know what
the right thing is.

00:34:16.820 --> 00:34:19.130
But I'm going to go on, and I
will put some optional readings

00:34:19.130 --> 00:34:20.880
online, especially if
you send me an email

00:34:20.880 --> 00:34:22.159
and tell me to do that.

00:34:22.159 --> 00:34:25.940
OK, so point is,
faces matter a lot.

00:34:25.940 --> 00:34:28.620
They matter for the
quality of life.

00:34:28.620 --> 00:34:30.800
They're important
because they convey

00:34:30.800 --> 00:34:33.620
a huge amount of information,
not just the identity

00:34:33.620 --> 00:34:37.880
of the person but also
their age, sex, mood, race,

00:34:37.880 --> 00:34:39.270
direction of attention.

00:34:39.270 --> 00:34:41.150
So if I'm lecturing
like this right now

00:34:41.150 --> 00:34:43.429
and I start doing that, you
guys are going to wonder,

00:34:43.429 --> 00:34:44.929
what the hell's
going on over there?

00:34:44.929 --> 00:34:46.190
Yeah, I saw a few heads turn.

00:34:46.190 --> 00:34:48.139
I'm just doing a little
demo here, right?

00:34:48.139 --> 00:34:51.560
We're very attuned to where
other people are looking, OK?

00:34:51.560 --> 00:34:54.110
So this is just one of
many different social cues

00:34:54.110 --> 00:34:55.070
we get from faces.

00:34:55.070 --> 00:35:00.200
There's just an incredibly rich
bunch of information in a face.

00:35:00.200 --> 00:35:02.660
We read in aspects of
people's personality

00:35:02.660 --> 00:35:04.572
from the shape of their
face even though it's

00:35:04.572 --> 00:35:06.530
been shown with some
interesting recent studies

00:35:06.530 --> 00:35:08.480
there's absolutely
nothing you can

00:35:08.480 --> 00:35:09.942
infer about a
person's personality

00:35:09.942 --> 00:35:11.150
from the shape of their face.

00:35:11.150 --> 00:35:14.957
We all do it, and we do
it in systematic ways.

00:35:14.957 --> 00:35:16.790
Another reason this is
important-- and faces

00:35:16.790 --> 00:35:18.380
are some of the
most common stimuli

00:35:18.380 --> 00:35:22.190
that we see in daily life,
starting from infancy where,

00:35:22.190 --> 00:35:24.590
I think, about 40%
of waking time,

00:35:24.590 --> 00:35:28.550
there's a face right in
front of an infant's eyes.

00:35:28.550 --> 00:35:32.570
And probably, these abilities
to extract all this information

00:35:32.570 --> 00:35:36.530
have been important throughout
our primate ancestry.

00:35:36.530 --> 00:35:39.260
So that's just to say, there's
a big space of face perception,

00:35:39.260 --> 00:35:41.093
and now we're going to
focus in on just face

00:35:41.093 --> 00:35:44.990
recognition, telling who
that person is, all right?

00:35:44.990 --> 00:35:48.530
So what questions do we want to
answer about face recognition?

00:35:48.530 --> 00:35:52.410
Well, a whole bunch of them, and
what methods do we want to use?

00:35:52.410 --> 00:35:54.380
So let's start with
some basic questions

00:35:54.380 --> 00:35:56.030
about face recognition.

00:35:56.030 --> 00:35:58.425
Well, first, as usual,
we want to know,

00:35:58.425 --> 00:36:00.800
what is the structure of the
problem in face recognition?

00:36:00.800 --> 00:36:01.620
What are the inputs?

00:36:01.620 --> 00:36:02.495
What are the outputs?

00:36:02.495 --> 00:36:03.500
Why is it hard, right?

00:36:03.500 --> 00:36:06.260
Just as we've been doing
for motion and color--

00:36:06.260 --> 00:36:08.480
that's Marr's
computational theory level.

00:36:08.480 --> 00:36:11.030
We want to know, how does
face recognition actually

00:36:11.030 --> 00:36:12.230
work in humans?

00:36:12.230 --> 00:36:13.700
What computations go on?

00:36:13.700 --> 00:36:16.250
What representations
are extracted?

00:36:16.250 --> 00:36:18.560
And is that answer different?

00:36:18.560 --> 00:36:20.540
Are we running different
code in our heads

00:36:20.540 --> 00:36:23.690
when we recognize faces from
when we recognize toasters

00:36:23.690 --> 00:36:27.298
and apples and dogs, OK?

00:36:27.298 --> 00:36:29.840
Another facet of that-- do we
have a totally different system

00:36:29.840 --> 00:36:31.850
for face recognition
from the recognition

00:36:31.850 --> 00:36:33.170
of all those other things?

00:36:33.170 --> 00:36:35.720
If so, then we might
want different theories

00:36:35.720 --> 00:36:38.000
of how face recognition works
from our theories of how

00:36:38.000 --> 00:36:39.230
object recognition works.

00:36:41.750 --> 00:36:44.840
How quickly do we detect
and recognize faces?

00:36:44.840 --> 00:36:47.120
That will help constrain
what kinds of computations

00:36:47.120 --> 00:36:50.450
might be going on.

00:36:50.450 --> 00:36:52.790
And of course, how
was face recognition

00:36:52.790 --> 00:36:54.920
actually implemented
in neurons in brains?

00:36:54.920 --> 00:36:57.170
So this is just some of the
big, wide-open questions

00:36:57.170 --> 00:36:58.500
we want to answer.

00:36:58.500 --> 00:37:01.670
So let's consider, what are
our tools for considering

00:37:01.670 --> 00:37:02.390
these things?

00:37:02.390 --> 00:37:04.310
And you guys should
all know what

00:37:04.310 --> 00:37:07.160
tools are available for
thinking at the level of Marr's

00:37:07.160 --> 00:37:08.270
computational theory--

00:37:08.270 --> 00:37:10.610
basically just thinking, right?

00:37:10.610 --> 00:37:12.110
You can collect
some images, too,

00:37:12.110 --> 00:37:15.020
but basically to understand
this, we just think.

00:37:15.020 --> 00:37:18.407
So for example,
as I keep saying,

00:37:18.407 --> 00:37:20.240
at the level of Marr's
computational theory,

00:37:20.240 --> 00:37:22.370
we want to know, what is
the problem to be solved?

00:37:22.370 --> 00:37:23.120
What is the input?

00:37:23.120 --> 00:37:23.912
What is the output?

00:37:23.912 --> 00:37:27.100
How might you go from that
input to that output, OK?

00:37:27.100 --> 00:37:29.410
So for example,
here's a stimulus

00:37:29.410 --> 00:37:33.010
that might hit a retina,
and then some magic happens.

00:37:33.010 --> 00:37:35.440
And then you just
say, Julia, OK?

00:37:35.440 --> 00:37:38.500
So we want to know, what's
going on in that magic, OK?

00:37:38.500 --> 00:37:41.597
And if a different image hits
your retina, you go, oh, Brad.

00:37:41.597 --> 00:37:42.430
That is, I wouldn't.

00:37:42.430 --> 00:37:43.953
I live in a cave.

00:37:43.953 --> 00:37:45.370
I barely get out
of the lab, but I

00:37:45.370 --> 00:37:47.662
understand that these are
people most people recognize.

00:37:47.662 --> 00:37:48.850
That's why I use them.

00:37:48.850 --> 00:37:49.683
That's the question.

00:37:49.683 --> 00:37:51.970
What goes on here in the middle?

00:37:51.970 --> 00:37:54.920
And your first thought
is, well, duh, easy.

00:37:54.920 --> 00:37:57.250
We could just make a
template, a kind of store

00:37:57.250 --> 00:38:00.280
the pixels that match that image
and take the incoming image

00:38:00.280 --> 00:38:02.440
and see if it exactly matches.

00:38:02.440 --> 00:38:06.020
And that's going to
work great, right?

00:38:06.020 --> 00:38:07.780
No.

00:38:07.780 --> 00:38:08.752
Why not?

00:38:08.752 --> 00:38:10.600
AUDIENCE: Different
angles [INAUDIBLE]..

00:38:10.600 --> 00:38:11.020
NANCY KANWISHER: Louder.

00:38:11.020 --> 00:38:12.450
AUDIENCE: Angles [INAUDIBLE].

00:38:12.450 --> 00:38:15.400
NANCY KANWISHER: Yeah,
yeah, absolutely.

00:38:15.400 --> 00:38:17.630
That's not going to work at all.

00:38:17.630 --> 00:38:21.550
And the problem is that we don't
just have one picture of Julia

00:38:21.550 --> 00:38:23.050
that we can match.

00:38:23.050 --> 00:38:25.900
There are loads of loads
of totally different kinds

00:38:25.900 --> 00:38:27.850
of pictures of
Julia, all of which

00:38:27.850 --> 00:38:31.600
we look at and immediately
go, Julia, no problem, OK?

00:38:31.600 --> 00:38:36.190
And so that means, what is it
that we're doing in our heads?

00:38:36.190 --> 00:38:39.790
If we're storing templates, we
have to store a lot of them,

00:38:39.790 --> 00:38:40.900
OK?

00:38:40.900 --> 00:38:44.810
So all those differences
in the images--

00:38:44.810 --> 00:38:46.750
so we could memorize
lots of templates.

00:38:46.750 --> 00:38:50.110
Well, that has long been
taken as like the reductio ad

00:38:50.110 --> 00:38:50.770
absurdum.

00:38:50.770 --> 00:38:52.150
That's the ridiculous
hypothesis.

00:38:52.150 --> 00:38:53.303
How could that be?

00:38:53.303 --> 00:38:55.720
How could there be room in
here to store lots of templates

00:38:55.720 --> 00:38:58.750
of each person, and
furthermore, how would that

00:38:58.750 --> 00:39:01.540
work for people we don't know?

00:39:01.540 --> 00:39:05.140
The other idea, which
is very vague right now,

00:39:05.140 --> 00:39:07.720
is that, well, maybe
we extract something

00:39:07.720 --> 00:39:11.260
that's common across all
of those, maybe something

00:39:11.260 --> 00:39:13.510
like the distance
between the eyes,

00:39:13.510 --> 00:39:16.030
something about the
shape of the mouth,

00:39:16.030 --> 00:39:17.800
other kinds of
properties that might

00:39:17.800 --> 00:39:20.560
be invariant across
those images, that is,

00:39:20.560 --> 00:39:23.290
that you could pull out
that information from any

00:39:23.290 --> 00:39:24.625
of those images, OK?

00:39:24.625 --> 00:39:26.500
It's sounding very vague
because it is vague.

00:39:26.500 --> 00:39:27.970
Nobody knows what
those would be,

00:39:27.970 --> 00:39:31.180
but the idea is, maybe
there's some image invariant

00:39:31.180 --> 00:39:34.000
properties of a face you can
get from here that you can then

00:39:34.000 --> 00:39:38.230
store and use to
recognize faces, OK?

00:39:38.230 --> 00:39:44.600
So now to think about this,
we can step back and say, OK,

00:39:44.600 --> 00:39:46.390
how is this done in machines?

00:39:46.390 --> 00:39:49.900
So machine face recognition
didn't work well at all

00:39:49.900 --> 00:39:52.240
until very recently, OK?

00:39:52.240 --> 00:39:55.398
And then all of a sudden,
a couple of years ago--

00:39:55.398 --> 00:39:57.190
here's another paper
from the different one

00:39:57.190 --> 00:39:58.550
that I showed you before.

00:39:58.550 --> 00:40:01.960
This one is VGGFace, one of
the major deep net systems

00:40:01.960 --> 00:40:02.890
for face recognition.

00:40:02.890 --> 00:40:04.667
It's widely used.

00:40:04.667 --> 00:40:06.250
There was another
one the year before.

00:40:06.250 --> 00:40:10.600
All of this since
2014, 2015-- hugely

00:40:10.600 --> 00:40:12.027
cited, widely influential.

00:40:12.027 --> 00:40:13.360
They're on all your smartphones.

00:40:13.360 --> 00:40:16.240
Boom, it all just happened
like nearly overnight

00:40:16.240 --> 00:40:19.840
with the availability of lots
of images to train deep nets.

00:40:19.840 --> 00:40:21.760
So now these things
are extremely

00:40:21.760 --> 00:40:26.240
effective and accurate,
and so in some sense,

00:40:26.240 --> 00:40:28.690
those networks are
possible models

00:40:28.690 --> 00:40:31.150
of what we're doing in our
heads when we recognize faces.

00:40:31.150 --> 00:40:33.160
It doesn't mean we do
it in the same way,

00:40:33.160 --> 00:40:34.490
but it's a possibility.

00:40:34.490 --> 00:40:37.240
It's a hypothesis
we could test, OK?

00:40:37.240 --> 00:40:37.990
Yeah.

00:40:37.990 --> 00:40:40.198
AUDIENCE: What is the current
state of the literature

00:40:40.198 --> 00:40:42.610
surrounding getting other
information from people's faces

00:40:42.610 --> 00:40:44.185
like moods or what they're--

00:40:44.185 --> 00:40:46.030
NANCY KANWISHER: Lots, lots.

00:40:46.030 --> 00:40:50.680
There's conferences and
machine vision competitions

00:40:50.680 --> 00:40:53.855
on extracting personality
properties, mood properties,

00:40:53.855 --> 00:40:55.480
every possible thing
you could imagine.

00:40:55.480 --> 00:40:57.550
This is a huge-- a lot of
people care about this.

00:40:57.550 --> 00:40:59.740
It's a huge field
in computer vision,

00:40:59.740 --> 00:41:01.840
and it's also a huge field
in cognitive science,

00:41:01.840 --> 00:41:03.787
asking what humans
pull from faces.

00:41:03.787 --> 00:41:05.620
AUDIENCE: How much
success that you've seen?

00:41:05.620 --> 00:41:07.390
NANCY KANWISHER: Oh god, others
would know that better than me.

00:41:07.390 --> 00:41:09.535
I bet it's pretty damn
good, a lot of it.

00:41:09.535 --> 00:41:11.410
Yeah, yeah, I mean,
these things are suddenly

00:41:11.410 --> 00:41:12.970
extremely effective.

00:41:12.970 --> 00:41:16.570
Yeah, OK, and there will be, by
the way, later in the course--

00:41:16.570 --> 00:41:19.060
my postdoc, Katharina Dobs,
who knows that literature much

00:41:19.060 --> 00:41:21.910
better than I do, will
talk about deep nets

00:41:21.910 --> 00:41:24.470
and their application in
human cognitive neuroscience.

00:41:24.470 --> 00:41:26.620
And she knows a lot about
the various networks

00:41:26.620 --> 00:41:29.080
that process face information.

00:41:29.080 --> 00:41:30.310
OK, so this is progress.

00:41:30.310 --> 00:41:32.890
Now we have some kind
of computational model.

00:41:32.890 --> 00:41:36.730
Trouble is, nobody really has an
intuitive understanding of what

00:41:36.730 --> 00:41:38.650
VGGFace is actually doing.

00:41:38.650 --> 00:41:39.910
You know how to train one up.

00:41:39.910 --> 00:41:43.870
There it is, but we don't really
understand what it's doing.

00:41:43.870 --> 00:41:46.210
And further, we have no
idea if what it's doing

00:41:46.210 --> 00:41:49.930
is anything like what
humans are doing, OK?

00:41:49.930 --> 00:41:51.970
So it's progress that
we have a model now

00:41:51.970 --> 00:41:54.730
that we didn't have
like five years ago,

00:41:54.730 --> 00:41:57.550
but we still have all
these questions open.

00:41:57.550 --> 00:42:01.120
OK, so on this first question,
what do we want to know?

00:42:01.120 --> 00:42:02.950
What we discover at
the level of Marr

00:42:02.950 --> 00:42:06.350
computational theory is a,
if not the, central challenge

00:42:06.350 --> 00:42:10.090
in face recognition is the huge
variation across images, which

00:42:10.090 --> 00:42:13.720
you know just by thinking about
it or trying to write the code.

00:42:13.720 --> 00:42:17.002
OK, so ooh, I'm
just barely able.

00:42:17.002 --> 00:42:18.460
I'm going to race
along, and Anya's

00:42:18.460 --> 00:42:20.252
going to tell me in
five minutes to switch.

00:42:20.252 --> 00:42:22.870
OK, so I want to talk
just a little bit

00:42:22.870 --> 00:42:23.900
about behavioral data.

00:42:23.900 --> 00:42:25.480
I'll run out of
time, and we'll roll

00:42:25.480 --> 00:42:26.855
this in less time,
because I want

00:42:26.855 --> 00:42:29.020
to include functional MRI
because you guys need it

00:42:29.020 --> 00:42:30.040
for the assignment.

00:42:30.040 --> 00:42:32.740
OK, so how are we going
to figure out what

00:42:32.740 --> 00:42:34.960
humans represent about faces?

00:42:34.960 --> 00:42:36.490
OK, so here we are.

00:42:36.490 --> 00:42:39.160
We consider this
possibility that one way

00:42:39.160 --> 00:42:41.650
to solve this problem is by
essentially memorizing lots

00:42:41.650 --> 00:42:43.720
of templates for each person.

00:42:43.720 --> 00:42:46.300
Another possibility is this
kind of vague, inchoate idea

00:42:46.300 --> 00:42:49.300
that maybe there's some abstract
representation that'll be

00:42:49.300 --> 00:42:51.370
the same across all of those.

00:42:51.370 --> 00:42:54.110
How are we going to figure
out which humans do?

00:42:54.110 --> 00:42:57.040
Well, if we're really
memorizing lots of templates

00:42:57.040 --> 00:42:59.350
for each person and that's
how we recognize them

00:42:59.350 --> 00:43:02.860
in all their different guises,
that wouldn't work for people

00:43:02.860 --> 00:43:03.760
we didn't know.

00:43:03.760 --> 00:43:06.940
That is, you wouldn't be able to
take two different photographs

00:43:06.940 --> 00:43:10.600
of the same person and know if
it's the same person or not,

00:43:10.600 --> 00:43:11.530
right?

00:43:11.530 --> 00:43:13.450
Because you could only
do this by memorizing.

00:43:13.450 --> 00:43:15.280
Does everybody get that idea?

00:43:15.280 --> 00:43:17.290
Whereas whatever
this other idea is,

00:43:17.290 --> 00:43:20.352
it should work somewhat
for novel individuals

00:43:20.352 --> 00:43:21.310
you don't already know.

00:43:21.310 --> 00:43:22.352
Here are two photographs.

00:43:22.352 --> 00:43:24.410
Same person or different person?

00:43:24.410 --> 00:43:27.790
So now let's ask,
can humans do this?

00:43:27.790 --> 00:43:30.430
Do we store lots of
templates for individuals,

00:43:30.430 --> 00:43:32.920
or can we do something
more abstract?

00:43:32.920 --> 00:43:35.680
Well, if we simply
deal with this problem

00:43:35.680 --> 00:43:38.320
by storing lots of templates
for each individual,

00:43:38.320 --> 00:43:41.830
maybe not literally pixel
templates but some kind

00:43:41.830 --> 00:43:45.520
of snapshot, then
the key test is,

00:43:45.520 --> 00:43:48.070
we shouldn't be able to do
this matching task if we

00:43:48.070 --> 00:43:49.480
don't know that person.

00:43:49.480 --> 00:43:51.280
Everybody get the logic here?

00:43:51.280 --> 00:43:52.720
OK, so let's try it.

00:43:52.720 --> 00:43:54.760
So this paper a few years ago--

00:43:54.760 --> 00:43:55.780
Jenkins et al.

00:43:55.780 --> 00:43:57.050
Asked that question.

00:43:57.050 --> 00:43:58.460
So here's what they did.

00:43:58.460 --> 00:44:00.550
They collected a whole
bunch of photographs

00:44:00.550 --> 00:44:03.430
of Dutch politicians
with multiple images

00:44:03.430 --> 00:44:05.680
of each politician, OK?

00:44:05.680 --> 00:44:08.650
Then they gave them to people
on cards, and they said,

00:44:08.650 --> 00:44:11.140
there are multiple
images of each person.

00:44:11.140 --> 00:44:13.660
And I'm not going to tell you
how many different politicians

00:44:13.660 --> 00:44:14.530
are in this deck.

00:44:14.530 --> 00:44:17.050
Just sort them in piles,
so there's a different pile

00:44:17.050 --> 00:44:19.148
for each person, OK?

00:44:19.148 --> 00:44:21.190
I'm going to show you a
low-tech version of this.

00:44:21.190 --> 00:44:23.190
I'm going to show you a
whole bunch of pictures,

00:44:23.190 --> 00:44:24.670
all in one array,
and you guys are

00:44:24.670 --> 00:44:28.690
going to try to figure out
how many people are there, OK?

00:44:28.690 --> 00:44:29.470
Everybody ready?

00:44:29.470 --> 00:44:30.890
I'm just going to leave
it up for a few seconds.

00:44:30.890 --> 00:44:32.432
There's going to be
lots of pictures.

00:44:32.432 --> 00:44:35.990
Your task is how many different
individuals are depicted here.

00:44:35.990 --> 00:44:36.490
Here we go.

00:44:42.470 --> 00:44:44.030
OK, write down your best guess.

00:44:44.030 --> 00:44:46.760
Just kind of look around.

00:44:46.760 --> 00:44:49.910
OK, everybody got a guess?

00:44:49.910 --> 00:44:53.290
OK, write down your guess.

00:44:53.290 --> 00:44:56.410
OK, how many people
think there were over 10

00:44:56.410 --> 00:44:59.110
different individuals there?

00:44:59.110 --> 00:44:59.620
One.

00:44:59.620 --> 00:45:01.660
OK, how many people
think over five?

00:45:04.170 --> 00:45:07.350
Yeah, probably half of you.

00:45:07.350 --> 00:45:10.800
How many people
think over three?

00:45:10.800 --> 00:45:12.165
Most of you.

00:45:12.165 --> 00:45:12.930
There are two.

00:45:16.860 --> 00:45:18.370
What does that mean?

00:45:18.370 --> 00:45:19.830
That means you can't do it.

00:45:19.830 --> 00:45:22.560
That means you can't
match different images

00:45:22.560 --> 00:45:27.090
of the same person if you
don't know that person.

00:45:27.090 --> 00:45:29.140
Pretty surprising, isn't it?

00:45:29.140 --> 00:45:31.080
We think we're so awesome
at face recognition

00:45:31.080 --> 00:45:32.830
because most of the
time, what we're doing

00:45:32.830 --> 00:45:34.770
is recognizing people
we know, people

00:45:34.770 --> 00:45:37.410
we've seen in all different
viewpoints and hair

00:45:37.410 --> 00:45:41.010
arrangements and stuff.

00:45:41.010 --> 00:45:42.780
If you don't have
lots of opportunity

00:45:42.780 --> 00:45:45.090
to store all those things
and it's a novel face,

00:45:45.090 --> 00:45:47.940
we're really bad at that, OK?

00:45:47.940 --> 00:45:48.440
Yeah.

00:45:48.440 --> 00:45:50.250
AUDIENCE: But there's
a constraint of time?

00:45:50.250 --> 00:45:51.090
NANCY KANWISHER:
Yeah, yeah, I was

00:45:51.090 --> 00:45:52.470
trying to make the demo work.

00:45:52.470 --> 00:45:54.447
But OK, so the way
they do this task,

00:45:54.447 --> 00:45:56.280
people have unlimited
time, and they're just

00:45:56.280 --> 00:45:57.360
kind of sorting them.

00:45:57.360 --> 00:46:00.240
The mean number of piles that
people made in this experiment

00:46:00.240 --> 00:46:01.920
was 7 and 1/2.

00:46:01.920 --> 00:46:03.973
Correct answer's two, OK?

00:46:03.973 --> 00:46:05.640
OK, now you might
say, well, maybe those

00:46:05.640 --> 00:46:07.770
are shitty photographs, right?

00:46:07.770 --> 00:46:09.660
OK, so here's the control.

00:46:09.660 --> 00:46:11.310
Those are Dutch politicians.

00:46:11.310 --> 00:46:14.550
They then did the same
experiment on Dutch people who

00:46:14.550 --> 00:46:16.740
look at that photograph
and in about two seconds

00:46:16.740 --> 00:46:21.010
say two, duh, OK?

00:46:21.010 --> 00:46:23.510
So if you know there's nothing
wrong with those photographs,

00:46:23.510 --> 00:46:26.510
it's just a matter of whether
you know those people or not.

00:46:26.510 --> 00:46:31.370
OK, so the point of all of
this is that this crazy story

00:46:31.370 --> 00:46:34.245
that, in fact, what a
lot of what we're doing--

00:46:34.245 --> 00:46:35.870
I'm sort of simplifying
here, but a lot

00:46:35.870 --> 00:46:37.160
of what we're doing
in face recognition,

00:46:37.160 --> 00:46:39.140
a lot of the way we
deal with all this image

00:46:39.140 --> 00:46:43.280
variability is not that we
have some very abstract, fancy,

00:46:43.280 --> 00:46:47.480
high-level representation
of each individual face.

00:46:47.480 --> 00:46:49.580
We just have lots of
experience with faces,

00:46:49.580 --> 00:46:52.190
and we use that so
that if we have a novel

00:46:52.190 --> 00:46:54.350
face that we don't have
all that experience with,

00:46:54.350 --> 00:46:55.400
we're not so good at it.

00:46:55.400 --> 00:46:56.990
I'm going to run out of time,
so I'll take one question

00:46:56.990 --> 00:46:57.780
and go on.

00:46:57.780 --> 00:46:59.405
AUDIENCE: How do they
control for-- you

00:46:59.405 --> 00:47:01.895
know the issue you said about
if you don't have experience

00:47:01.895 --> 00:47:04.100
with similar races.

00:47:04.100 --> 00:47:05.840
NANCY KANWISHER: Yeah.

00:47:05.840 --> 00:47:08.510
I'm sure whenever you do
face recognition experiments,

00:47:08.510 --> 00:47:11.360
you make sure that if
your dominant subject

00:47:11.360 --> 00:47:14.780
pool is Caucasian, you have
Caucasian faces or whatever.

00:47:14.780 --> 00:47:15.707
Yeah.

00:47:15.707 --> 00:47:17.540
Unless it's something
you don't understand--

00:47:17.540 --> 00:47:18.780
I'm going to hang
around after class.

00:47:18.780 --> 00:47:20.960
You can ask me questions
there, or if you have to go,

00:47:20.960 --> 00:47:21.980
you can email me
because I really

00:47:21.980 --> 00:47:23.563
want to get through
this next bit, OK?

00:47:28.190 --> 00:47:29.460
OK, so there we are with that.

00:47:29.460 --> 00:47:31.820
So what this suggests,
kind of, sort of,

00:47:31.820 --> 00:47:33.980
is that whatever
we're doing, it's

00:47:33.980 --> 00:47:35.690
something that
benefits enormously

00:47:35.690 --> 00:47:38.120
from lots and lots of
experience with that individual.

00:47:38.120 --> 00:47:40.760
Maybe it's not
literal memorization

00:47:40.760 --> 00:47:43.820
of actual pixel-like snapshots,
but it's something more

00:47:43.820 --> 00:47:45.590
like that than anybody
would have guessed

00:47:45.590 --> 00:47:49.670
before this experiment, OK?

00:47:49.670 --> 00:47:54.990
OK, all right, I'm going to
skip this awesome stuff here.

00:47:54.990 --> 00:47:57.020
OK.

00:47:57.020 --> 00:48:00.027
OK, so the benefits
of-- actually,

00:48:00.027 --> 00:48:02.360
I'm going to come back and
do that slide next time, too.

00:48:02.360 --> 00:48:04.890
And we're going to cut
straight to functional MRI.

00:48:04.890 --> 00:48:06.830
I'm sorry about this, but I just
really want you guys to have

00:48:06.830 --> 00:48:08.247
this background
in case you don't.

00:48:08.247 --> 00:48:09.980
You probably do.

00:48:09.980 --> 00:48:12.500
So functional MRI--
another cool method

00:48:12.500 --> 00:48:15.210
in cognitive neuroscience, and
how would it be useful here?

00:48:15.210 --> 00:48:17.150
OK, so first, what is it?

00:48:17.150 --> 00:48:19.880
Functional MRI is the
same as regular MRI

00:48:19.880 --> 00:48:22.040
that's in probably
tens of thousands

00:48:22.040 --> 00:48:24.140
of hospitals around the world.

00:48:24.140 --> 00:48:26.030
The big advances
in functional MRI

00:48:26.030 --> 00:48:28.310
were when some physicists
in the early '90s

00:48:28.310 --> 00:48:31.010
figured out how to take
those images really fast

00:48:31.010 --> 00:48:33.950
and how to make images
that reflect not just

00:48:33.950 --> 00:48:36.920
the density of tissue but
the activity of neurons

00:48:36.920 --> 00:48:38.720
at each point in the brain, OK?

00:48:38.720 --> 00:48:40.190
That was big stuff, OK?

00:48:40.190 --> 00:48:41.570
Early 1990s.

00:48:41.570 --> 00:48:43.190
And so the reason
it's a big deal

00:48:43.190 --> 00:48:47.570
it is the best, highest
spatial resolution

00:48:47.570 --> 00:48:51.290
method for making pictures
of human brain function

00:48:51.290 --> 00:48:52.610
noninvasively.

00:48:52.610 --> 00:48:55.070
That means without opening
up the head, all right?

00:48:55.070 --> 00:48:56.488
So that's an important thing.

00:48:56.488 --> 00:48:58.530
That's why there's lots
and lots of papers on it.

00:48:58.530 --> 00:49:00.697
That's why we're going to
spend a lot of time on it.

00:49:00.697 --> 00:49:04.400
The bare basics are that the
functional MRI signal that's

00:49:04.400 --> 00:49:06.020
used is called the BOLD signal.

00:49:06.020 --> 00:49:08.570
That stands for blood
oxygenation level

00:49:08.570 --> 00:49:10.640
dependent signal, OK?

00:49:10.640 --> 00:49:13.010
And what that means is this.

00:49:13.010 --> 00:49:15.680
Basic signal is blood flow.

00:49:15.680 --> 00:49:18.200
And so the way it works
is if a bunch of neurons

00:49:18.200 --> 00:49:20.990
someplace in your brain
start firing a lot,

00:49:20.990 --> 00:49:25.340
that's metabolically expensive
to make all those neurons fire,

00:49:25.340 --> 00:49:26.870
and so you have
to send more blood

00:49:26.870 --> 00:49:28.320
to that part of the brain.

00:49:28.320 --> 00:49:32.990
So it's just like if you go for
a run, the muscles in your legs

00:49:32.990 --> 00:49:35.960
need more blood delivered
to them to supply them

00:49:35.960 --> 00:49:38.030
metabolically for that
increased activity,

00:49:38.030 --> 00:49:40.820
and so the blood flow
increase to your leg muscles

00:49:40.820 --> 00:49:42.320
will increase, OK?

00:49:42.320 --> 00:49:44.360
Well, similarly, the
blood flow increases

00:49:44.360 --> 00:49:46.350
to active parts of the brain.

00:49:46.350 --> 00:49:49.580
Now, the weird part of it
is that for reasons nobody

00:49:49.580 --> 00:49:52.430
completely understands,
the blood flow increase

00:49:52.430 --> 00:49:54.920
more than compensates
for the oxygen use,

00:49:54.920 --> 00:49:56.990
so the signal is
actually backwards.

00:49:56.990 --> 00:50:02.390
Active parts of the brain
have less, not more,

00:50:02.390 --> 00:50:06.800
deoxygenated hemoglobin compared
to oxygenated hemoglobin,

00:50:06.800 --> 00:50:10.430
and the relevance of that is
that oxygenated hemoglobin

00:50:10.430 --> 00:50:12.920
and deoxygenated
hemoglobin are magnetically

00:50:12.920 --> 00:50:15.860
different in the way that
the MRI signal can see.

00:50:15.860 --> 00:50:17.930
So the basic signal
you're looking at

00:50:17.930 --> 00:50:20.430
is, how much oxygen
is there in the blood

00:50:20.430 --> 00:50:24.620
in that part of the
brain, and hence,

00:50:24.620 --> 00:50:26.360
how much blood flow went there?

00:50:26.360 --> 00:50:28.810
And hence, how much
neural activity was there?

00:50:28.810 --> 00:50:30.110
Did that sort of make sense?

00:50:30.110 --> 00:50:32.240
I'm not going to test you
on which is paramagnetic

00:50:32.240 --> 00:50:33.290
and which is diamagnetic.

00:50:33.290 --> 00:50:33.980
I never remember.

00:50:33.980 --> 00:50:35.270
I couldn't care less,
but you should know what

00:50:35.270 --> 00:50:36.800
the basic signal is, right?

00:50:36.800 --> 00:50:38.960
It's a magnetic
difference that results

00:50:38.960 --> 00:50:40.820
from oxygenation
differences that

00:50:40.820 --> 00:50:42.530
result from blood
flow differences that

00:50:42.530 --> 00:50:43.880
result from neural activity.

00:50:43.880 --> 00:50:44.653
Yeah.

00:50:44.653 --> 00:50:46.710
AUDIENCE: So when
there's more blood flow,

00:50:46.710 --> 00:50:48.095
there's more oxygenated?

00:50:48.095 --> 00:50:49.470
NANCY KANWISHER:
More oxygenated,

00:50:49.470 --> 00:50:53.730
and because it overcompensates
for the metabolic use

00:50:53.730 --> 00:50:58.860
of the neurons, the active parts
that you see with an MRI signal

00:50:58.860 --> 00:51:02.380
have more oxygenated
hemoglobin, right?

00:51:02.380 --> 00:51:06.100
OK, all right, so
that's the basic signal.

00:51:06.100 --> 00:51:08.320
And because that's
the basic signal,

00:51:08.320 --> 00:51:10.600
there's a bunch of things
we can tell already.

00:51:10.600 --> 00:51:13.400
So first of all--

00:51:13.400 --> 00:51:15.610
I'm just going to--
am I going to do this?

00:51:15.610 --> 00:51:17.290
Yeah, I'm going
to skip over this.

00:51:17.290 --> 00:51:19.450
It doesn't really matter.

00:51:19.450 --> 00:51:21.910
Because it's all based
on blood flow, one,

00:51:21.910 --> 00:51:24.070
it's extremely indirect--

00:51:24.070 --> 00:51:28.180
neural activity, blood flow
change, overcompensation,

00:51:28.180 --> 00:51:32.710
different magnetic
response, MRI image, right?

00:51:32.710 --> 00:51:35.500
So you would think with
all those different steps

00:51:35.500 --> 00:51:39.100
that you would get a really
weird, nonlinear, messy, crappy

00:51:39.100 --> 00:51:41.560
signal out the other end.

00:51:41.560 --> 00:51:43.780
And it is one of
the major challenges

00:51:43.780 --> 00:51:46.512
of my personal atheism,
but actually, you

00:51:46.512 --> 00:51:48.220
get a damn good signal
out the other end.

00:51:48.220 --> 00:51:50.303
And it's pretty linear
with neural activity, which

00:51:50.303 --> 00:51:51.970
seems like kind of
a freaking miracle,

00:51:51.970 --> 00:51:53.860
given how indirect it is, OK?

00:51:53.860 --> 00:51:55.870
But that has empowered
this whole huge field

00:51:55.870 --> 00:51:58.930
to discover cool things about
the organization of the brain.

00:51:58.930 --> 00:52:01.300
OK, nonetheless, there
are many caveats.

00:52:01.300 --> 00:52:03.460
Because it's blood
flow, the signal

00:52:03.460 --> 00:52:06.370
is limited in spatial
resolution down

00:52:06.370 --> 00:52:09.430
to-- people fight about this,
but around a millimeter.

00:52:09.430 --> 00:52:11.402
There are cowboys
in the field who

00:52:11.402 --> 00:52:13.360
think that they can get
less than a millimeter.

00:52:13.360 --> 00:52:13.930
Maybe.

00:52:13.930 --> 00:52:14.770
I don't know.

00:52:14.770 --> 00:52:16.540
It's debated.

00:52:16.540 --> 00:52:19.630
And the temporal
resolution is terrible.

00:52:19.630 --> 00:52:21.285
Blood flow changes
take a long time.

00:52:21.285 --> 00:52:21.910
Think about it.

00:52:21.910 --> 00:52:22.817
You start running.

00:52:22.817 --> 00:52:24.400
How long does it
take before the blood

00:52:24.400 --> 00:52:25.690
flow increases to your calves?

00:52:25.690 --> 00:52:28.150
Well, if you're really fit,
it's probably fast, but still

00:52:28.150 --> 00:52:29.440
going to take a few seconds.

00:52:29.440 --> 00:52:31.270
It takes about six
seconds for those blood

00:52:31.270 --> 00:52:34.510
flow changes in the brain
after neural activity.

00:52:34.510 --> 00:52:36.890
And it happens over a
big sloppy chunk of time,

00:52:36.890 --> 00:52:39.790
and so you don't have
much temporal resolution

00:52:39.790 --> 00:52:40.930
with functional MRI.

00:52:40.930 --> 00:52:42.760
Does that make sense?

00:52:42.760 --> 00:52:45.150
OK.

00:52:45.150 --> 00:52:49.300
OK, because it's this
very indirect signal,

00:52:49.300 --> 00:52:52.420
that also means that when we
get a change in the MRI signal,

00:52:52.420 --> 00:52:54.730
we don't exactly know
what's causing it.

00:52:54.730 --> 00:52:56.470
Is it synaptic activity?

00:52:56.470 --> 00:52:58.120
Is it actual neural firing?

00:52:58.120 --> 00:52:59.950
Is it one cell
inhibiting another?

00:52:59.950 --> 00:53:01.772
Is it a cell making protein?

00:53:01.772 --> 00:53:03.730
I mean, it could be any
of these things, right?

00:53:03.730 --> 00:53:06.880
So we don't know,
and that's a problem.

00:53:06.880 --> 00:53:09.070
And another problem is
the number you get out

00:53:09.070 --> 00:53:14.770
is just the intensity of the
detection of deoxyhemoglobin.

00:53:14.770 --> 00:53:17.650
It doesn't translate directly
into an absolute amount

00:53:17.650 --> 00:53:19.210
of neural activity.

00:53:19.210 --> 00:53:21.430
The consequence of
that is all you can do

00:53:21.430 --> 00:53:23.170
is compare two conditions.

00:53:23.170 --> 00:53:26.020
You can never say, there
was this exact amount

00:53:26.020 --> 00:53:27.970
of metabolic
activity right there.

00:53:27.970 --> 00:53:29.890
You can only say it was
more in this condition

00:53:29.890 --> 00:53:32.590
than that condition, OK?

00:53:32.590 --> 00:53:35.020
All right, so those
are the major caveats.

00:53:35.020 --> 00:53:38.140
Nonetheless, we can
discover some cool stuff.

00:53:38.140 --> 00:53:41.800
OK, so let's suppose, to get
back to face recognition,

00:53:41.800 --> 00:53:44.200
you wanted to know,
is face recognition

00:53:44.200 --> 00:53:48.290
a different problem in the brain
from object recognition, right?

00:53:48.290 --> 00:53:50.920
If it was, you might want
to write different code

00:53:50.920 --> 00:53:52.780
to try to understand
it from the code you're

00:53:52.780 --> 00:53:54.072
writing for object recognition.

00:53:54.072 --> 00:53:56.710
It's something you'd
kind of want to know, OK?

00:53:56.710 --> 00:53:58.870
So here's an experiment
I did-- god--

00:53:58.870 --> 00:53:59.840
20 years ago.

00:53:59.840 --> 00:54:02.442
Anyway, simplest
possible thing--

00:54:02.442 --> 00:54:04.900
so it's the easiest way I can
explain to you the bare bones

00:54:04.900 --> 00:54:06.610
of a simple MRI experiment.

00:54:06.610 --> 00:54:08.860
You pop the subject
in the scanner.

00:54:08.860 --> 00:54:12.160
You scan their head continuously
for about five minutes

00:54:12.160 --> 00:54:14.680
while they look at
a bunch of faces.

00:54:14.680 --> 00:54:16.347
For 20 seconds,
they stare at a dot.

00:54:16.347 --> 00:54:17.680
They look at a bunch of objects.

00:54:17.680 --> 00:54:18.930
They stare at a dot, OK?

00:54:18.930 --> 00:54:20.290
It's a five-minute experiment.

00:54:20.290 --> 00:54:22.370
You're scanning them
that whole time.

00:54:22.370 --> 00:54:25.570
And then you ask, of each
three-dimensional pixel,

00:54:25.570 --> 00:54:28.090
or voxel, in their
brain, whether the signal

00:54:28.090 --> 00:54:30.820
was higher in that voxel
while the subject was

00:54:30.820 --> 00:54:34.240
looking at faces than while they
were looking at objects, OK?

00:54:34.240 --> 00:54:36.695
And when you do
that, you get a blob.

00:54:36.695 --> 00:54:39.070
I've outlined it in green
here, but there's a little blob

00:54:39.070 --> 00:54:39.250
there.

00:54:39.250 --> 00:54:41.200
This is a slice through
the brain like this.

00:54:41.200 --> 00:54:44.230
That blob is right in here
on the bottom of the brain.

00:54:44.230 --> 00:54:46.270
And the statistics
are telling us

00:54:46.270 --> 00:54:49.150
that the MRI signal is
higher during the face epochs

00:54:49.150 --> 00:54:50.470
than the object epochs--

00:54:50.470 --> 00:54:51.860
everybody with me here--

00:54:51.860 --> 00:54:54.970
which implies very indirectly
that the neural activity

00:54:54.970 --> 00:54:57.970
of that region was higher when
this person was looking at

00:54:57.970 --> 00:55:00.610
faces than when they
were looking at objects.

00:55:00.610 --> 00:55:02.508
OK, now whenever you
see a blob like that,

00:55:02.508 --> 00:55:05.050
really, you want to see the data
that went into it, so here's

00:55:05.050 --> 00:55:06.070
mine.

00:55:06.070 --> 00:55:08.920
This is now the raw
average MRI signal

00:55:08.920 --> 00:55:12.070
intensity in that bit
of brain over the five

00:55:12.070 --> 00:55:13.870
minutes of the scan.

00:55:13.870 --> 00:55:17.380
You can see the
signal's higher in that

00:55:17.380 --> 00:55:20.782
region when the person
is looking at faces--

00:55:20.782 --> 00:55:22.240
these bars here--
than when they're

00:55:22.240 --> 00:55:23.740
looking at objects there.

00:55:23.740 --> 00:55:24.628
Everyone get that?

00:55:24.628 --> 00:55:26.170
That's what the
stats are telling us.

00:55:26.170 --> 00:55:27.920
This is just the reality
check of the data

00:55:27.920 --> 00:55:29.860
that produced those stats.

00:55:29.860 --> 00:55:34.120
OK, so now, in fact,
you can see something

00:55:34.120 --> 00:55:36.005
like that in pretty much
every normal person.

00:55:36.005 --> 00:55:38.380
I could pop any of you in the
scanner, and in 10 minutes,

00:55:38.380 --> 00:55:41.680
we'd find yours, OK?

00:55:41.680 --> 00:55:43.600
Now, here's the key question.

00:55:43.600 --> 00:55:46.300
Does this so far-- let's
suppose you find this in anyone.

00:55:46.300 --> 00:55:47.830
You do all the stats you like.

00:55:47.830 --> 00:55:50.230
It's as robust as you
could possibly want.

00:55:50.230 --> 00:55:54.550
Do these data alone tell us
that that region is specifically

00:55:54.550 --> 00:55:55.540
responsive to faces?

00:55:58.590 --> 00:55:59.090
No.

00:55:59.090 --> 00:56:00.552
Why not?

00:56:00.552 --> 00:56:02.520
AUDIENCE: Because it could--

00:56:02.520 --> 00:56:08.240
just like that certain
arrangement of features,

00:56:08.240 --> 00:56:12.140
or it could be reacting to
the variable light intensities

00:56:12.140 --> 00:56:16.113
that's reflecting off people's
skin-- coloration of the skin.

00:56:16.113 --> 00:56:17.030
NANCY KANWISHER: Good.

00:56:17.030 --> 00:56:18.270
Keep going.

00:56:18.270 --> 00:56:19.160
What else?

00:56:19.160 --> 00:56:19.850
Yes, you.

00:56:19.850 --> 00:56:22.723
AUDIENCE: Different faces of
anything, like even animals.

00:56:22.723 --> 00:56:24.890
NANCY KANWISHER: Yeah, then
it might still be faces,

00:56:24.890 --> 00:56:25.880
but it would be
different if it's

00:56:25.880 --> 00:56:27.410
human faces versus any faces.

00:56:27.410 --> 00:56:28.763
We kind of want to know, right?

00:56:28.763 --> 00:56:29.930
The code would be different.

00:56:29.930 --> 00:56:30.823
Yeah.

00:56:30.823 --> 00:56:32.618
AUDIENCE: It might be
because of the face

00:56:32.618 --> 00:56:35.765
is part of the bigger whole
and the object's a [INAUDIBLE]..

00:56:35.765 --> 00:56:38.140
NANCY KANWISHER: Uh-huh, the
face is a part of something,

00:56:38.140 --> 00:56:40.140
absolutely, where the
object is the whole thing.

00:56:40.140 --> 00:56:42.050
What else?

00:56:42.050 --> 00:56:42.550
Yeah.

00:56:42.550 --> 00:56:44.425
AUDIENCE: Maybe the
orientation of the object

00:56:44.425 --> 00:56:48.657
is so much simpler
than the human face is.

00:56:48.657 --> 00:56:50.740
NANCY KANWISHER: Just
objects are simpler or maybe

00:56:50.740 --> 00:56:52.240
just easier.

00:56:52.240 --> 00:56:53.950
Maybe it's just
hard to distinguish

00:56:53.950 --> 00:56:57.070
one face from another, and
so you need more blood flow.

00:56:57.070 --> 00:56:59.560
Really, what that thing
is-- that's a generic object

00:56:59.560 --> 00:57:02.200
recognition system, but
it has a harder time

00:57:02.200 --> 00:57:03.700
distinguishing faces
from each other

00:57:03.700 --> 00:57:05.120
because they're so similar.

00:57:05.120 --> 00:57:06.190
So there's more activity.

00:57:06.190 --> 00:57:07.960
Everybody get that?

00:57:07.960 --> 00:57:10.215
OK, what else?

00:57:10.215 --> 00:57:11.590
I'm going to go
two minutes over,

00:57:11.590 --> 00:57:13.540
so if people have
to leave, that's OK.

00:57:13.540 --> 00:57:16.235
I'll try not to go more
than two minutes over.

00:57:16.235 --> 00:57:16.735
What else?

00:57:20.100 --> 00:57:21.765
Yeah.

00:57:21.765 --> 00:57:23.140
AUDIENCE: It could
just as easily

00:57:23.140 --> 00:57:28.648
be responsible for seeing
maybe functional cues rather

00:57:28.648 --> 00:57:30.093
than face recognition.

00:57:30.093 --> 00:57:31.260
NANCY KANWISHER: Yeah, yeah.

00:57:31.260 --> 00:57:32.843
As I just said,
there's all this stuff

00:57:32.843 --> 00:57:36.720
we get from a face, not just
who is it, but are they healthy?

00:57:36.720 --> 00:57:38.102
What mood are they in?

00:57:38.102 --> 00:57:39.060
Where are they looking?

00:57:39.060 --> 00:57:41.400
All that stuff.

00:57:41.400 --> 00:57:43.830
OK, so what you
guys just did-- this

00:57:43.830 --> 00:57:46.320
is just basic common sense,
but it's also the essence

00:57:46.320 --> 00:57:47.522
of scientific reasoning.

00:57:47.522 --> 00:57:49.230
And we'll do a lot of
that in this class.

00:57:49.230 --> 00:57:52.420
And the crux of the matter
is, here's some data.

00:57:52.420 --> 00:57:53.580
Here's an inference.

00:57:53.580 --> 00:57:57.690
And so your job is
to think, is there

00:57:57.690 --> 00:58:00.900
any way that inference might
not follow from those data?

00:58:00.900 --> 00:58:03.660
How else might we account
for those data, OK?

00:58:03.660 --> 00:58:05.850
And you guys just did
that beautifully, OK?

00:58:05.850 --> 00:58:07.770
So the essence of
good science is

00:58:07.770 --> 00:58:11.820
whenever you see some data and
an inference, ask yourself,

00:58:11.820 --> 00:58:13.450
how might that
inference be wrong?

00:58:13.450 --> 00:58:15.750
How else might we
account for those data?

00:58:15.750 --> 00:58:17.940
OK, so that's what
you guys just did.

00:58:17.940 --> 00:58:21.000
I had previously made a list of
other things that might mean.

00:58:21.000 --> 00:58:22.590
It could respond
to anything human.

00:58:22.590 --> 00:58:24.270
You had said any kind of
face, but it could also

00:58:24.270 --> 00:58:25.978
be just anything human,
may be a response

00:58:25.978 --> 00:58:28.500
to hands, any body
part, anything

00:58:28.500 --> 00:58:32.100
we pay more attention to,
anything that has curves in it,

00:58:32.100 --> 00:58:35.100
or any of the suggestions
you guys made, OK?

00:58:35.100 --> 00:58:38.670
So the crux of the matter in
how you do a good functional MRI

00:58:38.670 --> 00:58:41.670
experiment or make a strong
claim about a part of the brain

00:58:41.670 --> 00:58:43.950
based on functional
MRI is to take

00:58:43.950 --> 00:58:46.630
all these alternative
accounts seriously.

00:58:46.630 --> 00:58:49.558
And so as just one example, what
we did in our very first paper

00:58:49.558 --> 00:58:51.600
is say, OK, there's lots
of alternative accounts.

00:58:51.600 --> 00:58:53.860
Let's try to tackle
a bunch of them.

00:58:53.860 --> 00:58:58.620
So we scan people looking at now
3/4 views of faces and hands,

00:58:58.620 --> 00:59:01.710
and we made them press a button
whenever two consecutive hands

00:59:01.710 --> 00:59:04.440
were the same-- that's
called a one-back task--

00:59:04.440 --> 00:59:07.020
or whenever two consecutive
faces are the same.

00:59:07.020 --> 00:59:09.750
By design, that task
is harder on the hands

00:59:09.750 --> 00:59:11.880
than the faces,
so we were forcing

00:59:11.880 --> 00:59:14.400
our subjects to pay more
attention to the hands

00:59:14.400 --> 00:59:17.100
than the faces, OK?

00:59:17.100 --> 00:59:20.010
And what we found is you get
the same blob, still responding

00:59:20.010 --> 00:59:21.970
more to faces than hands.

00:59:21.970 --> 00:59:24.210
And so the idea is
by showing that,

00:59:24.210 --> 00:59:26.603
we've ruled out every
one of those things.

00:59:26.603 --> 00:59:28.020
It's not just any
human body part.

00:59:28.020 --> 00:59:29.037
It doesn't go to hands--

00:59:29.037 --> 00:59:30.120
oh, sorry, anything human.

00:59:30.120 --> 00:59:31.692
It's not just any body part.

00:59:31.692 --> 00:59:33.150
It's not anything
we paid attention

00:59:33.150 --> 00:59:35.640
to because we made them pay
more attention to the hands.

00:59:35.640 --> 00:59:38.970
It's not anything with
curvy outline, OK?

00:59:38.970 --> 00:59:41.820
And so that's just a
little tiny example

00:59:41.820 --> 00:59:45.030
of how you can proceed
in a systematic way

00:59:45.030 --> 00:59:47.978
to try to analyze what
is actually driving

00:59:47.978 --> 00:59:49.020
this region of the brain.

00:59:49.020 --> 00:59:50.812
You come up with a
hypothesis, and then you

00:59:50.812 --> 00:59:52.680
think of alternatives
to the data.

00:59:52.680 --> 00:59:54.340
And you come up with
more hypotheses,

00:59:54.340 --> 00:59:56.250
and then you think
of ways to test them.

00:59:56.250 --> 00:59:58.470
And we'll do a lot
of that in here.

00:59:58.470 --> 01:00:01.170
OK, so that's what I
just said, and I'll just

01:00:01.170 --> 01:00:03.240
say that there's lots
of data since then.

01:00:03.240 --> 01:00:06.660
That region of the brain
actually extremely very much

01:00:06.660 --> 01:00:10.650
prefers faces, and it's
present in everyone.

01:00:10.650 --> 01:00:15.420
And next time, we will
talk about the fact

01:00:15.420 --> 01:00:17.070
that that looks
like it's suggesting

01:00:17.070 --> 01:00:19.470
that we have a different
system for face recognition

01:00:19.470 --> 01:00:20.880
than object recognition.

01:00:20.880 --> 01:00:23.190
But we haven't yet
nailed the case,

01:00:23.190 --> 01:00:25.830
and you guys should all
think about what remains.

01:00:25.830 --> 01:00:26.498
OK, thank you.

01:00:26.498 --> 01:00:27.540
Sorry I was racing there.

01:00:27.540 --> 01:00:30.830
I will hang out if you
guys have questions.