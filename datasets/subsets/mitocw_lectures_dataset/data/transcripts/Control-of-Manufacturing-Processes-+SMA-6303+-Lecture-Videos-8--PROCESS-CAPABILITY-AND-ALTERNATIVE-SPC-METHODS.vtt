WEBVTT

00:00:00.000 --> 00:00:02.460
The following content is
provided under a Creative

00:00:02.460 --> 00:00:03.740
Commons license.

00:00:03.740 --> 00:00:06.060
Your support will help
MIT OpenCourseWare

00:00:06.060 --> 00:00:10.090
continue to offer high quality
educational resources for free.

00:00:10.090 --> 00:00:12.690
To make a donation or to
view additional materials

00:00:12.690 --> 00:00:16.560
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:16.560 --> 00:00:17.904
at ocw.mit.edu.

00:00:22.070 --> 00:00:25.490
DUANE BONING: So let me
begin by just giving you

00:00:25.490 --> 00:00:27.620
a little bit of a
preview of what's

00:00:27.620 --> 00:00:29.450
coming up over the
next couple of weeks

00:00:29.450 --> 00:00:32.210
in terms of the calendar.

00:00:32.210 --> 00:00:35.180
The problem set that's
due today you'll notice

00:00:35.180 --> 00:00:37.520
is perhaps a little bit long.

00:00:37.520 --> 00:00:39.380
But the good news
is the next problem

00:00:39.380 --> 00:00:42.170
set you'll look at
and go, is that all?

00:00:42.170 --> 00:00:44.360
Because it will be a very short.

00:00:44.360 --> 00:00:47.780
Part of the reason is the
problem set that goes out today

00:00:47.780 --> 00:00:51.120
will be due a week from
today, next Tuesday.

00:00:51.120 --> 00:00:55.000
But a week from Thursday
will be our first quiz.

00:00:55.000 --> 00:00:57.230
And so the upcoming
problem set is

00:00:57.230 --> 00:01:02.120
meant to give you some time
to go back and re-integrate

00:01:02.120 --> 00:01:06.110
and re-digest the material
that we've covered so far.

00:01:06.110 --> 00:01:09.800
The material that is on this
problem set that's going out,

00:01:09.800 --> 00:01:13.292
it's going to be a couple
more, a few more problems still

00:01:13.292 --> 00:01:15.500
related to control charting,
some of the things we'll

00:01:15.500 --> 00:01:16.940
talk about today.

00:01:16.940 --> 00:01:19.170
And that will be
covered on the quiz.

00:01:19.170 --> 00:01:22.520
So the quiz will be
statistical basics

00:01:22.520 --> 00:01:27.830
up through statistical process
control and control charting.

00:01:27.830 --> 00:01:29.930
That'll be a week from Thursday.

00:01:29.930 --> 00:01:33.890
This Thursday I'll be
giving a lecture that shifts

00:01:33.890 --> 00:01:35.780
a little bit to yield modeling.

00:01:35.780 --> 00:01:41.570
And that will not be on either
this problem set or this quiz.

00:01:41.570 --> 00:01:45.740
So that's the plan.

00:01:45.740 --> 00:01:47.520
We'll have more
details on the quiz

00:01:47.520 --> 00:01:51.110
and I think even some
example quizzes that you

00:01:51.110 --> 00:01:54.050
can look at from the past.

00:01:54.050 --> 00:01:59.950
But other quick
warnings or previews

00:01:59.950 --> 00:02:04.900
on the quiz itself is the plan
is to make that closed book.

00:02:04.900 --> 00:02:07.750
But you will be able to
bring a page of notes in.

00:02:07.750 --> 00:02:12.880
So you can have a single page of
cheat sheet or notes, formulas,

00:02:12.880 --> 00:02:15.655
examples, whatever you
can fit for quiz one

00:02:15.655 --> 00:02:20.200
on the one side of one page.

00:02:20.200 --> 00:02:22.720
And also bring a calculator.

00:02:22.720 --> 00:02:28.120
We will provide for you
any statistics tables

00:02:28.120 --> 00:02:32.320
that might be needed, typically
out of the back of Montgomery

00:02:32.320 --> 00:02:34.540
or [INAUDIBLE].

00:02:34.540 --> 00:02:36.730
So we'll have those
if you need them.

00:02:36.730 --> 00:02:37.510
Yes?

00:02:37.510 --> 00:02:41.662
AUDIENCE: So [INAUDIBLE]

00:02:41.662 --> 00:02:43.370
DUANE BONING: We're
not giving you those.

00:02:43.370 --> 00:02:46.600
So those should go on
your one page of notes

00:02:46.600 --> 00:02:48.390
that you can bring in.

00:02:48.390 --> 00:02:50.817
Or you can just remember
them if you're really good.

00:02:50.817 --> 00:02:52.650
AUDIENCE: But you'll
give us like a Z table?

00:02:52.650 --> 00:02:54.275
DUANE BONING: We'll
give you a Z table.

00:02:54.275 --> 00:03:00.270
We'll give you, if you
need it, t, Chi squared.

00:03:00.270 --> 00:03:01.817
We'll give you those tables.

00:03:01.817 --> 00:03:04.883
AUDIENCE: Is there
a gamma table?

00:03:04.883 --> 00:03:06.800
DUANE BONING: I'm sure
there is a gamma table.

00:03:06.800 --> 00:03:09.838
I'm not sure whether
or not it's in the--

00:03:09.838 --> 00:03:13.430
AUDIENCE: [INAUDIBLE]
gamma of something--

00:03:13.430 --> 00:03:14.970
DUANE BONING: It arises in--

00:03:14.970 --> 00:03:17.066
AUDIENCE: [INAUDIBLE]

00:03:19.815 --> 00:03:21.440
DUANE BONING: And
that might be a clue.

00:03:21.440 --> 00:03:26.375
If you don't have a gamma table,
don't try to use the gamma.

00:03:26.375 --> 00:03:28.850
AUDIENCE: [INAUDIBLE]

00:03:28.850 --> 00:03:30.768
Can you put a sample
exam so that we know--

00:03:30.768 --> 00:03:31.560
DUANE BONING: Yeah.

00:03:31.560 --> 00:03:37.920
We'll try to dig up an example
exam so you can take a look.

00:03:37.920 --> 00:03:39.060
OK.

00:03:39.060 --> 00:03:40.950
So any other
questions on the quiz?

00:03:40.950 --> 00:03:45.180
Any questions from Singapore
on the upcoming quiz?

00:03:45.180 --> 00:03:49.140
You guys look so eager for it.

00:03:49.140 --> 00:03:49.800
OK.

00:03:49.800 --> 00:03:52.140
So today what I want to
do is pick up a little bit

00:03:52.140 --> 00:03:54.790
where Dave Hardt left off.

00:03:54.790 --> 00:03:58.590
So the plan here today
will be a very brief review

00:03:58.590 --> 00:03:59.970
on control charting.

00:03:59.970 --> 00:04:02.880
The basic idea I want
to be able to hit, now

00:04:02.880 --> 00:04:04.810
that you've seen
what he described

00:04:04.810 --> 00:04:07.890
and what you've been doing
hopefully on the problem

00:04:07.890 --> 00:04:10.050
set that's due at
the end of the day,

00:04:10.050 --> 00:04:14.100
is reinforce the
relationship here

00:04:14.100 --> 00:04:18.329
between hypothesis
tests and control chart.

00:04:18.329 --> 00:04:20.730
And so I'm going
to just remind us

00:04:20.730 --> 00:04:22.800
a little bit about
hypothesis tests

00:04:22.800 --> 00:04:25.290
and these different
types of errors.

00:04:25.290 --> 00:04:30.690
The probability of false alarm,
probability of a misalarm,

00:04:30.690 --> 00:04:33.720
missed alarm or beta,
and how the sample

00:04:33.720 --> 00:04:35.880
size plays with that.

00:04:35.880 --> 00:04:39.230
Because when you
design control charts,

00:04:39.230 --> 00:04:44.240
alpha, recall, basically sets
your upper control limits

00:04:44.240 --> 00:04:46.800
and lower control limits.

00:04:46.800 --> 00:04:52.880
Beta falls out,
beta being related

00:04:52.880 --> 00:04:55.700
to essentially the
probability, again,

00:04:55.700 --> 00:05:00.200
of missing an alarm when the
process has, in fact, shifted.

00:05:00.200 --> 00:05:04.680
And how do you change
or affect beta?

00:05:04.680 --> 00:05:06.430
By modifying the sample size.

00:05:06.430 --> 00:05:08.310
So I want to talk a
little bit about those.

00:05:08.310 --> 00:05:11.320
I think you might have
touched on them briefly.

00:05:11.320 --> 00:05:15.950
But I want to touch
on those and also

00:05:15.950 --> 00:05:17.600
remind us of the
definition or give you

00:05:17.600 --> 00:05:19.830
the definition of
average run length.

00:05:19.830 --> 00:05:22.820
And I think if you've gotten to
that point in the problem set,

00:05:22.820 --> 00:05:26.900
you've already read
about that as well.

00:05:26.900 --> 00:05:34.740
So let me start, again, with
the basic hypothesis test.

00:05:34.740 --> 00:05:38.760
Because remember, all
that a control chart is

00:05:38.760 --> 00:05:42.000
is a running hypothesis test
where every new sample comes

00:05:42.000 --> 00:05:45.360
in, you're doing a
hypothesis or you're

00:05:45.360 --> 00:05:47.190
asking the test
on the hypothesis,

00:05:47.190 --> 00:05:56.040
has the process shifted or is it
still my nominal distribution?

00:05:56.040 --> 00:06:03.880
So the basic situation with the
hypothesis test, if you recall,

00:06:03.880 --> 00:06:09.800
what we have is our
assumed distribution.

00:06:09.800 --> 00:06:14.240
I'm going to assume here
that it's a Gaussian.

00:06:14.240 --> 00:06:16.570
We'll call that h0.

00:06:16.570 --> 00:06:20.470
It's got some mu, some
true process mean.

00:06:20.470 --> 00:06:24.280
And what we said is we
pick control limits,

00:06:24.280 --> 00:06:28.600
or in the hypothesis test
case acceptance regions.

00:06:28.600 --> 00:06:32.110
But I'm going to use the control
chart terminology, since that's

00:06:32.110 --> 00:06:33.670
what we're dealing with here.

00:06:33.670 --> 00:06:36.490
We pick where these
limits are based

00:06:36.490 --> 00:06:41.870
on a probability of false alarm.

00:06:41.870 --> 00:06:43.460
Seems a little weird, right?

00:06:43.460 --> 00:06:48.440
But basically what
we do is we assign

00:06:48.440 --> 00:06:56.950
if alpha is our false alarm
rate, we put alpha over 2

00:06:56.950 --> 00:06:58.360
in each of the two tails.

00:06:58.360 --> 00:07:03.480
Now, that's based
on this key idea

00:07:03.480 --> 00:07:06.720
that if we're in
control, most of the time

00:07:06.720 --> 00:07:08.670
the data will be in here.

00:07:08.670 --> 00:07:10.440
Every now and then,
just by random

00:07:10.440 --> 00:07:14.730
chance alone, even when nothing
has changed, we're still in h0,

00:07:14.730 --> 00:07:18.590
I do get an unusual
point out in the tails.

00:07:18.590 --> 00:07:26.120
And if we do 3 sigma, we saw
that's about 3 in 1,000 points

00:07:26.120 --> 00:07:28.545
will fall way
outside in the tails.

00:07:28.545 --> 00:07:30.170
So when we get a
point out in the tail,

00:07:30.170 --> 00:07:35.350
we say something might
be unusual going on.

00:07:35.350 --> 00:07:39.550
Either something has changed
in the process or I got unlucky

00:07:39.550 --> 00:07:42.680
and I just pulled a
sample from the tails.

00:07:42.680 --> 00:07:45.260
So in fact, there
is, in a process

00:07:45.260 --> 00:07:48.700
that's always in control,
there is this false alarm rate.

00:07:48.700 --> 00:07:53.890
Now, another way of talking
about this false alarm rate

00:07:53.890 --> 00:07:57.460
is also the inverse
of the false alarm

00:07:57.460 --> 00:08:02.020
rate, which is how many
runs does it take on average

00:08:02.020 --> 00:08:03.730
before you get a false alarm?

00:08:03.730 --> 00:08:08.560
And this is referred to
as the average run length.

00:08:08.560 --> 00:08:15.190
So here sliding
back to the slides.

00:08:15.190 --> 00:08:18.380
Can we put the PowerPoint
slides on both screens?

00:08:18.380 --> 00:08:18.880
Great.

00:08:18.880 --> 00:08:19.713
Thank you very much.

00:08:22.270 --> 00:08:26.260
So looking back here, this
is our nominal probability.

00:08:26.260 --> 00:08:28.270
Again, 3 out of 1,000.

00:08:28.270 --> 00:08:33.580
But now if I turn it around
and ask the question, how often

00:08:33.580 --> 00:08:39.700
or what is the average number
of runs that we have to go?

00:08:39.700 --> 00:08:43.690
Well, it's just 1
over that probability

00:08:43.690 --> 00:08:50.680
on any one run of having
the point fall in the tails.

00:08:50.680 --> 00:08:53.920
So that's the definition
of our average run length.

00:08:53.920 --> 00:08:58.090
And actually, a notation
that I like in Montgomery

00:08:58.090 --> 00:09:01.900
is we'll define this as
average run length sub-zero.

00:09:01.900 --> 00:09:05.170
What that does is remind me
that that's the average run

00:09:05.170 --> 00:09:09.850
length associated with the
h0, the default probability

00:09:09.850 --> 00:09:11.006
distribution.

00:09:13.680 --> 00:09:15.120
So that's the alpha part.

00:09:15.120 --> 00:09:17.360
Where does beta come in?

00:09:17.360 --> 00:09:21.550
Beta comes in when we
actually have a shift.

00:09:21.550 --> 00:09:26.500
So the whole idea is now
imagine that I've got

00:09:26.500 --> 00:09:30.160
a shift in the distribution.

00:09:30.160 --> 00:09:35.215
I've had a shift from mu
to mu plus some delta.

00:09:38.550 --> 00:09:41.060
I'm going to label that
the alternative hypothesis.

00:09:41.060 --> 00:09:43.170
So the idea is
for a given delta,

00:09:43.170 --> 00:09:46.860
I'm now actually drawing parts
from a different distribution,

00:09:46.860 --> 00:09:50.030
my h1 distribution.

00:09:50.030 --> 00:09:52.250
I've already defined
the acceptance region

00:09:52.250 --> 00:09:54.800
based on my
acceptable average run

00:09:54.800 --> 00:09:58.570
length or my acceptable
false alarm rate.

00:09:58.570 --> 00:10:01.620
By the way, there's nothing
magic about plus minus 3 sigma.

00:10:01.620 --> 00:10:06.030
If you don't want to respond
once out of three times out

00:10:06.030 --> 00:10:11.340
of every 1,000 where
about once every 333 runs,

00:10:11.340 --> 00:10:13.560
you don't want to respond
to a false alarm rate,

00:10:13.560 --> 00:10:19.530
it doesn't cost you that much to
have a false alarm or it costs

00:10:19.530 --> 00:10:22.260
a lot to have a false alarm,
but it doesn't cost you

00:10:22.260 --> 00:10:25.290
that much to have a bad part,
then maybe you only want

00:10:25.290 --> 00:10:28.200
to respond 1/10 as much.

00:10:28.200 --> 00:10:30.780
In which case, you would
change your control limits,

00:10:30.780 --> 00:10:35.360
move them out a little
bit so that you set it

00:10:35.360 --> 00:10:37.490
based on the average--

00:10:37.490 --> 00:10:41.270
set based on the average run
length or the probability

00:10:41.270 --> 00:10:43.530
that you can accept
out of the tail.

00:10:43.530 --> 00:10:44.780
So that's set.

00:10:44.780 --> 00:10:48.830
But now we said there
is also the type two

00:10:48.830 --> 00:10:59.210
error or the beta error, which
is the probability of a missed

00:10:59.210 --> 00:11:00.020
alarm.

00:11:00.020 --> 00:11:04.490
And that's set by
those control limits.

00:11:04.490 --> 00:11:07.490
So now the probability
that I would

00:11:07.490 --> 00:11:12.710
have thought it was a good part
even though an alarm occurred

00:11:12.710 --> 00:11:15.590
is just the other side of
that same control limit

00:11:15.590 --> 00:11:19.670
but now drawn from
the h1 distribution.

00:11:19.670 --> 00:11:23.570
I really have to
shift, but the point

00:11:23.570 --> 00:11:26.760
fell in this part of the
tail of the h1 distribution.

00:11:26.760 --> 00:11:28.280
And I declared it good.

00:11:28.280 --> 00:11:31.820
I said there's no alarm.

00:11:31.820 --> 00:11:37.640
And we saw last time basically,
or a couple of classes

00:11:37.640 --> 00:11:42.050
ago, this is also it's
just the tail there

00:11:42.050 --> 00:11:43.070
in that distribution.

00:11:43.070 --> 00:11:53.960
It's UCL minus delta over
sigma over square root of n.

00:11:53.960 --> 00:11:56.510
By the way, this is just
this part of the tail.

00:11:56.510 --> 00:12:00.200
To be really careful, I
should also subtract off

00:12:00.200 --> 00:12:03.770
this tail, this
part right there,

00:12:03.770 --> 00:12:06.020
because I got something
way out there,

00:12:06.020 --> 00:12:08.600
I would call it a failed part.

00:12:08.600 --> 00:12:12.480
And so my beta is slightly
smaller than this.

00:12:12.480 --> 00:12:14.930
But as I've drawn it here,
I'm really only worried

00:12:14.930 --> 00:12:16.460
about this part of the tail.

00:12:16.460 --> 00:12:19.280
But there is the other
part of a lower control

00:12:19.280 --> 00:12:23.240
limit plus delta minus.

00:12:23.240 --> 00:12:23.740
Excuse me.

00:12:23.740 --> 00:12:25.390
It's still a lower
control limit still

00:12:25.390 --> 00:12:30.250
minus delta over
single route in.

00:12:30.250 --> 00:12:32.300
OK.

00:12:32.300 --> 00:12:32.900
Great.

00:12:32.900 --> 00:12:37.300
There is also another
definition of average run

00:12:37.300 --> 00:12:41.310
length associated with this.

00:12:41.310 --> 00:12:43.500
And the question
you would pose here

00:12:43.500 --> 00:12:51.660
is what is the average
run length to detect shift

00:12:51.660 --> 00:12:53.100
when it actually occurred?

00:13:03.170 --> 00:13:05.440
It's a little tricky here,
because there's actually

00:13:05.440 --> 00:13:06.700
two steps.

00:13:06.700 --> 00:13:13.080
You can imagine this would
be the probability of not

00:13:13.080 --> 00:13:16.050
missing an alarm and
then inverting it,

00:13:16.050 --> 00:13:19.440
just like we did here.

00:13:19.440 --> 00:13:23.220
So this little tail here is the
probability of missing alarm.

00:13:23.220 --> 00:13:25.740
There is also a
definition referred

00:13:25.740 --> 00:13:31.530
to as the power of a test,
which is 1 minus beta.

00:13:34.290 --> 00:13:36.690
And that's basically
the probability

00:13:36.690 --> 00:13:39.060
of catching an alarm
on any one run.

00:13:39.060 --> 00:13:43.060
And that's just, it's
everything to the right of this.

00:13:43.060 --> 00:13:47.310
And now if I draw from
h1 but not in the region

00:13:47.310 --> 00:13:51.630
where I would have declared
it h0, that's 1 minus beta.

00:13:51.630 --> 00:14:03.000
An average run length sub 1 is
defined as 1 over 1 minus beta.

00:14:03.000 --> 00:14:05.210
So this is the
average number of runs

00:14:05.210 --> 00:14:08.900
it takes to detect the shift
assuming it actually occurred.

00:14:11.470 --> 00:14:12.200
OK?

00:14:12.200 --> 00:14:14.660
Everybody happy
with this so far?

00:14:14.660 --> 00:14:17.100
Is it starting to fit together?

00:14:17.100 --> 00:14:20.850
Now how do I change--

00:14:20.850 --> 00:14:23.390
well, why would I change alpha?

00:14:23.390 --> 00:14:26.390
I change alpha
based on a decision

00:14:26.390 --> 00:14:31.730
outside on acceptance of
my acceptability of making

00:14:31.730 --> 00:14:32.690
a false alarm.

00:14:32.690 --> 00:14:33.800
How do I change beta?

00:14:36.770 --> 00:14:41.990
What if I say it's taking me
too long to detect a shift?

00:14:41.990 --> 00:14:45.490
I want it to take less time.

00:14:45.490 --> 00:14:46.270
Change n.

00:14:46.270 --> 00:14:49.390
So what changes when I change n?

00:14:49.390 --> 00:14:51.250
Does alpha change
when I change n?

00:14:54.350 --> 00:14:56.790
No.

00:14:56.790 --> 00:15:00.390
Alpha is set a priori based
on my false alarm rate.

00:15:00.390 --> 00:15:02.460
When I change beta--

00:15:02.460 --> 00:15:04.980
or when I change n,
I do change beta.

00:15:04.980 --> 00:15:06.540
Do I change the control limits?

00:15:11.240 --> 00:15:12.740
I do change the control limits.

00:15:12.740 --> 00:15:17.550
I change the control limits
because my sample size changed

00:15:17.550 --> 00:15:19.990
and I move them to
maintain the same alpha.

00:15:19.990 --> 00:15:23.410
So let me draw the picture.

00:15:23.410 --> 00:15:25.150
Let's see.

00:15:25.150 --> 00:15:29.540
If I draw down here, can people
in Singapore still see this?

00:15:29.540 --> 00:15:30.800
OK, great.

00:15:30.800 --> 00:15:34.760
So if I increase n,
what's going to happen

00:15:34.760 --> 00:15:39.170
is I have a sample size
that tightens by my sampling

00:15:39.170 --> 00:15:42.860
distribution for x bar.

00:15:42.860 --> 00:15:45.680
Remember, h0 is really
an x bar based on.

00:15:45.680 --> 00:15:49.070
And so that distribution
will have the same mean,

00:15:49.070 --> 00:15:50.910
but it will get tighter.

00:15:50.910 --> 00:15:52.670
So now I have an h0.

00:15:52.670 --> 00:15:56.180
But I'm still setting
my control limits

00:15:56.180 --> 00:15:57.785
based on an acceptable alpha.

00:16:03.230 --> 00:16:05.510
But the control limits
will get tighter,

00:16:05.510 --> 00:16:08.960
because I've got a
tighter distribution.

00:16:08.960 --> 00:16:11.840
I still have the same
alpha in the tail.

00:16:11.840 --> 00:16:13.460
And what's cool
here is I'm still

00:16:13.460 --> 00:16:17.240
detecting the same mean shift.

00:16:17.240 --> 00:16:18.560
Trying to draw down here.

00:16:18.560 --> 00:16:20.360
Boom, boom, boom.

00:16:20.360 --> 00:16:25.650
So now I've got a tighter
distribution here as well.

00:16:25.650 --> 00:16:27.050
Here's my h1.

00:16:27.050 --> 00:16:29.690
I've got a tighter
distribution on h1

00:16:29.690 --> 00:16:32.420
and I've still got just a
little bit of tiny stuff

00:16:32.420 --> 00:16:34.100
over here on the tail.

00:16:34.100 --> 00:16:36.530
And that's now my beta,
and it's much smaller

00:16:36.530 --> 00:16:37.760
if I've increased n.

00:16:41.260 --> 00:16:45.160
So I can basically
pick my sample size

00:16:45.160 --> 00:16:48.580
to give me the
acceptable false--

00:16:48.580 --> 00:16:52.510
or the acceptable missed alarm
rate or the acceptable average

00:16:52.510 --> 00:16:56.080
run length to detect the shift.

00:16:56.080 --> 00:16:59.650
And there are tables
in both Montgomery

00:16:59.650 --> 00:17:02.890
and [INAUDIBLE]
that are referred

00:17:02.890 --> 00:17:08.950
to as operating curves or
OC's that basically give you

00:17:08.950 --> 00:17:15.400
the relationship between
sample size, size of shift,

00:17:15.400 --> 00:17:22.000
and the beta or average run
length one that you can accept.

00:17:22.000 --> 00:17:23.770
You could go in
and calculate them.

00:17:23.770 --> 00:17:30.550
Turns out this is actually a
really nasty calculation to do,

00:17:30.550 --> 00:17:32.800
because there's no close--

00:17:32.800 --> 00:17:35.860
if you need to find the n
to give you a certain beta,

00:17:35.860 --> 00:17:37.750
you have to solve
it iteratively.

00:17:37.750 --> 00:17:40.450
There's no closed form for that.

00:17:40.450 --> 00:17:44.290
Q sum are the
cumulative probability

00:17:44.290 --> 00:17:47.080
that's associated with beta.

00:17:47.080 --> 00:17:49.580
So there's no formula for it.

00:17:49.580 --> 00:17:51.610
And these tables,
basically, or charts

00:17:51.610 --> 00:17:55.330
have basically
calculated that for you.

00:17:55.330 --> 00:17:58.300
You can set up Excel
or whatever to do that,

00:17:58.300 --> 00:18:01.090
to solve it iteratively
if you want.

00:18:01.090 --> 00:18:01.670
Yeah?

00:18:01.670 --> 00:18:04.770
AUDIENCE: [INAUDIBLE]

00:18:13.420 --> 00:18:15.400
DUANE BONING: So
the question was,

00:18:15.400 --> 00:18:18.820
we're changing alpha or
changing sample size.

00:18:18.820 --> 00:18:20.290
The distribution gets tighter.

00:18:20.290 --> 00:18:20.905
Why doesn't--

00:18:24.380 --> 00:18:26.140
AUDIENCE: Yeah,
why doesn't alpha?

00:18:26.140 --> 00:18:29.770
DUANE BONING: Why
doesn't alpha change?

00:18:29.770 --> 00:18:33.460
If we kept the same lower
control limit and upper control

00:18:33.460 --> 00:18:36.070
limit and I tightened this.

00:18:36.070 --> 00:18:39.340
So if I kept these the same
but the distribution tightened,

00:18:39.340 --> 00:18:41.620
then alpha would change.

00:18:41.620 --> 00:18:45.400
What's interesting about
classic control chart

00:18:45.400 --> 00:18:49.240
design is the upper control
limit and lower control limit

00:18:49.240 --> 00:18:51.320
are based on the statistics.

00:18:51.320 --> 00:18:54.070
So they are based on alpha.

00:18:54.070 --> 00:19:00.650
We nailed down alpha first
and then set the upper

00:19:00.650 --> 00:19:02.570
and lower control limits.

00:19:02.570 --> 00:19:04.070
And there's a really
important thing

00:19:04.070 --> 00:19:07.730
that we'll get to, actually,
in just a second about process

00:19:07.730 --> 00:19:13.400
capability where then we
start talking about upper

00:19:13.400 --> 00:19:21.710
and lower spec limits, which is
related to the notion of where

00:19:21.710 --> 00:19:25.070
the parts property
really needs to be

00:19:25.070 --> 00:19:27.540
for it to function correctly.

00:19:27.540 --> 00:19:30.800
And those are very different
than control limits

00:19:30.800 --> 00:19:32.750
but often confused.

00:19:32.750 --> 00:19:35.870
And so nominally,
your spec limits

00:19:35.870 --> 00:19:40.130
wouldn't actually change if
you're changing sample size.

00:19:40.130 --> 00:19:43.100
The part still needs
to function correctly

00:19:43.100 --> 00:19:45.980
within a certain band.

00:19:48.510 --> 00:19:51.930
So in that case, you
change sample size.

00:19:51.930 --> 00:19:53.640
Your spec limits would not.

00:19:53.640 --> 00:19:57.510
But what people often
do is erroneously

00:19:57.510 --> 00:20:00.360
set the control limits
to equal the spec limits.

00:20:00.360 --> 00:20:02.280
And that's wrong.

00:20:02.280 --> 00:20:05.190
What you want to do is set the
control limits based purely

00:20:05.190 --> 00:20:08.850
on the statistics based
on your false alarm rate.

00:20:08.850 --> 00:20:14.070
Chance of observing by chance
or by random probability,

00:20:14.070 --> 00:20:17.102
an unusual event.

00:20:17.102 --> 00:20:20.752
AUDIENCE: [INAUDIBLE]

00:20:22.050 --> 00:20:24.090
For example, in the
problem set, there

00:20:24.090 --> 00:20:26.082
was a question [INAUDIBLE].

00:20:40.253 --> 00:20:41.670
DUANE BONING: I
guess I don't want

00:20:41.670 --> 00:20:44.820
to say too much about the
problem set right now.

00:20:44.820 --> 00:20:51.570
But you can often relate
things back to the underlying

00:20:51.570 --> 00:20:52.570
probabilities.

00:20:52.570 --> 00:20:54.780
And what you should
usually do is relate them

00:20:54.780 --> 00:20:56.700
back to the underlying
probabilities

00:20:56.700 --> 00:21:02.250
and set up the hypothesis
test based on shifts,

00:21:02.250 --> 00:21:07.560
questions about there can be two
sided shifts, one sided shifts.

00:21:07.560 --> 00:21:09.550
Both things could shift
by a certain amount,

00:21:09.550 --> 00:21:11.258
and it might be just
worried about what's

00:21:11.258 --> 00:21:12.960
happening in the tail.

00:21:12.960 --> 00:21:14.590
There's other hypotheses.

00:21:14.590 --> 00:21:18.180
These are all for
changes in the mean.

00:21:18.180 --> 00:21:21.690
If I'm looking at some
other distribution,

00:21:21.690 --> 00:21:24.840
like changes in the
variance, I could set up

00:21:24.840 --> 00:21:27.840
a similar hypothesis test
and perhaps other probability

00:21:27.840 --> 00:21:30.120
distributions apply.

00:21:30.120 --> 00:21:32.520
Turns out with
the S chart, we're

00:21:32.520 --> 00:21:36.900
really tracking the mean
of standard deviation

00:21:36.900 --> 00:21:39.640
or the mean of the sample
standard deviation.

00:21:39.640 --> 00:21:42.000
And in those cases,
we typically still

00:21:42.000 --> 00:21:44.370
are assuming that
the distribution

00:21:44.370 --> 00:21:47.700
of standard deviation
for large samples

00:21:47.700 --> 00:21:51.570
is Gaussian within
correction factors to make

00:21:51.570 --> 00:21:54.750
it come close in setting
the control limits.

00:21:54.750 --> 00:21:59.650
Things like C4's and
those sorts of things.

00:21:59.650 --> 00:22:04.080
But going back, it's not
always exactly the same set up.

00:22:04.080 --> 00:22:06.420
But you can almost
always reason it out

00:22:06.420 --> 00:22:09.060
by going back and
setting up what

00:22:09.060 --> 00:22:12.540
is my normal situation, what is
the thing I'm trying to detect,

00:22:12.540 --> 00:22:14.970
and then look at the
probabilities associated

00:22:14.970 --> 00:22:17.515
with those cases.

00:22:17.515 --> 00:22:19.140
AUDIENCE: I have a
question, Professor.

00:22:19.140 --> 00:22:20.307
DUANE BONING: Yes, question.

00:22:22.350 --> 00:22:24.420
AUDIENCE: If I give
you a control chart,

00:22:24.420 --> 00:22:28.770
you can't really tell
whether it's a false alarm

00:22:28.770 --> 00:22:32.032
or it's a true alarm, can you?

00:22:32.032 --> 00:22:33.240
DUANE BONING: That's correct.

00:22:37.660 --> 00:22:39.820
AUDIENCE: Like how do you
use a control chart then?

00:22:39.820 --> 00:22:41.620
Because if I gave
a control chart,

00:22:41.620 --> 00:22:45.270
it could be with alpha
probability of a false alarm

00:22:45.270 --> 00:22:50.720
or with 1 minus beta
probability of a true alarm.

00:22:50.720 --> 00:22:52.720
[INAUDIBLE]

00:22:52.720 --> 00:22:57.280
DUANE BONING: This is a
fundamental important point

00:22:57.280 --> 00:23:01.030
about how control charting
works, which goes back

00:23:01.030 --> 00:23:04.000
to the idea that if
nothing changes, you do

00:23:04.000 --> 00:23:05.530
have false alarms.

00:23:05.530 --> 00:23:08.890
And if something changes,
you've got a real alarm.

00:23:08.890 --> 00:23:10.390
All you've got on
the control chart

00:23:10.390 --> 00:23:14.860
is a point that's lying
outside of your normal bounds.

00:23:14.860 --> 00:23:20.090
All by itself from the
control chart, you can't tell.

00:23:20.090 --> 00:23:22.080
You are exactly right.

00:23:22.080 --> 00:23:26.090
So in SPC methodology,
what you are supposed

00:23:26.090 --> 00:23:30.110
to do any time you
see an alarm is

00:23:30.110 --> 00:23:33.590
go and look at the equipment,
investigate the problem,

00:23:33.590 --> 00:23:36.800
and decide was
this a false alarm?

00:23:36.800 --> 00:23:39.800
In other words, is the process
still operating normally?

00:23:39.800 --> 00:23:43.710
Or has something
important changed?

00:23:43.710 --> 00:23:48.330
Now, it turns out that in
an awful lot of factories,

00:23:48.330 --> 00:23:54.460
people have adapted to
this notion of false alarm

00:23:54.460 --> 00:23:59.290
in a very qualitative way, which
is they see one alarm point.

00:23:59.290 --> 00:24:00.400
They look at it.

00:24:00.400 --> 00:24:01.870
They don't investigate
the process.

00:24:01.870 --> 00:24:04.330
They ignore it.

00:24:04.330 --> 00:24:05.530
And then they wait.

00:24:05.530 --> 00:24:07.340
They do the next point.

00:24:07.340 --> 00:24:09.670
If it's also an
alarm, then they say,

00:24:09.670 --> 00:24:12.160
ah, I think something
may really be wrong

00:24:12.160 --> 00:24:14.400
and then they investigate.

00:24:14.400 --> 00:24:16.315
What are they doing there?

00:24:16.315 --> 00:24:17.940
How could you change
your control chart

00:24:17.940 --> 00:24:19.590
to actually do what
they are doing?

00:24:24.010 --> 00:24:26.920
What essentially
they're doing is saying,

00:24:26.920 --> 00:24:30.670
I don't believe the evidence.

00:24:30.670 --> 00:24:35.710
The 3 in 1,000 chance of seeing
one point outside of a control

00:24:35.710 --> 00:24:39.520
limit is not a strong
enough evidence for me.

00:24:39.520 --> 00:24:41.020
What they're really
doing is looking

00:24:41.020 --> 00:24:47.470
for a smaller probability
of two points in a row

00:24:47.470 --> 00:24:49.930
being outside of
the control chart

00:24:49.930 --> 00:24:52.130
as much stronger evidence.

00:24:52.130 --> 00:24:56.380
So in this case, roughly
it's twice as infrequent.

00:24:56.380 --> 00:24:58.990
Or not twice, but the product.

00:25:03.370 --> 00:25:05.890
The probability of seeing
both of those points outside

00:25:05.890 --> 00:25:09.460
of the control limit, and again,
not alarming on the first one

00:25:09.460 --> 00:25:12.400
is the square of that.

00:25:12.400 --> 00:25:14.770
So that's a very
infrequent thing.

00:25:14.770 --> 00:25:17.740
Then they say I think there's
really something wrong.

00:25:17.740 --> 00:25:21.230
Now, you could actually have
adapted your control chart

00:25:21.230 --> 00:25:25.490
either by having it
only signal alarms

00:25:25.490 --> 00:25:30.170
once you've got two alarm
points or changing the control

00:25:30.170 --> 00:25:35.310
limits so that you basically
move the control limits out

00:25:35.310 --> 00:25:36.580
even further.

00:25:36.580 --> 00:25:41.370
So you've got to really have an
alpha over 2 that's that small.

00:25:41.370 --> 00:25:45.960
So maybe you go to 4, plus or
minus 4 sigma or 4.5 sigma,

00:25:45.960 --> 00:25:52.665
something like that, in order
to have that much evidence.

00:25:52.665 --> 00:25:56.930
AUDIENCE: What steps are
efficient to find [INAUDIBLE]??

00:25:56.930 --> 00:25:58.160
DUANE BONING: I'm sorry?

00:25:58.160 --> 00:25:59.635
What steps?

00:25:59.635 --> 00:26:00.260
AUDIENCE: Yeah.

00:26:00.260 --> 00:26:01.885
There is a huge
machine, and everything

00:26:01.885 --> 00:26:03.980
can be somewhat changed.

00:26:03.980 --> 00:26:05.590
I don't know where
I should start.

00:26:05.590 --> 00:26:07.760
DUANE BONING: Well,
I think the steps

00:26:07.760 --> 00:26:09.320
that you use to
actually investigate

00:26:09.320 --> 00:26:11.870
depend completely
on the process,

00:26:11.870 --> 00:26:14.090
but there's a few
rules of thumb.

00:26:14.090 --> 00:26:18.080
Many of you have probably
had actual process experience

00:26:18.080 --> 00:26:20.360
on different tools, different
manufacturing lines.

00:26:20.360 --> 00:26:22.250
What are some things
that you would do?

00:26:22.250 --> 00:26:25.520
I'll tell you the number
one thing I would do.

00:26:25.520 --> 00:26:27.290
AUDIENCE: Check the screen?

00:26:27.290 --> 00:26:28.700
DUANE BONING: Look
at the screen.

00:26:28.700 --> 00:26:29.570
Pull the power plug!

00:26:29.570 --> 00:26:30.290
Plug it back in!

00:26:30.290 --> 00:26:30.790
No.

00:26:33.090 --> 00:26:33.916
Yes?

00:26:33.916 --> 00:26:34.790
AUDIENCE: Check the measurement?

00:26:34.790 --> 00:26:35.540
DUANE BONING: Yes.

00:26:35.540 --> 00:26:37.290
That's exactly what I would do.

00:26:37.290 --> 00:26:40.350
I would always doubt my
measurement equipment first.

00:26:40.350 --> 00:26:42.050
They also have failures.

00:26:42.050 --> 00:26:45.230
Rather than pull the
process, turn the process off

00:26:45.230 --> 00:26:48.290
and panic with the
actual equipment,

00:26:48.290 --> 00:26:51.420
I'd want to verify
the measurement first.

00:26:51.420 --> 00:26:54.980
So I take it off, do an
independent measurement

00:26:54.980 --> 00:26:57.240
of those parts, and
try to understand it.

00:26:57.240 --> 00:27:02.720
So let's say that indeed I see,
yes, that part looks like it's

00:27:02.720 --> 00:27:04.880
outside of the control limits.

00:27:04.880 --> 00:27:06.377
What's another
thing it might do?

00:27:09.059 --> 00:27:11.790
Do you look just at
that one single point?

00:27:11.790 --> 00:27:13.593
What other data do you have?

00:27:13.593 --> 00:27:17.043
AUDIENCE: You can look
at the [INAUDIBLE]..

00:27:17.043 --> 00:27:17.960
DUANE BONING: Exactly.

00:27:17.960 --> 00:27:20.190
So now you can look back
at the control chart

00:27:20.190 --> 00:27:23.370
data that has been plotted
either manually in old systems

00:27:23.370 --> 00:27:25.740
or in the computer system.

00:27:25.740 --> 00:27:28.020
Track back historically
and start to look,

00:27:28.020 --> 00:27:32.790
is there some other evidence
that wasn't triggering an alarm

00:27:32.790 --> 00:27:35.050
but that might give you a sense?

00:27:35.050 --> 00:27:39.300
And a telltale one might
be, in fact, a slow drift

00:27:39.300 --> 00:27:40.620
in the points.

00:27:40.620 --> 00:27:43.190
Now, some people might
set up the WECO rules,

00:27:43.190 --> 00:27:46.930
the Western Electric rules,
to also watch for drifts.

00:27:46.930 --> 00:27:49.230
But that's pretty rare, frankly.

00:27:49.230 --> 00:27:52.950
People tend to just use
the upper and lower control

00:27:52.950 --> 00:27:57.010
limits in a single
point outside of them.

00:27:57.010 --> 00:28:00.210
So you might look back
at the rest of the data

00:28:00.210 --> 00:28:02.970
and try to see is there
any additional evidence.

00:28:02.970 --> 00:28:06.173
AUDIENCE: The trends can be
incorporated into [INAUDIBLE]..

00:28:06.173 --> 00:28:07.090
DUANE BONING: Exactly.

00:28:07.090 --> 00:28:09.090
So we could also have--

00:28:09.090 --> 00:28:12.330
the point was trends could be
incorporated in the control

00:28:12.330 --> 00:28:15.780
chart as well, either with
this and additional rules

00:28:15.780 --> 00:28:19.220
or, in fact, other
control charting

00:28:19.220 --> 00:28:22.310
approaches we'll
talk about later that

00:28:22.310 --> 00:28:28.040
are designed to track or
detect very subtle trends.

00:28:28.040 --> 00:28:30.800
But again, the
point would be often

00:28:30.800 --> 00:28:34.730
you are trying to debug
the process and look back.

00:28:34.730 --> 00:28:38.000
So you are a good
pattern detector,

00:28:38.000 --> 00:28:41.420
and you might detect a pattern.

00:28:44.030 --> 00:28:46.070
Failing that, I
think then you start

00:28:46.070 --> 00:28:53.240
looking around to see if maybe
there's not a systematic trend.

00:28:53.240 --> 00:28:55.940
What you're
hypothesizing, in fact,

00:28:55.940 --> 00:28:58.280
is that there was a
shift, not a trend.

00:28:58.280 --> 00:29:02.110
And you start talking to people,
looking at the equipment,

00:29:02.110 --> 00:29:03.680
talking with the
operators, and ask,

00:29:03.680 --> 00:29:05.570
did something unusual happen?

00:29:05.570 --> 00:29:08.960
The whole idea is you've
detected an unusual event.

00:29:08.960 --> 00:29:14.150
Might that be correlated with
something else in the operation

00:29:14.150 --> 00:29:15.440
of the machine changing?

00:29:15.440 --> 00:29:17.780
So you start investigating
and looking for things

00:29:17.780 --> 00:29:22.250
that might explain a shift.

00:29:22.250 --> 00:29:23.720
So you go to talk
to the operator,

00:29:23.720 --> 00:29:25.460
and it's a new operator.

00:29:25.460 --> 00:29:28.150
You've never seen
that operator before.

00:29:28.150 --> 00:29:29.210
Say when did you start?

00:29:29.210 --> 00:29:30.815
They say, an hour ago.

00:29:30.815 --> 00:29:31.315
Aha.

00:29:34.250 --> 00:29:37.017
And then it gets progressively
more complicated.

00:29:37.017 --> 00:29:38.600
You're going to have
to dive into more

00:29:38.600 --> 00:29:41.480
details on either the
process, the feedstock, all

00:29:41.480 --> 00:29:46.100
of those sources of variability
that we talked about earlier.

00:29:46.100 --> 00:29:48.150
Good questions.

00:29:48.150 --> 00:29:50.090
OK.

00:29:50.090 --> 00:29:53.185
So I've sort of flown by these.

00:29:53.185 --> 00:29:54.560
These are actually
slides, again,

00:29:54.560 --> 00:30:00.820
that were pulled out of
last Thursday's lecture

00:30:00.820 --> 00:30:03.370
but just illustrate
this notion of

00:30:03.370 --> 00:30:07.510
and give some example
numbers of exactly what we're

00:30:07.510 --> 00:30:10.210
talking about here with
average run length.

00:30:10.210 --> 00:30:14.590
The example plotted here
is the same chart but drawn

00:30:14.590 --> 00:30:18.370
a little bit differently
for kind of smaller shifts.

00:30:18.370 --> 00:30:24.790
Here we have a delta u
that moves the probability

00:30:24.790 --> 00:30:27.440
under the assumption of a shift.

00:30:27.440 --> 00:30:31.970
And this pe down here is
basically 1 minus beta.

00:30:31.970 --> 00:30:35.380
So this is the
probability of detecting

00:30:35.380 --> 00:30:42.640
the true shift under
the control limits again

00:30:42.640 --> 00:30:46.310
that were set based on h0.

00:30:46.310 --> 00:30:48.700
And so here is an example
where, again, the false alarm

00:30:48.700 --> 00:30:54.550
rate was 3 out of 1,000 with
this small one sigma shift.

00:30:54.550 --> 00:30:58.480
Now your tail probability
there only becomes 24 out

00:30:58.480 --> 00:31:01.780
of 1,000 or an
average run length,

00:31:01.780 --> 00:31:06.100
if I invert that,
of about 42 samples

00:31:06.100 --> 00:31:11.980
are required before you can
detect that small one sigma

00:31:11.980 --> 00:31:12.550
shift.

00:31:12.550 --> 00:31:16.450
Now, one sigma shift doesn't
seem all that small to me.

00:31:16.450 --> 00:31:23.910
If that's too long to detect
a shift, then what do you do?

00:31:26.580 --> 00:31:27.630
Increase sample size.

00:31:27.630 --> 00:31:28.530
That's exactly right.

00:31:32.730 --> 00:31:34.830
And here is simply
the definition.

00:31:34.830 --> 00:31:38.640
What's going on there if we
increase the sample size is

00:31:38.640 --> 00:31:44.280
the standard deviation, again,
of our charts, our x bar chart.

00:31:44.280 --> 00:31:48.690
h0 and h1 are shrinking.

00:31:48.690 --> 00:31:56.260
That moves our three sigma
limits closer together

00:31:56.260 --> 00:31:58.810
so that you get
smaller probabilities.

00:31:58.810 --> 00:31:59.566
Yeah?

00:31:59.566 --> 00:32:01.190
AUDIENCE: Even if
we increase the n,

00:32:01.190 --> 00:32:07.290
still we have to get
more samples [INAUDIBLE]..

00:32:07.290 --> 00:32:08.040
DUANE BONING: Yes.

00:32:08.040 --> 00:32:13.020
So the question was
to increase, then,

00:32:13.020 --> 00:32:15.160
you got to take more samples.

00:32:15.160 --> 00:32:18.180
So there is a little
bit of this two level

00:32:18.180 --> 00:32:21.120
hierarchy in
sampling assumed here

00:32:21.120 --> 00:32:22.350
when I'm talking about this.

00:32:22.350 --> 00:32:28.560
We'll also talk about ways to
deal with more continuously

00:32:28.560 --> 00:32:31.530
sampled, even driving
in smaller, not larger,

00:32:31.530 --> 00:32:34.710
so that you can
improve detectability.

00:32:34.710 --> 00:32:38.970
But the two level hierarchy
that I'm talking about here

00:32:38.970 --> 00:32:46.200
is basically this notion
that in many lines,

00:32:46.200 --> 00:32:49.690
you've got lots and lots of
parts coming off very quickly.

00:32:49.690 --> 00:32:54.540
So in time, you've got
parts coming very rapidly.

00:32:54.540 --> 00:32:56.730
And the basic picture
here is you're

00:32:56.730 --> 00:33:01.230
grabbing a sample of size
n at some point in time.

00:33:01.230 --> 00:33:03.870
Maybe it's a four point sample.

00:33:03.870 --> 00:33:08.150
And then much later, I'm
grabbing another four.

00:33:08.150 --> 00:33:11.200
So I am sampling from
a what's pictured

00:33:11.200 --> 00:33:13.770
as a much larger group.

00:33:13.770 --> 00:33:18.420
And so the idea
here is I actually

00:33:18.420 --> 00:33:21.060
have more than one
degree of freedom.

00:33:21.060 --> 00:33:24.030
We've been talking about I can
change just the sample size,

00:33:24.030 --> 00:33:28.080
but I might still in
time only be sampling--

00:33:31.020 --> 00:33:33.880
I could have actually
the same time period.

00:33:33.880 --> 00:33:36.090
So I'm not actually
changing if I just

00:33:36.090 --> 00:33:38.070
modify my sample size here.

00:33:38.070 --> 00:33:42.000
I might actually not change
the frequency at which

00:33:42.000 --> 00:33:45.840
I draw my size and sample.

00:33:45.840 --> 00:33:48.810
I might do this once an hour.

00:33:48.810 --> 00:33:51.390
And once every hour, I
draw a sample of size four.

00:33:55.640 --> 00:33:57.980
So there's an additional
degree of freedom

00:33:57.980 --> 00:34:02.840
that's not in here, which is
I could also shorten my sample

00:34:02.840 --> 00:34:05.190
time, if you will.

00:34:05.190 --> 00:34:11.159
Because then it takes
fewer minutes of clock time

00:34:11.159 --> 00:34:16.239
to get to my 42
runs that I sample.

00:34:16.239 --> 00:34:18.960
So I haven't really talked
about sort of the trade off

00:34:18.960 --> 00:34:20.130
between these two.

00:34:20.130 --> 00:34:22.469
But this is the assumed model.

00:34:22.469 --> 00:34:24.600
And you could also
do things, and I

00:34:24.600 --> 00:34:28.260
think what was kind of
implicit in your question,

00:34:28.260 --> 00:34:31.409
is I could continuously
sample everything.

00:34:31.409 --> 00:34:34.260
And these days of automated
inspection equipment

00:34:34.260 --> 00:34:37.770
on the equipment, on the
manufacturing equipment,

00:34:37.770 --> 00:34:41.670
you're very often sampling and
measuring virtually every part.

00:34:41.670 --> 00:34:48.000
In which case, it's sort of
like every part gets measured,

00:34:48.000 --> 00:34:50.639
in which case I have a
very interesting trade

00:34:50.639 --> 00:34:55.300
off between n, which is exactly
the point you were making.

00:34:55.300 --> 00:34:59.340
If I increase n,
then it takes me

00:34:59.340 --> 00:35:05.930
longer to get more runs
that would form each point

00:35:05.930 --> 00:35:06.870
on the control chart.

00:35:06.870 --> 00:35:08.787
So there is a little bit
of a trade off there.

00:35:11.340 --> 00:35:13.200
Is that clear?

00:35:13.200 --> 00:35:14.050
Almost?

00:35:14.050 --> 00:35:14.970
AUDIENCE: Almost.

00:35:14.970 --> 00:35:18.240
Because why we don't
simple increase--

00:35:18.240 --> 00:35:22.180
because [INAUDIBLE]

00:35:24.270 --> 00:35:28.350
So why we just don't
increase the coefficient?

00:35:28.350 --> 00:35:31.350
So it's more sensitive
to any change.

00:35:31.350 --> 00:35:34.090
Why we should increase the x?

00:35:34.090 --> 00:35:35.340
DUANE BONING: Well, let's see.

00:35:35.340 --> 00:35:38.490
If you increase, go
from 3 sigma to 5 sigma,

00:35:38.490 --> 00:35:42.270
you increase the control limit
and you make it less sensitive.

00:35:42.270 --> 00:35:45.070
You have to have even
more extreme low points.

00:35:45.070 --> 00:35:49.590
So in that case, you decrease
the sensitivity of the chart

00:35:49.590 --> 00:35:52.630
to false alarms.

00:35:52.630 --> 00:35:55.110
So in that case,
you would want to--

00:35:57.630 --> 00:36:00.390
so alpha and beta are different.

00:36:00.390 --> 00:36:03.360
When you play with them a
little bit on the problem set,

00:36:03.360 --> 00:36:05.580
it'll reinforce, I think,
a little bit of this.

00:36:11.160 --> 00:36:17.320
So again, this is basically the
example we just talked about.

00:36:17.320 --> 00:36:17.820
OK.

00:36:17.820 --> 00:36:21.270
So I want to shift
to kind of this point

00:36:21.270 --> 00:36:24.510
that I was making earlier about
the difference between control

00:36:24.510 --> 00:36:27.390
limits and spec limits.

00:36:27.390 --> 00:36:32.130
But I'm going to get to it
through this notion of process

00:36:32.130 --> 00:36:34.260
capability.

00:36:34.260 --> 00:36:38.820
And CP and CPK, if some
of you have run into that

00:36:38.820 --> 00:36:42.270
or played with that
yet on the problem set.

00:36:42.270 --> 00:36:43.950
The point here is
that once we've

00:36:43.950 --> 00:36:49.200
got a distribution like this,
based on historical data

00:36:49.200 --> 00:36:51.960
that we use to build
our control chart,

00:36:51.960 --> 00:36:55.750
we basically have a
model of our process.

00:36:55.750 --> 00:36:57.670
It's a very simple
statistical model

00:36:57.670 --> 00:37:00.020
that says it has
a particular mean

00:37:00.020 --> 00:37:02.920
and it has a particular variance
in the normal distribution

00:37:02.920 --> 00:37:04.000
case.

00:37:04.000 --> 00:37:06.820
And an important thing that
we can do with that model

00:37:06.820 --> 00:37:09.100
is answer a couple
of key questions

00:37:09.100 --> 00:37:11.320
that really come up a lot.

00:37:11.320 --> 00:37:12.325
How good is the process?

00:37:14.900 --> 00:37:16.280
Is the mean where we want it?

00:37:16.280 --> 00:37:20.170
Is the variance
where we want it?

00:37:20.170 --> 00:37:22.390
And in particular,
how good is it

00:37:22.390 --> 00:37:26.800
with respect to our
requirements for manufacturing

00:37:26.800 --> 00:37:30.220
a particular part or product?

00:37:30.220 --> 00:37:32.720
This is the notion of
process capability.

00:37:32.720 --> 00:37:36.580
And what this also pulls in,
then, for the first time,

00:37:36.580 --> 00:37:39.430
unlike all of this, which
is just probabilities

00:37:39.430 --> 00:37:44.701
on a normally operating or a
not normally operating process,

00:37:44.701 --> 00:37:47.620
process capability pulls
in for the first time

00:37:47.620 --> 00:37:52.410
the notion of design
specifications.

00:37:52.410 --> 00:37:55.710
Different than control limits.

00:37:55.710 --> 00:37:58.140
So the basic way to
characterize your process

00:37:58.140 --> 00:38:01.920
is you have to assure yourself
that the process is indeed

00:38:01.920 --> 00:38:04.030
in control.

00:38:04.030 --> 00:38:05.490
It's operating normally.

00:38:05.490 --> 00:38:09.630
No common cause events.

00:38:09.630 --> 00:38:13.080
When that's the case for a
normally distributed process,

00:38:13.080 --> 00:38:17.430
it's characterized entirely
by mean and variance

00:38:17.430 --> 00:38:20.490
or on our control
chart by x bar and s.

00:38:20.490 --> 00:38:24.210
So indeed, if the control
limits have been set correctly

00:38:24.210 --> 00:38:28.290
in relation to
known sample sizes

00:38:28.290 --> 00:38:31.410
and known probabilities
of error and so on,

00:38:31.410 --> 00:38:33.300
then x bar and s
tells you everything

00:38:33.300 --> 00:38:38.070
about the underlying
process mean and variance.

00:38:38.070 --> 00:38:39.900
And now we can
start to use those

00:38:39.900 --> 00:38:43.140
to look at where
the process sits

00:38:43.140 --> 00:38:45.240
and those statistics
sit with respect

00:38:45.240 --> 00:38:49.700
to tolerances or
quality loss functions.

00:38:49.700 --> 00:38:52.460
Quality loss functions may be
a little less familiar to you.

00:38:52.460 --> 00:38:55.670
Everybody has an intuitive
idea of tolerance.

00:38:55.670 --> 00:38:57.590
So let's talk about that first.

00:38:57.590 --> 00:39:00.350
Basic idea of a tolerance
on, say, a part measurement

00:39:00.350 --> 00:39:02.710
is that there is an
upper and lower limit.

00:39:02.710 --> 00:39:05.690
So let's say it's some
characteristic dimension.

00:39:05.690 --> 00:39:09.050
We have a nominal dimension
or a target dimension.

00:39:09.050 --> 00:39:11.000
I'll label that x star.

00:39:11.000 --> 00:39:14.270
And then we have an upper spec
limit and a lower spec limit

00:39:14.270 --> 00:39:17.600
that we simply say,
as long as the part

00:39:17.600 --> 00:39:20.360
dimension is within
that range, we're

00:39:20.360 --> 00:39:22.280
going to call it a good part.

00:39:22.280 --> 00:39:25.850
And it meets maybe the
design requirements.

00:39:25.850 --> 00:39:30.380
It meets our consumer's
requirements, what have you.

00:39:30.380 --> 00:39:33.410
And what we're
saying is in essence,

00:39:33.410 --> 00:39:36.440
anything, anywhere
within those spec limits

00:39:36.440 --> 00:39:41.420
is equally good and
equally shippable.

00:39:41.420 --> 00:39:44.150
So there's no notion
of better or worse.

00:39:44.150 --> 00:39:49.160
It's simply on off decision,
it's acceptable or not.

00:39:49.160 --> 00:39:51.440
So that's a spec limit or not.

00:39:51.440 --> 00:39:53.840
Now, the idea of a
quality loss function

00:39:53.840 --> 00:39:57.830
is a beautiful
idea that actually

00:39:57.830 --> 00:40:02.780
conforms a little bit better
to what we really care about.

00:40:02.780 --> 00:40:05.150
And one could think of
a quality loss function

00:40:05.150 --> 00:40:09.470
as this penalty function
for any deviation

00:40:09.470 --> 00:40:12.710
away from our target x star.

00:40:12.710 --> 00:40:15.560
Now graphically, a typical
quality loss function

00:40:15.560 --> 00:40:22.610
that is used is a quadratic
cost for these deviations.

00:40:22.610 --> 00:40:30.670
Where the further I
get away from x bar,

00:40:30.670 --> 00:40:32.200
I penalize more and more.

00:40:32.200 --> 00:40:33.790
Not just even in
a linear fashion

00:40:33.790 --> 00:40:35.470
but in a square fashion.

00:40:35.470 --> 00:40:41.090
And then I have some normalized
or some constant out front.

00:40:41.090 --> 00:40:43.840
And what this basically
says is you really

00:40:43.840 --> 00:40:46.120
want to be on target.

00:40:46.120 --> 00:40:49.270
So as many of your parts are
right close to the target,

00:40:49.270 --> 00:40:52.210
the lower your loss function is.

00:40:52.210 --> 00:40:54.910
And then the further
away you get,

00:40:54.910 --> 00:40:56.590
the worse and worse things are.

00:40:56.590 --> 00:41:01.160
So this really drives
you to do two things.

00:41:01.160 --> 00:41:02.990
To minimize quality
loss function,

00:41:02.990 --> 00:41:05.660
what do you need to do?

00:41:05.660 --> 00:41:08.870
Want to target your process,
get it means centered.

00:41:08.870 --> 00:41:11.600
And second, you want to
reduce the deviation,

00:41:11.600 --> 00:41:14.550
reduce the variance
in your process.

00:41:14.550 --> 00:41:16.640
So with one quality
loss function,

00:41:16.640 --> 00:41:21.420
you're really driving the
process towards both goals.

00:41:21.420 --> 00:41:26.960
Now, there are questions about
how do you calibrate this?

00:41:26.960 --> 00:41:30.380
And there's some rules of
thumb, for example, people

00:41:30.380 --> 00:41:35.160
take in upper and
lower spec limits,

00:41:35.160 --> 00:41:42.440
you can start to sort of relate
these to some cost associated

00:41:42.440 --> 00:41:47.480
with if a part actually
goes out of spec and either

00:41:47.480 --> 00:41:52.130
you shift it or not, what
is the cost of throwing away

00:41:52.130 --> 00:41:52.910
that part?

00:41:52.910 --> 00:41:55.310
That might give you
a point on the curve

00:41:55.310 --> 00:41:59.360
and establish an actual
dollar value, for example,

00:41:59.360 --> 00:42:01.340
right at those spec limits.

00:42:01.340 --> 00:42:07.520
But it actually isn't that
important what the number is.

00:42:07.520 --> 00:42:10.910
It's really that you've got
this quadratic dependence which

00:42:10.910 --> 00:42:18.680
drives you towards minimizing
both being off target

00:42:18.680 --> 00:42:20.480
and the variance.

00:42:20.480 --> 00:42:23.690
So here's an example on
the use of tolerances.

00:42:23.690 --> 00:42:25.463
And now pulling in
both our distributions

00:42:25.463 --> 00:42:26.255
and control charts.

00:42:29.930 --> 00:42:34.600
So here we're assuming
the process is normal.

00:42:34.600 --> 00:42:40.000
And what I've done here
is drawn, first off,

00:42:40.000 --> 00:42:46.720
our in control
process with its mean

00:42:46.720 --> 00:42:50.140
and its plus minus
3 sigma points.

00:42:50.140 --> 00:42:54.550
Now, notice here I've
superimposed x star.

00:42:54.550 --> 00:42:56.830
So this is the desired target.

00:42:56.830 --> 00:43:00.100
And notice we're off
target a little bit.

00:43:00.100 --> 00:43:02.860
There's a deviation between
where our process mean really

00:43:02.860 --> 00:43:04.750
sits and our target.

00:43:04.750 --> 00:43:07.405
And I've also drawn
these spec limits.

00:43:10.480 --> 00:43:15.160
So now you can start to evaluate
various questions like, well,

00:43:15.160 --> 00:43:18.670
what's the probability
not of a false alarm,

00:43:18.670 --> 00:43:23.980
but the probability of
a nonconforming part?

00:43:23.980 --> 00:43:24.830
And that's easy.

00:43:24.830 --> 00:43:28.490
You're just looking
at the tails.

00:43:28.490 --> 00:43:30.460
So I can actually
go in and say, OK,

00:43:30.460 --> 00:43:33.070
the probability of
a nonconforming part

00:43:33.070 --> 00:43:36.730
up on this side is just
that tail to the right

00:43:36.730 --> 00:43:41.190
based on the actual
performance I need.

00:43:41.190 --> 00:43:46.620
You can also start to
ask questions of, well,

00:43:46.620 --> 00:43:49.920
how does my probability of
getting nonconforming parts

00:43:49.920 --> 00:43:54.025
change as a function of
how far off target I am?

00:43:54.025 --> 00:43:55.650
That's just shifting
the thing and it's

00:43:55.650 --> 00:43:57.250
moving those tails around.

00:43:57.250 --> 00:43:59.220
So you can evaluate
both of these.

00:43:59.220 --> 00:44:03.660
And essentially, a very
nice, canonical way

00:44:03.660 --> 00:44:12.570
of talking about how far
apart your spec limits

00:44:12.570 --> 00:44:14.580
are compared to
your distribution,

00:44:14.580 --> 00:44:18.090
how good your
underlying process is

00:44:18.090 --> 00:44:20.370
compared to your spec limits.

00:44:20.370 --> 00:44:24.300
Is this notion of process
capability, or CP, and then

00:44:24.300 --> 00:44:26.530
CPK, as we'll see
in a minute, also

00:44:26.530 --> 00:44:30.430
pulls in the idea of
the mean difference.

00:44:30.430 --> 00:44:35.700
So first off, definition of
CP or process capability ratio

00:44:35.700 --> 00:44:40.670
is simply the distance between
your upper and spec limits

00:44:40.670 --> 00:44:46.260
and six sigma of your
underlying process.

00:44:46.260 --> 00:44:49.860
Now, you start to hear the six
sigma kinds of terminology.

00:44:49.860 --> 00:44:51.913
Yeah?

00:44:51.913 --> 00:44:52.830
AUDIENCE: [INAUDIBLE].

00:44:52.830 --> 00:44:55.605
For example, if you
have 30 or 40 steps,

00:44:55.605 --> 00:44:59.580
and each unit [INAUDIBLE]
has its own [INAUDIBLE],,

00:44:59.580 --> 00:45:04.570
is there a system [INAUDIBLE]?

00:45:04.570 --> 00:45:08.465
And the second question was,
for [INAUDIBLE],, for example,

00:45:08.465 --> 00:45:13.920
or for [INAUDIBLE],, you have
thousands of parameters,

00:45:13.920 --> 00:45:16.170
each of which has a spec.

00:45:16.170 --> 00:45:20.910
So what's the quality
loss function?

00:45:20.910 --> 00:45:27.420
Each of them is at
some point [INAUDIBLE]

00:45:27.420 --> 00:45:31.680
all of them as
something [INAUDIBLE]..

00:45:31.680 --> 00:45:33.687
Otherwise, you're not s

00:45:33.687 --> 00:45:36.270
DUANE BONING: So you're asking,
really, I think two questions.

00:45:36.270 --> 00:45:38.520
I'll give you a really quick
answer right now and then

00:45:38.520 --> 00:45:40.600
we'll defer some of those.

00:45:40.600 --> 00:45:44.010
So your first question is,
for example, in semiconductor

00:45:44.010 --> 00:45:46.380
manufacturing or
in many processes,

00:45:46.380 --> 00:45:48.750
we have multiple process steps.

00:45:48.750 --> 00:45:50.670
And what you can
often do is have

00:45:50.670 --> 00:45:53.700
control charts for each
individual one of those.

00:45:53.700 --> 00:45:56.010
You can talk about
the process capability

00:45:56.010 --> 00:45:57.910
of each one of those.

00:45:57.910 --> 00:46:01.770
And yes, indeed, there
is also very often

00:46:01.770 --> 00:46:03.570
the question about
sort of quality roll

00:46:03.570 --> 00:46:10.530
up of how quality loss
or deviations either add

00:46:10.530 --> 00:46:16.720
up or compound themselves
across multiple steps.

00:46:16.720 --> 00:46:18.070
So we'll talk about that.

00:46:18.070 --> 00:46:21.030
But the first quick
answer there is

00:46:21.030 --> 00:46:24.730
if each of those process
steps are independent,

00:46:24.730 --> 00:46:29.400
then it's simply an
additive function.

00:46:29.400 --> 00:46:32.820
Where it gets interesting is if
those two process steps are not

00:46:32.820 --> 00:46:33.580
independent.

00:46:33.580 --> 00:46:35.580
So for example, the
second process step

00:46:35.580 --> 00:46:38.770
may magnify or
shrink or, in fact,

00:46:38.770 --> 00:46:42.000
even be designed to
compensate for deviations

00:46:42.000 --> 00:46:44.010
in the first process
step, in which case

00:46:44.010 --> 00:46:47.140
there is intentional
correlation between the two.

00:46:47.140 --> 00:46:49.710
So it's not always
a trivial question

00:46:49.710 --> 00:46:54.420
of how multiple steps
interact to aggregate.

00:46:54.420 --> 00:46:56.560
The second question
is a little different,

00:46:56.560 --> 00:47:02.040
which is especially in today's
highly instrumented processes,

00:47:02.040 --> 00:47:05.730
there are a zillion parameters
that you can measure.

00:47:05.730 --> 00:47:07.680
There's a zillion
control charts.

00:47:07.680 --> 00:47:09.720
And what we've been
talking about so far

00:47:09.720 --> 00:47:12.840
is a single parameter
control chart.

00:47:12.840 --> 00:47:14.790
And there's some very
important corrections

00:47:14.790 --> 00:47:19.650
one has to make when you've
got many, many control charts

00:47:19.650 --> 00:47:21.870
and many, many parameters.

00:47:21.870 --> 00:47:24.120
And just to give
you the intuition,

00:47:24.120 --> 00:47:28.680
if the probability for a
single control chart with plus

00:47:28.680 --> 00:47:36.450
minus 3 sigma control limits of
a false alarm is 3 out of 1,000

00:47:36.450 --> 00:47:40.860
or an average run length
of 333 and you have 1,000

00:47:40.860 --> 00:47:44.220
independent control charts,
how often do you think you're

00:47:44.220 --> 00:47:47.050
going to see a false alarm?

00:47:47.050 --> 00:47:49.270
All the time.

00:47:49.270 --> 00:47:52.990
So the quick answer
there is you actually

00:47:52.990 --> 00:47:57.070
have to think about an
aggregate false alarm rate, not

00:47:57.070 --> 00:47:59.500
an individual alpha
on every chart.

00:47:59.500 --> 00:48:01.480
And you really should
make an adjustment.

00:48:01.480 --> 00:48:03.340
Basically spread
your control limits

00:48:03.340 --> 00:48:06.940
on every one of your
independent individual charts

00:48:06.940 --> 00:48:10.510
much wider so that you
don't drive the engineer

00:48:10.510 --> 00:48:13.580
nuts chasing false alarms.

00:48:13.580 --> 00:48:17.950
So we'll, I think, talk
about multivariate situations

00:48:17.950 --> 00:48:20.170
also a little bit later.

00:48:20.170 --> 00:48:22.880
Very good questions.

00:48:22.880 --> 00:48:25.280
So the key idea here on
the process capability

00:48:25.280 --> 00:48:27.950
is simply talking
about the spread

00:48:27.950 --> 00:48:30.170
in your process compared
to your spec limits

00:48:30.170 --> 00:48:32.090
and how good that is.

00:48:32.090 --> 00:48:37.190
Notice that if I'm off
target, this definition CP

00:48:37.190 --> 00:48:38.580
doesn't change.

00:48:38.580 --> 00:48:41.060
This is purely talking
about the inherent variance,

00:48:41.060 --> 00:48:43.280
the inherent spread
in your process

00:48:43.280 --> 00:48:45.650
compared to your spec limits.

00:48:45.650 --> 00:48:50.400
Your inherent six sigma spread
compared to your spec limits.

00:48:50.400 --> 00:48:55.260
If you also want to worry
about mean, and you often do,

00:48:55.260 --> 00:48:58.800
another measure is the CPK.

00:48:58.800 --> 00:49:02.490
And what that looks at is
the shortest distance either

00:49:02.490 --> 00:49:05.880
on the right or the left of the
chart between your spec limit

00:49:05.880 --> 00:49:07.500
and your mean.

00:49:07.500 --> 00:49:10.530
How many sigma
apart is your mean

00:49:10.530 --> 00:49:12.360
from one of the spec limits?

00:49:12.360 --> 00:49:18.240
And divide that by 3 sigma,
3 sigma on either side.

00:49:18.240 --> 00:49:25.745
And so now you're penalized for
being offset from your target.

00:49:25.745 --> 00:49:27.120
So let's look at
this graphically

00:49:27.120 --> 00:49:29.250
and get a little
bit of a feel here.

00:49:29.250 --> 00:49:31.180
And here I'm just
using tolerance limits.

00:49:31.180 --> 00:49:34.480
I'm not talking quality
loss at this point.

00:49:34.480 --> 00:49:37.200
CP, CPK are actually
just defined

00:49:37.200 --> 00:49:41.330
for pure plus minus
tolerance limits.

00:49:41.330 --> 00:49:46.830
So what's drawn here is a
set of spec limits that go--

00:49:46.830 --> 00:49:49.620
these are sigma markings.

00:49:49.620 --> 00:49:56.040
So if I look at, first off, CP
and CPK, my lower spec limit,

00:49:56.040 --> 00:50:00.420
upper spec limit, those
are six sigma apart

00:50:00.420 --> 00:50:03.330
compared to the variance
of my underlying process.

00:50:03.330 --> 00:50:05.550
That's just a CP of 1.

00:50:05.550 --> 00:50:09.360
So qualitatively, CP of 1
you should mesh in your mind

00:50:09.360 --> 00:50:14.250
just says my spread is such
that my plus minus 3 sigma tails

00:50:14.250 --> 00:50:21.050
are right at the spec limits
if I'm perfectly centered.

00:50:21.050 --> 00:50:23.600
And as pictured here, I
am perfectly centered.

00:50:23.600 --> 00:50:26.030
So my CPK is also 1.

00:50:26.030 --> 00:50:29.270
In other words, my minimum
distance between mu

00:50:29.270 --> 00:50:32.750
and my upper spec limit or
mu and my lower spec limit

00:50:32.750 --> 00:50:35.750
is both three sigma.

00:50:35.750 --> 00:50:39.800
So the minimum of
those two is still 1.

00:50:39.800 --> 00:50:43.490
So my CPK is 1.

00:50:43.490 --> 00:50:46.610
Now let's consider a
slightly different picture.

00:50:46.610 --> 00:50:53.530
Here I have my spec
limits different from my--

00:50:53.530 --> 00:50:58.610
or basically my x bar
or x star here and my mu

00:50:58.610 --> 00:50:59.810
are very different.

00:50:59.810 --> 00:51:05.090
My process spread is
exactly the same as before.

00:51:05.090 --> 00:51:08.360
I still have an upper spec
limit minus lower spec limit

00:51:08.360 --> 00:51:10.010
equal to 6 sigma.

00:51:10.010 --> 00:51:14.900
So my CP, in some sense,
my inherent capability

00:51:14.900 --> 00:51:17.570
of the process in
terms of spread

00:51:17.570 --> 00:51:20.180
is still exactly the same.

00:51:20.180 --> 00:51:22.940
But now because
I'm off centered,

00:51:22.940 --> 00:51:28.790
the distance from mu to
the lower spec limit is 0.

00:51:28.790 --> 00:51:32.840
So my CPK is 0 in this case.

00:51:32.840 --> 00:51:36.500
So qualitatively, these
are very different.

00:51:36.500 --> 00:51:39.230
Here's another situation.

00:51:39.230 --> 00:51:41.310
What's changed here?

00:51:41.310 --> 00:51:44.150
Now in this case, my
spread is much smaller.

00:51:44.150 --> 00:51:46.250
I've got a tighter distribution.

00:51:46.250 --> 00:51:49.010
And in fact, my upper spec
limit to lower spec limit

00:51:49.010 --> 00:51:50.570
is basically 12 sigma.

00:51:53.180 --> 00:51:55.970
That's great, right?

00:51:55.970 --> 00:51:58.350
So that's a CP of 2.

00:51:58.350 --> 00:52:00.620
Now, I'm still off centered.

00:52:00.620 --> 00:52:02.060
I'm mistargeted.

00:52:02.060 --> 00:52:05.030
And so in this case
my minimum distance

00:52:05.030 --> 00:52:07.700
from mean to the
lower spec limit,

00:52:07.700 --> 00:52:09.560
that is the smallest distance.

00:52:09.560 --> 00:52:11.600
And that's still three sigma.

00:52:11.600 --> 00:52:16.250
So when I divide that distance
there, I get CPK of 1.

00:52:21.450 --> 00:52:23.190
And here's one last example.

00:52:23.190 --> 00:52:25.020
In this case, now
I've re-centered,

00:52:25.020 --> 00:52:31.110
got my CPK up to my full
theoretical possible CP of 2

00:52:31.110 --> 00:52:34.440
by putting it back
on the target.

00:52:34.440 --> 00:52:36.030
Putting my mean
back on the target.

00:52:39.430 --> 00:52:45.400
So one of the things you
use these CPs and CPKs for

00:52:45.400 --> 00:52:48.310
is in manufacturing.

00:52:48.310 --> 00:52:50.230
And you also can
use it in design

00:52:50.230 --> 00:52:52.870
to get a quick feel
for the relationship

00:52:52.870 --> 00:52:56.320
between your manufacturing
capability, the inherent spread

00:52:56.320 --> 00:53:03.250
and mean to target centering,
compared to your design specs.

00:53:03.250 --> 00:53:08.680
So you can characterize your
CP and CPK and then evaluate,

00:53:08.680 --> 00:53:13.480
well, if I make some change
in my design spec, what

00:53:13.480 --> 00:53:15.640
probabilities would that do?

00:53:15.640 --> 00:53:18.940
If I tightened up my spec
limits and start to say,

00:53:18.940 --> 00:53:22.667
if you just even know just a
CP and CPK, you can back out,

00:53:22.667 --> 00:53:25.600
because that's all in terms of
normalized standard deviation,

00:53:25.600 --> 00:53:27.880
you can back
probabilities associated

00:53:27.880 --> 00:53:30.940
with the number of parts
that are not going to conform

00:53:30.940 --> 00:53:32.950
to those specifications.

00:53:32.950 --> 00:53:36.760
So if I purely tell
you CP and CPK,

00:53:36.760 --> 00:53:38.920
you know probabilities,
fractions

00:53:38.920 --> 00:53:41.500
of nonconforming parts.

00:53:41.500 --> 00:53:43.960
And then you can
start to also compare

00:53:43.960 --> 00:53:48.550
how much would it buy me to move
my mean in terms of fraction

00:53:48.550 --> 00:53:50.620
of nonconforming parts.

00:53:50.620 --> 00:53:52.300
How much would it buy me?

00:53:52.300 --> 00:53:55.090
It might cost a lot
of energy and time

00:53:55.090 --> 00:53:57.310
to move the mean a little
bit closer to the center

00:53:57.310 --> 00:53:58.630
of my spec limits.

00:53:58.630 --> 00:54:00.310
Is it worth it or not?

00:54:00.310 --> 00:54:04.170
Or would my time be better
spent shrinking my variance

00:54:04.170 --> 00:54:06.850
if I were looking for
possible process improvements?

00:54:06.850 --> 00:54:10.150
You can start to make
those comparisons.

00:54:10.150 --> 00:54:12.340
And the last thing I want
to do with the CP and CPK

00:54:12.340 --> 00:54:15.550
is give you a little bit of
qualitative feel for these.

00:54:15.550 --> 00:54:21.670
Because what is a good
value for CP and CPK?

00:54:21.670 --> 00:54:24.880
Well, the larger the better.

00:54:24.880 --> 00:54:29.710
A CP or CPK of 0,
that's not good.

00:54:29.710 --> 00:54:32.410
When you start getting
in the ranges of two,

00:54:32.410 --> 00:54:35.110
that's often what is talked
about in manufacturing

00:54:35.110 --> 00:54:36.880
as being a goal.

00:54:36.880 --> 00:54:38.890
But it really depends
on your process

00:54:38.890 --> 00:54:43.570
and what the costs of
nonconforming parts are.

00:54:43.570 --> 00:54:47.590
But just to give you a feel,
then, for these probabilities

00:54:47.590 --> 00:54:51.040
of nonconforming parts and
what kind of CPs or CPKs

00:54:51.040 --> 00:54:56.710
those correspond to, just
recognize that a CPK of 1

00:54:56.710 --> 00:55:03.430
corresponds to 3 sigma tail
right at the spec limit.

00:55:03.430 --> 00:55:11.050
And being a 3 sigma tail
off of whatever my mean is.

00:55:11.050 --> 00:55:12.310
So I might be off target.

00:55:12.310 --> 00:55:18.580
But 3 sigma out from the
process mean, that's 3 sigma.

00:55:18.580 --> 00:55:21.130
Think of this is the
number of sigmas.

00:55:21.130 --> 00:55:24.130
And that corresponds to
about 1 in 1,000 parts

00:55:24.130 --> 00:55:25.540
falling outside of our tail.

00:55:30.620 --> 00:55:33.713
In fact, you can ask it upper
spec limit, lower spec limit.

00:55:33.713 --> 00:55:35.880
And we actually know the
number much more carefully.

00:55:35.880 --> 00:55:41.120
It's really 3 out of
1,000, those probabilities.

00:55:41.120 --> 00:55:44.900
If I go to a CPK of about
1.33, that corresponds roughly

00:55:44.900 --> 00:55:46.801
to a Z of 4.

00:55:46.801 --> 00:55:47.810
4 sigmas.

00:55:47.810 --> 00:55:51.020
I'm four sigmas away
from the spec limit.

00:55:51.020 --> 00:55:56.240
You can see I get about
100x reduction in the number

00:55:56.240 --> 00:55:58.520
of nonconforming parts.

00:55:58.520 --> 00:56:00.920
If I can go another
standard deviation,

00:56:00.920 --> 00:56:06.060
or a CPK of about 1.67, I
get another factor of 100.

00:56:06.060 --> 00:56:09.440
And similarly, I get about
another factor of 100

00:56:09.440 --> 00:56:12.500
if I can get to a CPK of 2.

00:56:12.500 --> 00:56:20.190
CPK of 2 means my
mean is 6 sigma away

00:56:20.190 --> 00:56:26.360
from either my minimum
distance from my upper

00:56:26.360 --> 00:56:27.980
or lower spec limit.

00:56:27.980 --> 00:56:30.610
And that's a very small number.

00:56:30.610 --> 00:56:36.140
That's really 6 sigma away or
about one part per billion.

00:56:36.140 --> 00:56:39.050
10 to the minus 9.

00:56:39.050 --> 00:56:43.430
So why do you want something
like a CP of 2 or a CPK of 2?

00:56:43.430 --> 00:56:49.060
Because one part per
billion seems awfully small.

00:56:49.060 --> 00:56:52.180
And the reason, really,
is so that you are

00:56:52.180 --> 00:56:56.230
robust to small mean shifts.

00:56:56.230 --> 00:57:02.280
To get down to even if you've
got that small mistargeting

00:57:02.280 --> 00:57:06.540
that you've still got enough
room in an inherent process

00:57:06.540 --> 00:57:10.470
with, say, a CP of
2 or CPK of 2 to be

00:57:10.470 --> 00:57:17.790
able to have a very small
number of nonconforming parts.

00:57:17.790 --> 00:57:19.780
So whoops, what happened here?

00:57:19.780 --> 00:57:25.320
So here's the picture where
my CPK and CP are both 2.

00:57:25.320 --> 00:57:26.880
Perfectly mean centered.

00:57:26.880 --> 00:57:31.950
Again, that's the 6 sigma
distance here between the tail.

00:57:31.950 --> 00:57:35.790
And we said the
probability of having way

00:57:35.790 --> 00:57:43.170
out here in the tail, that's
my fraction of nonconforming.

00:57:43.170 --> 00:57:45.640
And it's about one
part per billion,

00:57:45.640 --> 00:57:48.640
maybe two parts per billion.

00:57:48.640 --> 00:57:51.990
But now let's say that I
actually have a mean shift.

00:57:51.990 --> 00:57:53.730
That was a little weird.

00:57:53.730 --> 00:57:55.920
Where I've actually
shifted by 2 sigma.

00:57:58.830 --> 00:58:01.410
And recall we said back
with control charting,

00:58:01.410 --> 00:58:03.570
detecting small mean
shifts at 1 or 2 sigma

00:58:03.570 --> 00:58:05.670
are actually kind of hard.

00:58:05.670 --> 00:58:07.590
You actually have to
derive sample size

00:58:07.590 --> 00:58:12.630
way high to have a
very rapid detection.

00:58:12.630 --> 00:58:14.850
And what we'd like to
do in this case now,

00:58:14.850 --> 00:58:18.660
I've got still the same inherent
variability or a CP of 2,

00:58:18.660 --> 00:58:21.480
but that 2 sigma
shift still leaves

00:58:21.480 --> 00:58:27.210
me 4 sigma before my
tail starts impinging

00:58:27.210 --> 00:58:29.580
very much on my spec limit.

00:58:29.580 --> 00:58:34.260
And so now I'm here at
about 31 or 32 parts

00:58:34.260 --> 00:58:36.780
per million nonconforming.

00:58:36.780 --> 00:58:40.380
So that's kind of the
drive towards six sigma

00:58:40.380 --> 00:58:45.240
is that it allows you
to accommodate or have

00:58:45.240 --> 00:58:49.050
a process that is capable enough
that it can accommodate both

00:58:49.050 --> 00:58:54.040
the variance and
target deviations.

00:58:54.040 --> 00:59:00.740
Both of them are hard to
maintain as small as possible.

00:59:00.740 --> 00:59:03.010
Now, one can also go
back and, if you're

00:59:03.010 --> 00:59:05.200
given the quality
loss function, you

00:59:05.200 --> 00:59:11.950
can also ask if you
have the probability.

00:59:11.950 --> 00:59:13.510
So think of this as the PDF.

00:59:13.510 --> 00:59:16.250
If I have the probability
distribution function,

00:59:16.250 --> 00:59:22.270
I've got my underlying process
with some mean and variance.

00:59:22.270 --> 00:59:25.690
You can also do that
integral and ask questions

00:59:25.690 --> 00:59:30.070
about what is my
expected quality loss

00:59:30.070 --> 00:59:35.530
and do the same kinds of things
for mean deviations or variance

00:59:35.530 --> 00:59:36.580
changes.

00:59:36.580 --> 00:59:40.360
You just have to go in and
essentially do the calculation.

00:59:40.360 --> 00:59:42.130
One thing that's
nice is if you look

00:59:42.130 --> 00:59:46.055
at the expectation of
that quality loss function

00:59:46.055 --> 00:59:47.680
and just go through
and do a little bit

00:59:47.680 --> 00:59:53.240
of the basic expectation math
on that where you split out,

00:59:53.240 --> 00:59:57.370
factor out x minus x
bar, do that squaring

00:59:57.370 --> 01:00:02.770
and then relate components
of the expansion, what

01:00:02.770 --> 01:00:06.760
you get is one component
that is clearly related

01:00:06.760 --> 01:00:13.660
to your variance in the process
and another component that

01:00:13.660 --> 01:00:17.560
is clearly related to your
mistargeting or deviation

01:00:17.560 --> 01:00:19.010
from the mean.

01:00:19.010 --> 01:00:22.120
So in one quality loss
function, again, you

01:00:22.120 --> 01:00:25.900
can see and relate in the
normal distribution sense

01:00:25.900 --> 01:00:34.510
how both variance and how
target deviation contributes

01:00:34.510 --> 01:00:35.530
to quality loss.

01:00:38.620 --> 01:00:41.680
OK, so just to
recap a little bit

01:00:41.680 --> 01:00:45.610
on process capability,
what you've got

01:00:45.610 --> 01:00:47.230
is some characterization.

01:00:47.230 --> 01:00:50.740
You've got a model
of your real process.

01:00:50.740 --> 01:00:53.110
Where it's really
operating when in control

01:00:53.110 --> 01:00:55.550
in the best situation.

01:00:55.550 --> 01:00:56.980
You know its mean.

01:00:56.980 --> 01:00:58.510
You know its variance.

01:00:58.510 --> 01:01:02.140
You've got the requirements,
the spec limits.

01:01:02.140 --> 01:01:06.130
You don't use the spec limits
to set your control limits.

01:01:06.130 --> 01:01:10.210
Control limits are based on
detection of unusual events.

01:01:10.210 --> 01:01:12.850
And that's based
purely on your model.

01:01:12.850 --> 01:01:16.540
You might superimpose
your spec limits

01:01:16.540 --> 01:01:18.580
to be able to quickly
calculate and think

01:01:18.580 --> 01:01:25.060
about what the probability
of nonconforming parts are.

01:01:25.060 --> 01:01:27.730
But those are different things.

01:01:27.730 --> 01:01:30.340
To relate the two, a good
way of thinking about it

01:01:30.340 --> 01:01:35.770
is the CP and CPK or an
expected loss function.

01:01:35.770 --> 01:01:39.250
What's nice about CP and CPK
is we basically normalized

01:01:39.250 --> 01:01:44.100
away so that if I came
up to you and told you

01:01:44.100 --> 01:01:52.150
my process has a CP of 3
and a CPK of 2.5, you'd go,

01:01:52.150 --> 01:01:55.430
wow, I'm impressed.

01:01:55.430 --> 01:02:00.410
That's a pretty good process
or awfully loose spec limits.

01:02:00.410 --> 01:02:01.503
One of the two.

01:02:01.503 --> 01:02:02.420
You don't really know.

01:02:04.970 --> 01:02:09.170
AUDIENCE: If each step is six
sigma [INAUDIBLE] but there

01:02:09.170 --> 01:02:17.695
is no-- and let's say, the
real determinant [INAUDIBLE]

01:02:17.695 --> 01:02:19.320
you could have six
sigma for everything

01:02:19.320 --> 01:02:22.870
and you could still have
zero, because [INAUDIBLE]..

01:02:22.870 --> 01:02:26.740
Is there a CP and
CPK for [INAUDIBLE]??

01:02:26.740 --> 01:02:29.920
DUANE BONING: I'm not quite
sure what you mean by tracking.

01:02:29.920 --> 01:02:33.460
If by that you mean
being on target.

01:02:33.460 --> 01:02:37.090
AUDIENCE: Yeah,
let's say [INAUDIBLE]

01:02:37.090 --> 01:02:41.290
horizontal and
vertical [INAUDIBLE]..

01:02:41.290 --> 01:02:43.720
And each step by itself
would to be six sigma.

01:02:43.720 --> 01:02:46.960
And if the tracking
is off, what would

01:02:46.960 --> 01:02:48.907
be within that [INAUDIBLE]?

01:02:48.907 --> 01:02:51.490
DUANE BONING: So that's, again,
another multivariate question,

01:02:51.490 --> 01:02:54.640
looking at how important
it is for two parameters

01:02:54.640 --> 01:02:58.150
to be close to each other,
even if one or the other

01:02:58.150 --> 01:02:59.260
could deviate.

01:02:59.260 --> 01:03:00.430
It's OK if they both.

01:03:00.430 --> 01:03:01.570
AUDIENCE: [INAUDIBLE]

01:03:01.570 --> 01:03:03.070
DUANE BONING: And
so what I would do

01:03:03.070 --> 01:03:07.270
is if the importance is the
difference between those two

01:03:07.270 --> 01:03:10.720
dimensions, that's what
I would characterize

01:03:10.720 --> 01:03:14.300
and that's what I would
chart, put a plot on,

01:03:14.300 --> 01:03:16.660
and that's what I would
set my spec limits on.

01:03:16.660 --> 01:03:19.780
I might also still care
about, say, channel length.

01:03:19.780 --> 01:03:21.792
And so I might have
another chart on that too.

01:03:21.792 --> 01:03:23.500
I have to be careful,
because they're not

01:03:23.500 --> 01:03:24.940
completely independent.

01:03:24.940 --> 01:03:32.110
But what you really want to do
is get your statistical process

01:03:32.110 --> 01:03:37.540
control apparatus targeted at
or applied to the things that

01:03:37.540 --> 01:03:38.650
are most critical to you.

01:03:38.650 --> 01:03:40.510
And so in your
scenario, the tracking

01:03:40.510 --> 01:03:41.990
between those two parameters.

01:03:41.990 --> 01:03:44.905
That's what I would monitor.

01:03:44.905 --> 01:03:45.405
OK.

01:03:47.970 --> 01:03:52.320
So this is just a recap of the
x bar chart with a little--

01:03:52.320 --> 01:03:53.940
that's kind of ugly.

01:03:53.940 --> 01:03:55.510
A little example.

01:03:55.510 --> 01:03:59.318
I wonder if I can just
get rid of all those.

01:03:59.318 --> 01:04:01.040
Let me just try to
go forward here.

01:04:04.960 --> 01:04:06.940
I think this is trying
to reveal a data point

01:04:06.940 --> 01:04:10.400
one at a time, which we
don't really want to do.

01:04:10.400 --> 01:04:12.610
So here's an example.

01:04:12.610 --> 01:04:18.590
Based on what you've seen
about an x bar chart,

01:04:18.590 --> 01:04:22.820
and assuming here we've really
set plus minus 3 sigma control

01:04:22.820 --> 01:04:26.180
limits on this chart,
if you were wandering,

01:04:26.180 --> 01:04:30.170
moving along, and
just triggering just

01:04:30.170 --> 01:04:33.710
on, say, extremal points,
what do you think?

01:04:33.710 --> 01:04:35.780
Was this an alarm?

01:04:39.750 --> 01:04:42.270
Well, sure.

01:04:42.270 --> 01:04:43.410
How about that one?

01:04:43.410 --> 01:04:44.730
That was an alarm.

01:04:44.730 --> 01:04:46.710
Do you think they were real?

01:04:46.710 --> 01:04:48.000
Was it a false alarm?

01:04:48.000 --> 01:04:50.190
A real alarm?

01:04:50.190 --> 01:04:51.570
Why do you think it was real?

01:04:51.570 --> 01:04:54.720
AUDIENCE: Because [INAUDIBLE].

01:04:54.720 --> 01:04:58.240
And those two are
[INAUDIBLE] as well.

01:04:58.240 --> 01:05:03.610
So I expect to have
more points [INAUDIBLE]..

01:05:03.610 --> 01:05:06.130
DUANE BONING: So the real
answer is we don't know,

01:05:06.130 --> 01:05:07.930
but you're doing
exactly the right thing

01:05:07.930 --> 01:05:09.580
going back to some
of the discussion we

01:05:09.580 --> 01:05:12.700
had earlier of looking
for additional evidence.

01:05:12.700 --> 01:05:16.330
So your evidence here was
saying if I got this alarm,

01:05:16.330 --> 01:05:21.310
I might look back and say one
of the previous points, hmm,

01:05:21.310 --> 01:05:24.550
there's an awful lot of them
to one side of the mean.

01:05:24.550 --> 01:05:26.950
And in fact, maybe adrift,
although at this point

01:05:26.950 --> 01:05:30.010
sort of looks like it's
counter to the drift.

01:05:30.010 --> 01:05:33.820
So there's a little
counter evidence.

01:05:33.820 --> 01:05:35.860
By the time I get
to this point, now

01:05:35.860 --> 01:05:38.770
I'm starting to get really
convinced that something

01:05:38.770 --> 01:05:41.930
is weird.

01:05:41.930 --> 01:05:43.600
Because now look at this.

01:05:43.600 --> 01:05:48.010
What's the probability of
having all of those points?

01:05:48.010 --> 01:05:54.230
Almost all except for that one
or two to one side of the mean.

01:05:54.230 --> 01:05:57.140
Well, you could actually go
and calculate that probability.

01:05:57.140 --> 01:05:59.980
Sounds like a
problem set problem.

01:05:59.980 --> 01:06:03.100
But qualitatively,
you know something

01:06:03.100 --> 01:06:06.850
looks like it's off target.

01:06:06.850 --> 01:06:11.070
So this is the kind of thinking
that you would want to do.

01:06:11.070 --> 01:06:15.480
Now, this is a little tricky.

01:06:15.480 --> 01:06:19.900
If you were just eyeballing it,
what do you think a CP or a CPK

01:06:19.900 --> 01:06:21.535
would be for this process?

01:06:24.780 --> 01:06:25.470
I have no idea.

01:06:25.470 --> 01:06:26.762
I haven't done the calculation.

01:06:26.762 --> 01:06:31.050
You could go and do the
calculation based on what?

01:06:31.050 --> 01:06:33.090
What would you do to
actually formally try

01:06:33.090 --> 01:06:37.053
to calculate a CP or a
CPK for this process?

01:06:37.053 --> 01:06:37.988
AUDIENCE: [INAUDIBLE]

01:06:37.988 --> 01:06:38.780
DUANE BONING: Yeah.

01:06:38.780 --> 01:06:41.900
You'd calculate the mean
and the standard deviation.

01:06:41.900 --> 01:06:44.480
First off, you
have the true mean

01:06:44.480 --> 01:06:48.710
and you have the
true control limits.

01:06:48.710 --> 01:06:51.200
Could you calculate
a CP or CPK based

01:06:51.200 --> 01:06:52.640
on mean and standard deviation?

01:06:52.640 --> 01:06:53.848
I can give you those numbers.

01:06:58.819 --> 01:07:01.362
AUDIENCE: Because you need
upper and lower spec limits.

01:07:01.362 --> 01:07:02.820
DUANE BONING: You
need spec limits.

01:07:02.820 --> 01:07:05.790
Control chart by itself
never gives you spec limits.

01:07:05.790 --> 01:07:07.350
So you don't know.

01:07:07.350 --> 01:07:10.800
It may well be I've got
really tight spec limits.

01:07:14.430 --> 01:07:17.665
My parts really need to
operate down in here.

01:07:17.665 --> 01:07:19.290
By the way, you also
have to be careful

01:07:19.290 --> 01:07:23.170
because if these are
based on n samples,

01:07:23.170 --> 01:07:26.350
the sample distribution is
tighter than the underlying

01:07:26.350 --> 01:07:28.580
process distribution.

01:07:28.580 --> 01:07:31.570
So if you see spec limits
also on your control chart,

01:07:31.570 --> 01:07:32.950
it's kind of a warning.

01:07:32.950 --> 01:07:36.820
Because usually a control
chart is based on sampling.

01:07:36.820 --> 01:07:41.230
So by itself, you can't really
know what your CP and CPK are.

01:07:44.120 --> 01:07:46.740
And I would be
cautious about assuming

01:07:46.740 --> 01:07:52.260
your spec limits are anywhere
near your control limit.

01:07:52.260 --> 01:07:55.140
Don't assume your spec limits
are plus minus 3 sigma.

01:07:55.140 --> 01:07:57.450
You're really
falling in the trap

01:07:57.450 --> 01:08:01.140
of sort of basically
assuming a CP

01:08:01.140 --> 01:08:05.663
or a CPK of 1 process,
which is not often the case.

01:08:05.663 --> 01:08:09.540
AUDIENCE: [INAUDIBLE]
outside your control limits?

01:08:09.540 --> 01:08:10.900
DUANE BONING: Absolutely.

01:08:10.900 --> 01:08:11.430
Why?

01:08:11.430 --> 01:08:15.880
Here you're having a huge
fraction of nonconforming.

01:08:15.880 --> 01:08:16.380
Yeah.

01:08:16.380 --> 01:08:18.345
So that's my point.

01:08:18.345 --> 01:08:19.890
Your spec limits
may be very tight

01:08:19.890 --> 01:08:21.720
and your process
would be horrible.

01:08:21.720 --> 01:08:23.744
This is not a capable process.

01:08:23.744 --> 01:08:25.800
So normally yes.

01:08:25.800 --> 01:08:31.569
What you normally want to
do is trigger on alarms

01:08:31.569 --> 01:08:35.470
much before you have parts that
are falling outside of spec so

01:08:35.470 --> 01:08:38.350
that you can react to
drifts in the process

01:08:38.350 --> 01:08:41.710
before they cause you disaster.

01:08:41.710 --> 01:08:43.710
Absolutely.

01:08:43.710 --> 01:08:44.640
That was implicit.

01:08:44.640 --> 01:08:46.439
I'm glad you made it explicit.

01:08:46.439 --> 01:08:49.010
Yes, yes.

01:08:49.010 --> 01:08:50.600
OK.

01:08:50.600 --> 01:08:53.180
So I think we've talked
about a lot of these.

01:08:53.180 --> 01:08:55.729
What I'd like to do is
give you a little glimpse

01:08:55.729 --> 01:09:01.640
of some additional control chart
ideas that are beyond the x bar

01:09:01.640 --> 01:09:04.100
s or x bar range chart.

01:09:04.100 --> 01:09:06.640
There's some very good
things about the x bar chart.

01:09:06.640 --> 01:09:11.479
It's relatively simple, fairly
transparent, meaning you

01:09:11.479 --> 01:09:13.265
sort of know what
each data point means.

01:09:13.265 --> 01:09:16.010
You know what an alarm means.

01:09:16.010 --> 01:09:19.700
It relates very closely to
assumptions of normality.

01:09:19.700 --> 01:09:24.590
And in fact, by the
act of sampling,

01:09:24.590 --> 01:09:27.649
you're enforcing some
of these assumptions.

01:09:27.649 --> 01:09:30.470
By sampling within
larger than 1,

01:09:30.470 --> 01:09:32.899
generally, you've got
the central limit theorem

01:09:32.899 --> 01:09:34.010
on your side.

01:09:34.010 --> 01:09:39.140
So x bar really trends
towards a normal distribution.

01:09:39.140 --> 01:09:41.340
And we'll talk about
this a little bit.

01:09:41.340 --> 01:09:45.050
If I've got long
times between samples,

01:09:45.050 --> 01:09:49.310
if there's very,
very short time scale

01:09:49.310 --> 01:09:51.710
correlation between
sample points,

01:09:51.710 --> 01:09:54.740
these samples are truly
independent measures

01:09:54.740 --> 01:09:56.190
of the process.

01:09:56.190 --> 01:09:59.930
So as long as I've got
reasonably long sampling times,

01:09:59.930 --> 01:10:02.330
each of my samples are
an independent draw

01:10:02.330 --> 01:10:05.450
from the process.

01:10:05.450 --> 01:10:10.130
The problems are, yes, sample
size, I need n more than one.

01:10:10.130 --> 01:10:12.980
And very often I may
be measuring every one.

01:10:12.980 --> 01:10:16.730
So I'd like to get to some
ideas or control charts that

01:10:16.730 --> 01:10:20.480
might be able to either react
faster, a shorter average run

01:10:20.480 --> 01:10:23.000
length, or be able to
utilize the fact that you've

01:10:23.000 --> 01:10:28.860
got more data and I can have
this faster time response.

01:10:28.860 --> 01:10:31.740
So in fact, let's think real
briefly about the limit.

01:10:31.740 --> 01:10:36.020
What if your sample
size is just one?

01:10:36.020 --> 01:10:37.910
Could you go to that limit?

01:10:37.910 --> 01:10:41.930
There's a couple of
tricky points here.

01:10:41.930 --> 01:10:44.160
The good thing is you
have a lot of data.

01:10:44.160 --> 01:10:46.860
I can look at every part
and respond quickly.

01:10:46.860 --> 01:10:50.880
But now a lot of these
assumptions on the statistics

01:10:50.880 --> 01:10:54.870
or the calculation of
these basic statistics

01:10:54.870 --> 01:10:56.490
don't have meaning anymore.

01:10:56.490 --> 01:11:01.540
For a sample of size
one, what is the average?

01:11:01.540 --> 01:11:03.910
Well, OK, it's just that value.

01:11:03.910 --> 01:11:08.260
What's the standard deviation?

01:11:08.260 --> 01:11:09.610
No, it's undefined.

01:11:09.610 --> 01:11:12.755
1 over n minus 1
in the standard.

01:11:12.755 --> 01:11:13.880
You don't have enough data.

01:11:13.880 --> 01:11:17.320
You have no estimate
of a variance

01:11:17.320 --> 01:11:19.810
from that sample size of one.

01:11:19.810 --> 01:11:22.870
So an approach
here is essentially

01:11:22.870 --> 01:11:27.400
use kind of a variant referred
to as a running control chart

01:11:27.400 --> 01:11:29.030
or a running average.

01:11:29.030 --> 01:11:31.630
So you look back not
just on the new sample

01:11:31.630 --> 01:11:34.540
but some number of previous
data points and kind of

01:11:34.540 --> 01:11:39.670
have a window that moves along
or some calculated average

01:11:39.670 --> 01:11:43.210
of past history where
I might weight my most

01:11:43.210 --> 01:11:46.630
recent measurement
either equally or more

01:11:46.630 --> 01:11:49.150
importantly than my
past measurements

01:11:49.150 --> 01:11:54.520
and use that as an estimate
for the process average

01:11:54.520 --> 01:11:56.880
and the process variance.

01:11:56.880 --> 01:11:59.260
AUDIENCE: Does that violate
the independent assumption?

01:11:59.260 --> 01:12:00.430
DUANE BONING: Absolutely.

01:12:00.430 --> 01:12:04.090
It does violate the
independent assumption.

01:12:04.090 --> 01:12:06.800
If, however, they
truly are independent,

01:12:06.800 --> 01:12:09.550
you can still formulate
statistics and probabilities

01:12:09.550 --> 01:12:10.450
associated with that.

01:12:10.450 --> 01:12:16.650
In other words, if your
process data was independent.

01:12:16.650 --> 01:12:21.190
What you've got is from
any one point in time,

01:12:21.190 --> 01:12:22.800
I can calculate
the probabilities

01:12:22.800 --> 01:12:24.060
associated with that.

01:12:24.060 --> 01:12:26.460
But if I then calculated
that for my next point

01:12:26.460 --> 01:12:28.260
in time using some
of the same data,

01:12:28.260 --> 01:12:31.650
I would never use
some of the WECO rules

01:12:31.650 --> 01:12:35.707
like the probability of two
points in a row falling above,

01:12:35.707 --> 01:12:37.290
because they're no
longer independent.

01:12:37.290 --> 01:12:41.530
I can't just multiply
those two probabilities.

01:12:41.530 --> 01:12:43.320
So it would get
really messy trying

01:12:43.320 --> 01:12:48.330
to calculate sequence
probabilities

01:12:48.330 --> 01:12:51.480
using a running average.

01:12:51.480 --> 01:12:53.190
Now, that's different
for we'll see

01:12:53.190 --> 01:12:55.590
this notion of a
cumulative sum chart

01:12:55.590 --> 01:12:58.320
where actually you want
to use the fact that there

01:12:58.320 --> 01:13:02.790
is dependence to
aggregate deviations

01:13:02.790 --> 01:13:04.780
and detect them even faster.

01:13:04.780 --> 01:13:09.340
So you use that very notion
of lack of independence.

01:13:09.340 --> 01:13:11.820
So let's talk about these sort
of running average or an n

01:13:11.820 --> 01:13:13.770
equals 1 kind of design.

01:13:13.770 --> 01:13:16.210
What you'd like to do is
have increased sensitivity.

01:13:16.210 --> 01:13:19.680
You want to detect small
changes, especially

01:13:19.680 --> 01:13:22.320
small delta changes.

01:13:22.320 --> 01:13:24.730
So small shifts are one class.

01:13:24.730 --> 01:13:30.570
There's also cases where you
want to detect a small shift.

01:13:30.570 --> 01:13:33.420
We'll come to that chart
a little bit later.

01:13:33.420 --> 01:13:36.420
You could also have
the desire to not

01:13:36.420 --> 01:13:37.810
have too many false alarms.

01:13:37.810 --> 01:13:40.130
So you still want
noise rejection.

01:13:40.130 --> 01:13:41.880
And one of the problems
with some of these

01:13:41.880 --> 01:13:48.060
are you might, in fact,
actually have higher variance

01:13:48.060 --> 01:13:49.030
estimators.

01:13:49.030 --> 01:13:51.210
And you've got to be
very careful, because you

01:13:51.210 --> 01:13:56.440
can get larger false alarm rates
unless you compensate for that.

01:13:56.440 --> 01:13:59.670
But the basic idea,
and this almost

01:13:59.670 --> 01:14:01.970
goes back to your
earlier question,

01:14:01.970 --> 01:14:05.640
imagine, first off, what we're
doing with just a regular x bar

01:14:05.640 --> 01:14:08.970
chart but where we're
measuring every part

01:14:08.970 --> 01:14:11.710
and then just aggregating in
samples of different size.

01:14:11.710 --> 01:14:13.650
So what I've got
here is the run data.

01:14:13.650 --> 01:14:16.020
And then in the
little purple pictures

01:14:16.020 --> 01:14:21.280
is an x bar calculated for each
group of four sample points.

01:14:21.280 --> 01:14:23.680
So every four I
calculate the average,

01:14:23.680 --> 01:14:27.250
then I move on to the next
four and calculate the average.

01:14:27.250 --> 01:14:31.440
And so in a typical sort of x
bar chart with sample size n

01:14:31.440 --> 01:14:39.420
equals 4 where my sample
time is also n equals 4,

01:14:39.420 --> 01:14:43.920
I've got a smoother curve
here with the x bar.

01:14:43.920 --> 01:14:46.330
And we didn't explicitly
talk about it,

01:14:46.330 --> 01:14:48.840
but one of the
advantages of the x bar

01:14:48.840 --> 01:14:53.310
chart, what we talked
about was the squeezing

01:14:53.310 --> 01:14:55.000
of the distribution down.

01:14:55.000 --> 01:14:59.730
Another way of looking at it is
we're filtering high frequency

01:14:59.730 --> 01:15:06.570
deviations down to try to
detect a smoother average that's

01:15:06.570 --> 01:15:10.530
more indicative of the
true mean of the process.

01:15:10.530 --> 01:15:13.710
I'm really not trying
to detect variances.

01:15:13.710 --> 01:15:16.170
I'd just as soon have one
signal that just told me

01:15:16.170 --> 01:15:18.570
how good my mean is.

01:15:18.570 --> 01:15:22.660
And so an x bar really does
filtering of your data.

01:15:25.390 --> 01:15:29.050
So if you think about it from
the filtering perspective,

01:15:29.050 --> 01:15:30.460
what does the filtering do?

01:15:30.460 --> 01:15:32.290
It reduces some of
these sharp peaks.

01:15:35.380 --> 01:15:38.500
It aggregates or hides some
of this intermediate data,

01:15:38.500 --> 01:15:40.450
reduces this high
frequency content

01:15:40.450 --> 01:15:45.580
to try to get to the low
frequency, either drift

01:15:45.580 --> 01:15:49.690
or a mean shift a single
mean shift that's applying

01:15:49.690 --> 01:15:52.730
to many data points together.

01:15:52.730 --> 01:15:57.710
And so you can also think about
these as filtering operations.

01:15:57.710 --> 01:15:59.770
And any time you've
got filtering,

01:15:59.770 --> 01:16:02.140
you do get at some
of these questions

01:16:02.140 --> 01:16:07.480
we were kind of asking
about about how related

01:16:07.480 --> 01:16:12.890
or how independent are samples
at different points in time.

01:16:12.890 --> 01:16:15.460
So one can actually
go in and have

01:16:15.460 --> 01:16:17.650
a measure of this independence.

01:16:17.650 --> 01:16:22.390
And if the process itself
within a sample, the points

01:16:22.390 --> 01:16:24.250
are highly correlated,
you actually

01:16:24.250 --> 01:16:27.670
have to explicitly think about
this and for control charting

01:16:27.670 --> 01:16:30.970
spread those data points out.

01:16:30.970 --> 01:16:34.750
There is an
autocorrelation function

01:16:34.750 --> 01:16:39.520
that's rxx as a function
of the time separation

01:16:39.520 --> 01:16:42.400
or the number of sample
points between them

01:16:42.400 --> 01:16:44.650
that has this definition here.

01:16:44.650 --> 01:16:49.600
If you think about it
in terms of the time

01:16:49.600 --> 01:16:51.700
for an autocorrelated
function, that

01:16:51.700 --> 01:16:56.770
is in fact inherent in a
linear first order system,

01:16:56.770 --> 01:17:00.580
the autocorrelation function
drops off fairly rapidly

01:17:00.580 --> 01:17:04.420
from 0 time gap.

01:17:04.420 --> 01:17:09.040
So if I sample at this point in
time and I go some time away,

01:17:09.040 --> 01:17:15.430
the similarity of those
sample points decays away.

01:17:15.430 --> 01:17:19.960
If it were truly uncorrelated,
each and every new data point

01:17:19.960 --> 01:17:22.370
is completely
independent of the other.

01:17:22.370 --> 01:17:24.280
And the point is simply
here that if you're

01:17:24.280 --> 01:17:26.770
sampling from some
physical processes,

01:17:26.770 --> 01:17:29.110
there may be an inherent
correlation that

01:17:29.110 --> 01:17:31.100
isn't the one you're
trying to get at.

01:17:31.100 --> 01:17:34.480
You're trying to detect
long term trends to not

01:17:34.480 --> 01:17:37.900
detect very small shifts.

01:17:37.900 --> 01:17:40.840
And so you can play
around with the sample

01:17:40.840 --> 01:17:46.810
time compared to the inherent
correlation of the process.

01:17:46.810 --> 01:17:48.940
All this is saying is you
want to pick your sample

01:17:48.940 --> 01:17:55.060
time ultimately such that if I
pick this sample here and then

01:17:55.060 --> 01:18:00.700
my sample time some point later,
this underlying short term

01:18:00.700 --> 01:18:03.760
correlation is not
what I'm detecting.

01:18:03.760 --> 01:18:08.950
I'm detecting again
a big mean shift.

01:18:08.950 --> 01:18:10.960
So we've been
talking about this.

01:18:10.960 --> 01:18:12.580
Again, what you're
looking for here

01:18:12.580 --> 01:18:16.850
is maybe pulling your small
sample out, using, in fact,

01:18:16.850 --> 01:18:18.520
some of the
correlation there to be

01:18:18.520 --> 01:18:21.940
able to say what the
best estimate of the mean

01:18:21.940 --> 01:18:26.290
is right at that point in time
and then have uncorrelated

01:18:26.290 --> 01:18:27.310
samples around that.

01:18:30.480 --> 01:18:37.530
OK, well what I think I'll
do is probably stop now.

01:18:37.530 --> 01:18:39.600
I think I'll pick
up next time, defer

01:18:39.600 --> 01:18:42.780
or wait a little bit on
diving into the yield

01:18:42.780 --> 01:18:45.882
until maybe mid time Thursday.

01:18:45.882 --> 01:18:47.340
Because I do want
you to get a feel

01:18:47.340 --> 01:18:49.410
for these alternative charts.

01:18:49.410 --> 01:18:52.170
So the charts we'll
talk about next time

01:18:52.170 --> 01:18:55.920
include a moving average chart.

01:18:55.920 --> 01:18:59.760
Essentially you aggregate
your data and use

01:18:59.760 --> 01:19:04.740
that to form estimates of
local means and variances.

01:19:04.740 --> 01:19:06.870
And then we'll look
at other clever ways

01:19:06.870 --> 01:19:11.370
of using more recent data
and weight towards those.

01:19:11.370 --> 01:19:13.560
So we'll talk about an
exponentially weighted

01:19:13.560 --> 01:19:17.825
moving average control
chart and what that idea is.

01:19:17.825 --> 01:19:20.070
And then finally,
we'll also give you

01:19:20.070 --> 01:19:24.420
a little bit of a feel for
a cumulative sum chart where

01:19:24.420 --> 01:19:27.510
we aggregate deviations
from the mean,

01:19:27.510 --> 01:19:30.750
because we're really looking
for was there a small change

01:19:30.750 --> 01:19:34.140
or is there a small drift
going on in the process?

01:19:34.140 --> 01:19:35.970
And how do we increase
the sensitivity

01:19:35.970 --> 01:19:40.750
to detect that small shift
as rapidly as possible?

01:19:40.750 --> 01:19:42.960
So we'll talk about
charts that are really

01:19:42.960 --> 01:19:49.552
meant to squeeze that average
run length for detection down.

01:19:49.552 --> 01:19:50.760
So we'll see you on Thursday.

01:19:53.310 --> 01:19:55.860
And again, while you're
working on today's problem set,

01:19:55.860 --> 01:19:58.380
if you want to feel
good, take a glance

01:19:58.380 --> 01:20:00.615
as soon as it's released
at next week's problem set.

01:20:00.615 --> 01:20:02.941
It will make you
feel much better.