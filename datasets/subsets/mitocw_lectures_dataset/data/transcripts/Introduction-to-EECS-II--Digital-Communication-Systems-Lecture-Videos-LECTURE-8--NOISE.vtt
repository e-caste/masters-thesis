WEBVTT

00:00:00.000 --> 00:00:02.490
The following content is
provided under a Creative

00:00:02.490 --> 00:00:04.059
Commons license.

00:00:04.059 --> 00:00:06.360
Your support will help
MIT OpenCourseWare

00:00:06.360 --> 00:00:10.720
continue to offer high quality
educational resources for free.

00:00:10.720 --> 00:00:13.350
To make a donation or
view additional materials

00:00:13.350 --> 00:00:17.290
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:17.290 --> 00:00:18.294
at ocw.mit.edu.

00:00:27.440 --> 00:00:30.590
PROFESSOR: Today we're
going to dig a little deeper

00:00:30.590 --> 00:00:32.820
into the system that
we've been talking about.

00:00:32.820 --> 00:00:36.770
So we've already talked about
source coding and source

00:00:36.770 --> 00:00:38.520
decoding.

00:00:38.520 --> 00:00:40.460
And then we talked
about channel coding

00:00:40.460 --> 00:00:44.990
as we've just finished talking
about block codes and Viterbi--

00:00:44.990 --> 00:00:47.360
convolutional codes
and Viterbi decoding.

00:00:47.360 --> 00:00:50.360
So that's the coding
here and the decoding.

00:00:50.360 --> 00:00:53.030
And now we're going to
drill down to the next level

00:00:53.030 --> 00:00:56.300
to start to talk about
the actual signals going

00:00:56.300 --> 00:00:58.233
across physical channels.

00:00:58.233 --> 00:00:59.900
So this is going to
actually extend over

00:00:59.900 --> 00:01:02.630
the entire next
module of the course.

00:01:02.630 --> 00:01:05.209
I want to describe this in
the context of something

00:01:05.209 --> 00:01:10.052
you're going to be doing
in labs 4 through 6.

00:01:10.052 --> 00:01:11.510
You're actually
going to experiment

00:01:11.510 --> 00:01:13.550
with a specific channel.

00:01:13.550 --> 00:01:18.860
What you'll have is bits coming
in, code words coming in,

00:01:18.860 --> 00:01:22.470
being translated to signals.

00:01:22.470 --> 00:01:24.290
In this case,
discrete time signals,

00:01:24.290 --> 00:01:27.110
and I'll give you an
example shortly of that.

00:01:27.110 --> 00:01:30.380
The signals will then be
adapted through the modulator

00:01:30.380 --> 00:01:34.670
for transmission on
an analog channel.

00:01:34.670 --> 00:01:37.250
So there's a modulation
process and there's

00:01:37.250 --> 00:01:40.430
a digital-to-analog
conversion process.

00:01:40.430 --> 00:01:42.500
You'll be generating
waveform that you apply

00:01:42.500 --> 00:01:44.570
to the speaker in your laptop.

00:01:44.570 --> 00:01:46.850
That's going to
be a transmitter.

00:01:46.850 --> 00:01:52.070
The channel is going to
be just the air around you

00:01:52.070 --> 00:01:55.250
with all the disturbances
of room acoustics and noise

00:01:55.250 --> 00:01:57.223
and all of that, all the
distortions from that.

00:01:57.223 --> 00:01:58.640
And then you'll
pick up the signal

00:01:58.640 --> 00:02:01.610
on the microphone on your
laptop or an external microphone

00:02:01.610 --> 00:02:03.080
if you want.

00:02:03.080 --> 00:02:07.520
Conversion from analog
to digital, demodulation

00:02:07.520 --> 00:02:09.289
and filtering to
undo the modulation,

00:02:09.289 --> 00:02:11.600
and we'll be talking
about this in more detail

00:02:11.600 --> 00:02:14.690
to get another
sequence of samples.

00:02:14.690 --> 00:02:16.190
After which you
have a decision rule

00:02:16.190 --> 00:02:19.460
that then looks at the samples
and says, did I get a 0 or a 1?

00:02:19.460 --> 00:02:22.470
And you spit out the
bits of your code word.

00:02:22.470 --> 00:02:26.490
OK, so this is what we're
going to be looking at.

00:02:26.490 --> 00:02:29.170
So here is what you
might be sending

00:02:29.170 --> 00:02:30.830
at the transmitting end.

00:02:30.830 --> 00:02:32.750
You've got the bits coming in.

00:02:32.750 --> 00:02:35.240
You're going to convert
them to signals,

00:02:35.240 --> 00:02:37.740
and we're going to think
of discrete time signals.

00:02:37.740 --> 00:02:41.520
So this is a signal x of
n-- n takes integer value,

00:02:41.520 --> 00:02:43.910
so that's my
discrete time clock.

00:02:43.910 --> 00:02:46.620
And the typical waveform
might look like this.

00:02:46.620 --> 00:02:53.360
I might decide just very simply
to have levels held at 0.5 for,

00:02:53.360 --> 00:02:56.780
let's say, 16 samples
per bit, and then held

00:02:56.780 --> 00:03:01.110
at 0 for 16 samples to denote
a 1 and a 0 respectively.

00:03:01.110 --> 00:03:07.550
So here's a 1, a 00, 111, 0101.

00:03:07.550 --> 00:03:11.070
So we're converting two samples.

00:03:11.070 --> 00:03:13.280
This is a sample number,
and then the next step

00:03:13.280 --> 00:03:15.020
will be to actually--

00:03:15.020 --> 00:03:17.390
in your computer you'll
send this to your digital

00:03:17.390 --> 00:03:19.100
to analog converter which will--

00:03:19.100 --> 00:03:24.548
with a particular clock cycle,
convert this to real time.

00:03:24.548 --> 00:03:26.840
What you might imagine is
that the actual waveform that

00:03:26.840 --> 00:03:28.280
goes out on the
channel is somehow

00:03:28.280 --> 00:03:30.710
related to the
continuous waveform

00:03:30.710 --> 00:03:35.030
that you get by just connecting
the tops of these discrete time

00:03:35.030 --> 00:03:37.400
values.

00:03:37.400 --> 00:03:40.220
The actual mechanism for
transmission through the air

00:03:40.220 --> 00:03:41.487
we'll talk about next time.

00:03:41.487 --> 00:03:43.070
So right now we're
just going to focus

00:03:43.070 --> 00:03:46.670
on the level of the
discrete time signals.

00:03:46.670 --> 00:03:50.780
And at the other end, after
you've done your transmissions

00:03:50.780 --> 00:03:53.150
through the channel and you've
demodulated and filtered,

00:03:53.150 --> 00:03:55.820
you get a sequence
which ideally is

00:03:55.820 --> 00:03:59.360
a replication of the
sequence that you sent in.

00:03:59.360 --> 00:04:03.880
It can't have a scale factor,
scale factors don't worry us.

00:04:03.880 --> 00:04:06.650
In this case, you see that
the amplitude is divided by 2.

00:04:06.650 --> 00:04:09.460
But basically you see
the trace of what was

00:04:09.460 --> 00:04:12.370
sent at the transmitting end.

00:04:12.370 --> 00:04:14.530
There's some distortion
that's introduced

00:04:14.530 --> 00:04:16.360
by the dynamics of
the channel, and we'll

00:04:16.360 --> 00:04:18.529
be talking about that
in more detail later.

00:04:18.529 --> 00:04:21.399
So we aren't getting
quite the straight edges.

00:04:21.399 --> 00:04:23.830
But after a brief
transient period,

00:04:23.830 --> 00:04:26.230
the waveform seems to
settle to the constant value

00:04:26.230 --> 00:04:28.240
that we had of the input.

00:04:28.240 --> 00:04:30.805
So this is our received
set of samples.

00:04:30.805 --> 00:04:32.680
Now in this figure, I've
assumed that there's

00:04:32.680 --> 00:04:34.630
no noise, only the distortion.

00:04:34.630 --> 00:04:36.760
This lecture is going
to be about the noise.

00:04:36.760 --> 00:04:39.750
I wanted you to get the sense
of what distortion does,

00:04:39.750 --> 00:04:42.250
and then we'll park that issue
and come back to it next time

00:04:42.250 --> 00:04:45.030
and actually for several
lectures after that.

00:04:45.030 --> 00:04:47.620
But this lecture we're
going to focus on noise.

00:04:47.620 --> 00:04:51.070
Before we look at noise,
this is what a noise-free

00:04:51.070 --> 00:04:53.800
received signal might look like
with just the distortion in it.

00:04:56.360 --> 00:04:56.860
OK.

00:04:56.860 --> 00:05:01.630
And now you've got to
convert to a bit sequence.

00:05:01.630 --> 00:05:04.660
So a simple way to do that
is pick an appropriate point

00:05:04.660 --> 00:05:06.460
in each bit slot.

00:05:06.460 --> 00:05:08.580
Each slot of 16 samples long.

00:05:08.580 --> 00:05:10.780
Pick an appropriate
point, taking

00:05:10.780 --> 00:05:12.280
account of these
transient effects

00:05:12.280 --> 00:05:14.610
and so on, and then sample.

00:05:14.610 --> 00:05:18.100
And if the sample value
is above a threshold,

00:05:18.100 --> 00:05:19.690
you'll declare a 1.

00:05:19.690 --> 00:05:23.990
If the sample values below the
threshold, you'll declare a 0.

00:05:23.990 --> 00:05:27.380
And so you reconstruct
the sequence that went in.

00:05:27.380 --> 00:05:31.450
So we have the sample and
threshold feature here.

00:05:31.450 --> 00:05:34.060
So we're just taking one of
the samples in the bit period,

00:05:34.060 --> 00:05:36.490
comparing with the threshold,
and making a declaration.

00:05:36.490 --> 00:05:38.500
That's a very simple-minded
decision rule.

00:05:41.210 --> 00:05:41.860
OK.

00:05:41.860 --> 00:05:45.880
So we'll come back
to distortion.

00:05:45.880 --> 00:05:48.920
Today I want to
talk about noise,

00:05:48.920 --> 00:05:51.120
and I want to then
suppress distortion.

00:05:51.120 --> 00:05:52.880
So let's forget
about distortion.

00:05:52.880 --> 00:05:54.530
Let's assume that
the received signal

00:05:54.530 --> 00:05:59.840
yn is exactly what was sent
except for some additive noise.

00:05:59.840 --> 00:06:04.430
So what we're
imagining is you send

00:06:04.430 --> 00:06:16.750
a nice clean set of samples
here into your digital-to-analog

00:06:16.750 --> 00:06:22.660
converter, and what
comes out ideally

00:06:22.660 --> 00:06:28.160
would be the same
set of samples,

00:06:28.160 --> 00:06:31.130
but actually what happens is
that each of these samples

00:06:31.130 --> 00:06:32.930
is perturbed by noise.

00:06:32.930 --> 00:06:39.000
And so you get something
that might look like this.

00:06:44.290 --> 00:06:50.185
OK, so this is y, then, and
what we had before was x of n.

00:06:55.080 --> 00:06:55.580
OK.

00:06:55.580 --> 00:06:57.872
So nominally you'd
get the same thing.

00:06:57.872 --> 00:06:59.330
The only thing
that's different now

00:06:59.330 --> 00:07:01.850
is you've got an additive noise.

00:07:01.850 --> 00:07:05.630
We're going to assume
that this noise sample

00:07:05.630 --> 00:07:10.130
wn is independent from
one sample to the next.

00:07:10.130 --> 00:07:12.030
So when the channel
and the processing

00:07:12.030 --> 00:07:15.710
and so on decides to put
a noise sample on this,

00:07:15.710 --> 00:07:18.350
it doesn't pay attention
to what noise sample was

00:07:18.350 --> 00:07:19.530
out of on either side.

00:07:19.530 --> 00:07:22.138
So every noise sample
is picked independently.

00:07:22.138 --> 00:07:23.930
And it's picked from
the same distribution.

00:07:23.930 --> 00:07:27.270
That's with the identically
distributed part of this mean.

00:07:27.270 --> 00:07:30.560
So the characteristics
of the noise

00:07:30.560 --> 00:07:32.310
are the same right
through our signal.

00:07:32.310 --> 00:07:33.200
That's what we're assuming.

00:07:33.200 --> 00:07:34.867
That's the identically
distributed part.

00:07:34.867 --> 00:07:37.250
It's a statement about the
stationarity of the noise

00:07:37.250 --> 00:07:40.070
characteristics.

00:07:40.070 --> 00:07:42.020
All of this can be
generalized, but this

00:07:42.020 --> 00:07:45.100
is where we're going
to have our story

00:07:45.100 --> 00:07:48.740
and that's all we're
going to consider.

00:07:48.740 --> 00:07:52.940
OK, a key metric,
then, is what's

00:07:52.940 --> 00:07:54.080
the signal-to-noise ratio?

00:07:54.080 --> 00:07:57.530
This is something that you see
all over the place, the SNR.

00:07:57.530 --> 00:08:01.490
Usually what people
mean is signal power,

00:08:01.490 --> 00:08:04.060
and power is usually the
square of a signal-- that's

00:08:04.060 --> 00:08:05.060
what you're thinking of.

00:08:05.060 --> 00:08:07.633
If you think of
voltages, for instance,

00:08:07.633 --> 00:08:10.050
the square of the voltage gives
you power in the resistor.

00:08:10.050 --> 00:08:12.290
So you think of the
signal as being x,

00:08:12.290 --> 00:08:15.002
Its power as being x squared.

00:08:15.002 --> 00:08:16.460
Except you've got
to decide, do you

00:08:16.460 --> 00:08:19.760
want to talk about the peak
power or the time average power

00:08:19.760 --> 00:08:22.560
or some other measurement
of the signal power?

00:08:22.560 --> 00:08:26.280
So that's the signal
part of this ratio.

00:08:26.280 --> 00:08:31.130
And then the noise part of the
ratio is the noise variance.

00:08:31.130 --> 00:08:33.799
So we have a noise
component wn, it's

00:08:33.799 --> 00:08:36.350
the expected squared
amplitude of that.

00:08:36.350 --> 00:08:38.299
Oh, by the way, I didn't--

00:08:38.299 --> 00:08:40.520
this is on my slide,
but I didn't say it yet.

00:08:40.520 --> 00:08:42.960
I'm going to assume
the noise is zero mean.

00:08:42.960 --> 00:08:45.740
Which means that these
excursions from what

00:08:45.740 --> 00:08:51.260
you expect on average are at 0.

00:08:51.260 --> 00:08:54.320
If there was a systematic
bias to the noise,

00:08:54.320 --> 00:08:56.810
if I knew that there
was a non-zero mean,

00:08:56.810 --> 00:08:58.700
I could just factor
that into my processing

00:08:58.700 --> 00:09:01.800
and think of my
expected received signal

00:09:01.800 --> 00:09:04.200
as taking account of
that non-zero mean.

00:09:04.200 --> 00:09:06.140
So there's no loss of
generality, really.

00:09:06.140 --> 00:09:10.010
I'm assuming a zero mean noise.

00:09:10.010 --> 00:09:12.700
OK.

00:09:12.700 --> 00:09:14.890
Now when you come to
actually computing numbers,

00:09:14.890 --> 00:09:16.575
this is another example--

00:09:16.575 --> 00:09:17.950
showing another
kind of waveform,

00:09:17.950 --> 00:09:21.070
this is the sum of
sinusoids, I assume,

00:09:21.070 --> 00:09:22.910
to which you're
adding some noise.

00:09:22.910 --> 00:09:24.370
And in this
particular simulation,

00:09:24.370 --> 00:09:28.030
by tweaking the value
of A there, that's the--

00:09:28.030 --> 00:09:30.040
it's a gain factor
on the signal.

00:09:30.040 --> 00:09:32.780
You can actually vary
the signal-to-noise ratio

00:09:32.780 --> 00:09:35.030
and get a feel for
what difference

00:09:35.030 --> 00:09:37.210
signal-to-noise
ratio is represented.

00:09:37.210 --> 00:09:40.600
So at high
signal-to-noise ratio,

00:09:40.600 --> 00:09:43.460
the noise isn't perturbing
what went down very much.

00:09:43.460 --> 00:09:46.150
But when you get the lower
signal-to-noise ratios,

00:09:46.150 --> 00:09:48.490
the noise is actually
distorting the signal

00:09:48.490 --> 00:09:52.190
that you started with
quite substantially.

00:09:52.190 --> 00:09:58.930
Now the SNR here is
described in dB, decibels.

00:09:58.930 --> 00:10:02.170
And so let me just
say a word about that.

00:10:02.170 --> 00:10:04.338
That's a unit you'll
see all the time.

00:10:04.338 --> 00:10:05.380
You've seen all the time.

00:10:08.060 --> 00:10:10.510
So we're really
trying to measure

00:10:10.510 --> 00:10:11.810
a signal-to-noise ratio.

00:10:11.810 --> 00:10:13.660
So this is what you
would normally think of.

00:10:13.660 --> 00:10:15.880
But in many applications,
a logarithmic scale

00:10:15.880 --> 00:10:18.340
is really what you
want to deal with.

00:10:18.340 --> 00:10:20.500
For instance, if
you're measuring

00:10:20.500 --> 00:10:24.760
the response of the ear
to noise intensities,

00:10:24.760 --> 00:10:27.310
it turns out there's a
logarithmic feature built

00:10:27.310 --> 00:10:29.120
into our sensors.

00:10:29.120 --> 00:10:31.750
So usually want to be measuring
power and power ratios

00:10:31.750 --> 00:10:33.940
in terms of a log scale.

00:10:33.940 --> 00:10:36.470
That should have had
a capital B there.

00:10:36.470 --> 00:10:40.600
So here's the definition
of what a ratio is on dB.

00:10:40.600 --> 00:10:43.430
It's the ratio log to
the base 10 times 10.

00:10:46.360 --> 00:10:47.770
One caution here.

00:10:47.770 --> 00:10:49.750
I told you that when
we talk about powers,

00:10:49.750 --> 00:10:52.100
that's the square
of the amplitude.

00:10:52.100 --> 00:10:54.430
So if you're going to
compare amplitudes,

00:10:54.430 --> 00:10:57.040
ratio of amplitudes
on a log scale, then

00:10:57.040 --> 00:10:59.200
actually what you end
up doing is taking 20

00:10:59.200 --> 00:11:01.560
log 10 ratio of amplitudes.

00:11:01.560 --> 00:11:03.910
So you'll sometimes see
this definition as 20 log

00:11:03.910 --> 00:11:06.310
to the base 10
ratio of amplitudes,

00:11:06.310 --> 00:11:09.880
and what people are doing, then,
is comparing amplitude ratios,

00:11:09.880 --> 00:11:10.630
not power ratios.

00:11:10.630 --> 00:11:11.505
You have a question?

00:11:11.505 --> 00:11:15.260
AUDIENCE: Why do we define
power as amplitude squared?

00:11:15.260 --> 00:11:17.080
PROFESSOR: In sum--
so the question was,

00:11:17.080 --> 00:11:19.750
why do we define power
as amplitude squared?

00:11:23.530 --> 00:11:29.350
If you think of an
electrical circuit

00:11:29.350 --> 00:11:31.990
with some signal
applied across it,

00:11:31.990 --> 00:11:34.030
a voltage, the
instantaneous power

00:11:34.030 --> 00:11:38.560
dissipated in the
resistor is given by that.

00:11:38.560 --> 00:11:42.700
So people start to think of
square of a quantity as power.

00:11:42.700 --> 00:11:44.200
In the continuous
time domain that's

00:11:44.200 --> 00:11:47.260
very natural in signals
that come from physics,

00:11:47.260 --> 00:11:49.510
and that terminology is
just being carried over

00:11:49.510 --> 00:11:51.190
to this kind of a
discrete time setting.

00:11:51.190 --> 00:11:55.105
So when people say power, they
mean square of the signal.

00:11:55.105 --> 00:11:56.730
It could've been
called something else.

00:11:59.526 --> 00:12:01.860
OK.

00:12:01.860 --> 00:12:05.450
So you can actually span
huge ratios in power

00:12:05.450 --> 00:12:10.210
on this log scale with much
more better behaved numbers.

00:12:10.210 --> 00:12:14.540
0 dB, then, is a ratio of 1.

00:12:14.540 --> 00:12:16.730
3 dB, this is good to
carry around in your head.

00:12:16.730 --> 00:12:19.830
3 dB, it's actually
3.01-something,

00:12:19.830 --> 00:12:23.270
but 3 dB is a factor of
2 on the power ratio,

00:12:23.270 --> 00:12:27.360
or square root of 2
on an amplitude ratio.

00:12:27.360 --> 00:12:30.050
So let's actually go
back to what I showed you

00:12:30.050 --> 00:12:31.560
on the previous slide.

00:12:31.560 --> 00:12:36.470
So here, for instance,
is an SNR of 0.4 dB.

00:12:36.470 --> 00:12:38.750
If I figure that
that's close to 0 dB,

00:12:38.750 --> 00:12:41.660
then I should expect that
the noise power and signal

00:12:41.660 --> 00:12:45.710
power are about equal, and
the noise amplitude and signal

00:12:45.710 --> 00:12:47.400
amplitude are about equal.

00:12:47.400 --> 00:12:49.430
So what I expect to
see is perturbations

00:12:49.430 --> 00:12:51.470
of the original signal
that are comparable

00:12:51.470 --> 00:12:53.750
with the signal
values themselves,

00:12:53.750 --> 00:12:55.250
and that's sort of
what we see here.

00:12:55.250 --> 00:12:59.180
The shape of the signal is
pretty distorted at this point

00:12:59.180 --> 00:13:02.960
because the typical
amplitude of the noise sample

00:13:02.960 --> 00:13:06.460
is comparable with the signal
sample that I'm interested in.

00:13:06.460 --> 00:13:08.120
OK, so when you
get to 0 dB, you're

00:13:08.120 --> 00:13:12.530
starting to get quite
disturbed-looking waveforms.

00:13:12.530 --> 00:13:16.290
When you have 20 dB in
power, that's actually 100--

00:13:16.290 --> 00:13:18.777
ratio of 100--
sorry, what is that?

00:13:18.777 --> 00:13:20.360
Yeah, that's a ratio
of 100, isn't it?

00:13:20.360 --> 00:13:22.490
On par?

00:13:22.490 --> 00:13:24.300
So it's a ratio of
10 on amplitudes,

00:13:24.300 --> 00:13:25.550
and that's what you're seeing.

00:13:25.550 --> 00:13:29.300
The noise excursions
are about a 10th of what

00:13:29.300 --> 00:13:31.625
the signal amplitudes are.

00:13:31.625 --> 00:13:32.125
All right.

00:13:32.125 --> 00:13:38.050
It takes a little getting used
to, but it's fairly standard.

00:13:38.050 --> 00:13:38.550
OK.

00:13:38.550 --> 00:13:40.900
So now we want to figure
out how to describe

00:13:40.900 --> 00:13:42.070
noise and work with it.

00:13:44.780 --> 00:13:49.420
So let's look at a typical
run of a noise sequence.

00:13:49.420 --> 00:13:53.380
What I've done is just
extracted the noise piece

00:13:53.380 --> 00:13:55.430
of a typical received signal.

00:13:55.430 --> 00:13:57.520
So it's got excursions
above and below 0.

00:13:57.520 --> 00:14:00.550
Remember, I said it was a zero
mean random variable that we're

00:14:00.550 --> 00:14:04.690
thinking of, zero mean noise.

00:14:04.690 --> 00:14:08.080
And you can describe how these
values are distributed by just

00:14:08.080 --> 00:14:10.510
doing a simple histogram.

00:14:10.510 --> 00:14:13.120
And if you only take a few
values like 100 samples,

00:14:13.120 --> 00:14:15.865
you get a pretty
messy-looking histogram,

00:14:15.865 --> 00:14:18.167
it doesn't seem to
have much structure.

00:14:18.167 --> 00:14:19.750
But as you take more
and more samples,

00:14:19.750 --> 00:14:23.140
you'll typically find that the
histogram actually settles out

00:14:23.140 --> 00:14:27.980
to a nice shape, to some
subtle kind of shape.

00:14:27.980 --> 00:14:30.670
Normalizing this to
have unit area under it

00:14:30.670 --> 00:14:32.950
gives you what's called
the probability density

00:14:32.950 --> 00:14:34.430
function for the noise.

00:14:34.430 --> 00:14:36.130
So this is a term--

00:14:36.130 --> 00:14:40.040
kind of notion that's critical
in working with noise.

00:14:40.040 --> 00:14:42.070
So here's a step
of idealization.

00:14:42.070 --> 00:14:44.615
We're stepping back from
thinking about histograms

00:14:44.615 --> 00:14:46.990
to just a mathematical
way of talking

00:14:46.990 --> 00:14:50.230
about how random quantities
distribute themselves.

00:14:50.230 --> 00:14:51.820
So we'll talk about--

00:14:51.820 --> 00:14:55.300
by the way, we've been
using W for the noise

00:14:55.300 --> 00:14:58.510
and X for the signal, but if
you look in probability books,

00:14:58.510 --> 00:15:01.150
the first variable that people--
the first symbol people reach

00:15:01.150 --> 00:15:03.597
for and they want to talk
about a random variable is X,

00:15:03.597 --> 00:15:05.680
and I got stuck with a
whole bunch of figures that

00:15:05.680 --> 00:15:08.830
had X in them, so I didn't
want to change it to W.

00:15:08.830 --> 00:15:09.700
This is anything.

00:15:09.700 --> 00:15:11.520
We're going to
apply it to our W,

00:15:11.520 --> 00:15:15.080
but for now it's some capital
X. The other convention

00:15:15.080 --> 00:15:16.580
when you talk about
random variables

00:15:16.580 --> 00:15:18.730
as you tend to use a
capital letter to denote

00:15:18.730 --> 00:15:21.010
the random variable.

00:15:21.010 --> 00:15:21.510
OK.

00:15:21.510 --> 00:15:23.980
So we say that X is a
random variable governed

00:15:23.980 --> 00:15:27.550
by a particular probability
density function.

00:15:27.550 --> 00:15:29.680
If you can compute
the probability

00:15:29.680 --> 00:15:32.050
that X lies in some
particular interval

00:15:32.050 --> 00:15:35.530
by taking the corresponding
area under that PDF.

00:15:35.530 --> 00:15:39.550
So the PDF is the object
that gives you probabilities

00:15:39.550 --> 00:15:41.300
from areas under the integrals.

00:15:41.300 --> 00:15:43.600
So if you want the probability
that the quantity X,

00:15:43.600 --> 00:15:47.380
take the numerical values
in this range, X1 to X2,

00:15:47.380 --> 00:15:50.590
then you integrate
the PDF from X1 to X2,

00:15:50.590 --> 00:15:53.320
and this area is what you call--

00:15:53.320 --> 00:15:56.587
that area is the probability.

00:15:56.587 --> 00:15:58.420
And the total area under
the PDF, of course,

00:15:58.420 --> 00:16:00.910
has to be 1 because
the probability

00:16:00.910 --> 00:16:03.130
that X lies somewhere is 1.

00:16:03.130 --> 00:16:07.070
The probability that X
takes some value is 1.

00:16:07.070 --> 00:16:10.180
So this is how we
work with PDFs.

00:16:10.180 --> 00:16:13.210
Again, you'll find when
people want to sketch a PDF,

00:16:13.210 --> 00:16:16.353
the reflex is to sketch one
of these bell-shaped things.

00:16:16.353 --> 00:16:18.520
And it turns out there's
actually a reason for that.

00:16:21.580 --> 00:16:24.040
This bell-shaped thing or a
specific bell-shaped thing

00:16:24.040 --> 00:16:26.440
called the Gaussian
tends to arise

00:16:26.440 --> 00:16:28.120
in all sorts of
applications, and that's

00:16:28.120 --> 00:16:30.760
a consequence of something
called the central limit

00:16:30.760 --> 00:16:32.290
theorem.

00:16:32.290 --> 00:16:37.823
This is considered one of
the most important results

00:16:37.823 --> 00:16:38.740
in probability theory.

00:16:38.740 --> 00:16:44.290
It actually dates back to about
the 1730s as a conjecture,

00:16:44.290 --> 00:16:46.820
but it was Laplace who--

00:16:46.820 --> 00:16:52.978
in I guess the late 1700s, early
1800s who actually proved it.

00:16:52.978 --> 00:16:55.270
And it wasn't actually called
the central limit theorem

00:16:55.270 --> 00:16:57.845
until much more recently,
till about 1930 or so.

00:16:57.845 --> 00:17:00.220
And was called that because
it was the limit theorem that

00:17:00.220 --> 00:17:01.720
was central to all
of probability,

00:17:01.720 --> 00:17:03.178
that was the thinking.

00:17:03.178 --> 00:17:04.720
So here is the
central limit theorem.

00:17:04.720 --> 00:17:08.530
It says that if you
sum up a whole bunch

00:17:08.530 --> 00:17:13.000
of little random quantities that
are not necessarily Gaussian,

00:17:13.000 --> 00:17:16.700
and if they each have finite
mean and finite variance,

00:17:16.700 --> 00:17:18.950
the sum is going to have a
distribution that's going

00:17:18.950 --> 00:17:21.319
to look increasingly Gaussian.

00:17:21.319 --> 00:17:22.760
So you could start,
for instance,

00:17:22.760 --> 00:17:29.430
with a random variable that's
described by this triangular

00:17:29.430 --> 00:17:29.930
PDF.

00:17:32.450 --> 00:17:34.880
Take a whole bunch of
random variables generated

00:17:34.880 --> 00:17:36.230
according to that PDF.

00:17:36.230 --> 00:17:38.750
When I say generated according
to that PDF, what I mean

00:17:38.750 --> 00:17:41.390
is that the probability that
you get a value between any two

00:17:41.390 --> 00:17:46.640
limits here is the area under
that piece of the triangle.

00:17:46.640 --> 00:17:49.250
Generate a whole bunch of
these and sum them together,

00:17:49.250 --> 00:17:50.900
you find that the
resulting histogram

00:17:50.900 --> 00:17:54.350
starts to look Gaussian.

00:17:54.350 --> 00:17:56.427
You can start with another
kind of distribution,

00:17:56.427 --> 00:17:58.010
and again, it starts
to look Gaussian.

00:17:58.010 --> 00:18:01.403
And the more of these you add,
the more it looks Gaussian.

00:18:01.403 --> 00:18:03.320
And so this can be
actually made very precise.

00:18:03.320 --> 00:18:05.450
There's a very
precise sense in which

00:18:05.450 --> 00:18:08.212
the limiting distribution
in a situation like this

00:18:08.212 --> 00:18:08.795
is a Gaussian.

00:18:11.420 --> 00:18:12.530
So what is a Gaussian?

00:18:12.530 --> 00:18:14.210
I've got to describe
that for you.

00:18:14.210 --> 00:18:16.370
I'll do it in more
detail in a second.

00:18:19.100 --> 00:18:22.030
First, let me tell you
how we defined these two

00:18:22.030 --> 00:18:22.810
key parameters.

00:18:22.810 --> 00:18:26.298
These are things that from
other sorts of contexts.

00:18:26.298 --> 00:18:28.465
The mean and the standard
deviation of the variance,

00:18:28.465 --> 00:18:30.670
you know it from
quiz scores at least,

00:18:30.670 --> 00:18:34.390
but here is the mathematical
definition in terms of a PDF.

00:18:34.390 --> 00:18:38.350
So if you have a PDF for a
random variable capital X,

00:18:38.350 --> 00:18:42.600
the mean value of capital X is--

00:18:42.600 --> 00:18:44.620
it's basically the
average value of X

00:18:44.620 --> 00:18:48.400
weighted by the probability,
which is what you expect.

00:18:48.400 --> 00:18:50.910
So it's X times
the PDF integrated

00:18:50.910 --> 00:18:52.300
over all possible values.

00:18:52.300 --> 00:18:55.060
That's the definition
of the expected value.

00:18:55.060 --> 00:18:57.060
And what we do when we
take the expected value

00:18:57.060 --> 00:19:01.240
of the mean value on a quiz is
a sort of discrete time version

00:19:01.240 --> 00:19:02.330
of this.

00:19:02.330 --> 00:19:06.050
So we're seeing how many
people in a particular bin

00:19:06.050 --> 00:19:08.510
and multiply by the score
for the people in that bin

00:19:08.510 --> 00:19:10.390
and sum over all possible bins.

00:19:10.390 --> 00:19:12.730
That's one way to think
of what this is doing,

00:19:12.730 --> 00:19:15.590
assuming you've got the right
normalization of the PDF.

00:19:18.710 --> 00:19:22.200
And the variance is the
expected squared deviation

00:19:22.200 --> 00:19:23.770
from the mean value.

00:19:23.770 --> 00:19:27.060
So here's a deviation
from the mean value.

00:19:27.060 --> 00:19:30.070
You square it, and now you want
to take its expected value,

00:19:30.070 --> 00:19:33.180
so you weight it by the
PDF of X and that gives you

00:19:33.180 --> 00:19:33.970
the variance.

00:19:33.970 --> 00:19:37.230
So the variance is the
expected squared deviation

00:19:37.230 --> 00:19:39.291
from the mean value.

00:19:39.291 --> 00:19:42.750
OK, so the PDF is valuable
in getting all of this.

00:19:46.410 --> 00:19:51.150
And to get a sense of what
means and standard deviations

00:19:51.150 --> 00:19:52.082
and variances do--

00:19:52.082 --> 00:19:54.540
I don't know if I said from
the previous slide, by the way,

00:19:54.540 --> 00:19:57.120
that standard deviation is the
square root of the variance.

00:19:57.120 --> 00:19:57.930
Did I say that?

00:19:57.930 --> 00:20:00.330
Maybe not.

00:20:00.330 --> 00:20:02.470
But I have it at the
bottom of the slide, right?

00:20:02.470 --> 00:20:02.970
OK.

00:20:07.280 --> 00:20:08.050
OK.

00:20:08.050 --> 00:20:11.170
So shifting the mean
of a random variable,

00:20:11.170 --> 00:20:15.127
if I define a new random
variable with the same PPF

00:20:15.127 --> 00:20:16.960
except for a different
mean, what that means

00:20:16.960 --> 00:20:19.390
is that-- what that signifies
is that the PDF has just

00:20:19.390 --> 00:20:21.040
shifted over by that amount.

00:20:21.040 --> 00:20:23.450
So changing the mean
and nothing else

00:20:23.450 --> 00:20:28.030
will just shift the PDF over
to the corresponding position.

00:20:28.030 --> 00:20:30.850
Changing the variance from a
small value to a large value

00:20:30.850 --> 00:20:33.240
will spread out the
PDF because you're

00:20:33.240 --> 00:20:35.890
the variance is capturing the
expected squared deviation

00:20:35.890 --> 00:20:37.040
from the mean.

00:20:37.040 --> 00:20:42.040
So a higher variance PDF has
got to have a larger spread.

00:20:42.040 --> 00:20:44.020
But because the areas
normalized to 1,

00:20:44.020 --> 00:20:46.780
if it spreads out this way,
it's got to come down on top,

00:20:46.780 --> 00:20:48.760
and that's what
you're seeing here.

00:20:48.760 --> 00:20:50.260
All these pictures
actually turn out

00:20:50.260 --> 00:20:52.900
to be drawn for the
Gaussian, but my statements

00:20:52.900 --> 00:20:56.028
are more general here.

00:20:56.028 --> 00:20:57.320
But here's the Gaussian itself.

00:21:00.260 --> 00:21:02.300
So now I'm going
back to my notation

00:21:02.300 --> 00:21:04.970
W. We're going to think
of a random variable

00:21:04.970 --> 00:21:09.997
W which is going to be typical
of all my noise samples.

00:21:09.997 --> 00:21:12.080
It's going to have some
mean which we'll be taking

00:21:12.080 --> 00:21:15.050
to be 0 and our examples.

00:21:15.050 --> 00:21:17.370
It's got a variance
sigma squared.

00:21:17.370 --> 00:21:21.860
So if a random variable
has this particular PDF,

00:21:21.860 --> 00:21:23.210
we call it Gaussian.

00:21:23.210 --> 00:21:26.330
That's the definition of a
Gaussian random variable.

00:21:26.330 --> 00:21:27.830
The number here,
while you've got

00:21:27.830 --> 00:21:30.860
to remember it at some
point, but all it's doing

00:21:30.860 --> 00:21:34.740
is normalizing to unit area.

00:21:34.740 --> 00:21:36.710
So the key thing
about a Gaussian

00:21:36.710 --> 00:21:40.730
is that it's an exponential
with a negative sign there

00:21:40.730 --> 00:21:43.880
of the squared deviation
from the mean normalized

00:21:43.880 --> 00:21:46.490
by the variance with that
extra factor 2 there.

00:21:49.730 --> 00:21:54.790
So different choices of variance
will give you different shapes

00:21:54.790 --> 00:21:55.290
here.

00:21:55.290 --> 00:21:57.350
So the smaller
variances correspond

00:21:57.350 --> 00:22:01.050
to the more peaked and
more sharply-falling PDFs.

00:22:03.758 --> 00:22:04.300
So let's see.

00:22:04.300 --> 00:22:06.620
How many standard deviations
away from the mean

00:22:06.620 --> 00:22:10.060
you have to go before you
have very low probability

00:22:10.060 --> 00:22:11.410
of reaching there?

00:22:16.430 --> 00:22:18.380
Anyone?

00:22:18.380 --> 00:22:20.480
There's no unique answer
to this, but yeah?

00:22:20.480 --> 00:22:21.180
AUDIENCE: 3?

00:22:21.180 --> 00:22:22.820
PROFESSOR: 3 is not about idea.

00:22:22.820 --> 00:22:24.320
So let's see.

00:22:24.320 --> 00:22:26.150
Let's take sigma
squared equals 1.

00:22:26.150 --> 00:22:28.950
That's variance of 1, so
the standard deviation is 1.

00:22:28.950 --> 00:22:31.580
So for the red
trace, by the time

00:22:31.580 --> 00:22:35.990
we get out to the number 3,
we expect to actually see

00:22:35.990 --> 00:22:37.550
a very low value for the PDF.

00:22:37.550 --> 00:22:39.700
So 3 sounds about right.

00:22:39.700 --> 00:22:42.320
Does that hold up
for the blue one?

00:22:42.320 --> 00:22:44.840
Sigma squared is 0.25.

00:22:44.840 --> 00:22:47.970
So the square root of that
is a standard deviation,

00:22:47.970 --> 00:22:50.120
which is 0.5, so 3 times that.

00:22:50.120 --> 00:22:54.440
So when we get out to about 1.5,
we should be essentially at 0.

00:22:54.440 --> 00:22:56.960
So don't forget the square root.

00:22:56.960 --> 00:23:00.320
The other thing-- actually, I
should have commented on this

00:23:00.320 --> 00:23:03.110
earlier, let me show it to you--

00:23:03.110 --> 00:23:08.540
on this slide that I
had, I labeled this arrow

00:23:08.540 --> 00:23:10.310
here just schematically
to show you

00:23:10.310 --> 00:23:12.350
that it's a measure of width.

00:23:12.350 --> 00:23:14.973
But the tag I put on it
is standard deviation.

00:23:14.973 --> 00:23:16.640
Standard deviation
is the thing that you

00:23:16.640 --> 00:23:19.010
want to use when you
want to measure width

00:23:19.010 --> 00:23:19.790
on a distribution.

00:23:19.790 --> 00:23:21.290
That has the right units.

00:23:21.290 --> 00:23:23.900
Standard deviation, the
square root of variance

00:23:23.900 --> 00:23:27.200
has the same units as
X. If X is a voltage,

00:23:27.200 --> 00:23:29.150
the standard deviation
is units of voltage.

00:23:29.150 --> 00:23:32.600
It would be a mistake to label
a spread here by the variance.

00:23:32.600 --> 00:23:34.910
You want to think in terms
of standard deviation

00:23:34.910 --> 00:23:38.410
when you're thinking
about spread.

00:23:38.410 --> 00:23:40.790
So you define the variance
and then take the square root

00:23:40.790 --> 00:23:44.210
to get the standard deviation.

00:23:44.210 --> 00:23:45.320
OK.

00:23:45.320 --> 00:23:49.220
So for our noise in
this kind of setting,

00:23:49.220 --> 00:23:50.720
in our communications
setting, we're

00:23:50.720 --> 00:23:54.350
going to assume that
every noise sample was

00:23:54.350 --> 00:23:56.510
drawn from a Gaussian
distribution with zero mean.

00:23:56.510 --> 00:23:59.190
Just the same kind of
distribution that I showed you.

00:23:59.190 --> 00:24:02.870
So the only thing that's going
to change from one example

00:24:02.870 --> 00:24:04.640
to another will be the variance.

00:24:04.640 --> 00:24:07.360
But for a given case, we're
talking about IID noise.

00:24:07.360 --> 00:24:10.162
You're going to fix the
variance, have zero mean,

00:24:10.162 --> 00:24:11.870
and all your noise
samples will be pulled

00:24:11.870 --> 00:24:13.430
from that same distribution.

00:24:16.910 --> 00:24:21.440
If you were actually looking at
data here for these excursions,

00:24:21.440 --> 00:24:23.690
if you were actually
looking at what

00:24:23.690 --> 00:24:27.620
the excursions from the
baseline are, and you wanted

00:24:27.620 --> 00:24:30.530
in a numerical experiment--
in a simulation setting,

00:24:30.530 --> 00:24:33.050
for instance, or in
a physical experiment

00:24:33.050 --> 00:24:35.830
to get an estimate of what
the mean and variance are,

00:24:35.830 --> 00:24:38.150
well, we've got very
familiar expressions.

00:24:38.150 --> 00:24:43.437
You would take the sample
mean or the sample variance.

00:24:43.437 --> 00:24:45.020
The square root of
the sample variance

00:24:45.020 --> 00:24:48.090
would then be your estimate
of the standard deviation.

00:24:48.090 --> 00:24:51.030
So we can come at
the same objects--

00:24:51.030 --> 00:24:54.380
well, we have the PDF, which
is the mathematical construct,

00:24:54.380 --> 00:24:57.040
but in an experimental
setting, this

00:24:57.040 --> 00:24:59.810
is how you would go
about estimating these.

00:24:59.810 --> 00:25:01.820
And there's a whole big
theory of estimation

00:25:01.820 --> 00:25:04.580
that tells you whether these
are good estimates or not

00:25:04.580 --> 00:25:06.188
and offers alternatives,
and we're not

00:25:06.188 --> 00:25:07.230
getting into any of that.

00:25:07.230 --> 00:25:09.740
We're staying
close to the basics

00:25:09.740 --> 00:25:12.470
and close to what
makes sense intuitively

00:25:12.470 --> 00:25:15.950
and what's essentially
used all over.

00:25:20.750 --> 00:25:26.150
So now we have the
task at the receiver

00:25:26.150 --> 00:25:28.975
of getting a bunch
of samples like this

00:25:28.975 --> 00:25:31.100
and then trying to decide
whether what we're seeing

00:25:31.100 --> 00:25:34.640
is a reflection of a 1 or a 0.

00:25:34.640 --> 00:25:40.760
If we had 0's sent
from here, what

00:25:40.760 --> 00:25:46.880
we're going to see after we
receive the noisy signal is

00:25:46.880 --> 00:25:49.738
perturbed samples.

00:25:49.738 --> 00:25:51.780
And so we're going to look
at a particular sample

00:25:51.780 --> 00:25:55.580
and try and decide whether in
that bit slot what was sent

00:25:55.580 --> 00:25:56.450
was a 0 or a 1.

00:25:59.570 --> 00:26:04.410
I'm going to actually use a
scheme for illustration here

00:26:04.410 --> 00:26:07.910
that's not the scheme
that I've suggested here.

00:26:07.910 --> 00:26:09.980
Here, I suggested
something that's sending 0.

00:26:09.980 --> 00:26:13.910
If I'm communicating a 0 and
I'm sending some other voltage

00:26:13.910 --> 00:26:19.130
level when I want to communicate
a 1, I'm going between 0 and 1.

00:26:19.130 --> 00:26:20.810
It turns out on the
physical channel,

00:26:20.810 --> 00:26:25.280
if you've got a transmitter
with a certain peak power,

00:26:25.280 --> 00:26:27.680
you're probably better
off using a plus V

00:26:27.680 --> 00:26:30.680
to indicate a 1 and
a minus V for a 0

00:26:30.680 --> 00:26:33.650
because you're using that
transmitter at full power

00:26:33.650 --> 00:26:34.250
all the time.

00:26:34.250 --> 00:26:36.980
So you're actually trying
to overcome the noise

00:26:36.980 --> 00:26:38.548
as strongly as possible.

00:26:38.548 --> 00:26:40.340
So that's the scheme
I'm going to consider.

00:26:40.340 --> 00:26:41.798
I'm going to consider
that when you

00:26:41.798 --> 00:26:45.650
want to signal a 1, what you're
doing at the transmitting end

00:26:45.650 --> 00:26:51.460
is sending out L samples at
plus some peak voltage Vp.

00:26:51.460 --> 00:26:56.420
And when you want to signal a 0,
you send L samples at minus Vp.

00:26:56.420 --> 00:27:01.160
So this is what we refer to
as a bipolar signaling scheme.

00:27:12.280 --> 00:27:15.520
So it would be
something like this.

00:27:21.690 --> 00:27:23.980
This is the xn.

00:27:23.980 --> 00:27:28.460
And this is what I'm
using to signal a 1,

00:27:28.460 --> 00:27:31.570
and this is what I'm
using to signal a 0.

00:27:31.570 --> 00:27:35.440
But in terms of
actual voltage levels,

00:27:35.440 --> 00:27:40.150
this is minus Vp and Vp here.

00:27:52.800 --> 00:27:55.440
And on the receiving
end, what I'm

00:27:55.440 --> 00:27:58.140
getting at any
particular samples--

00:27:58.140 --> 00:28:00.990
so I pick one particular
sample to look at,

00:28:00.990 --> 00:28:05.730
and when I look at that sample--
let's say at sample n sub j.

00:28:05.730 --> 00:28:08.370
So maybe I'm looking
in the j-th bit slot

00:28:08.370 --> 00:28:12.000
and I pick one particular sample
time, let we call that n sub j.

00:28:12.000 --> 00:28:16.620
And I have to decide, am I
looking at plus Vp with noise

00:28:16.620 --> 00:28:19.500
or am I looking at
minus Vp with noise?

00:28:19.500 --> 00:28:21.312
That's a decision.

00:28:21.312 --> 00:28:22.020
I know the Vp's--

00:28:24.527 --> 00:28:26.610
assume that we've taken
care of the scaling and so

00:28:26.610 --> 00:28:28.080
on across the channel.

00:28:28.080 --> 00:28:30.780
And I know the
characteristics of the noise.

00:28:30.780 --> 00:28:33.780
I know that the noise samples
are Gaussian, zero mean,

00:28:33.780 --> 00:28:34.530
and some variance.

00:28:39.460 --> 00:28:43.580
So if I draw a picture
that's turned sideways

00:28:43.580 --> 00:28:50.510
here in terms of the
received signal, let's see.

00:28:53.300 --> 00:28:55.820
I might get something
centered around

00:28:55.820 --> 00:29:01.010
minus Vp or something
centered around Vp.

00:29:01.010 --> 00:29:05.810
If a minus Vp was sent, then
it's got a noise added to it.

00:29:05.810 --> 00:29:07.880
The noise has a
Gaussian distribution.

00:29:11.730 --> 00:29:13.550
So this is the
distribution of values

00:29:13.550 --> 00:29:17.270
I expect if a 0 was sent.

00:29:17.270 --> 00:29:20.350
So this is-- let me call it--

00:29:20.350 --> 00:29:23.630
it's the distribution of Y--

00:29:23.630 --> 00:29:25.730
I'm not going to put all
the attachments here--

00:29:25.730 --> 00:29:28.780
if a 0 was sent.

00:29:28.780 --> 00:29:33.050
Because my shorthand notation
for the density of Y assuming

00:29:33.050 --> 00:29:34.760
a 0 was sent.

00:29:34.760 --> 00:29:38.720
I haven't drawn a very good
Gaussian, but you get the idea.

00:29:38.720 --> 00:29:46.740
And here's the distribution
of Y if a 1 was sent.

00:29:51.950 --> 00:29:55.800
So what I'm actually measuring
is some number out here.

00:29:55.800 --> 00:29:56.630
I get some number.

00:30:00.640 --> 00:30:02.230
And I've got to
decide, did that come

00:30:02.230 --> 00:30:08.260
from having sent a 0 and
getting this much noise

00:30:08.260 --> 00:30:11.140
or did it come from sending a
1 and getting this much noise?

00:30:14.688 --> 00:30:15.480
That's the problem.

00:30:18.910 --> 00:30:21.000
So if 0's and 1's
are equally likely,

00:30:21.000 --> 00:30:22.890
what do you think is
a sensible rule here?

00:30:26.620 --> 00:30:29.010
Just pick a threshold
where these two cross.

00:30:29.010 --> 00:30:32.350
Threshold in the middle.

00:30:32.350 --> 00:30:35.160
So if the sample is above the
threshold, you declare a 1.

00:30:35.160 --> 00:30:39.310
If it's below the
threshold, you declare a 0.

00:30:39.310 --> 00:30:42.160
What if 0's and 1's
were not equally likely?

00:30:42.160 --> 00:30:44.590
Suppose it was much more
likely that you would get a 1.

00:30:47.108 --> 00:30:49.650
And suppose we're still thinking
in terms of threshold rules,

00:30:49.650 --> 00:30:51.360
what might you want to do?

00:30:51.360 --> 00:30:54.273
Suppose it's much more
likely that we get a 1.

00:30:54.273 --> 00:30:55.690
AUDIENCE: Move the
threshold to --

00:30:55.690 --> 00:30:56.338
PROFESSOR: Sorry?

00:30:56.338 --> 00:30:57.970
AUDIENCE: Move the
threshold to the left.

00:30:57.970 --> 00:30:59.262
PROFESSOR: Move it to the left.

00:30:59.262 --> 00:31:03.130
So you want to actually
allow for the fact

00:31:03.130 --> 00:31:05.230
that most of the time
you're getting 1's,

00:31:05.230 --> 00:31:08.140
and so you really have
to get close to the 0

00:31:08.140 --> 00:31:10.050
before you going to declare a 0.

00:31:10.050 --> 00:31:11.950
So your bias kind
of gets built in.

00:31:11.950 --> 00:31:15.680
Now this is just thinking as
an engineer what you might do.

00:31:15.680 --> 00:31:18.760
It turns out that
for Gaussian noise,

00:31:18.760 --> 00:31:20.410
the optimum decision
rule in terms

00:31:20.410 --> 00:31:22.330
of minimizing the
probability of error

00:31:22.330 --> 00:31:25.210
is exactly a threshold
rule of this kind.

00:31:25.210 --> 00:31:27.910
And the analysis will tell you
where that threshold should be.

00:31:27.910 --> 00:31:29.920
So we're not
getting into proving

00:31:29.920 --> 00:31:32.080
that this is the
optimum, but it turns out

00:31:32.080 --> 00:31:34.990
with Gaussian noise, the minimum
probability of error decision

00:31:34.990 --> 00:31:39.100
rule for this kind of
a hypothesis test--

00:31:39.100 --> 00:31:41.380
this is a classic
hypothesis test--

00:31:41.380 --> 00:31:42.830
is to pick a threshold.

00:31:42.830 --> 00:31:44.860
Now that's not true
necessarily for other sorts

00:31:44.860 --> 00:31:47.530
of distributions, it's
not true for the settings,

00:31:47.530 --> 00:31:51.470
but for the Gaussian it turns
out it's what you have to do.

00:31:51.470 --> 00:31:54.590
So let's just assume
equal prior probabilities.

00:31:54.590 --> 00:31:57.570
So 0's and 1's come at you
with equal probability,

00:31:57.570 --> 00:32:01.150
and we now have to figure out
what the probability of error

00:32:01.150 --> 00:32:02.440
is.

00:32:02.440 --> 00:32:07.300
So there's a slide here
with some computation.

00:32:07.300 --> 00:32:08.950
Let me just walk
you through that.

00:32:08.950 --> 00:32:11.075
We don't have to follow
all the details and you can

00:32:11.075 --> 00:32:12.400
study it and more--

00:32:12.400 --> 00:32:14.740
I mean, you can
study it at leisure,

00:32:14.740 --> 00:32:17.580
but it's the same
picture I showed.

00:32:17.580 --> 00:32:18.080
OK?

00:32:18.080 --> 00:32:19.043
AUDIENCE: [INAUDIBLE]

00:32:19.043 --> 00:32:19.710
PROFESSOR: Yeah?

00:32:19.710 --> 00:32:20.915
AUDIENCE: [? Sorry ?] [? to ?]
[? interrupt, but I ?] have

00:32:20.915 --> 00:32:21.415
a question--

00:32:21.415 --> 00:32:22.082
PROFESSOR: Yeah.

00:32:22.082 --> 00:32:23.150
AUDIENCE: --the Gaussian.

00:32:23.150 --> 00:32:23.600
PROFESSOR: [? About ?]
[INAUDIBLE]??

00:32:23.600 --> 00:32:24.800
AUDIENCE: [INAUDIBLE].

00:32:24.800 --> 00:32:24.970
PROFESSOR: Yeah.

00:32:24.970 --> 00:32:26.922
AUDIENCE: Is that true
when the two Gaussians

00:32:26.922 --> 00:32:28.570
have different variances?

00:32:28.570 --> 00:32:29.770
PROFESSOR: No.

00:32:29.770 --> 00:32:32.140
OK, I'm assuming-- OK,
the question-- the comment

00:32:32.140 --> 00:32:35.980
was that this rule of the
threshold being the optimum

00:32:35.980 --> 00:32:38.650
is not necessarily true
if the Gaussians have

00:32:38.650 --> 00:32:41.980
unequal variances.

00:32:41.980 --> 00:32:43.960
But I'm assuming IID noise.

00:32:43.960 --> 00:32:46.300
I'm assuming Independent
Identically Distributed noise.

00:32:46.300 --> 00:32:49.000
So the noise samples are
governed by the same Gaussian

00:32:49.000 --> 00:32:51.130
right through, and
then this turns out

00:32:51.130 --> 00:32:52.840
to be the optimum rule.

00:32:52.840 --> 00:32:56.110
Thanks for catching that.

00:32:56.110 --> 00:33:00.910
So you can imagine
the picture with--

00:33:00.910 --> 00:33:03.670
suppose the noise
is very sharply

00:33:03.670 --> 00:33:10.270
peaked for one of these
cases and very shallow

00:33:10.270 --> 00:33:11.230
for the other one.

00:33:11.230 --> 00:33:15.040
So there's high variance for
the 1's and there's low variance

00:33:15.040 --> 00:33:16.090
for the 0's.

00:33:16.090 --> 00:33:20.170
You might then anticipate that
if you got a signal way over

00:33:20.170 --> 00:33:23.440
to the left here, you're
going to call it a 1, not a 0.

00:33:23.440 --> 00:33:27.220
So each case needs to be
dealt with separately.

00:33:27.220 --> 00:33:30.250
But assuming these are
equal variance, which

00:33:30.250 --> 00:33:33.870
goes with the IID case,
this is the optimum rule.

00:33:33.870 --> 00:33:35.800
OK.

00:33:35.800 --> 00:33:37.810
So let me just
step through this.

00:33:37.810 --> 00:33:40.150
What we're saying
now is that what's

00:33:40.150 --> 00:33:41.613
the probability of
making an error?

00:33:41.613 --> 00:33:43.780
Well, let me actually write
down an expression here.

00:33:50.050 --> 00:33:53.970
So the probability of an error--

00:33:53.970 --> 00:33:56.130
this is the general expression.

00:33:56.130 --> 00:33:59.460
It's the probability
that I send a 0--

00:33:59.460 --> 00:34:04.170
let me just say that this is
the probability of sending

00:34:04.170 --> 00:34:16.210
0 times the probability
of declaring 1

00:34:16.210 --> 00:34:17.449
given that 0 was sent.

00:34:20.905 --> 00:34:22.530
And then there's the
other possibility.

00:34:22.530 --> 00:34:26.449
The probability that
I sent a 1, and here's

00:34:26.449 --> 00:34:32.170
the probability of declaring
a 0 given 1 was sent.

00:34:35.517 --> 00:34:37.100
So it turns out these
are the only two

00:34:37.100 --> 00:34:40.933
ways you can make an error, and
these are mutually exclusive,

00:34:40.933 --> 00:34:42.350
and so what you're
doing is adding

00:34:42.350 --> 00:34:45.199
the probabilities of the
two ways of making an error.

00:34:45.199 --> 00:34:48.170
You can either have a 0
sent, and then the question

00:34:48.170 --> 00:34:51.770
is, what's the probability of
declaring a 1 if a 0 was sent?

00:34:51.770 --> 00:34:53.540
And then you have the
corresponding term

00:34:53.540 --> 00:34:56.560
on the other side.

00:34:56.560 --> 00:34:58.660
If P0 equals P1--

00:34:58.660 --> 00:35:01.090
in other words, if
both of them are 0.5,

00:35:01.090 --> 00:35:04.720
this is going to be 1 minus P0.

00:35:04.720 --> 00:35:07.780
If they're both 0.5, then
you can pull that out,

00:35:07.780 --> 00:35:10.720
and what you're looking at
for the probability of error

00:35:10.720 --> 00:35:15.982
is just the sum of the
areas under these two tails.

00:35:15.982 --> 00:35:17.440
Oh sorry, not the
sum of the areas.

00:35:17.440 --> 00:35:20.760
If these are both 0.5,
you pull out 0.5--

00:35:20.760 --> 00:35:21.260
yeah.

00:35:21.260 --> 00:35:24.260
It's the sum of those two areas.

00:35:24.260 --> 00:35:24.760
OK.

00:35:24.760 --> 00:35:28.170
So 0.5 times the sum
of those two areas.

00:35:28.170 --> 00:35:32.670
Well in the symmetric case,
these two areas are the same.

00:35:32.670 --> 00:35:36.310
The area to the right of this
threshold under the Gaussian

00:35:36.310 --> 00:35:37.990
here is the probability
of declaring

00:35:37.990 --> 00:35:40.540
a 1 given that a 0 was sent.

00:35:40.540 --> 00:35:42.010
The area under the
tail to the left

00:35:42.010 --> 00:35:43.510
here is the probability
of declaring

00:35:43.510 --> 00:35:45.580
a 0 given that a 1 was sent.

00:35:45.580 --> 00:35:47.750
Those two areas are the same.

00:35:47.750 --> 00:35:51.070
So you'll discover that
the probability of error

00:35:51.070 --> 00:35:53.715
is just the area under
one of these tails.

00:35:53.715 --> 00:35:55.340
Just the area under
one of those tails.

00:35:55.340 --> 00:35:56.757
So that's all you
have to compute.

00:35:59.290 --> 00:36:01.040
So how do we do that?

00:36:01.040 --> 00:36:03.820
Well, as the area
under a Gaussian.

00:36:03.820 --> 00:36:05.920
We write down the Gaussian.

00:36:05.920 --> 00:36:09.340
Let's pretend that this
was 0 and this was Vp.

00:36:09.340 --> 00:36:12.430
It doesn't make a difference as
far as the computation of areas

00:36:12.430 --> 00:36:16.280
goes, but it makes the
expressions easier to write.

00:36:16.280 --> 00:36:23.580
So I'm saying that the
area under the table

00:36:23.580 --> 00:36:36.810
here is equal to the area
under the tail there.

00:36:36.810 --> 00:36:38.010
I can do it either way.

00:36:38.010 --> 00:36:41.370
I can either center the
Gaussian at minus Vp and look

00:36:41.370 --> 00:36:44.730
at the area to the right of 0,
or I can center the Gaussian at

00:36:44.730 --> 00:36:48.138
0 and look at the area
to the right of Vp.

00:36:48.138 --> 00:36:49.930
And the way the expression
is written here,

00:36:49.930 --> 00:36:52.900
it chooses to do
it the second way.

00:36:52.900 --> 00:36:56.790
So what we're saying is,
here is the Gaussian.

00:36:56.790 --> 00:36:58.260
It's centered at
0, so I don't have

00:36:58.260 --> 00:37:02.130
to subtract any term off
that term in the numerator.

00:37:02.130 --> 00:37:04.940
Here's the 2 sigma squared
in the denominator.

00:37:04.940 --> 00:37:07.860
And I integrate it
from Vp onwards.

00:37:07.860 --> 00:37:09.320
There's this
notation introduced.

00:37:09.320 --> 00:37:11.640
Vp is square root of ES.

00:37:11.640 --> 00:37:13.590
The reason is that
we're thinking

00:37:13.590 --> 00:37:17.970
in terms of the energy
of a single sample--

00:37:17.970 --> 00:37:19.560
or the power of a
single sample, they

00:37:19.560 --> 00:37:21.518
turn out to be the same
thing because it's just

00:37:21.518 --> 00:37:22.300
a single sample.

00:37:22.300 --> 00:37:25.060
So it's just the notation
that's traditionally used.

00:37:25.060 --> 00:37:27.810
But what we're talking
about as Vp there.

00:37:27.810 --> 00:37:32.250
So the area from Vp to
infinity under the Gaussian

00:37:32.250 --> 00:37:36.430
with the normalization
factor here.

00:37:36.430 --> 00:37:39.460
Now this is not an integral you
can evaluate in closed form.

00:37:39.460 --> 00:37:41.530
It is a tabulated integral.

00:37:41.530 --> 00:37:44.080
Tabulated most conveniently
in terms of something

00:37:44.080 --> 00:37:45.708
called the error function.

00:37:45.708 --> 00:37:47.500
And so when you work
through the calculus--

00:37:47.500 --> 00:37:49.417
and I won't show it to
here, it's in the book,

00:37:49.417 --> 00:37:51.280
you might do it
in recitation, you

00:37:51.280 --> 00:37:53.410
discover that the
probability of error

00:37:53.410 --> 00:37:59.080
is this error function of the
square root of ES over N0.

00:37:59.080 --> 00:38:01.990
N0's notation for
2 sigma squared.

00:38:01.990 --> 00:38:04.240
If I translate that back to
notation we've been using,

00:38:04.240 --> 00:38:07.360
it's just Vp over sigma.

00:38:07.360 --> 00:38:09.670
So the error performance,
the probability of error

00:38:09.670 --> 00:38:13.330
is a function of the ratio
of the peak amplitude

00:38:13.330 --> 00:38:17.710
on the signal to the standard
deviation of the noise.

00:38:17.710 --> 00:38:20.730
That's sort of the
square root of the SNR.

00:38:20.730 --> 00:38:23.560
The SNR would be
square of the amplitude

00:38:23.560 --> 00:38:26.450
to square of the
standard deviation.

00:38:26.450 --> 00:38:29.110
So this is the square
root of the SNR.

00:38:29.110 --> 00:38:32.160
And what does this
function look like?

00:38:32.160 --> 00:38:35.440
We can plot it.

00:38:35.440 --> 00:38:39.730
So that's exactly
that computation.

00:38:39.730 --> 00:38:43.270
This is a simulation on the
theory overlaid on each other,

00:38:43.270 --> 00:38:45.100
but we have 0.5.

00:38:45.100 --> 00:38:47.950
This function is called the
complementary error function.

00:38:47.950 --> 00:38:52.250
The C is for complementary,
erf is for error function,

00:38:52.250 --> 00:38:54.700
and here's the square
root of ES over N0

00:38:54.700 --> 00:38:57.220
which we had in the
previous expression.

00:38:57.220 --> 00:39:01.120
So you're really thinking
of signal-to-noise ratio

00:39:01.120 --> 00:39:06.850
along this axis in dB and
the probability of error

00:39:06.850 --> 00:39:09.250
on a logarithmic
scale down here.

00:39:09.250 --> 00:39:12.130
So as the signal-to-noise
ratio increases,

00:39:12.130 --> 00:39:15.340
as a signal becomes more
powerful relative to the noise,

00:39:15.340 --> 00:39:18.670
the probability of
error decreases.

00:39:18.670 --> 00:39:21.080
Visually what's going on?

00:39:21.080 --> 00:39:22.330
Let's go back to this picture.

00:39:25.680 --> 00:39:29.175
When the noise decreases
relative to the signal, what's

00:39:29.175 --> 00:39:31.050
happening is that these
Gaussians are getting

00:39:31.050 --> 00:39:33.250
more peaked and they're
pulling in more tightly,

00:39:33.250 --> 00:39:36.300
and so there's less chance
of confusing the two cases.

00:39:36.300 --> 00:39:37.920
So it's as simple as that.

00:39:37.920 --> 00:39:41.730
It's the separation between
these two levels divided

00:39:41.730 --> 00:39:44.130
by standard deviation of
the noise that's really

00:39:44.130 --> 00:39:45.780
going to determine performance.

00:39:45.780 --> 00:39:48.070
How far apart are
these two cases

00:39:48.070 --> 00:39:50.070
relative to the standard
deviation of the noise?

00:39:50.070 --> 00:39:53.557
That's the square root of
the signal-to-noise ratio.

00:39:53.557 --> 00:39:55.140
That's what determines
the probability

00:39:55.140 --> 00:39:56.310
of error in this case.

00:39:59.500 --> 00:40:01.290
OK.

00:40:01.290 --> 00:40:07.140
So are we done or could
we be doing better?

00:40:07.140 --> 00:40:10.980
If you think of what we did,
we looked at the samples

00:40:10.980 --> 00:40:13.800
in a bit slice, in a bit slot.

00:40:13.800 --> 00:40:16.170
We took one of those
samples and we carried out

00:40:16.170 --> 00:40:18.030
this decision rule on it.

00:40:18.030 --> 00:40:20.920
Could we be doing
better than that?

00:40:20.920 --> 00:40:21.442
Yeah?

00:40:21.442 --> 00:40:23.234
AUDIENCE: We could look
at one more sample?

00:40:23.234 --> 00:40:25.290
PROFESSOR: We could look
at more than one sample.

00:40:25.290 --> 00:40:26.700
This was a little bit arbitrary.

00:40:26.700 --> 00:40:28.320
It was conservative.

00:40:28.320 --> 00:40:32.640
Why you often do that is because
the number of samples in a bit

00:40:32.640 --> 00:40:35.010
slot is small and you don't
want to get near the edges

00:40:35.010 --> 00:40:37.380
because you're little
worried about the transience.

00:40:37.380 --> 00:40:39.750
You've got a long enough--

00:40:39.750 --> 00:40:43.020
if you've got enough samples in
a bit slot and the transience

00:40:43.020 --> 00:40:45.120
have died out, then maybe
you can just pick out

00:40:45.120 --> 00:40:47.230
a bigger chunk in the middle.

00:40:47.230 --> 00:40:51.320
And so that's what we're
going to think to do here.

00:40:51.320 --> 00:40:53.370
OK.

00:40:53.370 --> 00:40:56.470
So it's the same
setting, but we're

00:40:56.470 --> 00:40:59.290
going to average M samples.

00:40:59.290 --> 00:41:03.290
We've got L samples per bit.

00:41:03.290 --> 00:41:05.440
We may not be confident
capturing all of those

00:41:05.440 --> 00:41:08.060
were averaging because there's
some stuff at the edges,

00:41:08.060 --> 00:41:09.340
so let's pick M of them.

00:41:09.340 --> 00:41:16.795
Maybe less than L. Take M of
them and compute the average.

00:41:16.795 --> 00:41:18.670
And I'm doing this just
for one of the cases.

00:41:18.670 --> 00:41:22.520
You'd have to do the same
thing for the minus Vp case.

00:41:22.520 --> 00:41:26.200
So the question is, what
does the average do?

00:41:26.200 --> 00:41:27.700
So why did you want
to average them?

00:41:27.700 --> 00:41:29.034
What was your intuition?

00:41:29.034 --> 00:41:32.497
AUDIENCE: Because that would--
it [? would be ?] [INAUDIBLE]..

00:41:32.497 --> 00:41:33.080
PROFESSOR: OK.

00:41:33.080 --> 00:41:34.430
So here's the key thing.

00:41:34.430 --> 00:41:38.000
If you've got independent noise
samples and you average them,

00:41:38.000 --> 00:41:39.730
you're going to
decrease the variance.

00:41:39.730 --> 00:41:42.560
If you've got M independent
noise samples from an IID

00:41:42.560 --> 00:41:45.740
process, you decrease
the variance by M.

00:41:45.740 --> 00:41:50.180
This doesn't hold if the noise
samples are not independent.

00:41:50.180 --> 00:41:54.050
In fact, if one noise sample
equals the other, then

00:41:54.050 --> 00:41:55.820
when you add the two,
you get something

00:41:55.820 --> 00:41:59.960
whose variances is four
times rather than just twice.

00:41:59.960 --> 00:42:02.330
So it's critical that
these be independent.

00:42:02.330 --> 00:42:05.960
So if we've got
independent samples--

00:42:05.960 --> 00:42:07.970
independent noise
samples from one sample

00:42:07.970 --> 00:42:11.090
to the next and we
average them-- well, let's

00:42:11.090 --> 00:42:12.920
just average both
sides of this equation.

00:42:12.920 --> 00:42:14.990
We've got the average
of Y going to be

00:42:14.990 --> 00:42:16.790
the average of these
values, which is just

00:42:16.790 --> 00:42:20.090
going to be Vp again
because it's constant at Vp,

00:42:20.090 --> 00:42:22.112
plus the average of W.

00:42:22.112 --> 00:42:23.570
Here's the other
interesting thing.

00:42:23.570 --> 00:42:25.070
We're not going to
try proving this,

00:42:25.070 --> 00:42:29.810
but it turns out
that the average

00:42:29.810 --> 00:42:33.107
of a sum of independent
Gaussians is, again, Gaussian.

00:42:33.107 --> 00:42:35.440
You might believe that if you
think of the central limit

00:42:35.440 --> 00:42:35.860
theorem.

00:42:35.860 --> 00:42:37.360
You think of each
of these Gaussians

00:42:37.360 --> 00:42:40.220
being approximated by
sums of random variables.

00:42:40.220 --> 00:42:41.860
So the sum of these
Gaussians is then

00:42:41.860 --> 00:42:43.960
a sum of just a larger
number of random variables

00:42:43.960 --> 00:42:45.790
that should still be Gaussian.

00:42:45.790 --> 00:42:48.030
So the sum of an
independent set of Gaussians

00:42:48.030 --> 00:42:50.530
is, again, Gaussian.

00:42:50.530 --> 00:42:53.710
So all I need to know
for this average W

00:42:53.710 --> 00:42:55.540
since it's Gaussian
is what is its mean

00:42:55.540 --> 00:42:57.310
and what is its variance?

00:42:57.310 --> 00:42:59.273
It turns out if you
add up a bunch of zero

00:42:59.273 --> 00:43:00.940
mean random variables,
you get something

00:43:00.940 --> 00:43:03.200
with zero mean, no surprise.

00:43:03.200 --> 00:43:05.200
And if you add--

00:43:05.200 --> 00:43:08.020
if you take the average,
then the variance actually

00:43:08.020 --> 00:43:12.353
drops by that factor M.

00:43:12.353 --> 00:43:13.770
So what you're
going to do is take

00:43:13.770 --> 00:43:16.770
the average of the signal,
average of the noise.

00:43:16.770 --> 00:43:18.995
That shrinks the
noise component.

00:43:18.995 --> 00:43:20.370
You have the same
kind of picture

00:43:20.370 --> 00:43:22.650
but now with a higher
signal-to-noise ratio.

00:43:22.650 --> 00:43:26.640
Now what you've got in the
numerator instead of ES

00:43:26.640 --> 00:43:29.240
is EB, which is M times ES.

00:43:29.240 --> 00:43:31.290
You're summing the
energies of all the samples

00:43:31.290 --> 00:43:32.550
that you've taken.

00:43:32.550 --> 00:43:35.450
And that's what we refer to as
EB, it's the energy of the bit.

00:43:40.730 --> 00:43:41.230
All right.

00:43:41.230 --> 00:43:46.390
It turns out that that has
all sorts of implications.

00:43:46.390 --> 00:43:48.292
You certainly want
to be averaging

00:43:48.292 --> 00:43:50.500
if you've got this kind of
setting, because otherwise

00:43:50.500 --> 00:43:53.590
you're leaving all these
samples on the table

00:43:53.590 --> 00:43:55.160
and not making good use of them.

00:43:55.160 --> 00:43:57.670
So if you're really
getting ambitious,

00:43:57.670 --> 00:43:59.590
you really want to be
extracting all of that.

00:44:03.280 --> 00:44:06.150
Also, if you want to maintain
the same error performance

00:44:06.150 --> 00:44:08.248
and the noise
intensity increases,

00:44:08.248 --> 00:44:10.540
then you're going to want to
have more samples per bit.

00:44:10.540 --> 00:44:12.415
You may want to slow
down your signaling rate

00:44:12.415 --> 00:44:14.230
so you can put more
samples per bit.

00:44:14.230 --> 00:44:18.460
It turns out in the deep
space probe examples

00:44:18.460 --> 00:44:20.830
that we've been
talking about, that's

00:44:20.830 --> 00:44:22.130
exactly what's happening.

00:44:22.130 --> 00:44:25.420
If you look at Voyager
2, it was transmitting

00:44:25.420 --> 00:44:28.547
at 115 kilobits in 1979.

00:44:28.547 --> 00:44:30.130
That's the year, I
joined the faculty,

00:44:30.130 --> 00:44:32.680
that's a long time ago.

00:44:32.680 --> 00:44:34.270
That was near Jupiter.

00:44:34.270 --> 00:44:40.492
Last month-- I mean, it's
gone past Jupiter, Saturn.

00:44:40.492 --> 00:44:41.950
The other planet
I only like to say

00:44:41.950 --> 00:44:44.408
the Greek name of because it
comes out wrong when I say it.

00:44:44.408 --> 00:44:45.550
It's Ouranos.

00:44:45.550 --> 00:44:48.590
And then Neptune.

00:44:48.590 --> 00:44:51.340
So it went past all of these.

00:44:51.340 --> 00:44:53.200
And now it's about 9
billion miles away.

00:44:53.200 --> 00:44:55.817
It's twice as far away
from the sun as Pluto is.

00:44:55.817 --> 00:44:57.400
But look at the
transmission rate now.

00:44:57.400 --> 00:45:00.320
It's 160 bits per second.

00:45:00.320 --> 00:45:01.480
So it's greatly reduced.

00:45:01.480 --> 00:45:07.720
And the reason is that over
this extended interval,

00:45:07.720 --> 00:45:10.150
the energy per sample
that arrives at Earth

00:45:10.150 --> 00:45:11.530
is just minuscule.

00:45:11.530 --> 00:45:14.850
I mean, it was small enough
to begin with from Jupiter

00:45:14.850 --> 00:45:16.660
and look at what it does now.

00:45:16.660 --> 00:45:19.600
So it's about 1,000
times less in power

00:45:19.600 --> 00:45:23.290
and you've gone down 1,000
times less more or less

00:45:23.290 --> 00:45:26.110
in your signaling
rate because you're

00:45:26.110 --> 00:45:30.640
trying to put that much
more time in the signal.

00:45:30.640 --> 00:45:32.650
So these trade-offs
are driven by trying

00:45:32.650 --> 00:45:37.450
to get the same energy
per bit for a given noise

00:45:37.450 --> 00:45:39.147
to maintain the performance.

00:45:41.770 --> 00:45:43.520
As I was reading
up on this, there

00:45:43.520 --> 00:45:46.875
were little references to
things that went wrong.

00:45:46.875 --> 00:45:48.250
The only a handful
of things that

00:45:48.250 --> 00:45:50.333
are listed as having gone
wrong, but they turn out

00:45:50.333 --> 00:45:53.200
to be related to decoding.

00:45:53.200 --> 00:45:57.610
So there was a command that was
incorrectly decoded and kept

00:45:57.610 --> 00:46:01.450
some heaters on for very long
and caused some malfunction.

00:46:01.450 --> 00:46:03.680
Here was a flipped bit.

00:46:03.680 --> 00:46:04.960
This is one of only--

00:46:04.960 --> 00:46:06.970
these are a few of
only a small list

00:46:06.970 --> 00:46:10.210
of things that are listed
as having gone wrong.

00:46:10.210 --> 00:46:14.530
But a flipped bit
here caused a problem.

00:46:14.530 --> 00:46:17.230
You've got very few bits in
these computers to begin with.

00:46:17.230 --> 00:46:19.040
Remember the numbers
we had last time.

00:46:19.040 --> 00:46:22.780
So a flipped bit
can cause trouble.

00:46:22.780 --> 00:46:24.160
OK.

00:46:24.160 --> 00:46:26.245
Let's do one last piece here.

00:46:29.380 --> 00:46:33.040
We're going to try and be
even less conservative.

00:46:33.040 --> 00:46:45.570
So suppose I know
that when a 1 is sent,

00:46:45.570 --> 00:46:48.610
what I receive is a waveform
of a particular type.

00:46:48.610 --> 00:46:52.140
So the piece of the response
corresponding to this

00:46:52.140 --> 00:46:53.400
has some particular shape.

00:46:53.400 --> 00:46:54.270
Suppose I know that.

00:46:58.860 --> 00:46:59.360
OK.

00:46:59.360 --> 00:47:00.760
So nothing is constant here.

00:47:00.760 --> 00:47:04.690
This is the actual
y of n sequence.

00:47:04.690 --> 00:47:06.340
And then to this,
I'm adding noise.

00:47:11.867 --> 00:47:12.700
So here's the thing.

00:47:12.700 --> 00:47:17.650
I've got a yn which is no longer
just a constant plus noise,

00:47:17.650 --> 00:47:20.250
it's some known
profile plus noise.

00:47:20.250 --> 00:47:21.825
That known profile
is actually what

00:47:21.825 --> 00:47:23.200
the xn is going
to look like when

00:47:23.200 --> 00:47:24.550
it goes through the channel.

00:47:24.550 --> 00:47:26.350
I should perhaps have
called it y0 of n,

00:47:26.350 --> 00:47:29.680
but let's stick to x0 of n.

00:47:29.680 --> 00:47:33.730
So x0 of n is known,
and we've got the noise.

00:47:33.730 --> 00:47:37.510
The question is, do you want
to just be averaging or do

00:47:37.510 --> 00:47:40.110
you want to try something else?

00:47:40.110 --> 00:47:44.620
If I've got this kind
of signal received

00:47:44.620 --> 00:47:48.858
and I've got the same amount
of noise added to each sample,

00:47:48.858 --> 00:47:50.650
which of these samples
is more trustworthy?

00:47:50.650 --> 00:47:52.317
Which sample do you
want to weight more?

00:47:55.850 --> 00:47:59.660
I've got some amount
of noise adding

00:47:59.660 --> 00:48:01.460
into all of these
samples, so there's

00:48:01.460 --> 00:48:04.520
some standard deviations'
worth on each of these.

00:48:07.890 --> 00:48:11.600
Which is the most
trustworthy sample here?

00:48:11.600 --> 00:48:12.318
Yeah?

00:48:12.318 --> 00:48:13.610
AUDIENCE: The one on the right?

00:48:13.610 --> 00:48:14.030
PROFESSOR: Yeah.

00:48:14.030 --> 00:48:15.290
It's the one on the
right because it's

00:48:15.290 --> 00:48:16.400
got the largest amplitude.

00:48:16.400 --> 00:48:19.170
By itself it has the largest
signal-to-noise ratio.

00:48:19.170 --> 00:48:21.260
So if you're going to
combine these samples,

00:48:21.260 --> 00:48:22.718
you would think
that you would want

00:48:22.718 --> 00:48:25.920
to put more weight on the
sample that was larger.

00:48:25.920 --> 00:48:28.020
So you can actually
formulate that analytically.

00:48:28.020 --> 00:48:30.860
So we're going to combine
the received samples

00:48:30.860 --> 00:48:33.115
with some set of weights an.

00:48:33.115 --> 00:48:35.240
Here's what it's going to
do on the right-hand side

00:48:35.240 --> 00:48:37.290
of that equation.

00:48:37.290 --> 00:48:39.540
Again, when you take
a weighted combination

00:48:39.540 --> 00:48:44.370
of zero mean Gaussians, as
you get a zero mean Gaussian.

00:48:44.370 --> 00:48:48.180
So all you need to know is
what's the variance of a scaled

00:48:48.180 --> 00:48:49.650
Gaussian?

00:48:49.650 --> 00:48:50.370
So let's see.

00:48:50.370 --> 00:48:58.680
If I have a wn having
variance sigma squared,

00:48:58.680 --> 00:49:01.110
what do you think is the
variance of 3 times wn?

00:49:08.930 --> 00:49:11.660
3 times wn means the
excursions are scaled by 3,

00:49:11.660 --> 00:49:13.810
so what's the variance?

00:49:13.810 --> 00:49:14.310
9.

00:49:20.080 --> 00:49:23.260
So scaling by a
particular number

00:49:23.260 --> 00:49:26.510
scales the variance by
the square of that number.

00:49:26.510 --> 00:49:28.660
So the Gaussian
you're adding in here

00:49:28.660 --> 00:49:32.050
has a variance which
is sigma squared

00:49:32.050 --> 00:49:34.855
times the sum of the W squared.

00:49:34.855 --> 00:49:35.355
Sorry.

00:49:35.355 --> 00:49:38.320
The sigma squared times
to sum of the A squareds.

00:49:38.320 --> 00:49:40.840
That's what the variance
of the Gaussian is.

00:49:40.840 --> 00:49:44.950
So you can actually write a very
simple optimization problem.

00:49:44.950 --> 00:49:49.130
What choice of weights maximizes
the signal-to-noise ratio?

00:49:49.130 --> 00:49:50.890
And you discover,
indeed, exactly

00:49:50.890 --> 00:49:55.680
that you're going to put the
largest weight on the largest

00:49:55.680 --> 00:49:57.510
sample.

00:49:57.510 --> 00:50:00.250
And when you do that, the
resulting signal-to-noise ratio

00:50:00.250 --> 00:50:04.540
is, again, energy of the
signal that was transmitted

00:50:04.540 --> 00:50:05.780
divided by the variance.

00:50:05.780 --> 00:50:09.580
So if you do the optimum
processing with this so-called

00:50:09.580 --> 00:50:13.630
matched filtering, you're
going to get to energy

00:50:13.630 --> 00:50:14.800
of the sample--

00:50:14.800 --> 00:50:17.680
sorry, energy of the
bit over the noise

00:50:17.680 --> 00:50:19.590
variance governing
the performance.

00:50:19.590 --> 00:50:22.420
So it's the bit energy
over the noise variance

00:50:22.420 --> 00:50:24.220
that's going to
determine performance

00:50:24.220 --> 00:50:27.700
provided you milk that bit
slot for everything it's

00:50:27.700 --> 00:50:29.660
worth by doing the
match filtering.

00:50:29.660 --> 00:50:30.160
OK.

00:50:30.160 --> 00:50:32.430
We'll leave it at
that for today.