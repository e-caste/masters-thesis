WEBVTT

00:00:00.000 --> 00:00:01.992
[SQUEAKING]

00:00:01.992 --> 00:00:03.984
[RUSTLING]

00:00:03.984 --> 00:00:05.976
[CLICKING]

00:00:11.200 --> 00:00:14.750
PROFESSOR: So let me
very briefly recap

00:00:14.750 --> 00:00:19.770
what we discussed last time.

00:00:19.770 --> 00:00:23.820
So we started off thinking
about choices and risk.

00:00:23.820 --> 00:00:27.500
How do people think
about economic behavior

00:00:27.500 --> 00:00:31.290
when things are uncertain,
when there's risk involved?

00:00:31.290 --> 00:00:33.380
I sort of discussed
and showed you

00:00:33.380 --> 00:00:37.220
sort of the main workhorse
model that economists

00:00:37.220 --> 00:00:38.120
use to study risk.

00:00:38.120 --> 00:00:40.340
That's sort of the
expected utility.

00:00:40.340 --> 00:00:44.000
That is a very commonly
extremely useful

00:00:44.000 --> 00:00:45.530
model for many situations.

00:00:45.530 --> 00:00:47.270
So it's like very widely used.

00:00:47.270 --> 00:00:50.250
If you ask 100
economists randomly

00:00:50.250 --> 00:00:52.250
which model explains
choices [INAUDIBLE]

00:00:52.250 --> 00:00:54.110
the majority will
surely tell you expected

00:00:54.110 --> 00:00:55.850
utility is what you should use.

00:00:55.850 --> 00:00:59.000
It explains a wide range of
phenomena in very useful ways.

00:00:59.000 --> 00:01:01.040
You can think about lots
of different things.

00:01:01.040 --> 00:01:03.380
You can, for example, think
about investment behavior,

00:01:03.380 --> 00:01:04.920
finance.

00:01:04.920 --> 00:01:08.150
When you think about what
should you invest in,

00:01:08.150 --> 00:01:14.060
if things become more risky,
if assets are more volatile,

00:01:14.060 --> 00:01:16.363
you need to have a
higher return for that,

00:01:16.363 --> 00:01:17.780
or you need to be
offered a higher

00:01:17.780 --> 00:01:20.760
return to invest in those
assets and so on and so forth.

00:01:20.760 --> 00:01:23.180
There's lots of useful
applications in finance

00:01:23.180 --> 00:01:24.725
using expected utility.

00:01:24.725 --> 00:01:26.600
You can think of a range
of different issues.

00:01:26.600 --> 00:01:28.880
You can think about, for
example, criminal behavior,

00:01:28.880 --> 00:01:30.590
about sort of the risk
of getting caught.

00:01:30.590 --> 00:01:32.600
What happens when the
risk of caught goes up?

00:01:32.600 --> 00:01:35.282
People engage in less crime
and so on and so forth.

00:01:35.282 --> 00:01:36.740
There's lots of
different behaviors

00:01:36.740 --> 00:01:39.780
that you can think about and
explain using expected utility.

00:01:39.780 --> 00:01:43.250
So I do want you to sort of take
away the expected utility model

00:01:43.250 --> 00:01:46.075
is a very useful model
for various applications.

00:01:46.075 --> 00:01:47.450
What we're trying
to do is trying

00:01:47.450 --> 00:01:50.660
to understand are there some
applications for which perhaps

00:01:50.660 --> 00:01:54.530
the expected utility model
has some limitation, perhaps

00:01:54.530 --> 00:01:57.080
because of its simplicity
or parsimony because there's

00:01:57.080 --> 00:01:58.730
only one parameter in there?

00:01:58.730 --> 00:02:01.130
Can we sort of alter
that in some ways

00:02:01.130 --> 00:02:05.790
and try to make it more
realistic in some situations?

00:02:05.790 --> 00:02:07.815
So the key parameter
of interest when

00:02:07.815 --> 00:02:09.440
you try to sort of
estimate this model,

00:02:09.440 --> 00:02:12.035
trying to sort of match
the data in some ways,

00:02:12.035 --> 00:02:13.910
the parameter that you'd
estimate here is you

00:02:13.910 --> 00:02:15.950
need to sort of assume
some functional form.

00:02:15.950 --> 00:02:18.420
This is what I did last time.

00:02:18.420 --> 00:02:19.460
I can show you this.

00:02:23.650 --> 00:02:26.850
So this here, one very
commonly functional form

00:02:26.850 --> 00:02:30.120
is the CRRA utility function
that's very widely used

00:02:30.120 --> 00:02:32.530
in a lot of range of settings.

00:02:32.530 --> 00:02:35.460
The feature of that is it
has a constant relative risk

00:02:35.460 --> 00:02:38.640
aversion, and that has a
bunch of useful properties

00:02:38.640 --> 00:02:41.423
for estimating things
or making predictions.

00:02:41.423 --> 00:02:42.840
So what you then
need to do is you

00:02:42.840 --> 00:02:45.220
try to sort of estimate
somebody's risk preferences.

00:02:45.220 --> 00:02:47.508
How do people behave under risk?

00:02:47.508 --> 00:02:49.800
What you need to do then sort
of assume some functional

00:02:49.800 --> 00:02:52.052
form, for example, this
CRRA utility function.

00:02:52.052 --> 00:02:53.760
And then the question
is kind of like how

00:02:53.760 --> 00:02:56.910
do we estimate gamma the
risk aversion parameter?

00:02:56.910 --> 00:02:59.280
That's the key
parameter in this model.

00:02:59.280 --> 00:03:02.338
Now, how do you do that?

00:03:02.338 --> 00:03:04.380
I showed you some different
choices [INAUDIBLE]..

00:03:04.380 --> 00:03:05.430
Essentially we
reveal preference.

00:03:05.430 --> 00:03:07.305
Economists believe when
we reveal preference,

00:03:07.305 --> 00:03:09.160
I give you some choices
that involve risk.

00:03:09.160 --> 00:03:10.618
And depending what
you choose, that

00:03:10.618 --> 00:03:12.390
reveals what your gamma is.

00:03:12.390 --> 00:03:14.592
So you can do like
small scale gambles,

00:03:14.592 --> 00:03:17.050
which is just like small choices
between different options.

00:03:17.050 --> 00:03:18.790
Some entail more
risks than others,

00:03:18.790 --> 00:03:20.165
and then you can
essentially just

00:03:20.165 --> 00:03:22.440
sort of estimate
using those choices,

00:03:22.440 --> 00:03:25.740
or people certainty
equivalent for such choices,

00:03:25.740 --> 00:03:27.270
what's people's gamma is.

00:03:27.270 --> 00:03:29.820
Now, what we found is that when
you have small-scale gambles,

00:03:29.820 --> 00:03:32.100
people look or appear
very risk-averse.

00:03:32.100 --> 00:03:39.917
They'll often decline gambles
with positive expected value,

00:03:39.917 --> 00:03:42.000
which makes them appear
quite risk-averse once you

00:03:42.000 --> 00:03:43.830
sort of estimate
this parameter gamma.

00:03:43.830 --> 00:03:47.580
Gamma looks like gamma is
above 10, above 20, above 30,

00:03:47.580 --> 00:03:48.990
really, really high.

00:03:48.990 --> 00:03:52.510
Now, at the same time, you
can look at large-scale risk.

00:03:52.510 --> 00:03:55.278
There, when you look
at large-scale choices,

00:03:55.278 --> 00:03:57.570
when you sort of think about
what's a reasonable gamma,

00:03:57.570 --> 00:04:00.780
people actually only appear
moderately risk-averse.

00:04:00.780 --> 00:04:03.430
It doesn't look like they're
particularly risk-averse.

00:04:03.430 --> 00:04:05.940
When you sort of think for
those large-scale choices

00:04:05.940 --> 00:04:08.610
and we look at finance or
housing or other applications

00:04:08.610 --> 00:04:11.010
that people have
estimated such models,

00:04:11.010 --> 00:04:15.030
you get like gamma sort of
between 0 and 2 roughly.

00:04:15.030 --> 00:04:17.970
Now, what that then
implies is if your gamma is

00:04:17.970 --> 00:04:21.240
between 0 and 2, that
means essentially

00:04:21.240 --> 00:04:23.083
for small-scale
gambles you should

00:04:23.083 --> 00:04:24.250
be essentially risk neutral.

00:04:24.250 --> 00:04:26.083
You should not care
about really small risks

00:04:26.083 --> 00:04:28.443
that are about a dollar or two.

00:04:28.443 --> 00:04:30.360
So that sort of poses a
problem because now we

00:04:30.360 --> 00:04:32.220
have sort of two
contradicting answers.

00:04:32.220 --> 00:04:34.950
We have for small-scale
risk, it looks like people

00:04:34.950 --> 00:04:36.180
are really risk-averse.

00:04:36.180 --> 00:04:38.550
For a large-scale gamble it
looks like people are not

00:04:38.550 --> 00:04:39.570
so risk-averse.

00:04:39.570 --> 00:04:41.130
Now, we only have
like one parameter,

00:04:41.130 --> 00:04:43.290
is this gamma, which is
coming from the concavity

00:04:43.290 --> 00:04:44.727
of the utility function.

00:04:44.727 --> 00:04:46.560
And when we only have
one parameter and sort

00:04:46.560 --> 00:04:48.510
of two contradicting
pieces of evidence,

00:04:48.510 --> 00:04:50.280
we can't sort of
match both, right?

00:04:50.280 --> 00:04:52.440
Because if you match
one, then essentially

00:04:52.440 --> 00:04:55.240
you can match the
other and vice versa.

00:04:55.240 --> 00:04:59.142
Now, I showed you a little
bit Matthew Rabin's--

00:04:59.142 --> 00:05:01.475
what he calls the calibration
theorem, which essentially

00:05:01.475 --> 00:05:04.590
is sort of calibrating, showing
in a fairly compelling way

00:05:04.590 --> 00:05:09.030
that, in fact, you
can formally show it's

00:05:09.030 --> 00:05:11.220
not about sort of assumptions
of a specific utility

00:05:11.220 --> 00:05:12.360
function or the like.

00:05:12.360 --> 00:05:15.930
For these very sort of
minimal assumptions, which

00:05:15.930 --> 00:05:20.640
is just the utility
function is weakly concave,

00:05:20.640 --> 00:05:24.390
you can essentially show that
declining small-scale gambles

00:05:24.390 --> 00:05:28.140
with positive
expected value implies

00:05:28.140 --> 00:05:35.767
that people make absurd choices
when the gambles become larger.

00:05:35.767 --> 00:05:37.350
The recitation will
discuss this a bit

00:05:37.350 --> 00:05:40.320
in more detail and sort
of walking you exactly,

00:05:40.320 --> 00:05:43.870
in somewhat slower speeds,
through the specific example.

00:05:43.870 --> 00:05:46.080
Now then, where we
started then last time

00:05:46.080 --> 00:05:48.330
was thinking about
insurance choices.

00:05:48.330 --> 00:05:52.320
This is a very nice
paper by Justin Sydnor,

00:05:52.320 --> 00:05:55.770
and one very nice
feature of this paper

00:05:55.770 --> 00:05:57.420
is that it involves
real-world choices.

00:05:57.420 --> 00:05:59.250
So it's not like
some lab experiments

00:05:59.250 --> 00:06:00.840
with some experiments.

00:06:00.840 --> 00:06:03.365
Of course, some people care
a lot about undergrads.

00:06:03.365 --> 00:06:04.740
Some people might
say, well, what

00:06:04.740 --> 00:06:06.210
are these undergrads
choosing anyway?

00:06:06.210 --> 00:06:08.293
What does this have to do
with real-world choices?

00:06:08.293 --> 00:06:10.440
I think undergrads are great.

00:06:10.440 --> 00:06:13.140
But one might wonder,
like, if you recruit people

00:06:13.140 --> 00:06:16.200
into some experiments and you
see some choices, like what

00:06:16.200 --> 00:06:17.580
do these choices really reveal?

00:06:17.580 --> 00:06:20.190
Like, do we really find that
these choices are predictive

00:06:20.190 --> 00:06:21.498
of real-world behaviors?

00:06:21.498 --> 00:06:23.040
So one answer to
that is, well, let's

00:06:23.040 --> 00:06:24.550
find some data from
the real world.

00:06:24.550 --> 00:06:26.133
Let's look at real
choices that people

00:06:26.133 --> 00:06:27.600
have made in
real-world settings,

00:06:27.600 --> 00:06:30.480
and this is exactly
what Sydnor does.

00:06:30.480 --> 00:06:34.380
So what he does is he has this
data set of from a large home

00:06:34.380 --> 00:06:35.220
insurance provider.

00:06:35.220 --> 00:06:37.290
This is sort of 50,000
standard policies

00:06:37.290 --> 00:06:39.210
that are sort of like
representative of what

00:06:39.210 --> 00:06:41.490
people choose overall.

00:06:41.490 --> 00:06:45.180
And the key outcome of
interest in this study

00:06:45.180 --> 00:06:47.257
is like people's
deductible choices.

00:06:47.257 --> 00:06:48.090
What's a deductible?

00:06:48.090 --> 00:06:50.010
Again, these are expenses
paid out-of-pocket

00:06:50.010 --> 00:06:52.900
before the insurer
pays any expenses.

00:06:52.900 --> 00:06:55.180
So you have like a
deductibles of $500,

00:06:55.180 --> 00:06:58.413
you have a damage
of like $200, you

00:06:58.413 --> 00:06:59.580
have to pay it all yourself.

00:06:59.580 --> 00:07:02.160
If you have a damage of
$1,000, you pay the 500

00:07:02.160 --> 00:07:05.080
and then the insurer
pays the rest.

00:07:05.080 --> 00:07:07.500
And so what he has
is he has choices

00:07:07.500 --> 00:07:14.490
of a menu of four deductibles
for each customer or client.

00:07:14.490 --> 00:07:16.470
So you can see both
people's choice sets,

00:07:16.470 --> 00:07:18.570
and you can see people's
preferred options.

00:07:18.570 --> 00:07:20.320
And that allows them
to sort of say, well,

00:07:20.320 --> 00:07:23.310
if you have four options,
you picked one of them.

00:07:23.310 --> 00:07:26.740
That means you preferred that
one over all three others.

00:07:26.740 --> 00:07:29.430
So we can sort of
essentially put some bounds

00:07:29.430 --> 00:07:31.470
on people's risk aversion.

00:07:31.470 --> 00:07:33.260
And so we looked
at this already.

00:07:33.260 --> 00:07:35.318
This is kind of what
this roughly looks like.

00:07:35.318 --> 00:07:37.860
There are different deductibles,
which is essentially, again,

00:07:37.860 --> 00:07:42.570
like how much how much
you have to pay yourself

00:07:42.570 --> 00:07:45.342
in case of damage until the
insurance payments kick in.

00:07:45.342 --> 00:07:47.550
You have the premium, which
is like how much for sure

00:07:47.550 --> 00:07:49.200
you have to pay every year.

00:07:49.200 --> 00:07:53.280
And then there is the premium
relative to the $1,000 policy.

00:07:53.280 --> 00:07:54.735
How much more expensive is it?

00:07:54.735 --> 00:07:55.860
That's in the third column.

00:07:55.860 --> 00:07:58.860
How much more expensive is it
to choose a lower deductible

00:07:58.860 --> 00:08:01.170
relative to the $1,000 premium?

00:08:01.170 --> 00:08:03.840
And then we have people's
choices which is, in this case,

00:08:03.840 --> 00:08:08.400
I guess, policyholder one
shows a deductible of $250.

00:08:08.400 --> 00:08:11.100
It was a premium
of $661, which is

00:08:11.100 --> 00:08:17.430
$157 more expensive than the
$1,000 deductible option.

00:08:17.430 --> 00:08:18.180
OK?

00:08:18.180 --> 00:08:22.710
And for each
policyholder, the company

00:08:22.710 --> 00:08:26.730
was, in fact, sort of
providing individual prices.

00:08:26.730 --> 00:08:29.530
So essentially they
were looking at where

00:08:29.530 --> 00:08:32.669
do they live, what's the housing
value, and so on and so forth.

00:08:32.669 --> 00:08:33.900
Sydnor knows all of that.

00:08:33.900 --> 00:08:35.490
So he knows the
full set of options

00:08:35.490 --> 00:08:37.950
that people had available
and their actual choices,

00:08:37.950 --> 00:08:40.200
and the options
available vary by person.

00:08:44.133 --> 00:08:45.800
How do we learn now
about risk aversion?

00:08:45.800 --> 00:08:47.810
Well, so the losses
to the customers

00:08:47.810 --> 00:08:49.610
are capped by the
deductible, right?

00:08:49.610 --> 00:08:52.910
So any loss you have from
any damage that you get,

00:08:52.910 --> 00:08:56.400
the losses are only
up to the deductible.

00:08:56.400 --> 00:08:58.550
So if you have a
deductible of $500,

00:08:58.550 --> 00:09:01.670
the most you can lose or
have to pay in any case

00:09:01.670 --> 00:09:04.800
if any loss occurs is $500.

00:09:04.800 --> 00:09:06.410
So choosing a lower
deductible then,

00:09:06.410 --> 00:09:08.810
what it does it amounts
to essentially reducing

00:09:08.810 --> 00:09:10.620
that loss in case
you have a damage.

00:09:10.620 --> 00:09:13.370
So if you have a
deductible of $500

00:09:13.370 --> 00:09:17.035
and decide to instead choose
a deductible of $250, that

00:09:17.035 --> 00:09:18.410
means essentially
in case there's

00:09:18.410 --> 00:09:21.050
a damage, in case you
have to pay something,

00:09:21.050 --> 00:09:22.490
you don't have to pay 500.

00:09:22.490 --> 00:09:25.490
You have to only pay 250.

00:09:25.490 --> 00:09:28.080
But of course, if you
lower your deductible,

00:09:28.080 --> 00:09:32.270
the price of your insurance
goes up, the premium goes up,

00:09:32.270 --> 00:09:34.703
and the premium you
have to pay for sure.

00:09:34.703 --> 00:09:36.370
So the way you can
think about this then

00:09:36.370 --> 00:09:38.750
is like, if you choose a
lower deductible, for sure

00:09:38.750 --> 00:09:40.580
you have to pay more money.

00:09:40.580 --> 00:09:42.920
But in case there's
some damage to you

00:09:42.920 --> 00:09:45.775
with some probability
that happens,

00:09:45.775 --> 00:09:47.150
if you have some
claims, you have

00:09:47.150 --> 00:09:50.880
to like pay less because
your deductible is now lower.

00:09:50.880 --> 00:09:52.268
OK?

00:09:52.268 --> 00:09:53.810
So now what info do
we actually need?

00:09:53.810 --> 00:09:55.542
We need the available
deductibles.

00:09:55.542 --> 00:09:58.000
Like, essentially what are the
deductibles for each choice?

00:09:58.000 --> 00:10:00.050
We need the premium
for each option.

00:10:00.050 --> 00:10:03.740
We need the claim probabilities
and people's wealth levels

00:10:03.740 --> 00:10:05.610
because you have a
utility function where

00:10:05.610 --> 00:10:06.610
there's wealth in there.

00:10:06.610 --> 00:10:08.550
I'll talk about
this in one second.

00:10:08.550 --> 00:10:09.520
Any questions so far?

00:10:16.160 --> 00:10:16.940
OK.

00:10:16.940 --> 00:10:19.400
So now one important
feature in this I think

00:10:19.400 --> 00:10:21.830
was asked like last
time about like, well,

00:10:21.830 --> 00:10:23.060
what about the claim rates?

00:10:23.060 --> 00:10:24.935
Well if the claim
rates are really high

00:10:24.935 --> 00:10:26.810
or if people think the
claim rates are really

00:10:26.810 --> 00:10:29.630
high, in some sense then,
having very low deductibles

00:10:29.630 --> 00:10:31.550
makes a lot of
sense because then--

00:10:31.550 --> 00:10:33.950
and very often it happens
that you have to pay.

00:10:33.950 --> 00:10:38.930
Then it makes lots of sense to
have like lower deductibles.

00:10:38.930 --> 00:10:41.960
But it turns out claims
rates are actually very low.

00:10:41.960 --> 00:10:44.173
So you can see overall--

00:10:44.173 --> 00:10:45.590
this is like the
full sample, this

00:10:45.590 --> 00:10:47.962
is everybody-- people's
claim rates is 4.2%.

00:10:47.962 --> 00:10:49.170
These are yearly claim rates.

00:10:49.170 --> 00:10:52.850
This is like out of 100
customers, 4.2 per year

00:10:52.850 --> 00:10:55.640
actually claim any damage.

00:10:55.640 --> 00:10:59.490
And then it varies a little bit
also by choice of deductible.

00:10:59.490 --> 00:11:01.340
So there's the
people who happened

00:11:01.340 --> 00:11:05.880
to choose in the end like
$1,000, $500, $250, and $100.

00:11:05.880 --> 00:11:07.880
But for each of them
essentially, the claim rate

00:11:07.880 --> 00:11:09.290
is below 5%.

00:11:09.290 --> 00:11:11.120
So it's very low.

00:11:11.120 --> 00:11:12.530
OK?

00:11:12.530 --> 00:11:16.070
The second factor from this data
is that reducing the deductible

00:11:16.070 --> 00:11:17.460
is very expensive.

00:11:17.460 --> 00:11:21.620
So for example, this is
the full sample again.

00:11:21.620 --> 00:11:24.860
On average, purchasing
the insurance

00:11:24.860 --> 00:11:28.760
where the deductible
of $1,000 cost $615,

00:11:28.760 --> 00:11:31.310
we can't say very
much about that choice

00:11:31.310 --> 00:11:33.882
because who knows how much the
actual damages are and so on.

00:11:33.882 --> 00:11:35.840
In some sense, that's
sort of irrelevant for us

00:11:35.840 --> 00:11:36.950
what that number is.

00:11:36.950 --> 00:11:38.840
What we're interested
in is like what

00:11:38.840 --> 00:11:41.270
are the differences in costs
of different deductibles?

00:11:41.270 --> 00:11:43.670
How much do you have to pay
to lower your deductible

00:11:43.670 --> 00:11:46.320
to like $500, $250, and so on?

00:11:46.320 --> 00:11:48.710
Now what you see here
is like on average,

00:11:48.710 --> 00:11:51.840
reducing the deductible
from $1,000 to $500,

00:11:51.840 --> 00:11:54.290
which is sort of what this
column shows that's shown

00:11:54.290 --> 00:11:59.720
in red, costs $999.91.

00:11:59.720 --> 00:12:02.840
So if you choose then
like $500, is this

00:12:02.840 --> 00:12:04.220
is a risk-averse choice or not?

00:12:04.220 --> 00:12:07.200
How do we think about that?

00:12:07.200 --> 00:12:09.165
Suppose your claim
rate is like, say, 5%.

00:12:17.430 --> 00:12:19.120
Yes.

00:12:19.120 --> 00:12:22.030
AUDIENCE: Well, I think it
would be a risk-averse decision

00:12:22.030 --> 00:12:25.500
because you're paying $100
more and your deductible

00:12:25.500 --> 00:12:27.965
has gone down by $500.

00:12:27.965 --> 00:12:31.356
So claim rate for that back
of the envelope calculation

00:12:31.356 --> 00:12:33.628
would need to be about 20.

00:12:33.628 --> 00:12:34.420
PROFESSOR: Exactly.

00:12:34.420 --> 00:12:37.000
So what you're saying is
you're reducing the deductible

00:12:37.000 --> 00:12:39.880
from $1,000 to $500.

00:12:39.880 --> 00:12:44.260
Now, if you think that happens
with a 5% chance, on average

00:12:44.260 --> 00:12:47.600
you're going to reduce
your payments by $25.

00:12:47.600 --> 00:12:54.310
So 5% times $500,
which is a $25.

00:12:54.310 --> 00:12:57.260
But people are willing to
pay about $100 for that.

00:12:57.260 --> 00:13:01.930
So for sure they're paying $100,
and the benefit that they get

00:13:01.930 --> 00:13:04.450
is with 5% chance, at
least the average customer,

00:13:04.450 --> 00:13:09.920
with 5% chance they're going
to pay $500 less in case

00:13:09.920 --> 00:13:10.960
there's some damage.

00:13:10.960 --> 00:13:13.323
That looks already
pretty risk-averse.

00:13:13.323 --> 00:13:14.740
Because as Ben
says, surely you're

00:13:14.740 --> 00:13:16.948
not risk-neutral, because
then you would not do that.

00:13:16.948 --> 00:13:19.000
You would choose the $1,000.

00:13:19.000 --> 00:13:21.160
It looks fairly risk-averse.

00:13:21.160 --> 00:13:26.410
Now if you go down then, if you
go to like from $250 to $200,

00:13:26.410 --> 00:13:30.840
there's an additional $133.22.

00:13:30.840 --> 00:13:35.100
So that's to say reducing your
deductible by another $150,

00:13:35.100 --> 00:13:39.930
from $250 to $100, makes you
for sure you have to pay $133.

00:13:39.930 --> 00:13:42.990
And now if your chance is like
5% of getting like essentially

00:13:42.990 --> 00:13:47.430
a damage, that is for a
5% chance of saving $150,

00:13:47.430 --> 00:13:51.200
people are willing
to pay $133 for sure.

00:13:51.200 --> 00:13:51.900
OK?

00:13:51.900 --> 00:13:54.090
So now if you try to calibrate
this, what we already

00:13:54.090 --> 00:13:55.830
know from this,
the simple example

00:13:55.830 --> 00:13:58.020
is that people look
extremely risk-averse.

00:13:58.020 --> 00:13:58.980
OK?

00:13:58.980 --> 00:14:05.182
So that's kind of like the
exercise that Sydnor is doing,

00:14:05.182 --> 00:14:07.640
just saying, look, let's take
these choices very seriously.

00:14:07.640 --> 00:14:09.580
Let's look what people have
done in real-world situations.

00:14:09.580 --> 00:14:10.997
These are repeat
customers, people

00:14:10.997 --> 00:14:13.080
who have done this for
a long time and so on.

00:14:13.080 --> 00:14:14.320
What are people choosing?

00:14:14.320 --> 00:14:19.550
And if you sort of assume
expected utility, what would

00:14:19.550 --> 00:14:21.180
people's gamma need
to look like to be

00:14:21.180 --> 00:14:23.460
able to explain this data?

00:14:23.460 --> 00:14:26.670
And we can do this customer--
this is like the average rates.

00:14:26.670 --> 00:14:29.780
They can do this sort
of customer by customer.

00:14:29.780 --> 00:14:33.240
Now, what he then finds
is the majority of people

00:14:33.240 --> 00:14:34.950
choose small deductibles.

00:14:34.950 --> 00:14:39.090
Lots of people
choose $250, $500.

00:14:39.090 --> 00:14:42.990
Very few people choose
$1,000, even among people,

00:14:42.990 --> 00:14:45.720
and this is on the x-axis,
who have been at the company

00:14:45.720 --> 00:14:47.493
for 15-plus years.

00:14:47.493 --> 00:14:49.410
You would say like the
first time you do this,

00:14:49.410 --> 00:14:50.970
maybe you don't understand
your claim rate,

00:14:50.970 --> 00:14:53.100
you don't understand what's
going on or whatever.

00:14:53.100 --> 00:14:56.400
But there's people who have been
at this company for 15 years.

00:14:56.400 --> 00:14:58.170
They should kind of
know at some point

00:14:58.170 --> 00:15:02.830
that claim rates are pretty
low, at least on average.

00:15:02.830 --> 00:15:04.950
And so like if you have
15 years at this company,

00:15:04.950 --> 00:15:08.280
it's hard to believe that you'd
still think that your claim

00:15:08.280 --> 00:15:10.260
rate is, say, about
10% or the like

00:15:10.260 --> 00:15:13.470
since it just doesn't
happen very often.

00:15:13.470 --> 00:15:16.160
OK.

00:15:16.160 --> 00:15:19.310
Now, how do we think about
people choosing a deductible?

00:15:19.310 --> 00:15:22.250
Again, what you need is like
the following parameter.

00:15:22.250 --> 00:15:24.320
You need to have
the yearly premium.

00:15:24.320 --> 00:15:27.350
You need to have the
deductible D. You're

00:15:27.350 --> 00:15:29.483
assuming no other risks
to lifetime wealth, which

00:15:29.483 --> 00:15:31.400
is a bit of a distraction,
but essentially you

00:15:31.400 --> 00:15:35.270
can diversify risk and
so on and so forth.

00:15:35.270 --> 00:15:38.427
You also can assume that's
at most one risk per year.

00:15:38.427 --> 00:15:40.010
This is, again, sort
of simplification

00:15:40.010 --> 00:15:44.900
and doesn't really matter very
much for the probability pi.

00:15:44.900 --> 00:15:48.110
And then for now
at least, we assume

00:15:48.110 --> 00:15:52.150
accurate, subjective beliefs
about the likelihood of a loss.

00:15:52.150 --> 00:15:56.200
Now then, what is then the
indirect utility function

00:15:56.200 --> 00:15:56.700
of wealth?

00:15:56.700 --> 00:15:58.158
What does the
utility function look

00:15:58.158 --> 00:16:00.290
like depending on
these parameters?

00:16:00.290 --> 00:16:02.290
Can somebody explain this
what I'm showing here?

00:16:02.290 --> 00:16:08.010
What is this equation?

00:16:08.010 --> 00:16:09.324
Yes.

00:16:09.324 --> 00:16:13.116
AUDIENCE: The first part
says pi [INAUDIBLE] w

00:16:13.116 --> 00:16:15.380
minus P minus D. The
w minus P minus D

00:16:15.380 --> 00:16:18.290
is your wealth if
something happens.

00:16:18.290 --> 00:16:19.270
[INAUDIBLE] pi.

00:16:19.270 --> 00:16:23.200
And the 1 minus pi is on
your utility of your wealth

00:16:23.200 --> 00:16:25.014
if nothing bad happens.

00:16:25.014 --> 00:16:26.448
[INAUDIBLE]

00:16:26.448 --> 00:16:27.240
PROFESSOR: Exactly.

00:16:27.240 --> 00:16:31.260
So for sure, so with probability
1 minus pi, nothing happens.

00:16:31.260 --> 00:16:34.080
You have your wealth
W that you had before.

00:16:34.080 --> 00:16:35.730
You have to pay the
premium for sure.

00:16:35.730 --> 00:16:37.813
So in that case, you also
have to pay the premium.

00:16:37.813 --> 00:16:40.950
So you're going to end up
with W minus P, the premium.

00:16:40.950 --> 00:16:43.380
And then with
probability pi, you also

00:16:43.380 --> 00:16:47.065
have to pay the premium,
which is W minus P,

00:16:47.065 --> 00:16:48.690
but also you have to
pay the deductible

00:16:48.690 --> 00:16:50.920
because some damage occurred.

00:16:50.920 --> 00:16:51.420
All right.

00:16:51.420 --> 00:16:54.600
And then your indirect
utility-- your expected utility

00:16:54.600 --> 00:16:56.718
for that year is
essentially then

00:16:56.718 --> 00:16:58.260
the weighted average
of these things.

00:16:58.260 --> 00:17:01.270
And pi is essentially
the weight on that,

00:17:01.270 --> 00:17:03.880
which is the subjective
or, in this case,

00:17:03.880 --> 00:17:06.910
assumed actual probability
of a damage occurring.

00:17:06.910 --> 00:17:11.670
Now, I sort of said the
indirect utility function,

00:17:11.670 --> 00:17:14.430
utility of wealth
function, what is that?

00:17:14.430 --> 00:17:16.170
What's A, an indirect
utility function?

00:17:16.170 --> 00:17:18.630
And B, why is there wealth
in it and not consumption?

00:17:18.630 --> 00:17:20.510
Usually we think
people eat stuff

00:17:20.510 --> 00:17:23.770
and there should be
like consumption here.

00:17:23.770 --> 00:17:25.020
Why do we have wealth in here?

00:17:25.020 --> 00:17:25.829
What is this?

00:17:35.840 --> 00:17:36.920
Yes.

00:17:36.920 --> 00:17:46.290
AUDIENCE: I think they might
be like assuming [INAUDIBLE]

00:17:46.290 --> 00:17:47.100
PROFESSOR: Exactly.

00:17:47.100 --> 00:17:50.014
What is the indirect
utility function?

00:17:50.014 --> 00:17:51.928
AUDIENCE: [INAUDIBLE]

00:17:51.928 --> 00:17:52.720
PROFESSOR: Exactly.

00:17:52.720 --> 00:17:56.280
So usually you think what
you do is if you go back

00:17:56.280 --> 00:17:59.520
to like 14.01 notes or
what was done in the first

00:17:59.520 --> 00:18:01.470
I think recitation,
usually what you

00:18:01.470 --> 00:18:05.430
do is you maximize
consumption with several goods

00:18:05.430 --> 00:18:07.630
or one good or
whatever over time,

00:18:07.630 --> 00:18:09.300
and usually there's
a budget constraint

00:18:09.300 --> 00:18:11.592
and wealth is usually in your
budget constraint, right?

00:18:11.592 --> 00:18:14.160
You can only consume as much
as how much money you have.

00:18:14.160 --> 00:18:16.050
Could be like your
income or your wealth

00:18:16.050 --> 00:18:17.700
if it's over your lifetime.

00:18:17.700 --> 00:18:19.510
Now when you do that
and maximize it,

00:18:19.510 --> 00:18:20.760
then you end up at an optimum.

00:18:20.760 --> 00:18:23.280
What you can then do is
like essentially express

00:18:23.280 --> 00:18:24.810
the optimum.

00:18:24.810 --> 00:18:26.520
Assuming that you
have chosen optimally,

00:18:26.520 --> 00:18:28.170
your consumption is
saying you already

00:18:28.170 --> 00:18:31.130
chose whether you wanted
apples or bananas or whatever.

00:18:31.130 --> 00:18:34.130
If we assume that you optimize,
I can then essentially just say

00:18:34.130 --> 00:18:36.390
assuming that you're
optimizing, what

00:18:36.390 --> 00:18:39.960
is your optimized utility for
different levels of wealth?

00:18:39.960 --> 00:18:43.140
And usually it's a function
of wealth and prices,

00:18:43.140 --> 00:18:48.120
and that's what the indirect
utility function is.

00:18:48.120 --> 00:18:50.920
We can very briefly also
go over that in recitation.

00:18:50.920 --> 00:18:55.720
But if you go back to
your 1401 or other notes,

00:18:55.720 --> 00:18:57.870
you will see
essentially that it's

00:18:57.870 --> 00:18:59.760
the outcome of a
maximization problem.

00:18:59.760 --> 00:19:01.710
Usually it's like for
two goods or whatever.

00:19:01.710 --> 00:19:03.540
Like, it's like income.

00:19:03.540 --> 00:19:06.980
In this case, it's wealth
because it's like over--

00:19:06.980 --> 00:19:10.320
Yeah, it's wealth, but it
could be available income

00:19:10.320 --> 00:19:11.850
as well if you wanted.

00:19:11.850 --> 00:19:14.280
Now, what the person
is then going to do

00:19:14.280 --> 00:19:19.740
is like each contract gives you
an expected utility in terms

00:19:19.740 --> 00:19:22.500
of how much do you expect your
utility to be if you choose

00:19:22.500 --> 00:19:24.450
that specific contract.

00:19:24.450 --> 00:19:26.220
And now the maximization
problem is now

00:19:26.220 --> 00:19:29.520
you choose the contract J
that maximizes the expected

00:19:29.520 --> 00:19:33.750
utility or the expected
indirect utility as a function

00:19:33.750 --> 00:19:36.180
of these parameters.

00:19:36.180 --> 00:19:38.960
Any questions on this?

00:19:38.960 --> 00:19:41.260
So for each contract,
we can write down what's

00:19:41.260 --> 00:19:43.795
the indirect utility function.

00:19:43.795 --> 00:19:45.273
It depends on people's wealth.

00:19:45.273 --> 00:19:46.690
So we have to make
some assumption

00:19:46.690 --> 00:19:47.982
of how wealthy people are.

00:19:47.982 --> 00:19:49.690
And it depends on
these other parameters.

00:19:49.690 --> 00:19:53.200
It depends on the premium,
it depends on the deductible,

00:19:53.200 --> 00:19:57.970
and it depends on the subjective
probability of a claim

00:19:57.970 --> 00:19:59.200
occurring in that year.

00:19:59.200 --> 00:20:01.158
We assume that there's
only one claim per year.

00:20:05.250 --> 00:20:06.090
OK.

00:20:06.090 --> 00:20:11.040
So now what we can do is then
we can back out the implied risk

00:20:11.040 --> 00:20:14.190
aversion from people's choices.

00:20:14.190 --> 00:20:15.720
And in fact, what
we can do is we

00:20:15.720 --> 00:20:18.150
can get upper and lower bounds
on people's risk aversion

00:20:18.150 --> 00:20:19.275
from what they have chosen.

00:20:19.275 --> 00:20:21.990
Let me sort of give you
an example for that.

00:20:21.990 --> 00:20:26.460
Suppose a person chooses
a $100 deductible.

00:20:26.460 --> 00:20:27.340
What does this mean?

00:20:27.340 --> 00:20:30.630
Well, this means essentially
that he or she preferred

00:20:30.630 --> 00:20:34.650
the $100 deductible over all
the other deductibles that

00:20:34.650 --> 00:20:35.470
were available.

00:20:35.470 --> 00:20:35.970
Right?

00:20:35.970 --> 00:20:37.990
So essentially, if
you choose the 100,

00:20:37.990 --> 00:20:39.180
you get three inequalities.

00:20:39.180 --> 00:20:41.430
You get like the $100
deductible is better

00:20:41.430 --> 00:20:43.890
than the $250
deductible, the $100

00:20:43.890 --> 00:20:46.650
is better than $500
deductible, and $100 is

00:20:46.650 --> 00:20:49.500
better than $1,000 deductibles.

00:20:49.500 --> 00:20:51.870
Now, this gives us a bound
on people's risk aversion.

00:20:51.870 --> 00:20:55.690
Is it the lower or an
upper bound and why?

00:21:15.690 --> 00:21:16.462
Yes.

00:21:16.462 --> 00:21:21.900
AUDIENCE: I think it's a lower
bound because $100 [INAUDIBLE]

00:21:21.900 --> 00:21:29.940
is like the lowest you can
go in a lower deductible

00:21:29.940 --> 00:21:32.470
can cause more risk-aversion.

00:21:32.470 --> 00:21:33.660
PROFESSOR: Right, exactly.

00:21:33.660 --> 00:21:34.952
Sort of like a corner solution.

00:21:34.952 --> 00:21:36.930
So like, if the person
who chooses $100--

00:21:36.930 --> 00:21:39.097
that's the person that I
just showed you previously.

00:21:39.097 --> 00:21:42.370
This is the example
that I showed you here.

00:21:42.370 --> 00:21:43.020
Where was it?

00:21:45.720 --> 00:21:49.170
This is a person who looks
extremely risk-averse, right?

00:21:49.170 --> 00:21:50.790
This is a person
essentially saying

00:21:50.790 --> 00:21:54.390
like, I'm choosing the
lowest possible deductible.

00:21:54.390 --> 00:21:56.640
I'm for sure paying
quite a bit of money

00:21:56.640 --> 00:21:59.850
compared to all these other
options, 4% or 5% chance

00:21:59.850 --> 00:22:02.740
of not having a damage.

00:22:02.740 --> 00:22:05.240
So this person will
look very risk-averse.

00:22:05.240 --> 00:22:06.620
It's the lowest possible option.

00:22:06.620 --> 00:22:10.350
So maybe the person, if there
had been a $50 or zero dollars

00:22:10.350 --> 00:22:12.300
option, would have even
chosen that option.

00:22:12.300 --> 00:22:14.940
We don't know because
that's not available.

00:22:14.940 --> 00:22:18.420
What you can then
do is, however, you

00:22:18.420 --> 00:22:20.190
can just write down
these inequalities.

00:22:20.190 --> 00:22:22.410
And so if you choose
the $100 deductible

00:22:22.410 --> 00:22:24.870
compared to the
$250 deductible, it

00:22:24.870 --> 00:22:27.480
will be the case that
if you solve for gamma,

00:22:27.480 --> 00:22:30.030
this gives you a lower
bound for or gamma.

00:22:30.030 --> 00:22:31.320
So what does that mean?

00:22:31.320 --> 00:22:33.480
We know that gamma
is at least as high

00:22:33.480 --> 00:22:37.650
as the solution of this
inequality will tell us,

00:22:37.650 --> 00:22:40.200
but in fact, their gamma
could be even higher.

00:22:40.200 --> 00:22:43.650
We just don't know it because we
don't have additional choices.

00:22:43.650 --> 00:22:47.200
Now, if you choose the $1,000
deductible on the other hand,

00:22:47.200 --> 00:22:49.830
you'll get like an upper bound.

00:22:49.830 --> 00:22:52.500
The reasoning is
exactly the same.

00:22:52.500 --> 00:22:54.840
Essentially, if you choose
the $1,000 deductible,

00:22:54.840 --> 00:22:56.310
that's like the
riskiest option you

00:22:56.310 --> 00:22:58.620
can choose because
essentially you're

00:22:58.620 --> 00:23:01.590
choosing something
you will choose not

00:23:01.590 --> 00:23:03.450
to reduce your risk in any way.

00:23:03.450 --> 00:23:05.330
So you're not willing
to pay to do that.

00:23:05.330 --> 00:23:08.790
It's kind of like
accepting a gamble

00:23:08.790 --> 00:23:10.208
and not choosing
the safe options.

00:23:10.208 --> 00:23:12.000
But we don't know
whether this person would

00:23:12.000 --> 00:23:15.000
have chosen like $1,000
on one deductible,

00:23:15.000 --> 00:23:17.640
or $2,000, or $5,000,
what deductible would

00:23:17.640 --> 00:23:19.980
have been chosen because
there's no other options

00:23:19.980 --> 00:23:22.540
of deductibles.

00:23:22.540 --> 00:23:24.913
And then in between, we
have essentially lower

00:23:24.913 --> 00:23:26.580
and upper bounds,
because essentially we

00:23:26.580 --> 00:23:28.680
know that if you
choose 500, we know

00:23:28.680 --> 00:23:30.960
that you didn't choose
1,000, and we know also

00:23:30.960 --> 00:23:32.610
that you didn't choose 250.

00:23:32.610 --> 00:23:35.250
So your gamma must be in
between those options whatsoever

00:23:35.250 --> 00:23:38.970
as implied by those two options.

00:23:38.970 --> 00:23:41.550
There's a previous problem
set that we posted that sort

00:23:41.550 --> 00:23:43.200
of walks you through that.

00:23:43.200 --> 00:23:46.890
We'll also go through
that in recitation

00:23:46.890 --> 00:23:50.200
to go through the
mechanics of that.

00:23:50.200 --> 00:23:51.240
Any questions on this?

00:23:54.990 --> 00:23:55.808
Yes.

00:23:55.808 --> 00:23:57.950
AUDIENCE: [INAUDIBLE]

00:24:06.270 --> 00:24:10.590
PROFESSOR: I think that's not
a problem because it's just

00:24:10.590 --> 00:24:12.180
sort of flipped.

00:24:12.180 --> 00:24:14.280
So we have the gamma--

00:24:14.280 --> 00:24:17.350
that's why you have the gamma
in the denominator as well.

00:24:17.350 --> 00:24:19.590
So you get a problem
if your gamma is 1,

00:24:19.590 --> 00:24:21.750
because in dividing
by 1, usually people

00:24:21.750 --> 00:24:24.796
use a log utility for that.

00:24:24.796 --> 00:24:26.260
Yeah.

00:24:26.260 --> 00:24:28.700
AUDIENCE: [INAUDIBLE]

00:24:39.703 --> 00:24:41.370
PROFESSOR: Yes, that's
a great question.

00:24:41.370 --> 00:24:43.440
So I'll get to this in a second.

00:24:43.440 --> 00:24:46.050
So what I have done
right now, and this

00:24:46.050 --> 00:24:48.350
is the typical way economists
think about these things,

00:24:48.350 --> 00:24:50.990
is sort of say, let's take
a model very seriously.

00:24:50.990 --> 00:24:52.760
Here's the model
that I'm using to try

00:24:52.760 --> 00:24:56.210
to explain people's
choices, and I'm essentially

00:24:56.210 --> 00:24:57.920
assuming everything else away.

00:24:57.920 --> 00:25:00.440
I'm essentially assuming
that the person optimizes.

00:25:00.440 --> 00:25:01.310
Those no mistakes.

00:25:01.310 --> 00:25:02.910
There's no framing of facts.

00:25:02.910 --> 00:25:05.570
There's no other stuff going
on, liquidity constraints

00:25:05.570 --> 00:25:06.322
and so on.

00:25:06.322 --> 00:25:07.780
And I'm taking this
very seriously.

00:25:07.780 --> 00:25:09.380
I'm estimating gamma.

00:25:09.380 --> 00:25:11.900
Now, the typical-- and this
is section four of the paper,

00:25:11.900 --> 00:25:12.590
in fact--

00:25:12.590 --> 00:25:14.117
the typical thing
then that happens

00:25:14.117 --> 00:25:15.950
is other people are
going to say like, well,

00:25:15.950 --> 00:25:19.040
what about if people don't
understand what they're doing?

00:25:19.040 --> 00:25:20.850
What about people
misperceiving the risk?

00:25:20.850 --> 00:25:23.840
What about people framing
effects in terms of the way

00:25:23.840 --> 00:25:25.880
you present the choices?

00:25:25.880 --> 00:25:28.130
People like to not choose
extremes, but like to choose

00:25:28.130 --> 00:25:29.240
and the middle.

00:25:29.240 --> 00:25:33.060
Can that explain the results?

00:25:33.060 --> 00:25:35.360
There's some
concerns about that.

00:25:35.360 --> 00:25:37.790
I think one thing that your
explanation, for example,

00:25:37.790 --> 00:25:41.450
would be able perhaps to explain
is people choosing $500 over

00:25:41.450 --> 00:25:42.560
like $1,000.

00:25:42.560 --> 00:25:44.690
It's hard for the
framing effects

00:25:44.690 --> 00:25:47.850
to explain why people
are choosing 250.

00:25:47.850 --> 00:25:51.950
So if you look at this figure
in particular, the people who

00:25:51.950 --> 00:25:55.010
are in the company for
15 years, lots of people

00:25:55.010 --> 00:25:57.770
choose deductibles of 250.

00:25:57.770 --> 00:26:00.170
It's a little bit harder to
explain with framing effects.

00:26:00.170 --> 00:26:04.830
Why wouldn't you choose
like 500 as opposed to 250?

00:26:04.830 --> 00:26:07.395
Sydnor argues that's
really not what's going on.

00:26:07.395 --> 00:26:09.020
I think at the end
of the day, probably

00:26:09.020 --> 00:26:11.420
it's the case that people
do not want these extremes

00:26:11.420 --> 00:26:13.033
and, to some
degree, I think some

00:26:13.033 --> 00:26:14.450
of what's going
on in this perhaps

00:26:14.450 --> 00:26:17.000
at least contributed by some
form of framing effect, maybe

00:26:17.000 --> 00:26:17.510
marketing.

00:26:17.510 --> 00:26:21.660
People who sell the insurance
choices really want people--

00:26:21.660 --> 00:26:24.933
they get paid presumably
if they sell essentially

00:26:24.933 --> 00:26:27.350
these low deductibles because
that's how the company makes

00:26:27.350 --> 00:26:28.930
a lot of money.

00:26:28.930 --> 00:26:30.680
What Sydnor says there
is like, well, it's

00:26:30.680 --> 00:26:33.138
actually hard to sell people
on stuff that they don't like.

00:26:33.138 --> 00:26:35.390
It seems like people really
seem to want these things,

00:26:35.390 --> 00:26:37.973
and maybe some of that is sort
of sales pressure, but probably

00:26:37.973 --> 00:26:38.942
not everything.

00:26:38.942 --> 00:26:40.400
So I think some of
what is going on

00:26:40.400 --> 00:26:44.040
is a little bit hard to rule
out all of those things,

00:26:44.040 --> 00:26:46.943
but if you read the
paper it's reasonable.

00:26:46.943 --> 00:26:48.860
And since it's what I'm
going to show you next

00:26:48.860 --> 00:26:51.590
is like the implied
gamma's like so large

00:26:51.590 --> 00:26:54.050
that even if you sort of
said, OK, half of this effect

00:26:54.050 --> 00:26:56.240
is driven by other things,
you would get still

00:26:56.240 --> 00:27:00.263
like really absurdly large
estimates of risk aversion.

00:27:00.263 --> 00:27:01.430
But that's a great question.

00:27:01.430 --> 00:27:02.195
Yes.

00:27:02.195 --> 00:27:04.520
AUDIENCE: [INAUDIBLE]

00:27:22.160 --> 00:27:24.420
PROFESSOR: Well, to some
degree, in some sense,

00:27:24.420 --> 00:27:26.030
I think the way they sort of--

00:27:26.030 --> 00:27:28.340
The question was like, is
the company deliberately

00:27:28.340 --> 00:27:31.580
giving people choices that leads
them to choose low deductibles?

00:27:31.580 --> 00:27:36.020
And therefore, do we sort of
overestimate people's gamma?

00:27:36.020 --> 00:27:37.460
To some degree,
yes, but it's not

00:27:37.460 --> 00:27:40.460
like just we have low
deductibles available.

00:27:40.460 --> 00:27:42.560
There's a $1,000
option available.

00:27:42.560 --> 00:27:44.090
I think maybe what
you're alluding

00:27:44.090 --> 00:27:46.945
to is like there's some sales
pressure and so on going on.

00:27:46.945 --> 00:27:48.320
That may well be
true, and people

00:27:48.320 --> 00:27:51.620
sort of might emphasize risk
and make it particularly salient

00:27:51.620 --> 00:27:54.210
and make customers nervous and
say like, look, these floods

00:27:54.210 --> 00:27:56.540
and so on are going on,
and really, low deductibles

00:27:56.540 --> 00:27:57.800
are good.

00:27:57.800 --> 00:27:59.730
I think to some
degree, that's true,

00:27:59.730 --> 00:28:03.920
but you have to be pretty
compelling in your reasoning.

00:28:03.920 --> 00:28:06.230
One other comment
is like, there's

00:28:06.230 --> 00:28:08.990
lots of other examples of
people choosing low deductibles

00:28:08.990 --> 00:28:10.850
and sort of extended
warranties and so on.

00:28:10.850 --> 00:28:12.690
And in lots of
cases, for example,

00:28:12.690 --> 00:28:16.160
if you look at like iPhones or
iPads and so on and so forth,

00:28:16.160 --> 00:28:19.580
laptops, et cetera,
Apple in particular,

00:28:19.580 --> 00:28:23.000
but other companies try to
sell extended warranties

00:28:23.000 --> 00:28:26.420
that, if you actually did
this exact same calculation,

00:28:26.420 --> 00:28:30.020
are not worth
engaging in or that

00:28:30.020 --> 00:28:33.740
reveal essentially extreme
risk aversion among customers.

00:28:33.740 --> 00:28:35.953
I myself am looking at
this kind of research.

00:28:35.953 --> 00:28:38.370
There's lots of research that
argues that people shouldn't

00:28:38.370 --> 00:28:39.830
choose extended warranties.

00:28:39.830 --> 00:28:46.298
Of course, then when I bought
a laptop the last time,

00:28:46.298 --> 00:28:48.590
I was like, of course I don't
need extended warranties.

00:28:48.590 --> 00:28:55.610
And then, of course, soon after,
the laptop broke and so on,

00:28:55.610 --> 00:28:57.390
and I didn't have a
warranty and so on.

00:28:57.390 --> 00:29:00.140
So of course, in specific
examples that may happen,

00:29:00.140 --> 00:29:05.780
but on average it's
not a good idea to do.

00:29:05.780 --> 00:29:06.280
OK.

00:29:06.280 --> 00:29:09.560
So now what does Sydnor find?

00:29:09.560 --> 00:29:14.080
So now here in this table, you
can see the implied estimates

00:29:14.080 --> 00:29:15.065
of gamma.

00:29:15.065 --> 00:29:16.690
Remember, what we
said is what we think

00:29:16.690 --> 00:29:19.270
is a reasonable gamma is
somewhere between 0 and 2

00:29:19.270 --> 00:29:22.150
for large-scale choices.

00:29:22.150 --> 00:29:23.980
He has different
types of assumptions.

00:29:23.980 --> 00:29:27.520
He has like lower bounds
and upper bounds of gammas.

00:29:27.520 --> 00:29:30.970
And what essentially you
see is the gammas are like,

00:29:30.970 --> 00:29:33.220
depending what the assumptions
are, in the hundreds

00:29:33.220 --> 00:29:34.405
or in the thousands.

00:29:34.405 --> 00:29:36.490
It depends a little bit
what people's wealth is.

00:29:36.490 --> 00:29:39.110
So essentially depending on
how much you assume people--

00:29:39.110 --> 00:29:40.810
So the data that
he does not have

00:29:40.810 --> 00:29:43.990
is what is people's
wealth actually like.

00:29:43.990 --> 00:29:45.880
How much money do
people actually have?

00:29:45.880 --> 00:29:48.050
So he makes some reasonable
assumptions by saying,

00:29:48.050 --> 00:29:50.755
look, these are people whose
houses are like $200,000,

00:29:50.755 --> 00:29:52.960
$300,000, $400,000 worth.

00:29:52.960 --> 00:29:54.880
So presumably, and we
know from other data

00:29:54.880 --> 00:29:57.800
sets roughly how much money
people have available.

00:29:57.800 --> 00:30:00.160
So then depending on essentially
what utility function

00:30:00.160 --> 00:30:04.390
you assume, he is mostly
using like CRRA utility,

00:30:04.390 --> 00:30:09.910
like then is wealth
of like $1,000,000.

00:30:09.910 --> 00:30:15.050
You can look at like $100,000,
$50,000, $5,000, and so on.

00:30:15.050 --> 00:30:17.890
You can also use CARA utility,
which is constant absolute risk

00:30:17.890 --> 00:30:18.970
aversion, and so on.

00:30:18.970 --> 00:30:21.370
And essentially, what
you have to assume

00:30:21.370 --> 00:30:25.690
is like extremely low levels of
wealth or something like $5,000

00:30:25.690 --> 00:30:27.670
to get into like
sort of single digits

00:30:27.670 --> 00:30:29.980
or double digits of gamma.

00:30:29.980 --> 00:30:32.620
It's extremely
hard to sort of get

00:30:32.620 --> 00:30:34.960
estimates that are
reasonable in the sense

00:30:34.960 --> 00:30:39.940
that we think are actually
reasonable parameters of gamma.

00:30:39.940 --> 00:30:40.990
Any questions on this?

00:30:45.500 --> 00:30:46.420
OK.

00:30:46.420 --> 00:30:50.560
So now, why do people choose
those small deductibles?

00:30:50.560 --> 00:30:52.360
And this is what you
were saying before.

00:30:52.360 --> 00:30:56.257
So one classical
explanation would

00:30:56.257 --> 00:30:58.340
be, well, they must be
really, really risk-averse.

00:30:58.340 --> 00:31:00.580
Their gamma must be really high.

00:31:00.580 --> 00:31:02.920
There you might
sort of say, well,

00:31:02.920 --> 00:31:04.780
we know already that
gamma shouldn't be that

00:31:04.780 --> 00:31:06.100
high from some other choices.

00:31:06.100 --> 00:31:07.600
So that's sort of
hard to reconcile.

00:31:07.600 --> 00:31:08.560
Was there a question?

00:31:08.560 --> 00:31:09.060
No.

00:31:11.498 --> 00:31:13.290
Second, you could say,
well, there's really

00:31:13.290 --> 00:31:15.035
high objective
probability of claims.

00:31:15.035 --> 00:31:16.910
We know that the objectives
for probabilities

00:31:16.910 --> 00:31:19.063
are only 4% or 5%.

00:31:19.063 --> 00:31:20.730
You'd have to sort
of have probabilities

00:31:20.730 --> 00:31:23.550
of claims that are
like 20%, 30%, 40%

00:31:23.550 --> 00:31:25.470
to be able to match these data.

00:31:25.470 --> 00:31:27.570
Now, it be that people
have risk misperception.

00:31:27.570 --> 00:31:29.280
It could be that
people really think

00:31:29.280 --> 00:31:32.660
the probability is actually 20%
when it's, at the end, only 5

00:31:32.660 --> 00:31:33.580
or 4.

00:31:33.580 --> 00:31:35.790
Now, what's sort of
inconsistent with that

00:31:35.790 --> 00:31:37.710
is that like repeat
customers, people that

00:31:37.710 --> 00:31:40.410
have been at the company
for 10, 15, 20 years,

00:31:40.410 --> 00:31:42.510
are making very similar choices.

00:31:42.510 --> 00:31:45.360
In some sense, it's hard
to reconcile that everybody

00:31:45.360 --> 00:31:48.180
is misperceiving this risk
year after year after year

00:31:48.180 --> 00:31:50.202
and spending lots of money.

00:31:50.202 --> 00:31:52.410
There's some questions on
is it borrowing constraint?

00:31:52.410 --> 00:31:54.750
Is it like people just
don't have enough money?

00:31:54.750 --> 00:32:00.400
Are they worried about having
to pay these deductibles?

00:32:00.400 --> 00:32:02.430
That also seems quite
unlikely because in fact,

00:32:02.430 --> 00:32:04.740
the deductibles
are particularly--

00:32:04.740 --> 00:32:06.600
we're not talking
about like $5,000.

00:32:06.600 --> 00:32:08.100
People could sort
of, if they really

00:32:08.100 --> 00:32:09.030
faced these borrowing
constraints,

00:32:09.030 --> 00:32:10.450
they could save and so on.

00:32:10.450 --> 00:32:13.830
So Sydnor also argues
that that's not going on.

00:32:13.830 --> 00:32:16.110
There's some questions about
marketing social pressure

00:32:16.110 --> 00:32:17.153
and so on, which is--

00:32:17.153 --> 00:32:19.320
of course, the company has
very much like incentives

00:32:19.320 --> 00:32:22.430
to sell people these
kinds of deductibles.

00:32:22.430 --> 00:32:24.180
I think some of that
is probably going on,

00:32:24.180 --> 00:32:26.340
and it's hard to
rule out entirely.

00:32:26.340 --> 00:32:28.810
Again, like, it's hard to
actually sell people stuff they

00:32:28.810 --> 00:32:31.230
don't really want to.

00:32:31.230 --> 00:32:34.750
So there must be lots of social
pressure to, in fact, do that.

00:32:34.750 --> 00:32:35.470
Menu effects.

00:32:35.470 --> 00:32:38.400
Already talked about
it a little bit.

00:32:38.400 --> 00:32:41.730
There we think that
maybe menu effects

00:32:41.730 --> 00:32:44.550
can explain why
people choose to 500

00:32:44.550 --> 00:32:47.100
or, like the interior choices,
why they don't choose 1,000.

00:32:47.100 --> 00:32:48.810
It's hard to explain
with menu effects

00:32:48.810 --> 00:32:52.260
why people choose 50 over 500.

00:32:52.260 --> 00:32:54.240
So then Sydnor's
preferred explanation

00:32:54.240 --> 00:32:56.670
is then reference-dependent
preferences and loss aversion,

00:32:56.670 --> 00:32:58.560
which we're going
to talk about next.

00:32:58.560 --> 00:32:59.470
Yes.

00:32:59.470 --> 00:33:02.298
AUDIENCE: Is there any data
on whether the probability

00:33:02.298 --> 00:33:07.040
of the claim is correlated with
which menu option the people

00:33:07.040 --> 00:33:08.340
chose?

00:33:08.340 --> 00:33:10.320
PROFESSOR: Yes, there is.

00:33:10.320 --> 00:33:13.500
So you have it here.

00:33:13.500 --> 00:33:17.610
So it's sort of weakly
correlated in a sense.

00:33:17.610 --> 00:33:19.200
Like, if you choose--

00:33:19.200 --> 00:33:21.210
or weekly negatively correlated.

00:33:21.210 --> 00:33:32.700
So people who choose $1,000 have
2.5% claim rates, and the $100

00:33:32.700 --> 00:33:34.060
are 4.7.

00:33:34.060 --> 00:33:36.730
So that's kind of not quite
explaining things either.

00:33:36.730 --> 00:33:38.165
Yeah.

00:33:38.165 --> 00:33:40.290
AUDIENCE: I think the use
of [INAUDIBLE] regression

00:33:40.290 --> 00:33:43.700
to control for fact that
those with lower deductibles

00:33:43.700 --> 00:33:45.612
may claim multiple times.

00:33:45.612 --> 00:33:46.320
PROFESSOR: I see.

00:33:46.320 --> 00:33:48.480
Yes.

00:33:48.480 --> 00:33:49.200
Great, yes.

00:33:54.670 --> 00:33:56.510
OK.

00:33:56.510 --> 00:33:57.760
So what do we learn from this?

00:33:57.760 --> 00:34:00.400
I think this is very
much sort of confirming

00:34:00.400 --> 00:34:04.030
some of the lab evidence data on
relatively small-scale gambles.

00:34:04.030 --> 00:34:06.670
These are not like small-scale
gambles of like a dollar

00:34:06.670 --> 00:34:08.199
or 2 or 5 or 10.

00:34:08.199 --> 00:34:10.989
These are about several
hundreds of dollars,

00:34:10.989 --> 00:34:13.900
but they're not about hundreds
of thousands of dollars.

00:34:13.900 --> 00:34:16.840
So these are relatively small
relative to like people's

00:34:16.840 --> 00:34:17.584
lifetime wealth.

00:34:17.584 --> 00:34:19.459
And what we see for
those kinds of choices is

00:34:19.459 --> 00:34:24.219
it really looks
like people appear

00:34:24.219 --> 00:34:27.280
to be very, very risk-averse.

00:34:27.280 --> 00:34:29.262
This is what I was saying
already previously.

00:34:29.262 --> 00:34:31.179
When looking at sort of
reasonably small-scale

00:34:31.179 --> 00:34:31.780
choices--

00:34:31.780 --> 00:34:34.505
and I count the Sydnor evidence
as reasonably small-scale

00:34:34.505 --> 00:34:36.880
choices because it's not about
like hundreds of thousands

00:34:36.880 --> 00:34:37.690
of dollars--

00:34:37.690 --> 00:34:42.550
you essentially see
that such choices imply

00:34:42.550 --> 00:34:43.900
people seem very risk-averse.

00:34:43.900 --> 00:34:47.080
These choices imply
enormous risk aversion

00:34:47.080 --> 00:34:49.000
for large-scale risks.

00:34:49.000 --> 00:34:51.940
But people are not avoiding
all sorts of large-scale risks.

00:34:51.940 --> 00:34:53.780
People take on lots
of large-scale risk.

00:34:53.780 --> 00:34:56.989
So in some sense, that
can't be true in some ways.

00:34:56.989 --> 00:34:59.080
We also find that
individuals are moderately

00:34:59.080 --> 00:35:00.470
risk-averse a large-scale risk.

00:35:00.470 --> 00:35:02.740
People are taking on
some risk, as I said.

00:35:02.740 --> 00:35:04.750
Now, if you sort of
take that seriously

00:35:04.750 --> 00:35:08.500
and say, well, people might not
be risk-averse, that in turn

00:35:08.500 --> 00:35:10.870
implies that people are
nearly risk-neutral.

00:35:10.870 --> 00:35:12.670
They should be
nearly risk-neutral

00:35:12.670 --> 00:35:14.633
for small-scale risk.

00:35:14.633 --> 00:35:16.300
So it can't be that
both of these things

00:35:16.300 --> 00:35:18.060
are true at the same time.

00:35:18.060 --> 00:35:21.790
Now, in fact, there's
a much older paper

00:35:21.790 --> 00:35:25.750
that's a very famous and seminal
paper by Kahneman and Tversky.

00:35:25.750 --> 00:35:29.000
Kahneman got the
Nobel Prize, in fact,

00:35:29.000 --> 00:35:32.440
for this work and similar work.

00:35:32.440 --> 00:35:34.210
Kahneman is a
psychologist and they

00:35:34.210 --> 00:35:36.100
were doing psychology
experiments,

00:35:36.100 --> 00:35:39.080
and it just turned out that
these psychology experiments

00:35:39.080 --> 00:35:42.610
they're extremely influential in
affecting how economists think

00:35:42.610 --> 00:35:46.330
about risk and risk
preferences, and in particular,

00:35:46.330 --> 00:35:49.390
sort of preference-dependent
preferences.

00:35:49.390 --> 00:35:52.708
And so what Kahneman and Tversky
were doing at the time way

00:35:52.708 --> 00:35:54.250
before a lot of this
other literature

00:35:54.250 --> 00:35:56.160
that I just showed you--

00:35:56.160 --> 00:35:58.540
A, they showed even
more sort of evidence

00:35:58.540 --> 00:36:01.325
against the expected
utility model.

00:36:01.325 --> 00:36:02.950
But perhaps more
importantly, they also

00:36:02.950 --> 00:36:04.840
proposed an alternative
model in saying,

00:36:04.840 --> 00:36:06.370
look, here's some
choices that we

00:36:06.370 --> 00:36:10.330
think are hard to explain
and hard to rationalize

00:36:10.330 --> 00:36:11.930
using expected utility.

00:36:11.930 --> 00:36:16.510
Now here's a different model
that can explain things perhaps

00:36:16.510 --> 00:36:19.400
better in some situations.

00:36:19.400 --> 00:36:21.400
So what did Kahneman
and Tversky actually do?

00:36:21.400 --> 00:36:23.320
The experiments are
actually extremely

00:36:23.320 --> 00:36:24.490
simple in various ways.

00:36:24.490 --> 00:36:26.808
They're very clever
and very clean,

00:36:26.808 --> 00:36:27.850
but actually very simple.

00:36:27.850 --> 00:36:29.710
And so what do these
experiments look like?

00:36:29.710 --> 00:36:31.245
These are survey responses.

00:36:31.245 --> 00:36:32.620
Like, essentially
they just asked

00:36:32.620 --> 00:36:35.050
people about what would you
do in different situations.

00:36:35.050 --> 00:36:36.820
This is not what economists
were doing at the time.

00:36:36.820 --> 00:36:38.410
Economists at the
time were saying

00:36:38.410 --> 00:36:40.450
like, revealed
preferences are important.

00:36:40.450 --> 00:36:44.032
I need to make you
do actual choices.

00:36:44.032 --> 00:36:45.490
Whatever you say
in surveys doesn't

00:36:45.490 --> 00:36:48.922
matter because who knows whether
you actually mean what you say.

00:36:48.922 --> 00:36:51.130
It turns out that the survey
responses are actually--

00:36:51.130 --> 00:36:53.290
these are hypothetical
stakes, but it turns out

00:36:53.290 --> 00:36:54.970
that if you do this
with actual stakes,

00:36:54.970 --> 00:36:57.940
you find very similar results.

00:36:57.940 --> 00:37:01.630
And the experiments
were as follows.

00:37:01.630 --> 00:37:05.230
There were things like questions
like, which of the following

00:37:05.230 --> 00:37:08.740
would you prefer, kind of like
I showed you in the first class

00:37:08.740 --> 00:37:11.962
at the end of the
survey that you did.

00:37:11.962 --> 00:37:13.420
Would you prefer
option A, which is

00:37:13.420 --> 00:37:17.800
a 50% of winning $1,000 or
50% chance of winning nothing,

00:37:17.800 --> 00:37:22.550
versus option B is
like $450 for sure?

00:37:22.550 --> 00:37:25.130
And so they did a series of
these types of questions.

00:37:25.130 --> 00:37:29.340
Now, one of the
things that then they

00:37:29.340 --> 00:37:32.220
showed is one key prediction
of expected utility

00:37:32.220 --> 00:37:34.830
is that, as I said
before, people only

00:37:34.830 --> 00:37:39.592
care about final outcomes and
their associated probabilities.

00:37:39.592 --> 00:37:41.010
Kahneman and
Tversky show a bunch

00:37:41.010 --> 00:37:44.013
of different striking
contradictions of that.

00:37:44.013 --> 00:37:46.680
I want you to focus on the first
row-- problem three and problem

00:37:46.680 --> 00:37:47.700
three prime.

00:37:47.700 --> 00:37:51.930
And look at that and
tell me what of that

00:37:51.930 --> 00:37:55.745
example is contradicting
expected utility.

00:37:55.745 --> 00:37:57.120
I don't know if
you can see this.

00:38:02.770 --> 00:38:05.010
So let me just read this
for you if it's hard to see,

00:38:05.010 --> 00:38:07.050
but think of them for a second.

00:38:07.050 --> 00:38:12.780
Problem three says,
"Would you prefer

00:38:12.780 --> 00:38:18.435
an 80% chance of $4,000
over $3,000 for sure?"

00:38:18.435 --> 00:38:19.560
What do you see below then?

00:38:19.560 --> 00:38:21.210
These are always 100 people.

00:38:21.210 --> 00:38:23.010
You see the number of
people who preferred

00:38:23.010 --> 00:38:24.580
one option over the other.

00:38:24.580 --> 00:38:27.810
So 80 people preferred
the 3,000 option.

00:38:27.810 --> 00:38:32.560
20 people said I'd rather
have the 80% chance of $4,000.

00:38:32.560 --> 00:38:33.930
That's problem three.

00:38:33.930 --> 00:38:37.110
Problem three
prime is about what

00:38:37.110 --> 00:38:39.090
they call negative prospects.

00:38:39.090 --> 00:38:43.920
Would you prefer an 80%
chance of losing $4,000,

00:38:43.920 --> 00:38:47.970
or for sure losing $3,000?

00:38:47.970 --> 00:38:55.650
And there you see 92% choose
the first option and 80%

00:38:55.650 --> 00:39:01.790
choose the $3,000 loss for sure.

00:39:01.790 --> 00:39:05.840
Let's start with
the left example.

00:39:05.840 --> 00:39:08.150
What did we learn
from the left example?

00:39:08.150 --> 00:39:10.610
Are people risk-averse,
risk-neutral, risk-loving,

00:39:10.610 --> 00:39:13.963
or what have we
learned from that?

00:39:13.963 --> 00:39:14.880
AUDIENCE: Risk-averse.

00:39:14.880 --> 00:39:17.610
PROFESSOR: Risk-averse,
and why is that?

00:39:17.610 --> 00:39:22.306
AUDIENCE: If you calculate
the expected monetary value,

00:39:22.306 --> 00:39:25.470
I guess you would
expect to [INAUDIBLE]..

00:39:29.056 --> 00:39:29.850
PROFESSOR: Right.

00:39:29.850 --> 00:39:31.860
So we said before
somebody is risk neutral

00:39:31.860 --> 00:39:34.770
if the person is indifferent
between two options

00:39:34.770 --> 00:39:37.830
when they have the same
expected monetary value.

00:39:37.830 --> 00:39:40.530
And so in this case,
an 80% chance of $4,000

00:39:40.530 --> 00:39:44.477
is $3,200 in expectation, right?

00:39:44.477 --> 00:39:46.060
But there's a bunch
of people who say,

00:39:46.060 --> 00:39:51.120
I'd rather get the $3,000, which
is less than 3,200, for sure.

00:39:51.120 --> 00:39:52.920
Which means essentially
they take a surer--

00:39:52.920 --> 00:39:55.950
like an option that's
for sure they get.

00:39:55.950 --> 00:39:57.690
They prefer that
over the uncertainty

00:39:57.690 --> 00:40:01.230
of getting 0 versus 4,000,
which on average gets them more,

00:40:01.230 --> 00:40:03.210
3,200.

00:40:03.210 --> 00:40:05.580
Expected utility would say
if you choose that option,

00:40:05.580 --> 00:40:09.690
it must be that you
are risk-averse.

00:40:09.690 --> 00:40:12.510
Now, not everybody
seems risk-averse.

00:40:12.510 --> 00:40:15.120
There's 80 people out
of 100 choose that,

00:40:15.120 --> 00:40:20.520
or I think it's 80% choose
that and the remaining ones

00:40:20.520 --> 00:40:21.870
choose the other option.

00:40:21.870 --> 00:40:25.170
These other people we
don't know much about.

00:40:25.170 --> 00:40:26.900
They're not very
risk-averse, but they

00:40:26.900 --> 00:40:28.650
could be also risk-averse
just like it was

00:40:28.650 --> 00:40:30.310
revealed in their choice there.

00:40:30.310 --> 00:40:30.810
OK.

00:40:30.810 --> 00:40:32.352
So from the left
side, we say we know

00:40:32.352 --> 00:40:36.485
at least 80 people, or 80% of
the sample, is risk-averse.

00:40:36.485 --> 00:40:37.860
Now let's look at
the right side.

00:40:37.860 --> 00:40:39.277
What do you see
on the right side?

00:40:53.800 --> 00:40:54.567
Yes.

00:40:54.567 --> 00:40:56.650
AUDIENCE: Well, they seem
to be risk [INAUDIBLE],,

00:40:56.650 --> 00:41:03.570
because unexpected risk minus
3,200 versus minus 3,000,

00:41:03.570 --> 00:41:07.585
but they prefer the minus
3,200 on expectation.

00:41:07.585 --> 00:41:09.260
PROFESSOR: Right.

00:41:09.260 --> 00:41:13.740
So if you look at the left
option, it's minus 3,200.

00:41:13.740 --> 00:41:16.240
So the left option
has more risk,

00:41:16.240 --> 00:41:18.958
and it also has a lower
expected monetary value.

00:41:18.958 --> 00:41:20.750
As you said in the
expected monetary value,

00:41:20.750 --> 00:41:23.920
it's just a flip of
the problem three.

00:41:23.920 --> 00:41:29.440
It's minus 3,200, and the
other one is minus 3,000.

00:41:29.440 --> 00:41:31.540
So if you're in a
risk-neutral, surely you

00:41:31.540 --> 00:41:33.640
would choose the minus 3,000.

00:41:33.640 --> 00:41:36.860
If instead you choose
the minus 3,200, well,

00:41:36.860 --> 00:41:38.860
it must be that you really
appreciate that there

00:41:38.860 --> 00:41:40.280
is additional risk there.

00:41:40.280 --> 00:41:43.270
So it looks like
you're risk-loving.

00:41:43.270 --> 00:41:43.990
OK.

00:41:43.990 --> 00:41:47.170
And now we find here
that 92% of the sample

00:41:47.170 --> 00:41:50.800
choose the first option,
but at the same time,

00:41:50.800 --> 00:41:55.720
with the same people, when
they're offered or given

00:41:55.720 --> 00:41:58.480
the choice between problem
three, 80% of people

00:41:58.480 --> 00:41:59.760
choose the other option.

00:41:59.760 --> 00:42:01.510
So there are a bunch
of people essentially

00:42:01.510 --> 00:42:05.350
that choose the $3,000
for sure in problem three.

00:42:05.350 --> 00:42:07.600
At the same time, they
choose the minus 4,000

00:42:07.600 --> 00:42:12.050
with an 80% chance when
given problem three prime.

00:42:12.050 --> 00:42:14.350
So that means essentially
we have two choices here.

00:42:14.350 --> 00:42:18.160
One choice says people
are risk-averse.

00:42:18.160 --> 00:42:21.810
The other choice says
people are risk-loving.

00:42:21.810 --> 00:42:25.840
In expected utility terms in
our world, this cannot happen.

00:42:25.840 --> 00:42:28.560
The reason being that we have
one parameter, which is gamma.

00:42:28.560 --> 00:42:31.260
Gamma tells us how
risk-averse or how risk-loving

00:42:31.260 --> 00:42:32.970
or the risk-neutral alike.

00:42:32.970 --> 00:42:34.800
That parameter
tells us everything

00:42:34.800 --> 00:42:37.140
about your risk
preferences for all choices

00:42:37.140 --> 00:42:38.340
that I'm giving you.

00:42:38.340 --> 00:42:40.800
It cannot be that you're
simultaneously risk-loving

00:42:40.800 --> 00:42:41.970
and risk-averse.

00:42:41.970 --> 00:42:45.750
So this evidence essentially
sort of rejecting the expected

00:42:45.750 --> 00:42:49.195
utility model or cannot
explain this behavior.

00:42:49.195 --> 00:42:50.070
Does this make sense?

00:42:52.978 --> 00:42:54.520
I sort of wrote this
down here, but I

00:42:54.520 --> 00:42:57.370
think I said everything
that's to be said.

00:43:00.730 --> 00:43:01.720
Any questions on this?

00:43:08.070 --> 00:43:10.080
So where we're going
to go is essentially A,

00:43:10.080 --> 00:43:12.330
these people seem to
be behaving differently

00:43:12.330 --> 00:43:15.630
for gains versus
losses and, in addition

00:43:15.630 --> 00:43:18.630
to that, so not only do people
seem to dislike losses--

00:43:18.630 --> 00:43:20.530
I'll show you some
evidence of that--

00:43:20.530 --> 00:43:23.550
but in addition, people seem
to be risk-loving for losses

00:43:23.550 --> 00:43:25.193
and risk-averse for gains.

00:43:25.193 --> 00:43:27.610
And that's kind of what Kahneman
and Tversky are claiming.

00:43:27.610 --> 00:43:29.130
And once you make
that claim, you'll

00:43:29.130 --> 00:43:33.390
be able to explain these
patterns in the data.

00:43:33.390 --> 00:43:37.080
Now, a second thing that
they show is the following.

00:43:37.080 --> 00:43:38.970
They show in problem
11 and problem 12.

00:43:38.970 --> 00:43:43.480
I'll let you read
it for yourself.

00:43:43.480 --> 00:43:45.780
And essentially,
you see that 84%--

00:43:45.780 --> 00:43:50.885
so the choice here is in
addition to whatever you have,

00:43:50.885 --> 00:43:52.260
for sure you're
going to be given

00:43:52.260 --> 00:43:55.230
$1,000, or shekels I guess.

00:43:55.230 --> 00:43:59.040
You are now asked to choose
between 1,000 with a 50% chance

00:43:59.040 --> 00:44:01.410
and 500 for sure.

00:44:01.410 --> 00:44:01.980
OK?

00:44:01.980 --> 00:44:04.290
84% say option B.

00:44:04.290 --> 00:44:06.690
Problem two is, in addition
to whatever your own,

00:44:06.690 --> 00:44:09.480
you have been given 2,000.

00:44:09.480 --> 00:44:11.670
You're now asked to
choose between option

00:44:11.670 --> 00:44:14.490
C, which is minus
1,000 with 50% chance,

00:44:14.490 --> 00:44:17.928
and option D,
which is minus 500.

00:44:17.928 --> 00:44:25.670
69% of people here say
they choose option C. OK?

00:44:25.670 --> 00:44:27.200
So what's the problem with this?

00:44:32.320 --> 00:44:33.130
Yes.

00:44:33.130 --> 00:44:35.470
AUDIENCE: I mean,
at the end of it,

00:44:35.470 --> 00:44:38.056
you would say that
problem 11 option

00:44:38.056 --> 00:44:41.210
B, you end up with 1,500.

00:44:41.210 --> 00:44:46.200
Whereas problem 12 option B,
you still end up with 1,500.

00:44:46.200 --> 00:44:49.210
People were inconsistent
with these decisions

00:44:49.210 --> 00:44:51.068
based on how the
question's framed.

00:44:51.068 --> 00:44:51.860
PROFESSOR: Exactly.

00:44:51.860 --> 00:44:55.910
So framing matters, or
reference to points matter.

00:44:55.910 --> 00:44:58.330
And so what assumption of
the expected utility model

00:44:58.330 --> 00:45:01.805
is that rejecting?

00:45:01.805 --> 00:45:03.588
AUDIENCE: [INAUDIBLE]

00:45:03.588 --> 00:45:04.380
PROFESSOR: Exactly.

00:45:04.380 --> 00:45:10.640
So we postulated that only
final outcomes matter.

00:45:10.640 --> 00:45:13.280
Here, if you write
this down, it turns out

00:45:13.280 --> 00:45:15.770
option A and option C are
exactly the same in terms

00:45:15.770 --> 00:45:17.690
of final outcomes.

00:45:17.690 --> 00:45:20.480
Turns out option B and D
are also exactly the same.

00:45:20.480 --> 00:45:23.051
I'll let you look at
this for a second.

00:45:23.051 --> 00:45:26.150
So A and C are the same in
terms of the final outcomes.

00:45:26.150 --> 00:45:29.360
B and D are also
exactly the same

00:45:29.360 --> 00:45:31.380
in terms of final outcomes.

00:45:31.380 --> 00:45:34.370
So that means essentially when
you compare A versus B and C

00:45:34.370 --> 00:45:37.400
versus D, If you only
cared about final outcomes,

00:45:37.400 --> 00:45:41.180
you cannot choose different
things here in this lottery.

00:45:41.180 --> 00:45:44.150
It cannot be that your utility
is defined over just final

00:45:44.150 --> 00:45:46.610
outcomes, because you just
told me you like two different

00:45:46.610 --> 00:45:49.980
things for the exact same thing.

00:45:49.980 --> 00:45:54.000
And there's 84% who choose
option B while 69% choose

00:45:54.000 --> 00:45:57.000
option C. So there's a bunch of
people who essentially switch.

00:46:00.107 --> 00:46:00.690
Is that clear?

00:46:03.400 --> 00:46:04.090
OK.

00:46:04.090 --> 00:46:06.430
So now, what is that?

00:46:06.430 --> 00:46:07.670
What's going on here?

00:46:07.670 --> 00:46:09.490
Well, where we're going
to go is like well,

00:46:09.490 --> 00:46:11.733
it seems like, exactly as
you say, framing matters.

00:46:11.733 --> 00:46:13.150
How you frame the
problem matters,

00:46:13.150 --> 00:46:14.270
but why does it matter?

00:46:14.270 --> 00:46:16.840
Well, it's because we setting
like a reference point.

00:46:16.840 --> 00:46:19.180
In the first place,
in the first example,

00:46:19.180 --> 00:46:22.420
the reference point is like
1,000 and there is like a gain

00:46:22.420 --> 00:46:23.720
relative to 1,000.

00:46:23.720 --> 00:46:27.340
We look at like people evaluate
this gamble as like gains.

00:46:27.340 --> 00:46:29.350
In the second example,
the reference point

00:46:29.350 --> 00:46:32.080
is set to like 2,000, and
now people essentially

00:46:32.080 --> 00:46:35.800
evaluate this gamble as
losses relative to the 2,000.

00:46:35.800 --> 00:46:37.390
And of depending
on how people think

00:46:37.390 --> 00:46:39.860
about gains versus
losses, it turns out

00:46:39.860 --> 00:46:42.625
that people make
different choices.

00:46:42.625 --> 00:46:44.920
I'll be more formal about
this, but that's essentially

00:46:44.920 --> 00:46:47.140
the idea of, like in
Kahneman and Tversky,

00:46:47.140 --> 00:46:49.730
their prospect
theory they proposed,

00:46:49.730 --> 00:46:55.600
why can explain these behaviors
as opposed to expected utility?

00:46:55.600 --> 00:46:56.170
OK.

00:46:56.170 --> 00:46:58.570
So now what are sort of
the most important points?

00:46:58.570 --> 00:46:59.750
There's three of them.

00:46:59.750 --> 00:47:01.333
I'm going to show
you two and then get

00:47:01.333 --> 00:47:03.910
to the third one at the end.

00:47:03.910 --> 00:47:07.570
So one is like what matters
a lot for people's behaviors

00:47:07.570 --> 00:47:09.920
is changes rather than levels.

00:47:09.920 --> 00:47:13.450
So what they argue is utility
is not defined by people's final

00:47:13.450 --> 00:47:15.910
status-- how much
they end up with--

00:47:15.910 --> 00:47:19.660
but as opposed to
like how much changes

00:47:19.660 --> 00:47:21.820
relative to some
reference point.

00:47:21.820 --> 00:47:24.650
That could be changes
relative to like a status quo.

00:47:24.650 --> 00:47:27.400
How much do I have
right now and how much

00:47:27.400 --> 00:47:29.800
does it change
positively or negatively?

00:47:29.800 --> 00:47:32.050
Or, it could be relative
to some expectation

00:47:32.050 --> 00:47:33.490
or some other reference point.

00:47:33.490 --> 00:47:35.350
What I showed you
here in this example

00:47:35.350 --> 00:47:38.140
is like this is not the status
quo that people evaluate

00:47:38.140 --> 00:47:41.440
their utility against because
the outcome is always the same.

00:47:41.440 --> 00:47:43.930
The reference point here is
kind of like the expectation

00:47:43.930 --> 00:47:46.450
I sent you and say,
OK, here's 1,000.

00:47:46.450 --> 00:47:48.490
And now what seems
to be happening

00:47:48.490 --> 00:47:51.820
is that people's expectations,
in fact, become 1,000.

00:47:51.820 --> 00:47:53.410
And then relative
to that expectation,

00:47:53.410 --> 00:47:57.220
people are going to
evaluate gains and losses.

00:47:57.220 --> 00:48:00.250
And similarly, if I give
you-- like, say you get 2,000,

00:48:00.250 --> 00:48:02.560
again, people will
evaluate the utility

00:48:02.560 --> 00:48:08.080
or the outcomes as gains
and losses relative to that.

00:48:08.080 --> 00:48:12.400
Second, there seems
to be loss aversion.

00:48:12.400 --> 00:48:14.500
Losses loom larger than gains.

00:48:14.500 --> 00:48:16.870
That is to say people
dislike a loss that's

00:48:16.870 --> 00:48:21.850
as large as a gain by a lot
more than they like the gain.

00:48:21.850 --> 00:48:22.370
OK?

00:48:22.370 --> 00:48:26.470
So it's like if you say you
lose 100 versus you gain 100,

00:48:26.470 --> 00:48:28.840
people really dislike
losing 100 a lot more

00:48:28.840 --> 00:48:32.350
than they like gaining 100.

00:48:32.350 --> 00:48:34.160
And once you sort
of postulate that,

00:48:34.160 --> 00:48:37.000
well then, that can explain why
you reject a bunch of gambles.

00:48:37.000 --> 00:48:41.260
Because if say if the gamble is
like minus 10 with 50% chance

00:48:41.260 --> 00:48:44.410
and plus 11 with
50% chance, well,

00:48:44.410 --> 00:48:46.810
if you dislike the losses
a lot more, if you dislike

00:48:46.810 --> 00:48:51.430
losing $10 a lot more than
gaining $10 or $11, well then,

00:48:51.430 --> 00:48:54.130
you're going to
reject this gamble

00:48:54.130 --> 00:48:59.440
even if the expected
value is positive.

00:48:59.440 --> 00:49:00.192
OK?

00:49:00.192 --> 00:49:02.650
We'll write down sort of like
a utility function next time.

00:49:02.650 --> 00:49:04.750
That's sort of does
this more formally,

00:49:04.750 --> 00:49:07.270
but essentially those
are the two key areas.

00:49:07.270 --> 00:49:10.370
There's a third one, which I'm
going to get to in a second.

00:49:10.370 --> 00:49:13.300
Now, the key part here is
like there is essentially

00:49:13.300 --> 00:49:14.260
reference dependence.

00:49:14.260 --> 00:49:18.880
People evaluate their outcomes
relative to some reference

00:49:18.880 --> 00:49:20.038
point.

00:49:20.038 --> 00:49:21.580
What kinds of examples
do we actually

00:49:21.580 --> 00:49:22.540
have a reference point?

00:49:22.540 --> 00:49:23.920
When you look in the
world-- and again,

00:49:23.920 --> 00:49:26.110
that's kind of very much
what I'd like you to do--

00:49:26.110 --> 00:49:28.510
when you think about things
that you see in the world,

00:49:28.510 --> 00:49:31.270
what kinds of examples do
we have that people care

00:49:31.270 --> 00:49:34.270
about reference points as
opposed to final outcomes?

00:49:46.140 --> 00:49:47.354
Yeah.

00:49:47.354 --> 00:49:51.448
AUDIENCE: My first thought was,
you know how we have to pay

00:49:51.448 --> 00:49:54.900
$0.10 for shopping bags?

00:49:54.900 --> 00:49:57.226
Where I was from a
couple years ago,

00:49:57.226 --> 00:50:00.653
we did it so that if
you used a reusable bag,

00:50:00.653 --> 00:50:02.006
you got $0.05 back.

00:50:02.006 --> 00:50:05.470
That was like not effective.

00:50:05.470 --> 00:50:09.970
[INAUDIBLE]

00:50:09.970 --> 00:50:11.350
PROFESSOR: Right.

00:50:11.350 --> 00:50:13.540
Yeah.

00:50:13.540 --> 00:50:14.200
So exactly.

00:50:14.200 --> 00:50:16.420
When you think about pricing
of different options,

00:50:16.420 --> 00:50:17.830
it matters a lot to people.

00:50:17.830 --> 00:50:20.440
It seems that when
you add $0.05,

00:50:20.440 --> 00:50:23.410
were to subtract $0.05
or $0.10 or whatever,

00:50:23.410 --> 00:50:26.530
the same changes about like
it's just $0.05 at the end

00:50:26.530 --> 00:50:29.392
of the day that you have
to pay more or less,

00:50:29.392 --> 00:50:30.850
depending on whether
you use a bag,

00:50:30.850 --> 00:50:33.400
your choice of using a bag
versus not should not be

00:50:33.400 --> 00:50:37.990
affected by whether it's framed
or put as like a loss versus

00:50:37.990 --> 00:50:42.045
a gain or like a whatever rebate
or whatever that you might get.

00:50:42.045 --> 00:50:44.170
And I think that's true
for many different-- that's

00:50:44.170 --> 00:50:48.900
true for shopping plastic bags
or any other shopping bags,

00:50:48.900 --> 00:50:50.650
but it's also true for
many other options.

00:50:50.650 --> 00:50:53.050
It depends a lot whether you
sort of add on that option

00:50:53.050 --> 00:50:54.730
or whether you get
sort of essentially

00:50:54.730 --> 00:50:57.025
subtracted the option
as some discount.

00:50:59.660 --> 00:51:01.535
Yes.

00:51:01.535 --> 00:51:03.510
AUDIENCE: [INAUDIBLE]

00:51:12.992 --> 00:51:13.700
PROFESSOR: Right.

00:51:13.700 --> 00:51:15.710
I think some of it
is not exactly--

00:51:15.710 --> 00:51:18.020
I think there's some
legal constraints

00:51:18.020 --> 00:51:20.565
to that specific behavior
because you're not

00:51:20.565 --> 00:51:22.940
supposed to sort of trick
people, but surely some of that

00:51:22.940 --> 00:51:24.530
is going on.

00:51:24.530 --> 00:51:25.770
People love discounts.

00:51:25.770 --> 00:51:27.950
Like, they love
making good deals.

00:51:27.950 --> 00:51:29.990
They love sort of
getting things cheaper

00:51:29.990 --> 00:51:32.342
in various ways compared
to some reference point.

00:51:32.342 --> 00:51:33.800
And the reference
point, since it's

00:51:33.800 --> 00:51:35.810
hard to tell what actually
should the price be,

00:51:35.810 --> 00:51:37.280
is often like the
previous price.

00:51:37.280 --> 00:51:39.500
It's something that
seems really expensive

00:51:39.500 --> 00:51:42.590
and now they're getting
it for less money.

00:51:42.590 --> 00:51:44.532
And so if you get it
for half the price,

00:51:44.532 --> 00:51:45.990
even if it's still
quite expensive,

00:51:45.990 --> 00:51:50.030
you think like you saved
half of the price somehow.

00:51:50.030 --> 00:51:51.530
AUDIENCE: So how
you feel about what

00:51:51.530 --> 00:51:54.482
you have may depend on what
you see your neighbor having.

00:51:54.482 --> 00:51:55.190
PROFESSOR: Right.

00:51:55.190 --> 00:51:56.810
So reference points
could not just

00:51:56.810 --> 00:52:00.980
be prices or sort of
things that affect yourself

00:52:00.980 --> 00:52:04.100
in certain ways, and price
is the key example overall.

00:52:04.100 --> 00:52:06.440
The reference point could
just be your environment,

00:52:06.440 --> 00:52:07.520
your social environment.

00:52:07.520 --> 00:52:09.228
We kind of talk about
this to some degree

00:52:09.228 --> 00:52:11.120
also when we talk about
social preferences.

00:52:11.120 --> 00:52:12.920
People care a lot
about what others do,

00:52:12.920 --> 00:52:14.628
and their reference
point might very much

00:52:14.628 --> 00:52:16.490
be formed by what
their neighbors do.

00:52:16.490 --> 00:52:17.760
Like, how big is their house?

00:52:17.760 --> 00:52:19.430
How big is their car?

00:52:19.430 --> 00:52:21.065
Do they have a
swimming pool or not?

00:52:21.065 --> 00:52:22.940
It could be also like
your neighbor's-- like,

00:52:22.940 --> 00:52:24.107
how good are they at school?

00:52:24.107 --> 00:52:26.000
Are they smart,
how good looking?

00:52:26.000 --> 00:52:27.220
And so on and so forth.

00:52:27.220 --> 00:52:29.030
So when people evaluate
certain outcomes,

00:52:29.030 --> 00:52:32.390
they often don't evaluate
the levels, but rather

00:52:32.390 --> 00:52:34.940
kind of how much
another person makes

00:52:34.940 --> 00:52:37.160
or whatever other
people's outcomes are.

00:52:37.160 --> 00:52:38.750
And part of the
reason might be that

00:52:38.750 --> 00:52:40.857
like evaluating absolute
outcomes is really hard.

00:52:40.857 --> 00:52:42.440
It's very hard to
actually understand.

00:52:42.440 --> 00:52:45.920
Is $10,000, $50,000,
$100,000, a million--

00:52:45.920 --> 00:52:49.850
how much money do you
actually need to be happy

00:52:49.850 --> 00:52:53.330
or that you like, or
what kinds of outcomes

00:52:53.330 --> 00:52:55.400
are you excited about and so on?

00:52:55.400 --> 00:52:57.350
But it's much easier to
say you have something

00:52:57.350 --> 00:52:59.270
and another person does not
or the other way around.

00:52:59.270 --> 00:53:01.130
It's much easier to
compare with others.

00:53:01.130 --> 00:53:04.040
It's very hard to actually
evaluate absolute outcomes,

00:53:04.040 --> 00:53:07.890
because who knows how you
should feel about that.

00:53:07.890 --> 00:53:09.440
Much easier to say
I have this, they

00:53:09.440 --> 00:53:12.600
don't, or the other way around.

00:53:12.600 --> 00:53:13.860
Yeah.

00:53:13.860 --> 00:53:15.660
AUDIENCE: People
sometimes don't want

00:53:15.660 --> 00:53:20.730
to let go of fallen
stock they own

00:53:20.730 --> 00:53:27.390
because they don't want to see
that they sold at a good price.

00:53:27.390 --> 00:53:28.450
PROFESSOR: Right.

00:53:28.450 --> 00:53:28.950
Exactly.

00:53:28.950 --> 00:53:31.950
That's called what people
call the disposition effect.

00:53:31.950 --> 00:53:34.200
There's actually
pretty large literature

00:53:34.200 --> 00:53:35.725
studying this and
lots of debates

00:53:35.725 --> 00:53:37.350
on whether it's really
going on or not,

00:53:37.350 --> 00:53:40.420
how important it is,
and so on and so forth.

00:53:40.420 --> 00:53:42.000
But sort of one
very basic stylized

00:53:42.000 --> 00:53:45.060
fact is that when people are
looking at stocks that they

00:53:45.060 --> 00:53:48.420
might want to sell, they
are much more likely to sell

00:53:48.420 --> 00:53:50.440
winners compared to losers.

00:53:50.440 --> 00:53:54.070
Now, why is that bad, or
should you not do that?

00:53:57.120 --> 00:54:04.957
AUDIENCE: Is that because
the winners [INAUDIBLE]

00:54:04.957 --> 00:54:06.540
PROFESSOR: There's
a bit of a question

00:54:06.540 --> 00:54:09.300
of is there like momentum
or reversal or the like.

00:54:09.300 --> 00:54:11.610
But if you believe in
efficient markets, which

00:54:11.610 --> 00:54:14.670
many economists do,
or at least some

00:54:14.670 --> 00:54:18.600
of them who are in Chicago--

00:54:18.600 --> 00:54:20.280
if you believe in
efficient markets,

00:54:20.280 --> 00:54:24.630
then the current stock
price should essentially

00:54:24.630 --> 00:54:26.700
not be-- or like
the previous losses

00:54:26.700 --> 00:54:28.920
and so on should not be
informative of what's

00:54:28.920 --> 00:54:30.220
going on in the future.

00:54:30.220 --> 00:54:32.730
So in expectations, your
losers and your winners,

00:54:32.730 --> 00:54:35.130
if there's any information
about the future,

00:54:35.130 --> 00:54:37.320
about the future valuation,
that should already

00:54:37.320 --> 00:54:39.065
be incorporated in the price.

00:54:39.065 --> 00:54:40.440
So if you look at
two stocks, one

00:54:40.440 --> 00:54:42.660
lost some money and
one gained some money,

00:54:42.660 --> 00:54:45.730
they're equally likely
to make you money.

00:54:45.730 --> 00:54:48.210
And so if anything, what
they show in these papers

00:54:48.210 --> 00:54:49.440
is like there's momentum.

00:54:49.440 --> 00:54:51.732
This is what you're saying
is that the winners actually

00:54:51.732 --> 00:54:55.020
are, in fact, going to be more
likely to try to increase value

00:54:55.020 --> 00:54:56.250
compared to the losers.

00:54:56.250 --> 00:55:00.720
But people tend to essentially
want to sort of realize gains.

00:55:00.720 --> 00:55:02.790
People seem to be happy
about making gains.

00:55:02.790 --> 00:55:06.960
They seem to be very
reluctant to sell the losers,

00:55:06.960 --> 00:55:09.900
and there's some questions on
how costly is that actually,

00:55:09.900 --> 00:55:13.022
but that's a very robust
pattern in the data.

00:55:13.022 --> 00:55:13.980
The same is also true--

00:55:13.980 --> 00:55:16.050
I'm going to show you
on Monday, the same

00:55:16.050 --> 00:55:17.520
is also true for houses.

00:55:17.520 --> 00:55:20.970
People are much less
likely to sell their house

00:55:20.970 --> 00:55:24.420
when it has lost value compared
to when it gained value.

00:55:24.420 --> 00:55:27.120
I'm controlling for
a bunch of things.

00:55:27.120 --> 00:55:28.600
Yeah.

00:55:28.600 --> 00:55:30.839
AUDIENCE: [INAUDIBLE]

00:55:38.118 --> 00:55:39.910
PROFESSOR: Yeah, so
that's a little tricky.

00:55:39.910 --> 00:55:42.170
There might be other things
going on, but exactly.

00:55:42.170 --> 00:55:45.500
It could just be that like
if you don't go to a movie

00:55:45.500 --> 00:55:47.390
when you have bought
a ticket yourself,

00:55:47.390 --> 00:55:50.300
it's sort of perceived as a loss
and you really don't like that.

00:55:50.300 --> 00:55:51.800
It's a little
complicated to think

00:55:51.800 --> 00:55:53.883
about this in simple terms,
because in some sense,

00:55:53.883 --> 00:55:55.697
the loss is still there.

00:55:55.697 --> 00:55:56.780
But anyway, yeah, exactly.

00:55:56.780 --> 00:55:57.990
It's like you lost money.

00:55:57.990 --> 00:56:00.230
It's almost like as if
you lost the movie ticket

00:56:00.230 --> 00:56:01.965
and didn't get
some value from it.

00:56:01.965 --> 00:56:04.340
There's a bit of a question
whether you sort of integrate

00:56:04.340 --> 00:56:06.233
these two things or not
because people think

00:56:06.233 --> 00:56:08.150
about monetary terms
versus other things often

00:56:08.150 --> 00:56:10.400
in separation, but I think
some of that is exactly.

00:56:10.400 --> 00:56:13.460
People feel like they have
a loss if they don't take

00:56:13.460 --> 00:56:14.877
advantage of the movie tickets.

00:56:14.877 --> 00:56:17.210
So I'm going to show you some
examples that are actually

00:56:17.210 --> 00:56:19.610
much more basic in some ways.

00:56:19.610 --> 00:56:22.360
When you think about
like visual illusions,

00:56:22.360 --> 00:56:24.860
this is what's called like
the size contrast illusion.

00:56:24.860 --> 00:56:26.030
So one of those
things is like when

00:56:26.030 --> 00:56:27.530
you look at circles
or things that's

00:56:27.530 --> 00:56:30.257
supposed to look the same,
and in fact are the same size,

00:56:30.257 --> 00:56:31.340
they look quite different.

00:56:31.340 --> 00:56:33.263
Depending on what you
contrast things with,

00:56:33.263 --> 00:56:34.430
things look quite different.

00:56:34.430 --> 00:56:36.770
For example, if you look at
these circles in the middle,

00:56:36.770 --> 00:56:38.540
they're in fact
exactly the same size,

00:56:38.540 --> 00:56:40.800
but in fact, they don't
really look like that.

00:56:40.800 --> 00:56:42.290
Similarly, if you look
at these two circles,

00:56:42.290 --> 00:56:43.290
it's perhaps more stark.

00:56:43.290 --> 00:56:45.560
These circles are, in
fact, exactly the same size

00:56:45.560 --> 00:56:46.530
that are in the middle.

00:56:46.530 --> 00:56:47.840
Every time I'm
teaching this class,

00:56:47.840 --> 00:56:50.173
I sort of have to convince
myself that they are actually

00:56:50.173 --> 00:56:50.760
the same size.

00:56:50.760 --> 00:56:52.370
So I print it out
and sort of measure

00:56:52.370 --> 00:56:54.870
to make sure that it's actually
the same size, and they are.

00:56:54.870 --> 00:56:56.720
I checked.

00:56:56.720 --> 00:56:59.190
But when you see this, even
if you know it's an illusion,

00:56:59.190 --> 00:57:02.090
it's very hard to convince
yourself that it's not.

00:57:02.090 --> 00:57:03.535
But they are the same size.

00:57:03.535 --> 00:57:04.910
When you look at
these bars, when

00:57:04.910 --> 00:57:07.243
you look at the upper black
bar and the lower black bar,

00:57:07.243 --> 00:57:10.467
again, it seems like somehow
the upper one is like wider,

00:57:10.467 --> 00:57:12.050
but in fact, it's
not, and again, it's

00:57:12.050 --> 00:57:15.050
all about the contrast
to the other side.

00:57:15.050 --> 00:57:17.240
There's this one which
throws me off the most.

00:57:17.240 --> 00:57:22.190
When you look at fields A and
B, they are the same color.

00:57:22.190 --> 00:57:24.595
That's hard to believe
once you see it.

00:57:24.595 --> 00:57:25.970
They are actually
the same color,

00:57:25.970 --> 00:57:27.710
and again, I would print
it out and actually

00:57:27.710 --> 00:57:28.877
put them next to each other.

00:57:28.877 --> 00:57:30.140
They are the exact same color.

00:57:30.140 --> 00:57:30.865
I checked.

00:57:30.865 --> 00:57:32.240
There's also a
video that you can

00:57:32.240 --> 00:57:34.460
watch that sort of shows this.

00:57:34.460 --> 00:57:36.650
And exactly what's
happening is the way

00:57:36.650 --> 00:57:41.360
we perceive colors is
coming from contrast, right?

00:57:41.360 --> 00:57:45.440
So like black does not or gray
doesn't look as gray when white

00:57:45.440 --> 00:57:46.730
or darker gray is next to it.

00:57:50.090 --> 00:57:53.030
Similarly here, if you look
at the gray of this bar,

00:57:53.030 --> 00:57:54.410
this is the exact same gray.

00:57:54.410 --> 00:57:56.120
Again, you can print it
out and sort of look at it.

00:57:56.120 --> 00:57:56.960
It's the exact same.

00:57:56.960 --> 00:57:58.335
When you look at
it, it just does

00:57:58.335 --> 00:58:01.580
look like on the right side
it's darker than on the left.

00:58:01.580 --> 00:58:04.730
Now, there's plenty of examples
of reference dependence

00:58:04.730 --> 00:58:05.685
for vision.

00:58:05.685 --> 00:58:06.560
There's tons of them.

00:58:06.560 --> 00:58:08.540
They're kind of
quite interesting.

00:58:08.540 --> 00:58:10.820
At some level, it tells us
something about the brain.

00:58:10.820 --> 00:58:14.210
Like, in some ways, we can learn
about essentially the way we

00:58:14.210 --> 00:58:17.030
evaluate certain outcomes
overall is essentially--

00:58:17.030 --> 00:58:19.370
it's much easier for us, or
we think about in contrast,

00:58:19.370 --> 00:58:22.332
it's very hard for us often
to think about levels.

00:58:22.332 --> 00:58:23.540
But of course, that's vision.

00:58:23.540 --> 00:58:26.510
So in some sense, what did we
learn really about utility?

00:58:26.510 --> 00:58:31.370
One thing you can look at is
bronze and silver medal winners

00:58:31.370 --> 00:58:32.270
at the Olympics.

00:58:32.270 --> 00:58:35.090
Presumably or arguably,
winning a silver medal

00:58:35.090 --> 00:58:37.370
is better than a
bronze medal winner.

00:58:37.370 --> 00:58:39.710
If you look at these
two women, one of them

00:58:39.710 --> 00:58:43.003
won the silver medal,
one of them won bronze.

00:58:43.003 --> 00:58:45.170
So what people have done
and psychologists have done

00:58:45.170 --> 00:58:46.940
is like took actually
a bunch of pictures

00:58:46.940 --> 00:58:50.600
from like ceremonies of
bronze and silver winners

00:58:50.600 --> 00:58:52.680
and just looked at
who looks happier.

00:58:52.680 --> 00:58:55.180
And when you do
that, essentially

00:58:55.180 --> 00:58:57.680
the bronze medalists look on
average happier than the silver

00:58:57.680 --> 00:58:58.580
medalists.

00:58:58.580 --> 00:59:01.160
Presumably it's because
they just missed gold

00:59:01.160 --> 00:59:03.860
while the other person
sort of won bronze

00:59:03.860 --> 00:59:09.080
because it's great to be
third as opposed to fourth.

00:59:09.080 --> 00:59:09.950
And there's more.

00:59:09.950 --> 00:59:12.620
You can read more about this,
but that's sort of like one

00:59:12.620 --> 00:59:13.610
finding.

00:59:13.610 --> 00:59:19.790
Now, it's true for lots
of different feelings,

00:59:19.790 --> 00:59:22.330
perceptions,
judgment, and so on.

00:59:22.330 --> 00:59:25.413
People compare stimuli
in various ways

00:59:25.413 --> 00:59:26.830
when it comes to
temperature, when

00:59:26.830 --> 00:59:30.800
it comes to all sorts of things
relative to reference levels.

00:59:30.800 --> 00:59:34.970
It's very hard for them to
evaluate it in absolute terms.

00:59:34.970 --> 00:59:37.220
So it's easy to say--
when you look at water,

00:59:37.220 --> 00:59:39.160
it's very hard to say
what's the temperature

00:59:39.160 --> 00:59:41.080
even if you sort of
practice it a lot.

00:59:41.080 --> 00:59:43.480
It's very easy to say
one bucket of water

00:59:43.480 --> 00:59:44.690
is warmer than another.

00:59:44.690 --> 00:59:48.760
It's very hard to sort of say
it's like 70 degrees or 60

00:59:48.760 --> 00:59:49.270
or whatever.

00:59:49.270 --> 00:59:50.770
It's very hard to
sort of understand

00:59:50.770 --> 00:59:52.630
what absolute
temperatures are, and I

00:59:52.630 --> 00:59:59.770
think that's the same in some
ways for a lot of consumption

00:59:59.770 --> 01:00:00.650
or other decisions.

01:00:00.650 --> 01:00:02.858
It's very hard to say how
much would you pay for this

01:00:02.858 --> 01:00:05.590
or how happy should I be
with certain outcomes,

01:00:05.590 --> 01:00:07.090
because people need
some reference,

01:00:07.090 --> 01:00:08.470
and often then the
reference are either

01:00:08.470 --> 01:00:10.210
some expectations,
or their neighbors,

01:00:10.210 --> 01:00:15.500
or just comparing between
different options.

01:00:15.500 --> 01:00:18.470
So that's the example
I mentioned previously,

01:00:18.470 --> 01:00:20.050
which is it's much
easier to compare

01:00:20.050 --> 01:00:21.980
your income or any
outcomes-- your grades,

01:00:21.980 --> 01:00:24.580
et cetera-- compared to
what your friend has.

01:00:24.580 --> 01:00:28.780
It's much harder to say how
much an extra $1,000 or having

01:00:28.780 --> 01:00:31.870
$50,000 per year,
is that good or bad?

01:00:31.870 --> 01:00:36.472
It depends a lot on what
you compare it against.

01:00:36.472 --> 01:00:37.930
So what people tend
to do, and this

01:00:37.930 --> 01:00:40.990
is exactly what Kahneman and
Tversky were postulating,

01:00:40.990 --> 01:00:43.270
is that people
compare the outcomes

01:00:43.270 --> 01:00:46.452
relative to reference points.

01:00:46.452 --> 01:00:48.160
And again, we're going
to write this down

01:00:48.160 --> 01:00:51.055
in more detail at the
beginning of next class,

01:00:51.055 --> 01:00:53.980
but what Kahneman and
Tversky were postulating

01:00:53.980 --> 01:00:55.960
were essentially two things.

01:00:55.960 --> 01:00:58.420
They're postulating A,
there's a reference level

01:00:58.420 --> 01:01:00.385
of consumption or any outcomes.

01:01:00.385 --> 01:01:03.280
We can talk a little bit about
what actually is this reference

01:01:03.280 --> 01:01:04.600
level, where it's coming from.

01:01:04.600 --> 01:01:08.320
For now, we just assume there's
a reference level that people

01:01:08.320 --> 01:01:10.430
compare the outcomes against.

01:01:10.430 --> 01:01:14.440
And then against that,
outcomes are compared.

01:01:14.440 --> 01:01:17.770
And in particular,
the function is

01:01:17.770 --> 01:01:20.890
like steeper on the left
compared to the right, right?

01:01:20.890 --> 01:01:24.730
Essentially losses
loom larger than gains.

01:01:24.730 --> 01:01:28.990
So like going down by 10
units on the left or 1 unit

01:01:28.990 --> 01:01:32.650
on the left is more
painful than going up

01:01:32.650 --> 01:01:35.400
on the right relative to
those reference points.

01:01:35.400 --> 01:01:36.580
OK?

01:01:36.580 --> 01:01:39.460
And so that's all said.

01:01:39.460 --> 01:01:43.540
So then what experimental
evidence do we in fact

01:01:43.540 --> 01:01:45.480
have for that?

01:01:45.480 --> 01:01:49.612
So one example-- so we have some
experimental evidence of this.

01:01:49.612 --> 01:01:51.070
And I'm going to
show you next time

01:01:51.070 --> 01:01:54.220
a bunch of different
examples of real choices,

01:01:54.220 --> 01:01:56.500
starting from golfing,
to selling houses,

01:01:56.500 --> 01:01:58.000
to lots of other outcomes.

01:01:58.000 --> 01:02:00.443
But sort of the experimental
evidence, the earlier

01:02:00.443 --> 01:02:02.110
experimental evidence
that people showed

01:02:02.110 --> 01:02:04.510
were preferences
over risky gambles.

01:02:04.510 --> 01:02:07.430
I showed you some of those
already, and then in particular

01:02:07.430 --> 01:02:10.297
unwillingness to trade
different options compared

01:02:10.297 --> 01:02:12.130
to like an alternative
option that I showed.

01:02:12.130 --> 01:02:15.178
That's what people refer
to as the endowment effect.

01:02:15.178 --> 01:02:16.720
So let me show you
first the gambles.

01:02:16.720 --> 01:02:17.990
We already had
that in some sense.

01:02:17.990 --> 01:02:19.448
These are like
essentially gambles.

01:02:19.448 --> 01:02:21.640
People seem really
risk averse when

01:02:21.640 --> 01:02:24.070
they are offered these gambles.

01:02:24.070 --> 01:02:26.890
Kahneman and Tversky would
say people are essentially

01:02:26.890 --> 01:02:27.490
loss averse.

01:02:27.490 --> 01:02:30.760
People really dislike
the loss of $10

01:02:30.760 --> 01:02:32.170
relative to the gain of $11.

01:02:32.170 --> 01:02:33.580
We can explain that behavior.

01:02:33.580 --> 01:02:36.247
And that's a very robust finding
that people decline these kinds

01:02:36.247 --> 01:02:37.130
of types of gambles.

01:02:37.130 --> 01:02:39.310
Kahneman and Tversky
would say that's evidence

01:02:39.310 --> 01:02:40.557
of loss aversion.

01:02:40.557 --> 01:02:42.640
Now, you might say these
are really small gambles.

01:02:42.640 --> 01:02:43.990
Do we really care about them?

01:02:43.990 --> 01:02:46.803
Well, once you do
this with $500, $550

01:02:46.803 --> 01:02:48.970
or whatever, large amounts,
people also do the same.

01:02:48.970 --> 01:02:50.890
It's hard to do this with
like real-world money

01:02:50.890 --> 01:02:52.240
because it's quite
a bit of money.

01:02:52.240 --> 01:02:53.860
It turns out there's
actually some studies who

01:02:53.860 --> 01:02:54.527
do that as well.

01:02:54.527 --> 01:02:57.670
They did this for real with MBA
students, financial analysts,

01:02:57.670 --> 01:02:58.810
and rich investors.

01:02:58.810 --> 01:03:02.992
And even those people like
tend to then turn down

01:03:02.992 --> 01:03:03.950
those kinds of gambles.

01:03:03.950 --> 01:03:05.410
So there seems to
be quite evidence

01:03:05.410 --> 01:03:08.928
of lots of loss aversion.

01:03:08.928 --> 01:03:10.720
Given the choices
between gains and losses,

01:03:10.720 --> 01:03:13.455
people seem to really
dislike losses.

01:03:13.455 --> 01:03:14.830
Now, what's the
endowment effect?

01:03:14.830 --> 01:03:20.410
That's sort of the perhaps most
famous evidence in this domain.

01:03:20.410 --> 01:03:24.250
It essentially is people, when
endowed with a certain item,

01:03:24.250 --> 01:03:26.380
they really do
not like to trade.

01:03:26.380 --> 01:03:30.235
And so the way you
would do an experiment--

01:03:30.235 --> 01:03:32.620
you would give people
an item, and then

01:03:32.620 --> 01:03:37.420
you ask them essentially-- and
a randomly selected fraction

01:03:37.420 --> 01:03:39.910
of people are given an item.

01:03:39.910 --> 01:03:41.980
And then either people
are offered the choice--

01:03:41.980 --> 01:03:43.480
you would you either
keep your item,

01:03:43.480 --> 01:03:45.938
or would you get another item
that's sort of the same value

01:03:45.938 --> 01:03:46.960
overall?

01:03:46.960 --> 01:03:50.770
Or, what people do is like
they have some experiments--

01:03:50.770 --> 01:03:52.750
there's many of
these experiments--

01:03:52.750 --> 01:03:56.470
you give some people one
item and some other people

01:03:56.470 --> 01:03:57.970
another item, and
then see would you

01:03:57.970 --> 01:03:59.380
like to trade with each other.

01:03:59.380 --> 01:04:01.040
And what you see is
essentially people

01:04:01.040 --> 01:04:03.560
are extremely
reluctant to trade.

01:04:03.560 --> 01:04:06.190
There's many of these
kinds of examples.

01:04:06.190 --> 01:04:12.170
Like, for example, so
one example-- sorry,

01:04:12.170 --> 01:04:13.800
I skipped that.

01:04:13.800 --> 01:04:16.037
One example is like if you
just give people an item--

01:04:16.037 --> 01:04:18.120
usually it's like a mug--
if you give people a mug

01:04:18.120 --> 01:04:21.030
and asked like, how much
do I have to pay you

01:04:21.030 --> 01:04:24.270
to give me to sell me this mug?

01:04:24.270 --> 01:04:25.510
People say large amounts.

01:04:25.510 --> 01:04:27.110
They would say like $5.

01:04:27.110 --> 01:04:28.860
If, instead, you asked
them, here's a mug.

01:04:28.860 --> 01:04:30.360
Would you like to
purchase this mug?

01:04:30.360 --> 01:04:33.432
And sort of controlling for how
much money they have and so on.

01:04:33.432 --> 01:04:35.640
If you ask them like, would
you like to buy this mug,

01:04:35.640 --> 01:04:37.380
people would say
like $2 or something.

01:04:37.380 --> 01:04:40.110
It's the same mug, and
essentially their choice

01:04:40.110 --> 01:04:43.410
depends on whether you give
them the mug versus whether you

01:04:43.410 --> 01:04:45.910
endow them with the mug, which
is where the endowment effect

01:04:45.910 --> 01:04:48.420
name comes from, or whether
you just sort of like

01:04:48.420 --> 01:04:51.040
ask their willingness to
pay when they don't have it.

01:04:51.040 --> 01:04:52.590
There's tons of
different experiments

01:04:52.590 --> 01:04:54.630
that do exactly that.

01:04:54.630 --> 01:04:56.340
Now, a different
version of that is like

01:04:56.340 --> 01:04:59.940
so this one is about like
buying a mug, selling a mug,

01:04:59.940 --> 01:05:01.660
buying and selling prices.

01:05:01.660 --> 01:05:03.600
Different versions
of that is to say

01:05:03.600 --> 01:05:07.823
you have a population of
students or different people,

01:05:07.823 --> 01:05:09.240
and what you do
is essentially you

01:05:09.240 --> 01:05:12.230
find two items that on average
have the same valuation.

01:05:12.230 --> 01:05:13.870
Knetsch was doing that.

01:05:13.870 --> 01:05:16.470
And so he had like
mugs and pens,

01:05:16.470 --> 01:05:19.170
and he sort of calibrated such
that on average, when you just

01:05:19.170 --> 01:05:21.828
asked, people's willingness
to pay for these mugs and pens

01:05:21.828 --> 01:05:23.370
are like on average
roughly the same.

01:05:23.370 --> 01:05:26.610
Of course, there's variation,
but on average it's the same.

01:05:26.610 --> 01:05:28.620
And then you offered
half the students mugs

01:05:28.620 --> 01:05:31.740
and half the students pens
and then offered in exchange.

01:05:31.740 --> 01:05:35.915
He also offered in exchange
for an addition of $0.05

01:05:35.915 --> 01:05:37.290
and saying like,
OK, maybe you're

01:05:37.290 --> 01:05:38.332
just exactly indifferent.

01:05:38.332 --> 01:05:40.898
So I'm giving you $0.05
in case you exchange.

01:05:40.898 --> 01:05:43.440
And it turns out that the mug
people like to keep their mugs,

01:05:43.440 --> 01:05:47.190
and the pen people like to keep
the pens, and 90% of people

01:05:47.190 --> 01:05:47.910
do that.

01:05:47.910 --> 01:05:50.670
That's one of the
most robust findings

01:05:50.670 --> 01:05:54.650
in experimental economics.

01:05:54.650 --> 01:05:56.660
There are some questions
on expectations.

01:05:56.660 --> 01:05:59.130
Do people expect to keep the
mugs and so on and so forth?

01:05:59.130 --> 01:06:00.630
There's some
complications for that,

01:06:00.630 --> 01:06:02.570
but the basic result
is very robust

01:06:02.570 --> 01:06:05.510
and has been shown in
many different settings.

01:06:05.510 --> 01:06:07.400
Any questions on this?

01:06:14.200 --> 01:06:14.730
OK.

01:06:14.730 --> 01:06:15.870
So what's going on here?

01:06:15.870 --> 01:06:18.560
Well, essentially,
the reference point

01:06:18.560 --> 01:06:22.330
seems to be essentially
affected by ownership.

01:06:22.330 --> 01:06:24.360
So if you own a mug,
your reference point

01:06:24.360 --> 01:06:25.800
is owning a mug.

01:06:25.800 --> 01:06:29.040
So now if I asked you, would
you like to sell me this mug?

01:06:29.040 --> 01:06:30.750
Well, now you have
a loss of a mug.

01:06:30.750 --> 01:06:32.860
So essentially, you're in
the lost domain of mugs.

01:06:32.860 --> 01:06:36.540
So I have to pay you more money
to compensate you for that.

01:06:36.540 --> 01:06:39.330
If, in contrast, you do not own
the mug, so like you have zero

01:06:39.330 --> 01:06:41.910
mugs, your reference point is
zero mugs, and I'm asking you,

01:06:41.910 --> 01:06:46.050
would you like to receive a
mug or like buy a mug from me,

01:06:46.050 --> 01:06:48.240
then you're essentially
on the gain domain.

01:06:48.240 --> 01:06:51.630
And sort of what I showed
you here previously--

01:06:51.630 --> 01:06:52.690
one second.

01:06:52.690 --> 01:06:55.080
So when I'm asking you
to purchase your mug,

01:06:55.080 --> 01:06:57.357
essentially you're on the
left side of this figure.

01:06:57.357 --> 01:06:58.440
You're in the loss domain.

01:06:58.440 --> 01:07:00.600
Your marginal utility
of mugs is very high.

01:07:00.600 --> 01:07:04.530
I have to pay you a lot of money
to receive the mug from you.

01:07:04.530 --> 01:07:06.750
In contrast, if you have
zero mugs, I'm asking you,

01:07:06.750 --> 01:07:08.125
would you like to
purchase a mug,

01:07:08.125 --> 01:07:10.125
you're on the right side
of the suffering point.

01:07:10.125 --> 01:07:12.130
So essentially now you're
in the gain domain.

01:07:12.130 --> 01:07:15.480
Now you're not willing
to pay a lot of money

01:07:15.480 --> 01:07:17.940
because you're gaining a mug.

01:07:17.940 --> 01:07:20.670
On top of that, you also on
the gain loss domain of money,

01:07:20.670 --> 01:07:23.950
but like I'm setting
that sort of aside.

01:07:23.950 --> 01:07:25.450
Does that make any sense?

01:07:27.750 --> 01:07:28.250
OK.

01:07:32.740 --> 01:07:35.125
And so people hate losses
more than they like the gain.

01:07:35.125 --> 01:07:36.250
So they stick with the mug.

01:07:36.250 --> 01:07:40.610
And similarly, that's the
same thing for the pen owners.

01:07:40.610 --> 01:07:42.140
There's lots of
different examples

01:07:42.140 --> 01:07:44.300
of those kinds of behaviors.

01:07:44.300 --> 01:07:46.610
For example, law
school students were

01:07:46.610 --> 01:07:50.000
asked to assess compensation
for pain and suffering damages

01:07:50.000 --> 01:07:51.710
in one study.

01:07:51.710 --> 01:07:53.600
This is expected to last--

01:07:53.600 --> 01:07:56.210
or in this example-- is
expected to last three years

01:07:56.210 --> 01:07:58.040
and be quite unpleasant.

01:07:58.040 --> 01:08:01.740
There's no impact on
earnings capacity.

01:08:01.740 --> 01:08:04.390
For example, it would be extreme
stiffness in the upper back

01:08:04.390 --> 01:08:04.890
and neck.

01:08:04.890 --> 01:08:07.530
I think that would probably
affect your earning capacity,

01:08:07.530 --> 01:08:12.040
but anyway let's assume
that's not the case.

01:08:12.040 --> 01:08:13.740
So then some students
are led to imagine

01:08:13.740 --> 01:08:15.060
they were being injured.

01:08:15.060 --> 01:08:19.210
How much would you be
willing to pay to get better?

01:08:19.210 --> 01:08:19.710
OK.

01:08:19.710 --> 01:08:21.750
So your reference point
is like you're injured.

01:08:21.750 --> 01:08:25.470
How much are you willing
to pay to get better?

01:08:25.470 --> 01:08:30.270
And people said
151,448 on average.

01:08:30.270 --> 01:08:31.770
Now, another group
of students was--

01:08:31.770 --> 01:08:35.250
this is randomized-- was led
to imagine being uninjured.

01:08:35.250 --> 01:08:39.600
Like, how much would I need to
pay you to accept the injury?

01:08:39.600 --> 01:08:40.920
You're not injured.

01:08:40.920 --> 01:08:43.535
Now I'm asking you, how
much are you willing to pay,

01:08:43.535 --> 01:08:44.910
or how much do I
have to pay you,

01:08:44.910 --> 01:08:48.390
how much do I have to compensate
you to accept this injury?

01:08:48.390 --> 01:08:52.529
Now, you would think this
is like the same thing,

01:08:52.529 --> 01:08:54.870
but what's your price
of not being injured?

01:08:54.870 --> 01:08:57.600
The price of health should be
independent on which way I'm

01:08:57.600 --> 01:08:58.380
asking you.

01:08:58.380 --> 01:09:01.859
Turns out, when people
have their good health,

01:09:01.859 --> 01:09:05.319
they're willing to pay
a lot more to keep it

01:09:05.319 --> 01:09:08.490
or to not lose it compared to
like when they have lost it

01:09:08.490 --> 01:09:11.729
and they want to regain it.

01:09:11.729 --> 01:09:13.890
There's another
quite nice example.

01:09:13.890 --> 01:09:15.930
I'm showing you here.

01:09:15.930 --> 01:09:16.930
One second.

01:09:16.930 --> 01:09:20.189
So in case you didn't
see, that was Dan Ariely,

01:09:20.189 --> 01:09:22.260
who has several nice books.

01:09:22.260 --> 01:09:24.160
One of them is
predictably irrational,

01:09:24.160 --> 01:09:27.580
which is a quite nice read.

01:09:27.580 --> 01:09:32.736
So there's lots of these kinds
of examples of loss aversion

01:09:32.736 --> 01:09:34.319
or what's called the
endowment effect.

01:09:34.319 --> 01:09:36.808
When people have
stuff essentially,

01:09:36.808 --> 01:09:38.100
they're not willing to part it.

01:09:38.100 --> 01:09:41.760
When they don't have it,
they're willing to pay less.

01:09:41.760 --> 01:09:44.279
One nice thing that Ariely was
also talking about-- a little

01:09:44.279 --> 01:09:45.840
bit about how
people then explain

01:09:45.840 --> 01:09:46.950
these kinds of behaviors.

01:09:46.950 --> 01:09:49.260
We're going to get
back to that in I

01:09:49.260 --> 01:09:50.837
think lecture 20
or something, which

01:09:50.837 --> 01:09:52.920
is about kind of like I
think what's going on here

01:09:52.920 --> 01:09:53.670
is very obvious.

01:09:53.670 --> 01:09:55.200
Potentially, people
are loss-averse

01:09:55.200 --> 01:09:57.150
and they're randomized
into like gains or losses,

01:09:57.150 --> 01:09:59.317
and that's kind of what's
explaining their behavior.

01:09:59.317 --> 01:10:01.058
But people don't
necessarily understand

01:10:01.058 --> 01:10:03.600
that they're being randomized
in one of the other conditions,

01:10:03.600 --> 01:10:05.700
and they then sort of try
to explain their behavior

01:10:05.700 --> 01:10:08.158
in various ways and saying, I
want to tell my grandchildren

01:10:08.158 --> 01:10:08.783
about this.

01:10:08.783 --> 01:10:10.200
But in some sense,
they're sort of

01:10:10.200 --> 01:10:12.150
like rationalizing
their preferences

01:10:12.150 --> 01:10:14.688
and they don't necessarily sort
of understand these stories,

01:10:14.688 --> 01:10:16.230
and they don't
necessarily understand

01:10:16.230 --> 01:10:19.320
where these preferences are
coming from even though we kind

01:10:19.320 --> 01:10:21.450
of know, because the
person has been manipulated

01:10:21.450 --> 01:10:23.040
into those kinds of choices.

01:10:23.040 --> 01:10:24.430
We'll get back to that.

01:10:24.430 --> 01:10:28.440
Now, the third part of Kahneman
and Tversky's prospect theory

01:10:28.440 --> 01:10:29.940
of their paper is
essentially what's

01:10:29.940 --> 01:10:31.615
called diminishing sensitivity.

01:10:31.615 --> 01:10:33.990
I'm going to get back to this
and sort of write this down

01:10:33.990 --> 01:10:36.720
in more detail on Monday,
but essentially it's

01:10:36.720 --> 01:10:40.290
like people are risk-averse
in the gain domain,

01:10:40.290 --> 01:10:42.600
but risk-loving and
the loss region.

01:10:42.600 --> 01:10:43.720
What does it look like?

01:10:43.720 --> 01:10:45.930
Essentially the
utility function is

01:10:45.930 --> 01:10:48.390
a little bit different from
what I showed you before.

01:10:48.390 --> 01:10:50.370
It's not only like
more steep on the left

01:10:50.370 --> 01:10:52.800
relative to the reference
point compared to the right,

01:10:52.800 --> 01:10:57.090
but it's also concave on the
right and convex on the left.

01:10:57.090 --> 01:10:59.490
And what that gets you and
what that buys you essentially

01:10:59.490 --> 01:11:05.070
is that people are risk
averse in the gain domain,

01:11:05.070 --> 01:11:08.430
but people are risk-loving
in the loss domain,

01:11:08.430 --> 01:11:10.890
and that sort of is needed
to be able to explain some

01:11:10.890 --> 01:11:12.930
of the behaviors
that I showed you

01:11:12.930 --> 01:11:16.320
early on in the Kahneman
and Tversky evidence.

01:11:16.320 --> 01:11:19.440
So again, I'll tell you
about this in more detail

01:11:19.440 --> 01:11:22.043
and sort of write this
down more precisely.

01:11:22.043 --> 01:11:23.460
And then we're
going to talk about

01:11:23.460 --> 01:11:25.230
like many different
applications.

01:11:25.230 --> 01:11:27.180
And particularly, there's
the endowment effect

01:11:27.180 --> 01:11:29.760
which is kind of what I talked
to you already about before.

01:11:29.760 --> 01:11:31.808
There's labor
supply, employment,

01:11:31.808 --> 01:11:33.600
and effort depending
on what people expect,

01:11:33.600 --> 01:11:34.590
how much they earn.

01:11:34.590 --> 01:11:40.110
They make different choices
about how many hours

01:11:40.110 --> 01:11:42.120
they actually work depending
on the expectation,

01:11:42.120 --> 01:11:44.460
whether it's above or
below a reference point.

01:11:44.460 --> 01:11:46.230
People are reluctant
to sell their houses

01:11:46.230 --> 01:11:47.850
if they're at a loss
compared to again

01:11:47.850 --> 01:11:50.040
even for very similar houses.

01:11:50.040 --> 01:11:52.260
And marathon-running
people are trying

01:11:52.260 --> 01:11:54.480
to reach essentially
certain targets.

01:11:54.480 --> 01:11:56.580
People like to be below
four hours or three

01:11:56.580 --> 01:11:58.470
hours and the like.

01:11:58.470 --> 01:12:00.580
There's the disposition
effect that we mentioned,

01:12:00.580 --> 01:12:03.060
which essentially is
people like to sell winners

01:12:03.060 --> 01:12:05.280
compared to losers.

01:12:05.280 --> 01:12:08.230
And the insurance choice that
I already showed you before--

01:12:08.230 --> 01:12:13.800
there's some evidence on
violence in the household.

01:12:13.800 --> 01:12:17.940
The particular example is about
football or games essentially--

01:12:17.940 --> 01:12:20.310
I think football games
where essentially depending

01:12:20.310 --> 01:12:24.210
on whether people expected
a loss or win of their team

01:12:24.210 --> 01:12:29.490
and when then a loss or a win
of the team actually happens,

01:12:29.490 --> 01:12:32.220
there's more violence when there
are unexpected losses compared

01:12:32.220 --> 01:12:35.380
to expected losses
which is, again,

01:12:35.380 --> 01:12:38.590
consistent with loss aversion.

01:12:38.590 --> 01:12:39.090
What's next?

01:12:39.090 --> 01:12:41.215
So next we're going to talk
about many applications

01:12:41.215 --> 01:12:44.310
of reference
dependence on Monday.

01:12:44.310 --> 01:12:47.400
I'd like you to sort of read
the Kahneman and Tversky

01:12:47.400 --> 01:12:49.830
paper, at least the
first pages to get

01:12:49.830 --> 01:12:51.083
some sense of their work.

01:12:51.083 --> 01:12:52.500
And then on
Wednesday, we're going

01:12:52.500 --> 01:12:54.282
to start talking about
social preferences.

01:12:54.282 --> 01:12:56.490
In particular, we can have
some experiments in class.

01:12:56.490 --> 01:13:00.210
There's going to be
no readings for that.

01:13:00.210 --> 01:13:02.030
There's the opportunity
to make money

01:13:02.030 --> 01:13:04.720
in class, at least some of it.