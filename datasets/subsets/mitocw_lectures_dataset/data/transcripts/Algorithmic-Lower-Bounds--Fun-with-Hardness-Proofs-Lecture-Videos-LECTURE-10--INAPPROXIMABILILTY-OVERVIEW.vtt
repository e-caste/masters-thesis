WEBVTT

00:00:00.080 --> 00:00:02.430
The following content is
provided under a Creative

00:00:02.430 --> 00:00:03.810
Commons license.

00:00:03.810 --> 00:00:06.060
Your support will help
MIT OpenCourseWare

00:00:06.060 --> 00:00:10.150
continue to offer high-quality
educational resources for free.

00:00:10.150 --> 00:00:12.690
To make a donation, or to
view additional materials

00:00:12.690 --> 00:00:16.600
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:16.600 --> 00:00:17.258
at ocw.mit.edu.

00:00:25.744 --> 00:00:27.570
PROFESSOR: All
right, welcome back.

00:00:27.570 --> 00:00:29.710
Today, we start a
series of lectures

00:00:29.710 --> 00:00:31.820
on approximation algorithms.

00:00:31.820 --> 00:00:34.382
So something rather
different, although

00:00:34.382 --> 00:00:36.090
we're still going to
be doing reductions.

00:00:36.090 --> 00:00:39.820
They're just going to be
stronger sense of reductions.

00:00:39.820 --> 00:00:42.244
So let me start
with reminding you--

00:00:42.244 --> 00:00:43.660
I assume you've
seen approximation

00:00:43.660 --> 00:00:45.910
algorithms at some
level before, but I

00:00:45.910 --> 00:00:49.050
want to define a few
things which we'll

00:00:49.050 --> 00:00:51.510
need for the lower bound side.

00:00:56.580 --> 00:01:00.057
So pretty much throughout, we've
been-- in the context of NP,

00:01:00.057 --> 00:01:01.890
we've been thinking
about decision problems,

00:01:01.890 --> 00:01:03.590
because that's
where NP make sense.

00:01:03.590 --> 00:01:08.240
Now, we need to change the
setup to where the output is

00:01:08.240 --> 00:01:10.010
not just yes or no,
but it's some kind

00:01:10.010 --> 00:01:12.200
of solution with a cost.

00:01:12.200 --> 00:01:15.620
So in general, the goal
in an optimization problem

00:01:15.620 --> 00:01:20.150
is going to be to go
from some instance

00:01:20.150 --> 00:01:31.420
to a solution with
min or max cost.

00:01:31.420 --> 00:01:33.220
So there's
minimization problems,

00:01:33.220 --> 00:01:34.740
maximization problems.

00:01:34.740 --> 00:01:37.910
And so you are given--
in order to specify this,

00:01:37.910 --> 00:01:43.240
you're given a set of
possible instances.

00:01:43.240 --> 00:01:45.230
That's your usual
notion of input.

00:01:45.230 --> 00:01:56.540
And then for each instance, we
are given a set of solutions.

00:01:56.540 --> 00:01:59.120
Usually, these are
called feasible

00:01:59.120 --> 00:02:00.890
solutions or valid solutions.

00:02:00.890 --> 00:02:02.775
I'll just call them solutions.

00:02:02.775 --> 00:02:08.810
Those are the allowable
outputs to that problem.

00:02:08.810 --> 00:02:11.120
And then for each
of those solutions,

00:02:11.120 --> 00:02:12.560
we're going to define some cost.

00:02:12.560 --> 00:02:18.290
I will assume that
they are non-negative,

00:02:18.290 --> 00:02:20.950
so I don't have to
worry about signs.

00:02:20.950 --> 00:02:24.940
And then you're
also given what's

00:02:24.940 --> 00:02:29.270
usually called the objective,
which is either min or max.

00:02:31.961 --> 00:02:34.210
OK, so you're going to be
given an item from this set.

00:02:34.210 --> 00:02:36.770
You want to produce an item
from this set that minimizes

00:02:36.770 --> 00:02:39.095
or maximizes this cost.

00:02:39.095 --> 00:02:41.720
That's not going to be possible,
because all these problems are

00:02:41.720 --> 00:02:42.720
going to be NP complete.

00:02:42.720 --> 00:02:44.940
But the point of
approximation is

00:02:44.940 --> 00:02:47.890
to relax getting
the best solution,

00:02:47.890 --> 00:02:51.160
and aim to get an approximately
best solution, which

00:02:51.160 --> 00:02:55.570
we will define in a moment.

00:02:55.570 --> 00:02:59.890
But let me start with some
useful notation, which

00:02:59.890 --> 00:03:10.810
is opt of x, x is an instance,
this is going to be two things.

00:03:10.810 --> 00:03:14.590
No definition should
be unambiguous.

00:03:14.590 --> 00:03:22.782
So it's going to be the cost
of a min or a max solution.

00:03:28.730 --> 00:03:30.330
Cost solution.

00:03:30.330 --> 00:03:33.660
And it's also going to be such
a solution itself, sometimes.

00:03:36.870 --> 00:03:39.160
Cool.

00:03:39.160 --> 00:03:41.120
So this is sort of
the usual setup.

00:03:41.120 --> 00:03:45.190
Now let's make it a little bit
more interesting by defining

00:03:45.190 --> 00:03:46.870
an NP optimization problem.

00:03:53.090 --> 00:03:58.480
This is going to be the
analog of NP for optimization.

00:03:58.480 --> 00:04:03.630
So one thing is
that all solutions

00:04:03.630 --> 00:04:09.720
have polynomial length,
polynomial in the length

00:04:09.720 --> 00:04:11.285
of the input, the instance.

00:04:14.180 --> 00:04:20.920
Another thing is that
instances and solutions

00:04:20.920 --> 00:04:21.729
can be recognized.

00:04:30.902 --> 00:04:32.485
So there's a polynomial
time algorithm

00:04:32.485 --> 00:04:35.220
that tells you yes,
that's a solution, yes,

00:04:35.220 --> 00:04:38.340
that's an instance, or no.

00:04:38.340 --> 00:04:42.650
And let's see, the
cost function should

00:04:42.650 --> 00:04:45.360
be polynomially-computable.

00:04:45.360 --> 00:04:48.560
And that's it.

00:04:48.560 --> 00:04:51.160
OK, so just making
these actually

00:04:51.160 --> 00:04:53.900
reasonable from an
algorithmic standpoint.

00:04:53.900 --> 00:04:57.700
And so now, you know,
thinking like NP,

00:04:57.700 --> 00:05:02.640
the solution is going to be
something like the certificate

00:05:02.640 --> 00:05:04.580
that you had a yes answer.

00:05:04.580 --> 00:05:07.070
Here, it's going to be a
certificate that the cost is

00:05:07.070 --> 00:05:08.460
a particular thing.

00:05:08.460 --> 00:05:10.920
So given an empty
optimization problem,

00:05:10.920 --> 00:05:15.010
you can define a
decision problem,

00:05:15.010 --> 00:05:20.320
or decision version of NPO.

00:05:20.320 --> 00:05:27.450
So NPO is going to be the
class of all such problems, NP

00:05:27.450 --> 00:05:30.340
optimization problems.

00:05:30.340 --> 00:05:33.580
Just like NP was
all the problems

00:05:33.580 --> 00:05:37.170
solvable in nondeterministic
polynomial time.

00:05:37.170 --> 00:05:41.890
These things I claim, if
we convert to the decision

00:05:41.890 --> 00:05:47.471
version, which is for a min
problem, the question is

00:05:47.471 --> 00:05:53.760
is opt of x, the cost, less than
or equal to some query value.

00:05:53.760 --> 00:05:58.690
And for a max
version, it is is opt

00:05:58.690 --> 00:06:03.740
of x greater than or
equal to the query value.

00:06:03.740 --> 00:06:05.450
This thing is an NP.

00:06:08.030 --> 00:06:12.050
So let's see why.

00:06:12.050 --> 00:06:15.270
If you have these properties,
then the resulting decision

00:06:15.270 --> 00:06:18.490
problem will be an NP because
if I give you a solution,

00:06:18.490 --> 00:06:20.320
and solutions have
polynomial length,

00:06:20.320 --> 00:06:24.290
you can compute its cost, verify
that it was a valid solution,

00:06:24.290 --> 00:06:27.340
and then you know that
opt-- opt is some min,

00:06:27.340 --> 00:06:29.030
so it's going to be
less than or equal

00:06:29.030 --> 00:06:30.700
to any particular solution.

00:06:30.700 --> 00:06:34.040
So if I give you a solution,
then I verify that opt of x

00:06:34.040 --> 00:06:36.830
is less than or equal
to q in polynomial time.

00:06:36.830 --> 00:06:39.850
For max, it's the reverse.

00:06:39.850 --> 00:06:41.962
And for example, if
you wanted to check--

00:06:41.962 --> 00:06:43.670
if you wanted to know
whether the min was

00:06:43.670 --> 00:06:45.840
greater than or
equal to something,

00:06:45.840 --> 00:06:47.600
well that's a co NP problem.

00:06:47.600 --> 00:06:51.170
So that's-- these are the
correct decision versions

00:06:51.170 --> 00:06:53.980
of those NP optimization
problems in the sense that you

00:06:53.980 --> 00:06:56.230
get a problem in NP.

00:06:56.230 --> 00:06:59.770
So this is a strict
generalization, in some sense,

00:06:59.770 --> 00:07:04.380
of NP problems, or
specialization I suppose.

00:07:04.380 --> 00:07:08.670
This is a particular-- well,
we have optimization versions

00:07:08.670 --> 00:07:10.030
which still give us NP problems.

00:07:10.030 --> 00:07:13.390
We can still talk about NP
completeness of these problems.

00:07:13.390 --> 00:07:16.310
But we're interested--
we're setting all this up

00:07:16.310 --> 00:07:19.400
so we can actually talk about
costs of not just the best

00:07:19.400 --> 00:07:24.800
solution, but costs of
suboptimal solutions.

00:07:24.800 --> 00:07:26.930
So let's get to approximation.

00:07:41.500 --> 00:07:46.500
So we're going to call
an algorithm, ALG, a c

00:07:46.500 --> 00:07:47.950
approximation.

00:07:47.950 --> 00:07:51.096
I write c, but it's not
necessarily a constant.

00:07:51.096 --> 00:08:01.010
It could be a function of
N. If-- in the min case,

00:08:01.010 --> 00:08:05.910
what we want is the cost
of the algorithm applied

00:08:05.910 --> 00:08:12.690
to an input, x, divided by
the cost of the optimal of x.

00:08:12.690 --> 00:08:14.810
Here, for whatever
reason, I write cost

00:08:14.810 --> 00:08:16.960
of opt instead of just opt.

00:08:16.960 --> 00:08:18.690
That should be less
than or equal to c.

00:08:23.110 --> 00:08:25.220
We look at the ratio,
we want that to be

00:08:25.220 --> 00:08:26.970
bounded by some value.

00:08:26.970 --> 00:08:30.630
In the max case, there
are two definitions

00:08:30.630 --> 00:08:34.080
that are standard in
the literature that

00:08:34.080 --> 00:08:38.950
correspond to different ways
of looking at the same thing.

00:08:38.950 --> 00:08:50.200
One would be cost of OPT
divided by cost of ALG.

00:08:50.200 --> 00:08:51.720
That should be, at most, c.

00:08:51.720 --> 00:08:57.470
This would correspond to c being
greater than or equal to 1.

00:08:57.470 --> 00:09:01.150
And the alternative is to
flip the inequality instead

00:09:01.150 --> 00:09:02.570
of the ratio.

00:09:13.150 --> 00:09:14.550
First, let's think about min.

00:09:14.550 --> 00:09:16.790
With a minimization problem,
whatever the algorithm produces

00:09:16.790 --> 00:09:18.331
will be greater than
or equal to OPT.

00:09:18.331 --> 00:09:22.080
So this is a ratio that will
give something greater than

00:09:22.080 --> 00:09:22.930
or equal to 1.

00:09:22.930 --> 00:09:26.550
And so usually we think
about two approximation,

00:09:26.550 --> 00:09:29.140
1.5 approximation,
100 approximation,

00:09:29.140 --> 00:09:31.190
whatever, just saying
the algorithm is

00:09:31.190 --> 00:09:33.460
within a factor of
100 of the optimal.

00:09:33.460 --> 00:09:36.080
For maximization, the thing
that the algorithm produces

00:09:36.080 --> 00:09:38.090
will be less than
or equal to OPT.

00:09:38.090 --> 00:09:41.010
And so if you want something
that is greater than 1,

00:09:41.010 --> 00:09:43.520
you have to flip the ratio.

00:09:43.520 --> 00:09:47.670
In this situation, c would
be less than or equal to 1.

00:09:47.670 --> 00:09:53.590
So some people call-- if
you have an algorithm that

00:09:53.590 --> 00:09:56.859
produces a solution that is
at least 1/2 times optimal,

00:09:56.859 --> 00:09:58.400
you might call it
a two approximation

00:09:58.400 --> 00:10:00.150
or you might call it
a 1/2 approximation,

00:10:00.150 --> 00:10:01.600
depending on the paper.

00:10:01.600 --> 00:10:03.440
I think this is, by
now, more common,

00:10:03.440 --> 00:10:07.310
but it really depends on the
era and the person and so on.

00:10:07.310 --> 00:10:08.490
So good to be aware of both.

00:10:08.490 --> 00:10:10.198
It's measuring the
same thing, of course,

00:10:10.198 --> 00:10:12.500
just c is either bigger
than 1 or less than 1.

00:10:12.500 --> 00:10:13.000
Yeah?

00:10:13.000 --> 00:10:15.200
AUDIENCE: Is that
over all instances?

00:10:15.200 --> 00:10:17.170
PROFESSOR: Oh yeah, this
should be for all x.

00:10:22.340 --> 00:10:25.360
For all valid instances x.

00:10:25.360 --> 00:10:28.080
Good.

00:10:28.080 --> 00:10:41.150
So usually when we say a
c approximation algorithm,

00:10:41.150 --> 00:10:43.960
usually we're interested in
polynomial time c approximation

00:10:43.960 --> 00:10:44.620
algorithm.

00:10:44.620 --> 00:10:46.344
Almost all the time
that is the case.

00:10:46.344 --> 00:10:48.260
I will probably forget
to say polynomial time,

00:10:48.260 --> 00:10:51.560
but I always mean it
unless I say otherwise,

00:10:51.560 --> 00:10:52.880
which will probably be never.

00:10:52.880 --> 00:10:54.910
But there are a few papers
that look at exponential time

00:10:54.910 --> 00:10:56.618
approximation algorithms
that have better

00:10:56.618 --> 00:11:00.750
exponent than the exact
counterparts, and so on.

00:11:00.750 --> 00:11:04.250
So this is interesting
because while

00:11:04.250 --> 00:11:09.070
your-- the straight-up decision
problem, which is basically

00:11:09.070 --> 00:11:12.800
deciding OPT exactly,
that might be NP complete.

00:11:12.800 --> 00:11:15.050
The approximation version,
like finding a two

00:11:15.050 --> 00:11:17.160
approximate solution,
might be polynomial.

00:11:17.160 --> 00:11:20.760
And that would be observed
by finding a polynomial

00:11:20.760 --> 00:11:22.130
to approximation algorithm.

00:11:24.810 --> 00:11:32.390
Sometimes you can do even
better than con-- well.

00:11:32.390 --> 00:11:35.740
So here, we've been thinking
about constant factors for c.

00:11:35.740 --> 00:11:38.257
Sometimes you can do even
better than any constant factor,

00:11:38.257 --> 00:11:40.090
or you can achieve any
constant factor would

00:11:40.090 --> 00:11:42.760
be another way of saying it.

00:11:42.760 --> 00:11:46.821
This is called a polynomial time
approximation scheme, usually

00:11:46.821 --> 00:11:47.320
PTAS.

00:11:58.300 --> 00:12:00.610
You can think of this as a
1 plus epsilon approximation

00:12:00.610 --> 00:12:02.340
for any epsilon.

00:12:02.340 --> 00:12:04.880
But the general-- or
you can think of it

00:12:04.880 --> 00:12:07.060
as an algorithm that,
given epsilon, produces a 1

00:12:07.060 --> 00:12:09.010
plus epsilon
approximation algorithm.

00:12:09.010 --> 00:12:11.490
Or you can think of
it as an algorithm

00:12:11.490 --> 00:12:15.885
with an additional input, not
just the instance, but also

00:12:15.885 --> 00:12:19.200
a value epsilon.

00:12:19.200 --> 00:12:23.950
It's a rational epsilon
greater than zero.

00:12:23.950 --> 00:12:38.110
And then the-- let's say
produces a solution that's

00:12:38.110 --> 00:12:43.360
a 1 plus epsilon approximation.

00:12:47.240 --> 00:12:49.190
So this would be a sort
of ideal situation.

00:12:49.190 --> 00:12:51.800
You get to specify what
the error bound is,

00:12:51.800 --> 00:12:56.430
and the algorithm will
find a suitable solution.

00:12:56.430 --> 00:12:59.960
And the polynomial time
part is that this algorithm

00:12:59.960 --> 00:13:11.310
must be polynomial time for
any fixed epsilon, which

00:13:11.310 --> 00:13:13.990
means that the dependence on
epsilon could be horrible.

00:13:13.990 --> 00:13:15.640
You could have an
algorithm, if N

00:13:15.640 --> 00:13:19.336
is your input size, that runs
in something like N to the 2

00:13:19.336 --> 00:13:22.430
to the 2 to the 1 over
epsilon or whatever.

00:13:22.430 --> 00:13:26.370
That's polynomial time for
any fixed value of epsilon.

00:13:26.370 --> 00:13:29.500
It's rather large for any
reasonable value of epsilon,

00:13:29.500 --> 00:13:32.170
like a half, but anyway.

00:13:32.170 --> 00:13:35.470
Or even 1 is still
getting up there.

00:13:35.470 --> 00:13:36.820
But that's considered a P test.

00:13:36.820 --> 00:13:38.880
Now there are stronger
notions of P tests

00:13:38.880 --> 00:13:40.410
that prevent this kind of thing.

00:13:40.410 --> 00:13:42.493
We will get to that when
we get to fixed parameter

00:13:42.493 --> 00:13:43.820
tractability.

00:13:43.820 --> 00:13:47.690
But for now, this would be
considered the gold standard.

00:13:47.690 --> 00:13:52.240
This is the best you could hope
for at this level of detail.

00:13:52.240 --> 00:13:57.830
And for-- let me give
you some more classes.

00:13:57.830 --> 00:14:00.190
So we defined NPO.

00:14:00.190 --> 00:14:01.570
There's the class PTAS.

00:14:01.570 --> 00:14:10.810
Again, reuse of term, but this
is all problems-- NPO problems

00:14:10.810 --> 00:14:15.460
with a PTAS algorithm.

00:14:15.460 --> 00:14:27.140
And more generally, if we
have some class of functions,

00:14:27.140 --> 00:14:30.800
then I'm going to write
FAPX to be all the NPO

00:14:30.800 --> 00:14:45.650
problems with poly-time f
approximation algorithms.

00:14:45.650 --> 00:14:48.960
I'll write f of N
to be clear what

00:14:48.960 --> 00:15:04.970
we're depending on for some
little f in the class big F.

00:15:04.970 --> 00:15:09.510
So for example, f
of N could be 3.

00:15:09.510 --> 00:15:11.670
That would be a constant
factor approximation.

00:15:11.670 --> 00:15:14.220
In that case, we
just call it APX.

00:15:14.220 --> 00:15:18.209
APX is what you might otherwise
call order one APX, things

00:15:18.209 --> 00:15:20.250
that you can approximate
in some constant factor,

00:15:20.250 --> 00:15:22.120
any constant factor.

00:15:22.120 --> 00:15:26.200
Another class that's
commonly studied is log APX.

00:15:26.200 --> 00:15:33.789
This is log N approximable
some constant factor.

00:15:33.789 --> 00:15:34.580
And there are more.

00:15:34.580 --> 00:15:36.880
There's poly APX,
where you just want

00:15:36.880 --> 00:15:41.240
N to some constant, usually
less than one but not always.

00:15:41.240 --> 00:15:44.947
But I think this will
be enough for us.

00:15:44.947 --> 00:15:47.280
And so we're interested in
distinguishing these classes.

00:15:47.280 --> 00:15:49.650
We're going to do that by
defining reductions, using

00:15:49.650 --> 00:15:52.690
reductions, defining hardness,
and then getting the hardest

00:15:52.690 --> 00:15:54.550
problems in certains
of these classes

00:15:54.550 --> 00:15:58.880
to show that's the best kind
of approximability you can get.

00:15:58.880 --> 00:16:01.700
Today, we'll be thinking in
particular about the boundary

00:16:01.700 --> 00:16:04.580
between PTAS and APX.

00:16:04.580 --> 00:16:07.160
So PTAS, we can get 1 plus
epsilon for any epsilon.

00:16:07.160 --> 00:16:08.630
APX, you can get
a constant factor,

00:16:08.630 --> 00:16:10.170
but there's some
limit to how small

00:16:10.170 --> 00:16:13.570
that constant factor could be,
at least in the hardest case.

00:16:13.570 --> 00:16:18.030
APX includes PTAS, but there's
problems in APX minus PTAS.

00:16:18.030 --> 00:16:21.790
Those have a limit how
far down you can get.

00:16:21.790 --> 00:16:22.290
OK.

00:16:22.290 --> 00:16:29.240
We have PTAS is a subset of APX.

00:16:29.240 --> 00:16:34.640
And furthermore, if P is not
equal NP, it's a strict subset.

00:16:34.640 --> 00:16:40.140
And let's say log
APX or whatever.

00:16:40.140 --> 00:16:44.460
You pick your favorite
approximation factor,

00:16:44.460 --> 00:16:46.700
and there are problems
where you can achieve

00:16:46.700 --> 00:16:49.780
that and nothing better.

00:16:49.780 --> 00:16:53.310
It's actually an easy exercise
to come up with such a thing.

00:16:53.310 --> 00:16:58.980
This is if P does not equal NP.

00:16:58.980 --> 00:17:03.660
So you can take, I don't know,
Hamiltonian cycle or something,

00:17:03.660 --> 00:17:05.500
or pick your favorite
NP hard problem.

00:17:05.500 --> 00:17:08.160
And if the answer is
yes to that problem,

00:17:08.160 --> 00:17:11.785
then you construct a
solution with some cost.

00:17:11.785 --> 00:17:14.470
And if the answer is no,
it's way, way smaller.

00:17:14.470 --> 00:17:17.690
And so any approximation
within your desired factor

00:17:17.690 --> 00:17:21.829
will be hard to find.

00:17:21.829 --> 00:17:24.540
Let me get a little bit more
specific, and let's talk

00:17:24.540 --> 00:17:26.670
about some real problems.

00:17:26.670 --> 00:17:30.720
So in the world of graph
algorithm approximability,

00:17:30.720 --> 00:17:33.510
these are the typical kinds of
approximation factors you see.

00:17:33.510 --> 00:17:35.710
Of course, it depends
exactly what subdomain.

00:17:35.710 --> 00:17:38.890
This is not a complete list, but
it starts to give you a flavor.

00:17:38.890 --> 00:17:41.990
And today, we'll be thinking
mostly at the top level.

00:17:41.990 --> 00:17:47.290
But let me define some of
these problems for you.

00:17:47.290 --> 00:17:50.210
A lot of them we have seen,
or a few of them we have seen,

00:17:50.210 --> 00:17:53.140
such as Steiner tree.

00:17:53.140 --> 00:17:55.450
We talked about
rectilinear Steiner tree.

00:17:55.450 --> 00:17:56.940
That was a Euclidean problem.

00:17:56.940 --> 00:17:58.420
You were given
points in the plane,

00:17:58.420 --> 00:17:59.794
and you wanted to
connect them up

00:17:59.794 --> 00:18:03.790
by the shortest
connected network.

00:18:03.790 --> 00:18:08.916
In a graph-- so that problem
has a PTAS rectilinear Steiner

00:18:08.916 --> 00:18:10.540
tree, because it's
a Euclidean problem.

00:18:10.540 --> 00:18:13.030
A lot of Euclidean
problems have PTAS's.

00:18:13.030 --> 00:18:15.780
And I'm denoting PTAS
here by 1 plus epsilon.

00:18:15.780 --> 00:18:18.050
In general, epsilon here
means for all epsilon greater

00:18:18.050 --> 00:18:20.640
than zero.

00:18:20.640 --> 00:18:23.470
Steiner tree in a graph
is I give you a graph

00:18:23.470 --> 00:18:27.179
and you have a bunch of special
vertices, k special vertices.

00:18:27.179 --> 00:18:28.720
You want to find a
connected subgraph

00:18:28.720 --> 00:18:33.380
of that graph that hits all
of the special vertices.

00:18:33.380 --> 00:18:36.090
That problem has a constant
factor approximation,

00:18:36.090 --> 00:18:39.740
and there's no PTAS
for a general graph.

00:18:39.740 --> 00:18:41.380
Steiner forest is
almost the same.

00:18:41.380 --> 00:18:42.910
Instead of giving
vertices that all

00:18:42.910 --> 00:18:45.720
have to connect to each other,
you say which pairs of vertices

00:18:45.720 --> 00:18:47.220
need to connect to each other.

00:18:47.220 --> 00:18:49.610
This guy and this guy,
and this guy and this guy.

00:18:49.610 --> 00:18:51.319
You can still-- this
is a generalization.

00:18:51.319 --> 00:18:53.776
In the case where you want to
connect them in a [? Kleek ?]

00:18:53.776 --> 00:18:55.350
pattern, that's Steiner tree.

00:18:55.350 --> 00:18:59.770
But Steiner forest has
the same approximability

00:18:59.770 --> 00:19:01.449
up to constant factors.

00:19:01.449 --> 00:19:02.990
Traveling salesman
in a graph, you've

00:19:02.990 --> 00:19:05.490
probably seen a 1.5
approximation to that.

00:19:05.490 --> 00:19:08.499
Definitely two approximation
is really easy, like MST.

00:19:08.499 --> 00:19:10.790
And that's-- for in a graph,
that's the best you can do

00:19:10.790 --> 00:19:11.900
in 2D.

00:19:11.900 --> 00:19:15.390
Or in a planar graph or
in an H-minor free graph,

00:19:15.390 --> 00:19:17.490
there's a PTAS.

00:19:17.490 --> 00:19:22.530
I think H-minor free, traveling
salesman problem weighted was

00:19:22.530 --> 00:19:24.660
solved just a couple years ago.

00:19:24.660 --> 00:19:27.750
But that has a PTAS.

00:19:27.750 --> 00:19:29.150
Let's see.

00:19:29.150 --> 00:19:31.137
Usually most people
like to think

00:19:31.137 --> 00:19:32.720
about minimization
problems, but there

00:19:32.720 --> 00:19:34.882
are maximization problems, too.

00:19:34.882 --> 00:19:37.490
Let's see, what else
haven't I defined.

00:19:37.490 --> 00:19:40.910
Did we talk about set cover?

00:19:40.910 --> 00:19:43.310
You have-- I think
we did briefly.

00:19:43.310 --> 00:19:45.994
You have sets and
you have elements.

00:19:45.994 --> 00:19:48.410
You want to choose the fewest
sets that hits all elements.

00:19:48.410 --> 00:19:50.840
Each set contains
some of the elements.

00:19:50.840 --> 00:19:53.234
You can think of it as a
bipartite graph, sets on one

00:19:53.234 --> 00:19:54.400
side, elements on the other.

00:19:54.400 --> 00:19:56.170
You want to choose
the fewest vertices

00:19:56.170 --> 00:20:01.210
on the left that hit all
the vertices on the right.

00:20:01.210 --> 00:20:05.130
Dominating set is the
non-bipartite version

00:20:05.130 --> 00:20:05.820
of that problem.

00:20:05.820 --> 00:20:07.980
You're just given a graph.

00:20:07.980 --> 00:20:10.390
And if you choose a vertex,
it covers that vertex

00:20:10.390 --> 00:20:12.607
and its neighboring vertices.

00:20:12.607 --> 00:20:14.190
And your goal is to
cover all vertices

00:20:14.190 --> 00:20:16.220
using the smallest
dominating set,

00:20:16.220 --> 00:20:18.540
by choosing the fewest vertices.

00:20:18.540 --> 00:20:20.600
So these problems turn
out to be equivalent from

00:20:20.600 --> 00:20:22.310
an approximability standpoint.

00:20:22.310 --> 00:20:25.350
And in a stronger sense,
they're both log N approximable,

00:20:25.350 --> 00:20:27.410
and that's the best you can do.

00:20:27.410 --> 00:20:31.050
This is assuming p
does not equal NP.

00:20:31.050 --> 00:20:34.000
Some of these results assume
slightly stronger things

00:20:34.000 --> 00:20:39.410
than p versus NP, but we won't
worry about that too much.

00:20:39.410 --> 00:20:39.910
Let's see.

00:20:39.910 --> 00:20:43.280
Another fun problem-- I'm
just highlighting the ones you

00:20:43.280 --> 00:20:45.260
should know about.

00:20:45.260 --> 00:20:47.642
Chromatic number,
we've seen what we

00:20:47.642 --> 00:20:48.850
thought about three coloring.

00:20:48.850 --> 00:20:50.308
But the chromatic
number problem is

00:20:50.308 --> 00:20:52.230
to minimize the
number of colors, k,

00:20:52.230 --> 00:20:54.290
such that your graph
is k-colorable.

00:20:54.290 --> 00:20:57.231
And that's really
hard to approximate.

00:20:57.231 --> 00:20:58.980
The best approximation
algorithm, I think,

00:20:58.980 --> 00:21:03.200
is N divided by log N,
or something roughly N.

00:21:03.200 --> 00:21:04.140
That's not so good.

00:21:04.140 --> 00:21:05.590
And there's a lower
bound that you

00:21:05.590 --> 00:21:08.840
can't do better than N to the 1
minus epsilon for all epsilon.

00:21:08.840 --> 00:21:11.590
So you really--
that's pretty tight.

00:21:11.590 --> 00:21:15.700
Not completely tight,
but pretty close.

00:21:15.700 --> 00:21:20.056
And let's see.

00:21:20.056 --> 00:21:21.410
Other good problems.

00:21:21.410 --> 00:21:24.442
So maybe out here, another
really hard problem

00:21:24.442 --> 00:21:25.900
to approximate--
these are problems

00:21:25.900 --> 00:21:27.566
you should try to
avoid if you're trying

00:21:27.566 --> 00:21:29.170
to solve things approximately.

00:21:29.170 --> 00:21:30.750
Or if you're driving
hardness, you'd

00:21:30.750 --> 00:21:33.840
want to use these if you can,
if your problem looks like this.

00:21:33.840 --> 00:21:35.400
Independence set,
just find me a set

00:21:35.400 --> 00:21:37.800
of vertices that
induce no edges.

00:21:37.800 --> 00:21:39.310
So no edges connecting them.

00:21:39.310 --> 00:21:40.780
That's really hard.

00:21:40.780 --> 00:21:43.461
If you complement the
graph everywhere there's

00:21:43.461 --> 00:21:44.960
an edge deleted and
everywhere there

00:21:44.960 --> 00:21:47.620
wasn't an edge added, that's
the same problem as Kleek.

00:21:47.620 --> 00:21:53.400
So these are the same also from
an approximation standpoint.

00:21:53.400 --> 00:21:54.419
OK.

00:21:54.419 --> 00:21:55.960
This is annoying
because [? Kleeks ?]

00:21:55.960 --> 00:21:57.660
are useful in practice, but.

00:21:57.660 --> 00:21:59.910
There's another problem
called [? densest ?] subgraph,

00:21:59.910 --> 00:22:01.280
which is approximate
[? Kleeks. ?] That's actually

00:22:01.280 --> 00:22:01.955
easier to approximate.

00:22:01.955 --> 00:22:02.320
Yeah.

00:22:02.320 --> 00:22:04.069
AUDIENCE: What's the
hardness of something

00:22:04.069 --> 00:22:05.900
for the chromatic
number lower bound?

00:22:05.900 --> 00:22:09.860
PROFESSOR: I think that's
like P does not equal ZPP,

00:22:09.860 --> 00:22:12.920
or NP does not
equal ZPP, I think.

00:22:12.920 --> 00:22:13.670
Yeah.

00:22:13.670 --> 00:22:15.370
For that-- that's
strong lower bounds.

00:22:15.370 --> 00:22:17.828
There are weaker lower bounds
assuming P does not equal NP.

00:22:21.820 --> 00:22:22.320
OK.

00:22:22.320 --> 00:22:28.280
One other good one, fun one,
to think about is set cover.

00:22:28.280 --> 00:22:31.410
You want to choose the fewest
sets to hit all elements.

00:22:31.410 --> 00:22:35.215
Maximum coverage, you don't
have to hit all the elements

00:22:35.215 --> 00:22:37.290
but you want to hit as
many elements as possible

00:22:37.290 --> 00:22:39.450
using k sets.

00:22:39.450 --> 00:22:41.120
So in the decision
problem, these

00:22:41.120 --> 00:22:43.180
are the same in some sense.

00:22:43.180 --> 00:22:44.410
Here, I give you k sets.

00:22:44.410 --> 00:22:48.410
I want to know can I
hit all the elements.

00:22:48.410 --> 00:22:51.090
For example, here
I'm giving k sets

00:22:51.090 --> 00:22:55.350
and I want to know can I hit
at least J of the elements.

00:22:55.350 --> 00:22:58.160
So this you could think is a
generalization from a decision

00:22:58.160 --> 00:22:59.740
standpoint, but
it's actually easier

00:22:59.740 --> 00:23:02.410
to approximate because the
objective is different.

00:23:02.410 --> 00:23:05.980
Here we have to hit every
element, no questions asked.

00:23:05.980 --> 00:23:07.900
Here, it's OK if we miss
some of the elements

00:23:07.900 --> 00:23:11.277
because we only need a
constant factor approximation.

00:23:11.277 --> 00:23:12.860
And we can get a
constant factor here,

00:23:12.860 --> 00:23:15.488
whereas the best we
can do is log N here.

00:23:18.140 --> 00:23:19.510
Cool.

00:23:19.510 --> 00:23:23.450
But unique coverage,
this is a result of ours.

00:23:23.450 --> 00:23:25.452
If you're trying to
maximize coverage

00:23:25.452 --> 00:23:26.910
but if you double
cover an element,

00:23:26.910 --> 00:23:28.610
you no longer get points for it.

00:23:28.610 --> 00:23:31.230
That requires log
N approximation.

00:23:31.230 --> 00:23:35.120
So I think I will
leave it at that.

00:23:35.120 --> 00:23:38.010
Directed graphs are much
harder, although it's not

00:23:38.010 --> 00:23:38.980
known exactly how hard.

00:23:38.980 --> 00:23:42.500
Here's an example of a big gap
in what we between log squared

00:23:42.500 --> 00:23:45.260
and N to the epsilon.

00:23:45.260 --> 00:23:47.250
Something we will get
to in a later class

00:23:47.250 --> 00:23:48.370
is called label cover.

00:23:48.370 --> 00:23:50.161
I won't define the
problem here, because it

00:23:50.161 --> 00:23:51.275
takes a while to define.

00:23:51.275 --> 00:23:53.620
But there, there's very
strong lower bounds

00:23:53.620 --> 00:23:56.870
on-- in approximability.

00:23:56.870 --> 00:23:59.770
It's similar to N to the 1
minus epsilon, but it's not.

00:23:59.770 --> 00:24:00.710
It's a little smaller.

00:24:00.710 --> 00:24:05.520
So you see 2 to the log N would
be N. That would be great.

00:24:05.520 --> 00:24:07.600
And so you'd really
like 2 to the log N

00:24:07.600 --> 00:24:09.070
to the 1 minus epsilon.

00:24:09.070 --> 00:24:11.790
But what we have here is
something a little smaller

00:24:11.790 --> 00:24:15.330
than log N, log N to the
1 minus epsilon power.

00:24:15.330 --> 00:24:18.020
And then you exponentiate that.

00:24:18.020 --> 00:24:23.740
So this is somewhat smaller
than what we'd actually like,

00:24:23.740 --> 00:24:27.460
which is something like N to the
1 minus-- or N to some epsilon,

00:24:27.460 --> 00:24:29.230
I guess, would be ideal.

00:24:29.230 --> 00:24:31.899
This is smaller than
any N to the epsilon.

00:24:31.899 --> 00:24:33.940
The best upper bounds, it
depends on the problem.

00:24:33.940 --> 00:24:36.331
With N to some constant,
like for label cover,

00:24:36.331 --> 00:24:37.080
it's N to the 1/3.

00:24:37.080 --> 00:24:38.800
For directed Steiner
forest, which

00:24:38.800 --> 00:24:40.550
is you have pairs
of vertices you

00:24:40.550 --> 00:24:43.120
want to connect, but
with a directed path.

00:24:43.120 --> 00:24:45.316
So similar to a
problem we've seen.

00:24:45.316 --> 00:24:46.690
Best approximation
known for that

00:24:46.690 --> 00:24:48.440
is N to the 4/5 plus epsilon.

00:24:48.440 --> 00:24:51.040
And this is just a
couple years ago.

00:24:51.040 --> 00:24:53.339
So that seems very hard to
do better than any constant.

00:24:53.339 --> 00:24:55.380
Probably there's an N to
the epsilon lower bound,

00:24:55.380 --> 00:24:57.710
but this is the best we
know so far, a bit smaller.

00:25:01.170 --> 00:25:02.400
Cool.

00:25:02.400 --> 00:25:05.070
Any questions about that table?

00:25:05.070 --> 00:25:13.120
We will probably come back to it
a few times, but in more depth.

00:25:13.120 --> 00:25:15.860
So how do you prove
the lower bound side?

00:25:15.860 --> 00:25:17.450
We're going to use reductions.

00:25:17.450 --> 00:25:22.500
Now this field is a little bit
messier in terms of reductions.

00:25:22.500 --> 00:25:25.020
For NP completeness,
NP hardness, we just

00:25:25.020 --> 00:25:28.600
worried about [? carp ?]
style, one call reductions.

00:25:28.600 --> 00:25:30.140
That's all we had
to think about.

00:25:30.140 --> 00:25:36.270
In this universe, there
are 1, 2, 3, 4, 5, 6, 7, 8,

00:25:36.270 --> 00:25:41.800
9-- at least nine
definitions of reduction.

00:25:41.800 --> 00:25:43.470
You don't need to know them all.

00:25:43.470 --> 00:25:49.420
I will define four
of them, I think.

00:25:49.420 --> 00:25:51.822
They're all very
similar, and they all

00:25:51.822 --> 00:25:53.780
lead to slightly different
notions of hardness.

00:25:53.780 --> 00:25:55.196
So you have to be
a little careful

00:25:55.196 --> 00:25:58.080
when you say oh, this
problem is something hard.

00:25:58.080 --> 00:26:00.020
But I'll give you
some definitions

00:26:00.020 --> 00:26:03.520
that are pretty easy
to work with, I think.

00:26:03.520 --> 00:26:06.940
In general, this
family is called

00:26:06.940 --> 00:26:10.510
approximation-preserving
reductions.

00:26:18.970 --> 00:26:25.430
So let's say we want to
go from some problem A

00:26:25.430 --> 00:26:34.000
to some problem B. So you're
given an instance of A,

00:26:34.000 --> 00:26:40.250
let's call it x, and you want to
produce an instance of B. Let's

00:26:40.250 --> 00:26:43.990
call it x prime.

00:26:43.990 --> 00:26:45.790
And we're going to do
that by a function f,

00:26:45.790 --> 00:26:49.580
so x prime is f of x.

00:26:49.580 --> 00:26:53.810
So far, just like NP reductions.

00:26:53.810 --> 00:26:57.850
But now-- usually what we said
is that the answer to x equals

00:26:57.850 --> 00:26:59.510
the answer to x prime.

00:26:59.510 --> 00:27:02.500
So you could say OPT of x
equals the OPT of x prime.

00:27:02.500 --> 00:27:04.800
But that's not going
to be strong enough.

00:27:04.800 --> 00:27:08.960
What we want is that, if we
can find a solution to this B

00:27:08.960 --> 00:27:16.710
problem-- let's call it
y-prime to the problem x-prime.

00:27:16.710 --> 00:27:18.220
So the x-prime is
an instance of B.

00:27:18.220 --> 00:27:21.540
You can find some solution-- any
solution y-prime-- to x-prime,

00:27:21.540 --> 00:27:23.180
then we want to be
able to convert it

00:27:23.180 --> 00:27:32.430
back by a function, g,
into a solution to A, which

00:27:32.430 --> 00:27:33.990
we'll call y.

00:27:33.990 --> 00:27:38.420
And so it's g-- you might
think it's g of y-prime,

00:27:38.420 --> 00:27:40.890
but we'll make it g
of x comma y prime.

00:27:40.890 --> 00:27:45.390
So it can also depend on
what you started with.

00:27:45.390 --> 00:27:47.800
So this is the general flavor.

00:27:47.800 --> 00:27:50.570
A reduction consists
of two steps.

00:27:50.570 --> 00:27:52.560
First, like before, we
convert instances of A

00:27:52.560 --> 00:27:55.010
to instances of B.
Now in addition,

00:27:55.010 --> 00:27:57.570
we want to be able to
recover solutions of B

00:27:57.570 --> 00:28:01.590
into similarly good
solutions to A.

00:28:01.590 --> 00:28:05.980
And there's many ways to
define similarly good.

00:28:05.980 --> 00:28:14.730
Let me give you one.

00:28:21.000 --> 00:28:27.000
So if we're just interested
in PTAS's, then--

00:28:27.000 --> 00:28:30.040
like you want to know does
my problem have a PTAS

00:28:30.040 --> 00:28:33.290
or I want to prove
impossibility of PTAS's, then

00:28:33.290 --> 00:28:37.230
I think there's one clear
notion of reduction,

00:28:37.230 --> 00:28:40.015
which is obviously enough
called PTAS reduction.

00:28:45.100 --> 00:28:55.070
So what we want to say is--
I hope you've seen calculus

00:28:55.070 --> 00:28:56.196
at some point.

00:28:56.196 --> 00:28:59.050
In calculus, there's this
notion of epsilon delta proofs,

00:28:59.050 --> 00:29:00.660
or definitions.

00:29:00.660 --> 00:29:03.230
So I'm going to say-- you don't
have to think of it this way,

00:29:03.230 --> 00:29:06.010
but I find it useful to
think of it this way.

00:29:06.010 --> 00:29:09.390
Let me get to what
the statement is.

00:29:09.390 --> 00:29:21.090
If y-prime is a 1 plus
delta approximation to B,

00:29:21.090 --> 00:29:39.190
then y is a 1 plus epsilon
approximation to A.

00:29:39.190 --> 00:29:43.230
So ultimately we're
interested in PTAS's, and we

00:29:43.230 --> 00:29:46.630
want to say that if you
have a PTAS for B, then

00:29:46.630 --> 00:29:48.316
you get a PTAS for A.

00:29:48.316 --> 00:29:50.190
It used to be, when we
did a reduction from A

00:29:50.190 --> 00:29:55.110
to B, that showed that if B had
a polynomial time algorithm,

00:29:55.110 --> 00:29:58.470
then so did A, because you just
plug in these chains together.

00:29:58.470 --> 00:30:00.640
You convert A into
the B instance,

00:30:00.640 --> 00:30:02.710
you run your poly algorithm
and get a solution,

00:30:02.710 --> 00:30:04.554
and convert it back.

00:30:04.554 --> 00:30:06.470
Well, with NP, we didn't
even have to convert.

00:30:06.470 --> 00:30:07.470
The answer was the same.

00:30:07.470 --> 00:30:08.772
Now we could do that.

00:30:08.772 --> 00:30:10.480
We want to do the same
thing with PTAS's.

00:30:10.480 --> 00:30:14.205
So if we have a PTAS for
B, we can convert A into B,

00:30:14.205 --> 00:30:17.300
run the PTAS, get a solution,
and come back and get--

00:30:17.300 --> 00:30:19.750
what we want is for it
to be a 1 plus epsilon

00:30:19.750 --> 00:30:22.480
approximation to A.
So that's the goal,

00:30:22.480 --> 00:30:24.400
1 plus epsilon approximation.

00:30:24.400 --> 00:30:25.850
And what this is
saying is there's

00:30:25.850 --> 00:30:29.900
some function delta of
epsilon-- for any epsilon,

00:30:29.900 --> 00:30:33.760
there's some value
delta where you

00:30:33.760 --> 00:30:38.110
can give that value
to the PTAS for B,

00:30:38.110 --> 00:30:42.500
and it will give you
what you want for A.

00:30:42.500 --> 00:30:48.210
Because PTAS is supposed to
run for any fixed constant--

00:30:48.210 --> 00:30:51.060
we called it epsilon, but
it could also be delta.

00:30:51.060 --> 00:30:53.790
And so you plug in
this to the PTAS.

00:30:53.790 --> 00:30:55.710
You will get a 1 plus
delta approximation

00:30:55.710 --> 00:30:57.080
to your B problem.

00:30:57.080 --> 00:31:00.050
And then what we want is
that this conversion-- so y

00:31:00.050 --> 00:31:03.380
here is g of xy-prime.

00:31:05.990 --> 00:31:08.266
So we want g-- this
is a constraint on g.

00:31:08.266 --> 00:31:11.730
We want g to have the property
that if y-prime is that good,

00:31:11.730 --> 00:31:15.690
then y will still be
as good as we need.

00:31:15.690 --> 00:31:20.490
So maybe you just-- you want
to get a 1.1 approximation.

00:31:20.490 --> 00:31:22.660
You want to get within
10% of the right answer.

00:31:22.660 --> 00:31:25.070
Maybe you have to call B
with a much smaller thing.

00:31:25.070 --> 00:31:29.500
Maybe delta is 0.01.

00:31:29.500 --> 00:31:32.030
You need to be within
1% of the right answer.

00:31:32.030 --> 00:31:35.480
But there's-- for
a PTAS reduction,

00:31:35.480 --> 00:31:38.300
you want there to be some
approximability you asked

00:31:38.300 --> 00:31:41.374
for so that you get
what you actually want.

00:31:41.374 --> 00:31:43.540
This is sort of the obvious--
this the natural thing

00:31:43.540 --> 00:31:46.740
if you want to preserve
PTAS-ness in this direction,

00:31:46.740 --> 00:31:49.830
from B to A.

00:31:49.830 --> 00:31:53.940
One slight note is
that we also allow f

00:31:53.940 --> 00:31:59.060
and g to depend on epsilon.

00:31:59.060 --> 00:32:00.890
So this reduction-- it
would be hard to get

00:32:00.890 --> 00:32:05.550
this property for all epsilon
with the same instance

00:32:05.550 --> 00:32:06.432
conversion.

00:32:06.432 --> 00:32:08.390
So now we allow the
conversion of the instances

00:32:08.390 --> 00:32:12.220
to depend on epsilon, so you
can do fun things that way

00:32:12.220 --> 00:32:14.790
for the purposes
of PTAS reductions.

00:32:14.790 --> 00:32:21.350
But the key thing we get is
that if B is a PTAS, then--

00:32:21.350 --> 00:32:23.120
or has a PTAS--
then A as a PTAS,

00:32:23.120 --> 00:32:25.490
so it's in the
complexity class PTAS.

00:32:25.490 --> 00:32:28.560
Of course, what we care about
is the contrapositive of that.

00:32:28.560 --> 00:32:35.680
So if A is not in PTAS,
then B is not in PTAS.

00:32:35.680 --> 00:32:38.370
So we can use this-- we
reduced from our hard problem

00:32:38.370 --> 00:32:41.070
that we know is not in PTAS,
assuming P does not equal NP,

00:32:41.070 --> 00:32:44.530
and we get that our new
problem B is not in PTAS.

00:32:44.530 --> 00:32:46.360
That's the point.

00:32:49.210 --> 00:32:51.020
This is also true for
something like APX.

00:32:54.580 --> 00:32:57.200
So if you just want constant
factor approximation,

00:32:57.200 --> 00:32:59.010
this will convert
one constant factor

00:32:59.010 --> 00:33:01.880
into another constant factor.

00:33:01.880 --> 00:33:06.580
So yeah, if you
have-- I mean, it's

00:33:06.580 --> 00:33:09.000
maybe stronger than what
you need, but will give you

00:33:09.000 --> 00:33:11.340
constant factor
approximations, as well.

00:33:23.050 --> 00:33:25.330
So here's some easy
more definitions.

00:33:28.438 --> 00:33:32.250
Oh, OK, so a few more fun facts.

00:33:32.250 --> 00:33:34.970
If this statement
also holds true

00:33:34.970 --> 00:33:38.170
for epsilon equals 0 when you
plug in epsilon equals zero,

00:33:38.170 --> 00:33:40.420
delta equals zero,
then you're just

00:33:40.420 --> 00:33:43.300
getting the regular notion
of reduction from NP land.

00:33:43.300 --> 00:33:45.970
This is saying the
OPTs are equal.

00:33:45.970 --> 00:33:48.790
So usually your solution
also works in that setting.

00:33:48.790 --> 00:33:50.530
That's usually the
easy thing to do.

00:33:50.530 --> 00:33:54.620
So this is a strictly stronger
notion of NP reduction

00:33:54.620 --> 00:33:58.810
if you allow the 0,0 case.

00:33:58.810 --> 00:34:00.570
Also, reductions change.

00:34:00.570 --> 00:34:02.590
So if A reduces to B
and B reduces to C,

00:34:02.590 --> 00:34:05.635
then A reduces to C. All the
usual things you'd expect.

00:34:09.030 --> 00:34:14.159
Some special cases of
interest are AP reduction.

00:34:14.159 --> 00:34:16.664
This is when the delta
function is linear in epsilon.

00:34:21.929 --> 00:34:25.270
So this is nice because
it tells you, for example,

00:34:25.270 --> 00:34:32.630
if B is in-- has some order
f approximation, then A does,

00:34:32.630 --> 00:34:34.130
as well.

00:34:34.130 --> 00:34:38.210
It just changes this
constant factor.

00:34:38.210 --> 00:34:42.360
So this is useful if you care
about log N approximation.

00:34:42.360 --> 00:34:45.360
If you use this
definition, things

00:34:45.360 --> 00:34:47.170
could change, because
you're allowed

00:34:47.170 --> 00:34:52.580
to change what the factor
is out here in crazy ways.

00:34:52.580 --> 00:34:56.130
So over here, [? before-- ?] so
[? it's ?] just a linear blow

00:34:56.130 --> 00:34:58.970
up in the approximation factor,
then we only lose constant

00:34:58.970 --> 00:35:02.010
factors in approximability.

00:35:02.010 --> 00:35:05.080
And one more is
strict reduction.

00:35:08.600 --> 00:35:10.900
This is when there's no blowup.

00:35:13.730 --> 00:35:18.697
If you have a c
approximation here,

00:35:18.697 --> 00:35:20.030
you get a c approximation there.

00:35:23.060 --> 00:35:24.510
That's nice when you can get it.

00:35:24.510 --> 00:35:26.300
We're not going to aim
for this in particular,

00:35:26.300 --> 00:35:27.410
but there are a
bunch of reductions

00:35:27.410 --> 00:35:29.260
we'll talk about
today that are strict.

00:35:29.260 --> 00:35:31.926
So it's nice to just have a word
to say oh, this is even strict.

00:35:31.926 --> 00:35:35.490
You don't even need any
[? blowup ?] like this stuff.

00:35:35.490 --> 00:35:41.990
So that was strict AP
and PTAS reductions.

00:35:46.570 --> 00:35:51.585
There's one more, but maybe
first let me define hardness.

00:36:03.500 --> 00:36:07.800
So today, we will
focus on APX hardness.

00:36:07.800 --> 00:36:16.640
And these are supposed to be
the hardest problems in APX.

00:36:23.660 --> 00:36:25.600
And so, I mean,
because we're trying

00:36:25.600 --> 00:36:30.370
to distinguish between
PTAS-able problems and just

00:36:30.370 --> 00:36:33.150
constant factor
approximable problems,

00:36:33.150 --> 00:36:37.660
we are interested
in PTAS reductions.

00:36:37.660 --> 00:36:42.270
So it's going to turn out
that APX hard-- well I

00:36:42.270 --> 00:36:44.230
guess we sort of already know.

00:36:44.230 --> 00:36:51.310
This implies you're not in PTAS
if P equals NP-- P does not

00:36:51.310 --> 00:36:57.090
equal NP-- because of
this strict containment,

00:36:57.090 --> 00:37:00.710
P does not equal NP, then
PTAS is different from APX.

00:37:00.710 --> 00:37:03.880
And so if you show your
hardest problem in APX,

00:37:03.880 --> 00:37:07.210
using a reduction that
preserves PTAS-ability,

00:37:07.210 --> 00:37:12.600
then you know-- I mean, the
idea is that-- all right.

00:37:12.600 --> 00:37:15.280
When we're doing
these reductions,

00:37:15.280 --> 00:37:18.170
we know that if B had a
PTAS then A had a PTAS.

00:37:18.170 --> 00:37:21.330
And what we're saying is you
can re-- your APX hard means

00:37:21.330 --> 00:37:26.270
you can reduce from any
problem in APX to your problem.

00:37:26.270 --> 00:37:30.200
So if your problem had a PTAS,
that would mean all problems

00:37:30.200 --> 00:37:30.960
had PTAS's.

00:37:30.960 --> 00:37:32.560
All APX problems had PTAS's.

00:37:32.560 --> 00:37:35.010
But we know that's not the case.

00:37:35.010 --> 00:37:37.940
Therefore, your problem
does not have a PTAS.

00:37:37.940 --> 00:37:40.080
So it's just like
MP completeness,

00:37:40.080 --> 00:37:41.310
just with different letters.

00:37:44.020 --> 00:37:44.520
Cool.

00:37:44.520 --> 00:37:45.561
And different reductions.

00:37:48.396 --> 00:37:49.770
These reductions
are all a little

00:37:49.770 --> 00:37:54.405
bit awkward to work
with directly for--

00:37:54.405 --> 00:37:56.590
or these definitions
are a little awkward

00:37:56.590 --> 00:38:01.130
to check directly, although
ultimately it's the same thing.

00:38:01.130 --> 00:38:04.240
In practice, people seem
to use a different notion

00:38:04.240 --> 00:38:09.160
of reduction, for the most
part, called L reductions.

00:38:09.160 --> 00:38:14.260
That's stronger than AP
reduction, not as strong

00:38:14.260 --> 00:38:16.820
as strict.

00:38:16.820 --> 00:38:20.060
I guess it's not directly
related either way to strict,

00:38:20.060 --> 00:38:21.430
but, it's OK.

00:38:29.960 --> 00:38:33.400
So while I define this in
terms of PTAS reduction,

00:38:33.400 --> 00:38:36.540
because that's what you
need to get this result,

00:38:36.540 --> 00:38:38.040
most people think
about L reduction,

00:38:38.040 --> 00:38:55.590
which is going to imply--
it's going to be stronger

00:38:55.590 --> 00:38:58.815
than the other two notions
of reduction, AP and PTAS.

00:39:05.360 --> 00:39:07.010
So what's the definition?

00:39:07.010 --> 00:39:09.100
We want to satisfy
two properties.

00:39:09.100 --> 00:39:17.650
The first property is a kind
of blowup, going left to right.

00:39:26.310 --> 00:39:27.930
Funnily enough, the
other reductions

00:39:27.930 --> 00:39:29.520
do not have this property,
because they don't quite

00:39:29.520 --> 00:39:31.103
need it in the way
that they're going.

00:39:31.103 --> 00:39:36.030
But we want that the transform
problem, its optimal solution

00:39:36.030 --> 00:39:36.907
is not much bigger.

00:39:36.907 --> 00:39:38.490
It's bigger by only
a constant factor.

00:39:38.490 --> 00:39:40.239
This is, like, less
than or equal to alpha

00:39:40.239 --> 00:39:44.190
times the optimal-- the
original optimal solution given

00:39:44.190 --> 00:39:51.080
instance, so A.
Second property--

00:39:51.080 --> 00:39:56.860
and we'll see why we
need this in a moment--

00:39:56.860 --> 00:40:00.310
is if you look at the
absolute error instead

00:40:00.310 --> 00:40:07.690
of the ratio between the
computed solution y--

00:40:07.690 --> 00:40:11.050
and y was the thing that we
got in the lower left corner,

00:40:11.050 --> 00:40:15.550
we produced the solution
y-- we look at the cost

00:40:15.550 --> 00:40:18.794
that we get out of--
y is the solution.

00:40:18.794 --> 00:40:20.460
So we look at the
cost of that solution,

00:40:20.460 --> 00:40:22.000
we compare it to
the optimal solution

00:40:22.000 --> 00:40:23.541
to our original
instance-- because it

00:40:23.541 --> 00:40:25.910
is a solution to that instance.

00:40:25.910 --> 00:40:29.290
That should not be much
bigger than the error

00:40:29.290 --> 00:40:30.980
on the right side.

00:40:30.980 --> 00:40:33.060
Again, absolute error.

00:40:33.060 --> 00:40:41.940
So we want this to be big O of
cost in B's problem of y-prime

00:40:41.940 --> 00:40:47.230
minus OPT of B of x prime.

00:40:50.850 --> 00:40:52.337
So these are both in B land.

00:40:52.337 --> 00:40:53.920
Again, we look at
the optimal solution

00:40:53.920 --> 00:40:57.475
to the produced instance
versus some solution

00:40:57.475 --> 00:40:58.334
that we were given.

00:40:58.334 --> 00:41:00.000
We didn't produce
that solution, so this

00:41:00.000 --> 00:41:04.500
has to hold no matter
what solution y-prime

00:41:04.500 --> 00:41:07.960
you're given to the instance
x-prime, when you convert it

00:41:07.960 --> 00:41:12.520
using g to a solution
to x in problem A,

00:41:12.520 --> 00:41:17.480
you want that the gap--
absolute gap-- is not stretched

00:41:17.480 --> 00:41:19.770
by more than a constant factor.

00:41:19.770 --> 00:41:22.820
Again, this is at most some
constant times [? that ?]

00:41:22.820 --> 00:41:24.690
[? thing ?].

00:41:24.690 --> 00:41:25.410
OK?

00:41:25.410 --> 00:41:30.610
Now it should not be obvious
that this implies this,

00:41:30.610 --> 00:41:39.540
but it does, with delta equal
to, I think-- get some color.

00:41:39.540 --> 00:41:42.365
Let's say the constant
in here is alpha,

00:41:42.365 --> 00:41:45.840
the constant here is beta.

00:41:45.840 --> 00:41:50.510
And this should be alpha
times beta times epsilon.

00:41:50.510 --> 00:41:53.460
That is delta of epsilon.

00:41:53.460 --> 00:41:55.250
That's the claim.

00:41:55.250 --> 00:42:01.560
Let's prove the claim,
ideally without cheating.

00:42:01.560 --> 00:42:11.300
So I claim that if I
have such an L reduction,

00:42:11.300 --> 00:42:16.190
that this holds where-- when
delta is that linear function

00:42:16.190 --> 00:42:18.120
of epsilon.

00:42:18.120 --> 00:42:23.200
So I want to conclude that y
is a pretty good approximation.

00:42:23.200 --> 00:42:29.020
So to do that, I will look at
the cost of y divided by OPT.

00:42:35.120 --> 00:42:39.010
OPT of X would be
the original input.

00:42:39.010 --> 00:42:43.170
So we want to show that this
is, at most, 1 plus epsilon.

00:42:43.170 --> 00:42:44.976
Now what do we know?

00:42:44.976 --> 00:42:46.600
I should write A, I
guess, because this

00:42:46.600 --> 00:42:51.020
is all about problem
A. What do we know?

00:42:51.020 --> 00:42:56.550
We know this property-- I
wrote absolute value here,

00:42:56.550 --> 00:42:59.210
so this works for
maximization and minimization.

00:42:59.210 --> 00:43:00.642
You can even
convert minimization

00:43:00.642 --> 00:43:02.600
to maximization problems
using this definition.

00:43:02.600 --> 00:43:05.550
We will do that at some point.

00:43:05.550 --> 00:43:06.050
OK.

00:43:06.050 --> 00:43:11.320
So let's just try to
plug this into here.

00:43:11.320 --> 00:43:14.380
So I think I need
to split into cases,

00:43:14.380 --> 00:43:16.870
and I'll just worry about
the minimization case.

00:43:16.870 --> 00:43:20.650
So let's say that cost
of A is bigger than OPT.

00:43:20.650 --> 00:43:27.020
So then cost of A is going to
be OPT plus this thing, at most.

00:43:27.020 --> 00:43:41.079
So this is going to be at
most OPT of x plus beta--

00:43:41.079 --> 00:43:42.745
I'm going to not use
the big O notation,

00:43:42.745 --> 00:43:45.560
and just write the beta thing.

00:43:45.560 --> 00:43:49.390
Beta times-- I guess there's
no absolute value needed,

00:43:49.390 --> 00:43:51.670
because this is a
minimization problem.

00:43:51.670 --> 00:44:03.361
Let's say-- so this will be cos
sub B of y-prime minus OPT of B

00:44:03.361 --> 00:44:03.860
of x-prime.

00:44:06.850 --> 00:44:09.600
So that's what I get from
expanding out the numerator,

00:44:09.600 --> 00:44:13.150
and the denominator is the same.

00:44:13.150 --> 00:44:13.650
Clear?

00:44:16.300 --> 00:44:18.901
Just rearranging terms
here, essentially.

00:44:18.901 --> 00:44:19.400
OK.

00:44:19.400 --> 00:44:28.600
Now these two terms cancel,
so we get 1 plus that.

00:44:28.600 --> 00:44:32.210
That's good, because I
wanted that to be epsilon.

00:44:32.210 --> 00:44:32.710
Let's see.

00:44:32.710 --> 00:44:34.570
What else do we know.

00:44:34.570 --> 00:44:37.710
It's 1 plus this thing.

00:44:37.710 --> 00:44:40.505
Let me scroll down.

00:44:51.599 --> 00:44:52.890
OK, we have one other property.

00:44:52.890 --> 00:44:54.080
We've got to use it.

00:44:54.080 --> 00:44:56.460
The other property
is that OPT sub

00:44:56.460 --> 00:45:02.980
B is related to OPT sub A.
Now we have OPT sub A here,

00:45:02.980 --> 00:45:05.730
but it's in the denominator,
which flips the relation.

00:45:05.730 --> 00:45:08.990
So we can write that relation
up there as OPT sub A of x

00:45:08.990 --> 00:45:12.130
is omega of OPT
sub B of x-prime.

00:45:12.130 --> 00:45:14.470
That's the same
as statement one.

00:45:14.470 --> 00:45:16.750
And because the
denominator is omega,

00:45:16.750 --> 00:45:20.540
that means the whole thing is
big L. So this is going to be,

00:45:20.540 --> 00:45:28.880
at most, one plus-- so
now we have this thing.

00:45:28.880 --> 00:45:31.080
So the numerator is the same.

00:45:31.080 --> 00:45:37.623
We had before beta times
cost sub B of y-prime minus

00:45:37.623 --> 00:45:40.210
OPT sub B of x-prime.

00:45:40.210 --> 00:45:42.350
So that's unchanged.

00:45:42.350 --> 00:45:45.580
But now, instead of
dividing by OPT sub A,

00:45:45.580 --> 00:45:52.117
we're going to divide
by OPT sub B of x-prime.

00:45:52.117 --> 00:45:54.700
And we lost a constant factor,
that constant factor translated

00:45:54.700 --> 00:45:56.840
to B alpha in the
numerator because we

00:45:56.840 --> 00:45:58.380
inverted the equation.

00:45:58.380 --> 00:46:00.150
We divided by the
constant factor there.

00:46:03.390 --> 00:46:10.520
So what-- well now
this cancels with that.

00:46:10.520 --> 00:46:14.240
So this is going
to be 1 plus alpha

00:46:14.240 --> 00:46:27.890
beta times cost sub B y-prime
over OPT sub B of x-prime

00:46:27.890 --> 00:46:29.430
minus 1.

00:46:29.430 --> 00:46:31.440
Still looks kind of weird.

00:46:31.440 --> 00:46:33.210
But what we-- there's
one more thing

00:46:33.210 --> 00:46:36.900
we didn't use, which is we
assumed the-- we were given

00:46:36.900 --> 00:46:40.270
some solution y-prime that
was a 1 plus delta of epsilon

00:46:40.270 --> 00:46:42.544
approximation, where
delta is this thing

00:46:42.544 --> 00:46:43.460
that we defined there.

00:46:43.460 --> 00:46:46.590
It's obviously set to cancel out
everything that happened here.

00:46:46.590 --> 00:46:54.132
So this thing should be,
at most, 1 plus delta,

00:46:54.132 --> 00:46:55.840
because this is exactly
the approximation

00:46:55.840 --> 00:46:59.700
ratio for y-prime versus the
optimal solution to x-prime.

00:46:59.700 --> 00:47:07.910
And 1 plus delta is 1
plus alpha beta epsilon.

00:47:07.910 --> 00:47:12.890
So this 1 cancels with that
1, these alpha betas-- whoops.

00:47:12.890 --> 00:47:14.270
AUDIENCE: I think
it's backwards.

00:47:14.270 --> 00:47:16.030
PROFESSOR: Which is backwards?

00:47:16.030 --> 00:47:18.274
The definition of delta?

00:47:18.274 --> 00:47:19.021
OK.

00:47:19.021 --> 00:47:19.520
Whoops.

00:47:25.250 --> 00:47:29.270
Let's try epsilon
over alpha beta.

00:47:33.450 --> 00:47:37.920
That makes sense, because delta
should be smaller than epsilon

00:47:37.920 --> 00:47:42.810
probably, and alpha and beta
are probably bigger than 1.

00:47:42.810 --> 00:47:47.920
So now we get epsilon
over alpha beta here.

00:47:47.920 --> 00:47:50.830
And then that alpha beta
cancels with that alpha beta,

00:47:50.830 --> 00:47:54.420
and we are left
with 1 plus epsilon.

00:47:54.420 --> 00:47:56.380
OK.

00:47:56.380 --> 00:48:00.339
So this is why it's not
obvious, but it's just plugging

00:48:00.339 --> 00:48:01.630
in all the things and it works.

00:48:01.630 --> 00:48:03.890
And the funny thing is,
somehow L reductions,

00:48:03.890 --> 00:48:07.095
though they-- a little bit
less natural, I feel like.

00:48:07.095 --> 00:48:09.720
I mean, if you're thinking about
constant factor approximation,

00:48:09.720 --> 00:48:12.146
why should you care
about absolute error?

00:48:12.146 --> 00:48:14.520
The short answer that I've
seen written in various papers

00:48:14.520 --> 00:48:16.790
is because it's easier
to think that way

00:48:16.790 --> 00:48:20.900
for a lot of the typical
reductions that you do.

00:48:20.900 --> 00:48:25.640
So let us do some
of those reductions.

00:48:25.640 --> 00:48:28.340
And we'll get some intuition.

00:48:28.340 --> 00:48:32.440
L reductions also-- they
do work in the zero case.

00:48:32.440 --> 00:48:34.960
If you have an optimal
solution to x-prime,

00:48:34.960 --> 00:48:36.910
you will get an
optimal solution to x.

00:48:36.910 --> 00:48:39.050
So they are also NP reductions.

00:48:39.050 --> 00:48:41.810
Again, that's a generalization
of-- or strengthening

00:48:41.810 --> 00:48:46.460
of the type of reduction we
saw in all previous lectures.

00:48:46.460 --> 00:48:48.190
All right.

00:48:48.190 --> 00:48:50.930
So I want to
simultaneously tell you

00:48:50.930 --> 00:48:53.300
a bunch of problems
that are APX complete so

00:48:53.300 --> 00:48:54.830
that you know things
to reduce from,

00:48:54.830 --> 00:48:57.200
but also I'll show you
examples of such things.

00:48:57.200 --> 00:49:00.102
So some of these I will omit the
proofs, because they're messy

00:49:00.102 --> 00:49:02.060
and it's just useful to
know that they're there

00:49:02.060 --> 00:49:03.020
so you can reduce from them.

00:49:03.020 --> 00:49:05.110
Others I-- most of them,
I will cover the proofs.

00:49:35.610 --> 00:49:36.110
All right.

00:49:36.110 --> 00:49:39.730
And we're going to
return to an old issue

00:49:39.730 --> 00:49:43.460
from the first SAT lecture.

00:49:47.807 --> 00:49:50.800
And I'm going to introduce
some new notation.

00:49:50.800 --> 00:49:56.110
So a lot of the starting
points here are just max SAT.

00:49:56.110 --> 00:49:57.906
Or I should-- let's
say a max CNF SAT.

00:49:57.906 --> 00:49:59.030
You're given a CNF formula.

00:49:59.030 --> 00:50:00.630
It's picked out a
bunch of clauses.

00:50:00.630 --> 00:50:02.380
You want to maximize
the number of clauses

00:50:02.380 --> 00:50:05.140
that you satisfy with
some variable assignment.

00:50:05.140 --> 00:50:07.034
So that's going to
be APX complete.

00:50:07.034 --> 00:50:08.700
There are constant
factor approximations

00:50:08.700 --> 00:50:11.630
if the clauses are constant
size, I should say.

00:50:11.630 --> 00:50:15.340
So like max 3SAT has a
constant factor approximation,

00:50:15.340 --> 00:50:18.390
I forget what the
current best is.

00:50:18.390 --> 00:50:19.100
And no better.

00:50:19.100 --> 00:50:21.550
There's no PTAS for a max 3SAT.

00:50:21.550 --> 00:50:25.020
This is a stronger
form of max 3SAT,

00:50:25.020 --> 00:50:28.790
which we have not seen before,
though we've hinted around it.

00:50:28.790 --> 00:50:32.030
The E means every
clause has exactly

00:50:32.030 --> 00:50:34.030
three distinct literals.

00:50:48.540 --> 00:50:51.190
This is an issue that
we stumbled into.

00:50:51.190 --> 00:50:54.000
Oh, do we allow clauses
to have only two literals?

00:50:54.000 --> 00:50:56.220
I said no, in the
original definition.

00:50:56.220 --> 00:50:57.800
But I did allow
repeated literals,

00:50:57.800 --> 00:50:58.800
which is the same thing.

00:50:58.800 --> 00:51:02.520
So when I say three set, I
mean you can repeat literals,

00:51:02.520 --> 00:51:04.610
or you can have fewer
than three literals.

00:51:04.610 --> 00:51:09.330
When I say E three set, then
you're not allowed to do that.

00:51:09.330 --> 00:51:12.640
Then you may remember we
talked about 3SAT five.

00:51:12.640 --> 00:51:15.890
That meant every clause
has only three literals,

00:51:15.890 --> 00:51:20.290
and every variable appears,
at most, five times.

00:51:20.290 --> 00:51:25.060
E5 means exactly five times.

00:51:25.060 --> 00:51:38.304
Each variable appears
exactly five times.

00:51:38.304 --> 00:51:39.280
Do you have a question?

00:51:39.280 --> 00:51:41.071
AUDIENCE: [INAUDIBLE]
gives you a condition

00:51:41.071 --> 00:51:42.800
on the number of clauses.

00:51:42.800 --> 00:51:43.970
PROFESSOR: Oh, you're right.

00:51:43.970 --> 00:51:45.683
That gives you a linear relation
between the number of clauses

00:51:45.683 --> 00:51:46.849
and the number of variables.

00:51:46.849 --> 00:51:48.910
You can work it out.

00:51:48.910 --> 00:51:49.820
This is hard.

00:51:49.820 --> 00:51:51.350
I will not prove it.

00:51:51.350 --> 00:51:54.340
It's not hard, but it's
just a little bit messy.

00:51:54.340 --> 00:51:55.175
Sorry.

00:51:55.175 --> 00:51:58.250
It's not difficult.

00:51:58.250 --> 00:52:01.310
I will prove a slightly
different result, which

00:52:01.310 --> 00:52:06.580
this one is based on, which
is a little bit more familiar

00:52:06.580 --> 00:52:09.490
but also introduces something
new we hadn't seen before.

00:52:09.490 --> 00:52:11.070
This is the regular 3SAT.

00:52:11.070 --> 00:52:14.700
Each clause has, at
most, three literals.

00:52:14.700 --> 00:52:18.130
And every variable appears,
at most, three times.

00:52:18.130 --> 00:52:20.290
This is something I
didn't realize was hard.

00:52:20.290 --> 00:52:23.010
It's not written in most places.

00:52:23.010 --> 00:52:27.430
But it is here, in the
world of approximability.

00:52:27.430 --> 00:52:29.420
This is the reduction.

00:52:29.420 --> 00:52:31.870
It's kind of funny.

00:52:31.870 --> 00:52:33.330
So suppose you
have a variable, x,

00:52:33.330 --> 00:52:35.660
which appears a million times.

00:52:35.660 --> 00:52:40.704
You're going to make
a cycle, so to speak.

00:52:40.704 --> 00:52:42.120
It's a formula,
so it's not really

00:52:42.120 --> 00:52:45.260
a cycle, but of size 1 million.

00:52:45.260 --> 00:52:50.360
And you're going to write down
this 2SAT constraint-- not xi

00:52:50.360 --> 00:52:52.290
or xi plus 1 for all i.

00:52:52.290 --> 00:52:54.900
And do it around in
the cycle, so not x6

00:52:54.900 --> 00:52:58.670
or x1-- or x a million or x1.

00:52:58.670 --> 00:53:01.500
That's, of course, equivalent
to saying if xi is set to true,

00:53:01.500 --> 00:53:03.410
then xi plus 1 must
also be set to true.

00:53:03.410 --> 00:53:05.790
So if any of these are
set to true, they all are.

00:53:05.790 --> 00:53:07.280
So the only
satisfying assignments

00:53:07.280 --> 00:53:12.860
here are everybody true,
everybody not true.

00:53:12.860 --> 00:53:15.940
So if you're just worried
about NP reductions,

00:53:15.940 --> 00:53:19.070
this is a reduction
from 3SAT to 3SAT-3.

00:53:19.070 --> 00:53:21.050
Because every variable
now will appear

00:53:21.050 --> 00:53:24.360
in exactly three--
exactly three,

00:53:24.360 --> 00:53:28.130
in fact-- so constraints.

00:53:28.130 --> 00:53:31.810
This one, this one, and whatever
it originally-- whatever

00:53:31.810 --> 00:53:33.720
you want to plug it into.

00:53:33.720 --> 00:53:36.570
So each of these was an
occurrence of that variable.

00:53:36.570 --> 00:53:39.090
Maybe use the positive form,
maybe use the negative form,

00:53:39.090 --> 00:53:42.430
but each variable appears three
times in this first picture.

00:53:42.430 --> 00:53:46.060
Now that's a great
reduction for 3SAT-3.

00:53:46.060 --> 00:53:48.970
Notice I didn't write max,
because this reduction will not

00:53:48.970 --> 00:53:53.030
work as an L reduction, say.

00:53:53.030 --> 00:53:57.110
You cannot prove that max 3SAT
is hard using this reduction,

00:53:57.110 --> 00:54:02.890
because you could, for
example, just violate these two

00:54:02.890 --> 00:54:06.377
constraints and make all
of these guys true and all

00:54:06.377 --> 00:54:07.210
of these guys false.

00:54:07.210 --> 00:54:09.330
And if this is size
a million, that

00:54:09.330 --> 00:54:12.327
means you're saying--
I mean, you're

00:54:12.327 --> 00:54:14.160
setting the variable
half true, half false--

00:54:14.160 --> 00:54:17.920
or you could use any ratio you
want-- at a very small cost.

00:54:17.920 --> 00:54:21.310
You're only paying a penalty
of two constraints violated,

00:54:21.310 --> 00:54:24.310
and yet you're able to satisfy
1/2 of the clauses using

00:54:24.310 --> 00:54:26.800
xi equal to true, and
some other set of clauses

00:54:26.800 --> 00:54:28.980
using xi set to false.

00:54:28.980 --> 00:54:31.220
And there's no way to
bound how many clauses you

00:54:31.220 --> 00:54:33.740
can sort of misrepresent.

00:54:33.740 --> 00:54:35.795
And the objective in
max 3SAT is to maximize

00:54:35.795 --> 00:54:37.814
the number of clauses satisfied.

00:54:37.814 --> 00:54:39.230
So if you allow
this kind of thing

00:54:39.230 --> 00:54:41.390
where you can flip in
crazy ways and change

00:54:41.390 --> 00:54:44.910
a huge number of clauses one
way or the other in this sort

00:54:44.910 --> 00:54:46.910
of false way, how
would-- if you're

00:54:46.910 --> 00:54:50.790
trying to actually construct
a solution to max 3SAT

00:54:50.790 --> 00:54:53.780
from a solution to
this 3SAT-3 instance,

00:54:53.780 --> 00:54:56.087
you wouldn't know which way
to set the variable x to.

00:54:56.087 --> 00:54:57.920
If you set it to true,
there's a whole bunch

00:54:57.920 --> 00:54:59.360
of clauses that
wanted it to be false.

00:54:59.360 --> 00:55:01.240
If you set it to false, a whole
bunch you wanted to be true.

00:55:01.240 --> 00:55:01.450
Yeah.

00:55:01.450 --> 00:55:03.741
AUDIENCE: If we're just
talking about NP hardness here,

00:55:03.741 --> 00:55:05.914
can't you make those edges
very heavy by repeating

00:55:05.914 --> 00:55:07.045
the clause many times?

00:55:07.045 --> 00:55:09.170
PROFESSOR: If we're just
worried about NP hardness,

00:55:09.170 --> 00:55:10.110
this is fine.

00:55:10.110 --> 00:55:11.860
Because then all of
these have to be true.

00:55:11.860 --> 00:55:14.355
AUDIENCE: Uh, I mean for
the max decision problem.

00:55:14.355 --> 00:55:16.770
PROFESSOR: Well then
the decision problem

00:55:16.770 --> 00:55:18.590
is, can you satisfy all of them.

00:55:18.590 --> 00:55:20.380
That is a special
case of the max--

00:55:20.380 --> 00:55:21.381
AUDIENCE: Satisfy k of--

00:55:21.381 --> 00:55:23.254
PROFESSOR: Yeah, but if
you set k equal to N,

00:55:23.254 --> 00:55:25.185
that's a special case
of the general form.

00:55:25.185 --> 00:55:26.035
AUDIENCE: Oh, OK.

00:55:26.035 --> 00:55:26.660
PROFESSOR: So--

00:55:26.660 --> 00:55:28.118
AUDIENCE: I see
what you're saying.

00:55:28.118 --> 00:55:29.290
PROFESSOR: Yeah.

00:55:29.290 --> 00:55:31.700
If you didn't-- if you wanted
to make k and N different,

00:55:31.700 --> 00:55:34.510
you could just add a bunch of
things that are unsatisfiable.

00:55:34.510 --> 00:55:36.900
Lots of ways to change
how many there are.

00:55:39.840 --> 00:55:40.940
OK.

00:55:40.940 --> 00:55:44.410
Nonetheless, we can prove
max 3SAT-3 is hard with an L

00:55:44.410 --> 00:55:47.330
reduction from 3SAT.

00:55:47.330 --> 00:55:52.089
So I'm not going to prove
that 3SAT is APX hard.

00:55:52.089 --> 00:55:53.630
We might do that in
a future lecture,

00:55:53.630 --> 00:55:54.880
but just take that as given.

00:55:54.880 --> 00:55:57.430
What I want to show you
is how to convert max 3SAT

00:55:57.430 --> 00:56:00.020
into max 3SAT-3.

00:56:00.020 --> 00:56:05.400
Not like this, but
using a different trick.

00:56:05.400 --> 00:56:17.200
So reducing from max 3SAT.

00:56:17.200 --> 00:56:20.150
We're given a formula,
we're given some variables.

00:56:20.150 --> 00:56:22.175
Let's say-- let's
look at variable x.

00:56:26.380 --> 00:56:27.790
And let's say it
appears k times.

00:56:35.480 --> 00:56:38.420
What we're going to
do, just like before,

00:56:38.420 --> 00:56:41.380
we're going to make
k new variables.

00:56:41.380 --> 00:56:45.420
We're going to make k variables
x1 through xk that replace x,

00:56:45.420 --> 00:56:47.250
and we're going to use
those instead of x.

00:56:47.250 --> 00:56:49.541
And I want to force all the
values of x to be the same,

00:56:49.541 --> 00:56:53.370
but I want to force it even when
you're allowed to cheat and set

00:56:53.370 --> 00:56:55.770
some of the things incorrectly.

00:56:55.770 --> 00:56:59.029
And we're going to do this
using a powerful tool, which

00:56:59.029 --> 00:57:00.945
is good to know about,
called expander graphs.

00:57:08.130 --> 00:57:09.600
So there's two
things to tell you.

00:57:09.600 --> 00:57:10.850
One is, what is
an expander graph,

00:57:10.850 --> 00:57:12.530
and the other thing is, once
I have an expander graph,

00:57:12.530 --> 00:57:13.080
what do I do.

00:57:13.080 --> 00:57:15.580
Let me start with the latter,
because it's a little simpler.

00:57:20.091 --> 00:57:22.340
I guess a lot of you have
heard about expander graphs.

00:57:22.340 --> 00:57:24.860
And the short answer is they're
complicated and confusing,

00:57:24.860 --> 00:57:26.830
but really cool and powerful.

00:57:26.830 --> 00:57:29.517
We're not going to try to prove
that expanders exist here.

00:57:29.517 --> 00:57:31.100
I'm just going to
tell you they exist,

00:57:31.100 --> 00:57:32.510
and that's actually
pretty simple

00:57:32.510 --> 00:57:35.890
what they-- what
properties they have.

00:57:35.890 --> 00:57:40.580
So what I want to do, whenever
I have an edge in this graph--

00:57:40.580 --> 00:57:42.620
this is a graph
whose vertices are

00:57:42.620 --> 00:57:45.560
the x1 through xk--
I'm going to convert

00:57:45.560 --> 00:57:47.880
an edge into a constraint,
which is effectively

00:57:47.880 --> 00:57:56.030
xi equals xj, which in
reality is not xi or xj,

00:57:56.030 --> 00:57:59.120
and not xj or xi.

00:58:02.720 --> 00:58:05.719
So I really probably shouldn't
think of it as xi equals xj.

00:58:05.719 --> 00:58:06.760
That's what I want to do.

00:58:06.760 --> 00:58:08.720
I want to force lots
of things to be equal.

00:58:08.720 --> 00:58:10.210
But really, we have
to, in the end,

00:58:10.210 --> 00:58:12.410
think about it in
terms of constraints,

00:58:12.410 --> 00:58:15.940
because some of them
might be violated.

00:58:15.940 --> 00:58:17.687
So what is an expander graph?

00:58:17.687 --> 00:58:18.770
There are different types.

00:58:18.770 --> 00:58:24.000
But what we will use
is two properties.

00:58:29.510 --> 00:58:30.975
So this is for k nodes.

00:58:34.220 --> 00:58:42.596
We have bounded degree, and
we have, for every cut AB--

00:58:42.596 --> 00:58:44.840
and I think we've talked
about cuts in this context,

00:58:44.840 --> 00:58:46.440
in the context of max cut.

00:58:46.440 --> 00:58:49.030
So the idea is that
A and B are disjoint,

00:58:49.030 --> 00:58:53.180
and their union is the
set of all vertices.

00:58:53.180 --> 00:58:57.250
So A is 1 side of the cut, B
is the other side of the cut.

00:58:57.250 --> 00:58:59.690
We want the number
of cross edges,

00:58:59.690 --> 00:59:05.382
the number of edges
between A and B, is big.

00:59:05.382 --> 00:59:15.030
It's at least the
min of A and B.

00:59:15.030 --> 00:59:16.540
Some intuition.

00:59:16.540 --> 00:59:17.405
Imagine a Kleek.

00:59:17.405 --> 00:59:19.570
A [? Kleek ?] is
not bounded degree,

00:59:19.570 --> 00:59:21.510
but it has this property.

00:59:21.510 --> 00:59:24.041
If you look at any cut, there's
a lot of edges between them.

00:59:24.041 --> 00:59:25.540
It's actually more
like the product.

00:59:25.540 --> 00:59:27.280
But in particular,
it's at least the min.

00:59:27.280 --> 00:59:30.310
So you can think of the expander
as a sparse [? Kleek. ?]

00:59:30.310 --> 00:59:34.419
It's [? Kleek-y ?]
enough, in this sense,

00:59:34.419 --> 00:59:36.460
which we'll see why this
is the property we want.

00:59:36.460 --> 00:59:37.740
But it has bounded
degree, therefore

00:59:37.740 --> 00:59:38.730
linear number of edges.

00:59:38.730 --> 00:59:41.160
So it's very sparse.

00:59:41.160 --> 00:59:43.560
In the construction
we are applying,

00:59:43.560 --> 00:59:45.290
which is due to
[INAUDIBLE] Philips

00:59:45.290 --> 00:59:49.250
and [? Sarnak, ?]
the degree is 14.

00:59:49.250 --> 00:59:50.170
But it doesn't matter.

00:59:50.170 --> 00:59:50.670
Constant.

00:59:53.201 --> 00:59:57.630
It's actually 14
regular, so that's nice.

00:59:57.630 --> 00:59:59.924
So we take this
graph, which was known

00:59:59.924 --> 01:00:01.340
to be out there--
basically random

01:00:01.340 --> 01:00:03.130
graphs [INAUDIBLE]
property, but we

01:00:03.130 --> 01:00:04.920
won't worry about how
to construct it too

01:00:04.920 --> 01:00:07.564
much, although you do have to.

01:00:07.564 --> 01:00:09.730
But there's tons of papers
on how to construct them.

01:00:09.730 --> 01:00:11.330
And then for every
edge, we convert it

01:00:11.330 --> 01:00:13.810
into these two constraints.

01:00:13.810 --> 01:00:16.510
So let's prove that
this is an L reduction.

01:00:22.140 --> 01:00:30.030
Claim L reduction.

01:00:30.030 --> 01:00:35.579
So maybe-- I don't have
anything to point at,

01:00:35.579 --> 01:00:37.370
because I can't point
at an expander graph.

01:00:37.370 --> 01:00:41.110
But let me draw the graph
so I can point at something.

01:00:41.110 --> 01:00:42.776
Let's draw the
[? Kleek, ?] just so it's

01:00:42.776 --> 01:00:44.500
a little easier to think about.

01:00:44.500 --> 01:00:47.260
This is an expander, [? k4 ?].

01:00:47.260 --> 01:00:52.700
And let's say, in your solution,
some of these end up getting

01:00:52.700 --> 01:00:55.690
assigned true value--
let's represent that

01:00:55.690 --> 01:00:58.580
by red-- some of them not.

01:00:58.580 --> 01:01:00.550
Maybe three of them
are true, one of them

01:01:00.550 --> 01:01:01.790
is false or whatever.

01:01:01.790 --> 01:01:05.270
In general, I'm
going to choose--

01:01:05.270 --> 01:01:07.160
so I mean, we're given
some solution, right?

01:01:07.160 --> 01:01:11.230
This is the solution y-prime
to the constructed instance

01:01:11.230 --> 01:01:11.730
x-prime.

01:01:11.730 --> 01:01:13.480
X-prime has this expander.

01:01:13.480 --> 01:01:15.870
So we look at a solution
y-prime to x-prime,

01:01:15.870 --> 01:01:18.870
we have no control
over what it is.

01:01:18.870 --> 01:01:21.232
If you look at a variable,
some fraction of them

01:01:21.232 --> 01:01:23.190
are set to true, some of
them are set to false.

01:01:23.190 --> 01:01:25.950
We're going to
choose the majority.

01:01:25.950 --> 01:01:27.830
So here it's majority
red, so we're going

01:01:27.830 --> 01:01:31.640
to change this guy to be red.

01:01:31.640 --> 01:01:34.670
Now does that hurt us?

01:01:34.670 --> 01:01:36.170
Well, we can think
of there as being

01:01:36.170 --> 01:01:39.070
a cut of the red
nodes versus the not

01:01:39.070 --> 01:01:40.290
red nodes, the black nodes.

01:01:45.200 --> 01:01:49.440
And I claim the number
of edges here is big.

01:01:49.440 --> 01:01:51.539
It's at least the
minimum of A and B.

01:01:51.539 --> 01:01:53.830
Now what we were doing is
taking the minimum of A and B

01:01:53.830 --> 01:01:56.820
and recoloring that side
to be the other side.

01:01:56.820 --> 01:02:00.610
When we do that, we
have these constraints,

01:02:00.610 --> 01:02:02.640
which are supposed to
be equality constraints.

01:02:02.640 --> 01:02:05.445
These things are supposed to be
equal, but they weren't before.

01:02:05.445 --> 01:02:08.950
Before I recolored this, these
were-- at least one constraint

01:02:08.950 --> 01:02:11.350
here was violated, because
this one of them was red,

01:02:11.350 --> 01:02:12.850
one of them was black.

01:02:12.850 --> 01:02:15.640
When I fill this in,
I improve my solution,

01:02:15.640 --> 01:02:18.050
because-- by at least
the size of the cut.

01:02:20.830 --> 01:02:23.590
Each of these guys
now becomes satisfied.

01:02:23.590 --> 01:02:24.850
It wasn't before.

01:02:24.850 --> 01:02:27.930
So I improve my
solution by this much.

01:02:27.930 --> 01:02:31.160
I also worsen my
solution, potentially,

01:02:31.160 --> 01:02:37.290
because that node
appears in one clause.

01:02:37.290 --> 01:02:41.619
And so it gets worse by 1.

01:02:41.619 --> 01:02:43.660
So suppose there are B
nodes on the smaller side,

01:02:43.660 --> 01:02:45.160
and we're recoloring B nodes.

01:02:45.160 --> 01:02:49.680
So we got an improvement by
P, and also we worsened things

01:02:49.680 --> 01:02:55.780
by up to P. Because these P guys
appear in P different clauses.

01:02:55.780 --> 01:02:59.290
Each one potentially we mess
up is no longer satisfied.

01:02:59.290 --> 01:03:01.820
But for every one that we mess
up-- so these guys up here

01:03:01.820 --> 01:03:06.270
in some actual clause--
each one we mess up,

01:03:06.270 --> 01:03:09.090
we also make at least
one thing happy.

01:03:09.090 --> 01:03:11.460
Because we fixed the cut.

01:03:11.460 --> 01:03:14.010
So if we have any solution,
we can convert it into one

01:03:14.010 --> 01:03:16.060
where our variables are
all true or all false,

01:03:16.060 --> 01:03:17.950
and not lose anything.

01:03:17.950 --> 01:03:21.600
Therefore, there exists
an optimal solution.

01:03:21.600 --> 01:03:23.910
There exists an
optimal solution.

01:03:23.910 --> 01:03:28.300
And you do this variable by
variable, where variables

01:03:28.300 --> 01:03:32.170
are all true or all false.

01:03:40.450 --> 01:03:47.400
Now, we do change the
value of OPT here.

01:03:47.400 --> 01:03:50.170
Because we added a
ton of constraints,

01:03:50.170 --> 01:03:52.540
and we just said well the--
in the optimal solution,

01:03:52.540 --> 01:03:55.390
or an optimal solution-- in
fact, all of these constraints

01:03:55.390 --> 01:03:56.600
will be satisfied.

01:03:56.600 --> 01:03:59.090
Which means OPT has increased.

01:03:59.090 --> 01:04:04.200
So the OPT for
x-prime, this thing,

01:04:04.200 --> 01:04:06.400
is going to be larger
than the OPT for x.

01:04:08.955 --> 01:04:18.770
OPT of x-prime is going to
equal OPT of x plus order

01:04:18.770 --> 01:04:26.510
the total number of occurrences
of all variables, which

01:04:26.510 --> 01:04:29.290
is at most three times
the number of clauses.

01:04:36.170 --> 01:04:38.710
So we want to know,
does it satisfy

01:04:38.710 --> 01:04:40.170
this definition of L reduction.

01:04:40.170 --> 01:04:42.230
We need to know that
the OPT does not explode

01:04:42.230 --> 01:04:44.350
by more than a constant factor.

01:04:44.350 --> 01:04:46.690
And yet, we added this big term.

01:04:46.690 --> 01:04:49.630
But the good news
is, in 3SAT, you

01:04:49.630 --> 01:04:52.340
can always satisfy a constant
fraction of the clauses.

01:04:52.340 --> 01:04:56.270
So OPT is always at least--
I think I have written here,

01:04:56.270 --> 01:04:59.790
like, half of them.

01:04:59.790 --> 01:05:02.140
I think you just randomly
assign the variables,

01:05:02.140 --> 01:05:07.050
and some constant fraction will
be satisfied in expectation.

01:05:07.050 --> 01:05:09.710
So there's definitely a solution
where OPT of-- so OPT of x

01:05:09.710 --> 01:05:11.210
is definitely at
least some constant

01:05:11.210 --> 01:05:12.650
times the number of clauses.

01:05:12.650 --> 01:05:15.380
And so this adding some constant
times the number of clauses

01:05:15.380 --> 01:05:18.030
doesn't change the overall cos
by more than a constant factor.

01:05:18.030 --> 01:05:20.020
So property one holds.

01:05:20.020 --> 01:05:26.600
Property two holds, in
fact, with beta equal 1.

01:05:26.600 --> 01:05:28.410
You're not making
your solution in-- you

01:05:28.410 --> 01:05:30.370
see we have this additive
thing, because we

01:05:30.370 --> 01:05:33.040
add these gadgets and stuff.

01:05:33.040 --> 01:05:35.027
Multiplicatively,
it's confusing,

01:05:35.027 --> 01:05:36.860
and that's what we were
worrying about here.

01:05:36.860 --> 01:05:38.560
But additively, it's very clean.

01:05:38.560 --> 01:05:41.680
We always add the exact same
amount to your solution.

01:05:41.680 --> 01:05:43.200
So if you have a
solution y-prime

01:05:43.200 --> 01:05:45.460
and you convert it
back to a solution y,

01:05:45.460 --> 01:05:47.970
the gap-- the additive
gap between the cost

01:05:47.970 --> 01:05:52.430
of y versus OPT of x will
be equal to the additive gap

01:05:52.430 --> 01:05:55.761
between the cost of y-prime
versus OPT of x-prime.

01:05:55.761 --> 01:05:56.260
Question.

01:05:56.260 --> 01:05:59.182
AUDIENCE: So here,
how many times

01:05:59.182 --> 01:06:01.617
are you using each variable?

01:06:01.617 --> 01:06:05.300
PROFESSOR: We are using
each variable 29 times.

01:06:08.870 --> 01:06:13.020
We're using it-- why 29?

01:06:13.020 --> 01:06:16.430
Because-- right.

01:06:16.430 --> 01:06:18.820
We have degree 14, but
then for every edge

01:06:18.820 --> 01:06:20.310
we actually have
two constraints,

01:06:20.310 --> 01:06:21.950
the implication
[? in ?] both ways.

01:06:21.950 --> 01:06:22.760
So that's 28.

01:06:22.760 --> 01:06:24.640
Plus the vari--
each of those nodes

01:06:24.640 --> 01:06:28.440
actually appears in
one actual clause.

01:06:28.440 --> 01:06:28.940
OK.

01:06:28.940 --> 01:06:32.660
So this proves that
max 3SAT-29 is hard.

01:06:32.660 --> 01:06:34.080
Yep, good question.

01:06:34.080 --> 01:06:37.840
Why did I claim max 3SAT-3?

01:06:37.840 --> 01:06:40.670
Because we can use
this reduction now.

01:06:43.190 --> 01:06:45.710
So there is another
reason I showed you this.

01:06:45.710 --> 01:06:47.960
I haven't seen this explicitly
said in the literature,

01:06:47.960 --> 01:06:50.930
but all the pieces
are out there.

01:06:50.930 --> 01:06:57.670
So once you show max 3SAT
some constant is hard,

01:06:57.670 --> 01:06:59.990
then you can do an L
reduction from that problem

01:06:59.990 --> 01:07:03.090
to max 3SAT-3, just like this.

01:07:03.090 --> 01:07:04.760
So first we do the expander.

01:07:04.760 --> 01:07:07.270
And then, still these nodes
have too high a degree.

01:07:07.270 --> 01:07:09.540
They're degree 29.

01:07:09.540 --> 01:07:12.660
Now we're going to expand
those into little cycles

01:07:12.660 --> 01:07:14.850
of constraints.

01:07:14.850 --> 01:07:21.300
Now this is actually OK when
the cycle has constant length,

01:07:21.300 --> 01:07:25.090
because maybe-- suppose some
of these are set to true,

01:07:25.090 --> 01:07:26.697
some of them are set to false.

01:07:26.697 --> 01:07:28.030
Then just set them all to false.

01:07:28.030 --> 01:07:29.334
Don't even take majority.

01:07:29.334 --> 01:07:30.250
Set them all to false.

01:07:30.250 --> 01:07:31.669
How much does that hurt you?

01:07:31.669 --> 01:07:33.460
Well, you know that
each of these variables

01:07:33.460 --> 01:07:36.200
appeared in a constant
number of clauses.

01:07:36.200 --> 01:07:41.950
So it only hurt you
by a constant amount.

01:07:41.950 --> 01:07:45.530
Every time you flip a
variable from true to false,

01:07:45.530 --> 01:07:50.800
you only lose an additive
constant in your solution.

01:07:50.800 --> 01:07:53.260
So when we're converting from
a solution y-prime, which

01:07:53.260 --> 01:07:55.420
does weird things on
a cycle, potentially,

01:07:55.420 --> 01:07:58.060
when we convert it to y
and set them all to false,

01:07:58.060 --> 01:08:01.820
we-- I mean we know--
so there's two cases.

01:08:01.820 --> 01:08:04.030
One is, all of these
constraints are satisfied.

01:08:04.030 --> 01:08:05.988
Then, you should choose
exactly what's written,

01:08:05.988 --> 01:08:08.300
and then they will all
be true or all be false.

01:08:08.300 --> 01:08:10.300
But if at least one
of them is violated,

01:08:10.300 --> 01:08:13.980
you can charge to that violation
and set them all to false.

01:08:13.980 --> 01:08:16.310
And when you do that, I don't
know how many-- you know,

01:08:16.310 --> 01:08:19.979
at most 1,000 violations
happen, some constant.

01:08:19.979 --> 01:08:22.740
Like three times 20-- whatever.

01:08:22.740 --> 01:08:24.600
Some constant.

01:08:24.600 --> 01:08:27.340
And then you know
that you can charge

01:08:27.340 --> 01:08:30.800
that cost to the violation
that you were given in y-prime.

01:08:30.800 --> 01:08:33.439
And so you can get that L
reduction property, too,

01:08:33.439 --> 01:08:36.460
and say oh good,
the additive gap

01:08:36.460 --> 01:08:39.510
in my produced solution where
I just set all those to false

01:08:39.510 --> 01:08:41.810
is at most 1,000 times
the original additive gap.

01:08:41.810 --> 01:08:45.810
And so that's an L
reduction from max 3SAT

01:08:45.810 --> 01:08:49.439
constant to max 3SAT-3.

01:08:49.439 --> 01:08:51.505
Questions?

01:08:51.505 --> 01:08:52.004
Yeah.

01:08:52.004 --> 01:08:54.254
AUDIENCE: So how do you know
there's a constant number

01:08:54.254 --> 01:08:55.280
of violations?

01:08:55.280 --> 01:08:57.630
PROFESSOR: Because
now we were given

01:08:57.630 --> 01:08:59.770
an instance of 3SAT
constant, meaning

01:08:59.770 --> 01:09:02.850
each variable appears in a
constant number of clauses.

01:09:02.850 --> 01:09:07.529
So we were given a
situation-- right, sorry.

01:09:07.529 --> 01:09:10.890
Also, when we do this,
each-- we set it up

01:09:10.890 --> 01:09:16.529
so each of these variables
appears in one original clause.

01:09:16.529 --> 01:09:19.240
Yeah And we know the total
size of the cycle is constant.

01:09:19.240 --> 01:09:21.810
So each of these, every time
we turn one of these to false,

01:09:21.810 --> 01:09:23.187
we lose one point.

01:09:23.187 --> 01:09:25.020
And there's only a
constant number of these,

01:09:25.020 --> 01:09:27.870
so it's actually-- the
constant is only 27.

01:09:27.870 --> 01:09:28.810
Or 29, sorry.

01:09:28.810 --> 01:09:30.319
29.

01:09:30.319 --> 01:09:32.909
It's a little weird to
get used to L reductions.

01:09:32.909 --> 01:09:34.200
I'm still getting used to them.

01:09:34.200 --> 01:09:37.260
But as you can see,
it's pretty powerful.

01:09:37.260 --> 01:09:40.890
You can do a lot,
and it's just OK.

01:09:40.890 --> 01:09:43.010
You definitely
have to be careful.

01:09:43.010 --> 01:09:45.370
In general, you want things
to have bounded degree.

01:09:45.370 --> 01:09:46.359
That makes things
really helpful.

01:09:46.359 --> 01:09:48.525
That's why I'm telling you
about these two problems.

01:09:55.873 --> 01:09:56.830
What next.

01:09:56.830 --> 01:10:00.470
Let's just continue--
let me at least mention,

01:10:00.470 --> 01:10:02.590
in case I run out
of time, max not all

01:10:02.590 --> 01:10:11.310
equal 3SAT, also hard, positive
1 in 3SAT, even one in E3SAT.

01:10:11.310 --> 01:10:12.700
Also, APX hard.

01:10:12.700 --> 01:10:14.030
APX complete.

01:10:14.030 --> 01:10:18.820
So those are good friends from
3SAT land to carry over here.

01:10:18.820 --> 01:10:22.930
But let's prove some-- and I
will eventually prove those.

01:10:22.930 --> 01:10:27.690
Let's prove some
other fun problems.

01:10:27.690 --> 01:10:30.910
Next one is independent set.

01:10:30.910 --> 01:10:34.170
Now I have to be a
little bit careful here.

01:10:34.170 --> 01:10:38.740
Independent set is really
really, really hard.

01:10:38.740 --> 01:10:42.630
But an interesting special
case is bounded degree

01:10:42.630 --> 01:10:44.370
independent set.

01:10:44.370 --> 01:10:46.080
So that's what I'll
talk about next.

01:10:57.800 --> 01:11:00.970
I think I'm going to prove max
degree for independent set,

01:11:00.970 --> 01:11:12.110
although it's known that
max degree 3 is hard also.

01:11:12.110 --> 01:11:18.870
So constant degree independent
set is actually APX complete.

01:11:18.870 --> 01:11:23.520
So there is a constant
factor approximation,

01:11:23.520 --> 01:11:26.080
which is take any maximal
independent set-- just

01:11:26.080 --> 01:11:28.270
keep adding vertices
until you can't anymore.

01:11:28.270 --> 01:11:34.100
That will be within a factor of
this of optimal, you can show.

01:11:34.100 --> 01:11:37.400
That's pretty clear.

01:11:37.400 --> 01:11:40.990
So that puts it in APX.

01:11:40.990 --> 01:11:43.390
And furthermore, we
claim that's [INAUDIBLE].

01:11:43.390 --> 01:11:45.990
So there's a constant
factor, and there's also

01:11:45.990 --> 01:11:47.540
a constant factor
in approximability.

01:11:47.540 --> 01:11:50.600
Some constant you
cannot go below.

01:11:50.600 --> 01:11:54.220
And the proof is this,
although it's a little bit--

01:11:54.220 --> 01:11:58.140
so it's a reduction from,
let's say, the one we just did,

01:11:58.140 --> 01:12:00.874
max 3SAT-3.

01:12:00.874 --> 01:12:02.540
And it's going to be
a strict reduction.

01:12:02.540 --> 01:12:06.190
We're not going to lose
anything if I did things right.

01:12:06.190 --> 01:12:08.727
Now in fact-- so I drew
here six recurrences of xi,

01:12:08.727 --> 01:12:10.310
but there's really
only three of them.

01:12:10.310 --> 01:12:12.620
But the idea is
complete bipartite graph

01:12:12.620 --> 01:12:14.780
between the positive
instances of xi

01:12:14.780 --> 01:12:16.420
and the negative instances.

01:12:16.420 --> 01:12:18.520
And then these are going
to be plugged in directly

01:12:18.520 --> 01:12:20.700
to the clause gadgets.

01:12:20.700 --> 01:12:22.080
These are the same variable.

01:12:24.340 --> 01:12:25.840
And so the idea is
each of these was

01:12:25.840 --> 01:12:27.190
plugged into only one clause.

01:12:27.190 --> 01:12:28.820
So I need to make copies.

01:12:28.820 --> 01:12:31.310
I do this complete bipartite
graph between them.

01:12:31.310 --> 01:12:35.560
And now we're trying to do
max independent set, which

01:12:35.560 --> 01:12:40.470
means whatever solution we find,
it will be an independent set.

01:12:40.470 --> 01:12:41.040
That's cool.

01:12:41.040 --> 01:12:42.850
There's no slack in
independence here.

01:12:42.850 --> 01:12:44.350
It's just about
how many we choose.

01:12:44.350 --> 01:12:44.850
Question?

01:12:44.850 --> 01:12:46.190
AUDIENCE: What is
an independent set?

01:12:46.190 --> 01:12:47.110
PROFESSOR:
Independence set is you

01:12:47.110 --> 01:12:48.160
want to choose a set
of variables that

01:12:48.160 --> 01:12:49.770
have no edges between them.

01:12:49.770 --> 01:12:52.990
Sorry, instead of vertices,
they have no edges between them.

01:12:52.990 --> 01:12:56.610
So that means that if I choose
any one of the blue nodes here,

01:12:56.610 --> 01:12:59.200
I can't choose any of the
red nodes, and vice versa.

01:12:59.200 --> 01:13:01.290
So I may not be able to
choose all of the blue

01:13:01.290 --> 01:13:03.940
or all of the red, but
that's how it goes.

01:13:03.940 --> 01:13:06.440
Now if we look at a clause,
there's-- in this case,

01:13:06.440 --> 01:13:09.726
we have to actually handle
the case of clauses of size 2

01:13:09.726 --> 01:13:11.535
and clauses of size 3.

01:13:11.535 --> 01:13:13.120
If you have a clause
of size 3, you'd

01:13:13.120 --> 01:13:14.940
just build a triangle on them.

01:13:14.940 --> 01:13:19.240
And the idea is only one
of those can be chosen.

01:13:19.240 --> 01:13:22.340
But it could be zero get chosen,
because you might be screwed.

01:13:22.340 --> 01:13:24.749
Maybe you chose a
red xi, and then you

01:13:24.749 --> 01:13:26.290
won't be able to
choose this blue xi.

01:13:26.290 --> 01:13:27.580
Maybe you chose a red xj.

01:13:27.580 --> 01:13:29.980
Maybe you chose a blue xk.

01:13:29.980 --> 01:13:31.740
In that case, you
won't be able to choose

01:13:31.740 --> 01:13:33.004
any of these vertices.

01:13:33.004 --> 01:13:34.670
But if at least one
of these [INAUDIBLE]

01:13:34.670 --> 01:13:38.070
is true, if you chose--
if you're either

01:13:38.070 --> 01:13:41.630
choosing the blue xi's or the
blue xj's or the red xk's, then

01:13:41.630 --> 01:13:44.140
in fact you get one
point for each clause.

01:13:44.140 --> 01:13:46.420
And in general, the
number of points you get,

01:13:46.420 --> 01:13:47.710
the number of things
you'll be able to put

01:13:47.710 --> 01:13:49.251
into your independent
set, is exactly

01:13:49.251 --> 01:13:52.360
the number of clauses
you'll be able to satisfy.

01:13:52.360 --> 01:13:57.230
And just by looking at whether
any of the blue xi's are true,

01:13:57.230 --> 01:14:00.340
then you set xi to true, looking
at whether any of the red xi's

01:14:00.340 --> 01:14:03.310
are set-- are chosen
in the independent set,

01:14:03.310 --> 01:14:04.581
then you set xi to false.

01:14:04.581 --> 01:14:05.955
That will recover
the assignment,

01:14:05.955 --> 01:14:09.370
and it'll have exactly the same
cost as the independent set

01:14:09.370 --> 01:14:10.187
size.

01:14:10.187 --> 01:14:12.520
Number of clauses you satisfy
will be exactly the number

01:14:12.520 --> 01:14:14.895
of independent set size.

01:14:17.850 --> 01:14:21.120
And similarly, for
a clause of size 2.

01:14:21.120 --> 01:14:21.880
So that's cool.

01:14:21.880 --> 01:14:27.300
Independent set is really easy
to reduce from max 3SAT-3.

01:14:27.300 --> 01:14:29.550
In fact, it would work
for max 3SAT constant.

01:14:29.550 --> 01:14:32.070
But if you do a max at
3SAT-3, there's only three

01:14:32.070 --> 01:14:33.270
of these guys.

01:14:33.270 --> 01:14:36.130
Then I think the biggest
degree you get is 4.

01:14:36.130 --> 01:14:38.697
This guy maybe is
attached to two things,

01:14:38.697 --> 01:14:40.280
and then also to two
things over here.

01:14:43.580 --> 01:14:46.250
Great.

01:14:46.250 --> 01:14:51.680
Next problem is vertex cover.

01:14:51.680 --> 01:14:53.050
So this is a funny one.

01:14:56.100 --> 01:14:59.395
So let's do a constant
degree vertex cover.

01:14:59.395 --> 01:15:01.770
In general, there's a two
approximation for vertex cover.

01:15:01.770 --> 01:15:07.076
So we don't need the constant
degree to be an APX, but.

01:15:07.076 --> 01:15:09.420
This is also APX complete.

01:15:09.420 --> 01:15:13.230
And it's kind of identical
to the independent set

01:15:13.230 --> 01:15:18.140
in a funny way, which
is for any graph,

01:15:18.140 --> 01:15:22.360
if you look at a vertex
cover, its complement

01:15:22.360 --> 01:15:23.990
is an independent set.

01:15:23.990 --> 01:15:25.510
If you look at any
independent set,

01:15:25.510 --> 01:15:28.910
its complement is
a vertex cover.

01:15:28.910 --> 01:15:31.590
Sorry, any maximal independent
set, its complement

01:15:31.590 --> 01:15:34.240
is a vertex cover.

01:15:34.240 --> 01:15:36.660
So they're kind of
duel in that, if you

01:15:36.660 --> 01:15:38.700
look at the size
of a vertex cover

01:15:38.700 --> 01:15:42.470
plus size of a maximal
independent set,

01:15:42.470 --> 01:15:45.700
it will always equal
the number of vertices.

01:15:45.700 --> 01:15:50.040
So maximizing this is the
same as minimizing this.

01:15:50.040 --> 01:15:52.320
But approximating this
is not necessarily

01:15:52.320 --> 01:15:53.700
the same as approximating this.

01:15:53.700 --> 01:15:56.740
One's a maximization problem,
one's a minimization problem.

01:15:56.740 --> 01:16:03.290
But it's still an L reduction
for bounded degree graphs,

01:16:03.290 --> 01:16:06.800
because if you have
degree at most delta,

01:16:06.800 --> 01:16:10.220
there's always an independent
set size of at least N

01:16:10.220 --> 01:16:11.390
over delta.

01:16:11.390 --> 01:16:14.300
And there's always a
vertex cover of size N.

01:16:14.300 --> 01:16:16.940
So they're within constant
factors of each other.

01:16:16.940 --> 01:16:23.700
In fact, these are both always
theta the number of vertices.

01:16:23.700 --> 01:16:24.950
OK?

01:16:24.950 --> 01:16:28.640
So the reduction is you give me
an instance of independent set,

01:16:28.640 --> 01:16:31.920
I give you that exact same
instance to vertex cover.

01:16:31.920 --> 01:16:35.260
And then-- so f is trivial.

01:16:35.260 --> 01:16:37.240
G takes the complement.

01:16:37.240 --> 01:16:39.630
Whatever you had in
the vertex cover,

01:16:39.630 --> 01:16:41.420
you don't put it in
the independent set,

01:16:41.420 --> 01:16:43.010
and vice versa.

01:16:43.010 --> 01:16:45.570
And then you just have to check
that this is an L reduction.

01:16:45.570 --> 01:16:47.880
So the first thing is the OPTs
are within a constant factor

01:16:47.880 --> 01:16:48.170
of each other.

01:16:48.170 --> 01:16:50.586
That's true, because they're
both within a constant factor

01:16:50.586 --> 01:16:52.120
of the number of vertices.

01:16:52.120 --> 01:16:56.430
And then you prove that
the additive gap is fixed.

01:16:56.430 --> 01:17:00.140
And it's the same thing if you
decrement this accidentally,

01:17:00.140 --> 01:17:02.150
then you increment
this accidentally.

01:17:02.150 --> 01:17:04.530
They're one for one.

01:17:04.530 --> 01:17:06.100
So this is kind of cool.

01:17:06.100 --> 01:17:08.730
It feels a little scary, but
we can convert a maximization

01:17:08.730 --> 01:17:11.250
problem into a minimization
problem with L reductions.

01:17:11.250 --> 01:17:13.940
This would be very hard
to even think about

01:17:13.940 --> 01:17:16.742
in the other
reduction types, which

01:17:16.742 --> 01:17:18.950
is one of the reasons L
reductions are so successful,

01:17:18.950 --> 01:17:19.450
I think.

01:17:22.336 --> 01:17:24.430
That was vertex cover.

01:17:24.430 --> 01:17:25.430
We can do one more.

01:17:29.940 --> 01:17:32.030
OK, really easy.

01:17:32.030 --> 01:17:32.850
Dominating set.

01:17:36.100 --> 01:17:38.140
Remember, dominating
set-- with vertex cover,

01:17:38.140 --> 01:17:40.236
when you put a
vertex in your cover,

01:17:40.236 --> 01:17:41.860
you cover all the
edges incident to it,

01:17:41.860 --> 01:17:43.130
and you want to
cover all the edges.

01:17:43.130 --> 01:17:44.838
Dominating set, when
you put a vertex in,

01:17:44.838 --> 01:17:46.980
you cover all the
neighboring vertices.

01:17:46.980 --> 01:17:49.070
You want to cover all vertices.

01:17:49.070 --> 01:17:51.765
So I'm going to reduce
from vertex cover.

01:17:51.765 --> 01:17:55.460
If you have an edge,
what you do is convert it

01:17:55.460 --> 01:18:00.360
into a path of length
2 plus that edge.

01:18:00.360 --> 01:18:03.580
So then you know that, if
this is in the dominating set,

01:18:03.580 --> 01:18:06.660
you can just move it
over to [INAUDIBLE] or w.

01:18:06.660 --> 01:18:09.260
It will cover all the things
it could cover before,

01:18:09.260 --> 01:18:11.420
and maybe even more.

01:18:11.420 --> 01:18:13.480
So then in the optimal
solution over here,

01:18:13.480 --> 01:18:15.385
you'd never need to choose
one of these vertices, which

01:18:15.385 --> 01:18:17.968
means we can assume that on the
original vertices, which means

01:18:17.968 --> 01:18:21.100
you are just solving
vertex cover,

01:18:21.100 --> 01:18:25.150
because covering that is the
same as covering that edge.

01:18:25.150 --> 01:18:26.344
Good.

01:18:26.344 --> 01:18:27.510
I think that's good for now.

01:18:27.510 --> 01:18:30.330
We'll do a bunch more
reductions next time.