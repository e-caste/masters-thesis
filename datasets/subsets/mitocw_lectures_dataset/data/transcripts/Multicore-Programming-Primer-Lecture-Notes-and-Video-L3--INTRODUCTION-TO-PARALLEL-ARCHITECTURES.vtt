WEBVTT

00:00:00.030 --> 00:00:02.420
The following content is
provided under a Creative

00:00:02.420 --> 00:00:03.850
Commons license.

00:00:03.850 --> 00:00:06.860
Your support will help MIT
OpenCourseWare continue to

00:00:06.860 --> 00:00:10.550
offer high quality educational
resources for free.

00:00:10.550 --> 00:00:13.420
To make a donation or view
additional materials from

00:00:13.420 --> 00:00:17.510
hundreds of MIT courses, visit
MIT OpenCourseWare at

00:00:17.510 --> 00:00:18.760
ocw.mit.edu.

00:00:21.140 --> 00:00:23.450
PROFESSOR: So we'll
get started.

00:00:23.450 --> 00:00:27.240
So today we are going to
dive into some parallel

00:00:27.240 --> 00:00:28.610
architectures.

00:00:28.610 --> 00:00:36.070
So the way, if you look at the
big world, is there's --

00:00:36.070 --> 00:00:39.370
just counting parallelism, you
can do it implicitly, either

00:00:39.370 --> 00:00:40.770
by hardware or the compiler.

00:00:40.770 --> 00:00:42.220
So the user won't see it.

00:00:42.220 --> 00:00:44.870
It will be done behind the
user's back, but can be done

00:00:44.870 --> 00:00:46.070
by hardware or compiler.

00:00:46.070 --> 00:00:48.980
Or explicitly, visible
to the user.

00:00:48.980 --> 00:00:53.440
So the hardware part is done in
superscalar processors, and

00:00:53.440 --> 00:00:55.550
all those things will have
explicitly parallel

00:00:55.550 --> 00:00:56.590
architecture.

00:00:56.590 --> 00:01:02.010
So what I am going to do is
spend some time just talking

00:01:02.010 --> 00:01:05.320
about implicitly parallel
superscalar processors.

00:01:05.320 --> 00:01:08.290
Because probably the entire time
you guys were born till

00:01:08.290 --> 00:01:11.220
now, this has been the
mainstream, people are

00:01:11.220 --> 00:01:13.220
building these things,
and we are use to it.

00:01:13.220 --> 00:01:14.980
And now we are kind
of doing a switch.

00:01:14.980 --> 00:01:17.480
Then we'll go into explicit
parallelism processors and

00:01:17.480 --> 00:01:20.310
kind of look at different types
in there, and get a feel

00:01:20.310 --> 00:01:23.270
for the big picture.

00:01:23.270 --> 00:01:26.140
So let's start at implicitly
parallel superscalar

00:01:26.140 --> 00:01:27.350
processors.

00:01:27.350 --> 00:01:29.160
So there are two types of
superscalar processors.

00:01:29.160 --> 00:01:31.140
One is what we call statically
scheduled.

00:01:31.140 --> 00:01:34.330
Those are kind of simpler ones,
where you use compiler

00:01:34.330 --> 00:01:36.620
techniques to figure out where
the parallelism is.

00:01:36.620 --> 00:01:40.280
And what happens is the computer
keeps executing,

00:01:40.280 --> 00:01:42.600
intead of one instruction at a
time, the few instructions

00:01:42.600 --> 00:01:44.440
next to each other
in one bunch.

00:01:44.440 --> 00:01:47.180
Like a bundle after
bundle type thing.

00:01:47.180 --> 00:01:49.570
On the other hand, dynamically
scheduled processors --

00:01:49.570 --> 00:01:52.260
things like the current Pentiums
-- are a lot more

00:01:52.260 --> 00:01:52.850
complicated.

00:01:52.850 --> 00:01:55.540
They have to extract instruction
level parallelism.

00:01:55.540 --> 00:01:58.020
ILP doesn't mean integer linear
programming, it's

00:01:58.020 --> 00:02:00.290
instruction level parallelism.

00:02:00.290 --> 00:02:03.840
Schedule them as soon as
operands become available,

00:02:03.840 --> 00:02:06.440
when the data is able to
run these instructions.

00:02:06.440 --> 00:02:08.520
Then there's just a bunch of
things that get more about

00:02:08.520 --> 00:02:10.610
parallelism, things like rename
registers to eliminate

00:02:10.610 --> 00:02:11.840
some dependences.

00:02:11.840 --> 00:02:14.170
You execute things
out of order.

00:02:14.170 --> 00:02:16.580
If later instructions the
operands become available

00:02:16.580 --> 00:02:19.300
early, you'll get those things
done instead of waiting.

00:02:19.300 --> 00:02:20.770
You can speculate to execute.

00:02:20.770 --> 00:02:23.430
I'll go through a little bit in
detail to kind of explain

00:02:23.430 --> 00:02:24.680
what these things might be.

00:02:27.774 --> 00:02:29.680
Why is this not going down.

00:02:29.680 --> 00:02:32.180
Oops.

00:02:32.180 --> 00:02:36.540
So if you look at a
normal pipeline.

00:02:36.540 --> 00:02:40.230
So this is a 004
type pipeline.

00:02:40.230 --> 00:02:43.790
What I have is a very
simplistic four

00:02:43.790 --> 00:02:45.210
stage pipeline in there.

00:02:45.210 --> 00:02:48.050
So a normal microprocessor,
a single-issue, will do

00:02:48.050 --> 00:02:50.080
something like this.

00:02:50.080 --> 00:02:52.050
And if you look at it, there's
still a little bit of

00:02:52.050 --> 00:02:53.680
parallelism here.

00:02:53.680 --> 00:02:56.320
Because you don't wait till the
first thing finishes to go

00:02:56.320 --> 00:02:58.250
to the second thing.

00:02:58.250 --> 00:03:01.670
If you look at a superscalar,
you have something like this.

00:03:01.670 --> 00:03:03.760
This is an in-order
superscalar.

00:03:03.760 --> 00:03:07.020
What happens is in every cycle
instead of doing one, you

00:03:07.020 --> 00:03:10.360
fetch two, you decode two,
you execute two, and

00:03:10.360 --> 00:03:12.080
so on and so forth.

00:03:12.080 --> 00:03:15.070
In an out-of-order super-scalar,
these are not

00:03:15.070 --> 00:03:16.670
going in these very
nice boundaries.

00:03:16.670 --> 00:03:19.930
You have a fetch unit that
fetches like hundreds ahead,

00:03:19.930 --> 00:03:22.580
and it keeps issuing as soon
as things are fetched and

00:03:22.580 --> 00:03:24.540
decoded to the execute unit.

00:03:24.540 --> 00:03:26.600
And it's a lot more of a complex
picture in there.

00:03:26.600 --> 00:03:29.260
I'm not going to show too much
of the picture there, because

00:03:29.260 --> 00:03:31.830
it's a very complicated thing.

00:03:31.830 --> 00:03:35.910
So the first thing the processor
has to do is, it has

00:03:35.910 --> 00:03:38.890
to look for true data
dependences.

00:03:38.890 --> 00:03:42.300
True data dependence says that
this instruction in fact is

00:03:42.300 --> 00:03:45.990
using something produced
by the previous guy.

00:03:45.990 --> 00:03:50.395
So this is important because
if the two instructions are

00:03:50.395 --> 00:03:53.410
data dependent, they cannot be
executed simultaneously.

00:03:53.410 --> 00:03:54.590
You to wait till the first
guy finishes to

00:03:54.590 --> 00:03:55.470
get the second guy.

00:03:55.470 --> 00:03:58.480
It cannot be completely
overlapped, and you can't

00:03:58.480 --> 00:03:59.910
execute them in out-of-order.

00:03:59.910 --> 00:04:01.930
You have to make sure the
data comes in before you

00:04:01.930 --> 00:04:03.180
actually use it.

00:04:05.190 --> 00:04:09.360
In computer architecture jargon,
this is called a

00:04:09.360 --> 00:04:10.340
pipeline hazard.

00:04:10.340 --> 00:04:12.120
And this is called a
Read After Write

00:04:12.120 --> 00:04:13.750
hazard, or RAW hazard.

00:04:13.750 --> 00:04:18.370
What that means is that the
write has to finish before you

00:04:18.370 --> 00:04:20.800
can do the read.

00:04:20.800 --> 00:04:23.660
In a microprocessor, people try
very hard to minimize the

00:04:23.660 --> 00:04:26.490
time you have to wait to do
that, and you really have to

00:04:26.490 --> 00:04:27.740
honor that.

00:04:32.780 --> 00:04:34.550
In hardware/software what you
have to do is you have to

00:04:34.550 --> 00:04:38.330
preserve this program
ordering.

00:04:38.330 --> 00:04:41.820
The program has to be executed
sequentially, determined by

00:04:41.820 --> 00:04:42.630
the source program.

00:04:42.630 --> 00:04:44.560
So if the source program says
some order of doing things,

00:04:44.560 --> 00:04:44.900
you better --

00:04:44.900 --> 00:04:46.330
if there's some reason for
doing that, you better

00:04:46.330 --> 00:04:48.200
actually adhere to that order.

00:04:48.200 --> 00:04:51.390
You can't go and just do things
in a haphazard way.

00:04:51.390 --> 00:04:55.410
And dependences are basically
a fact of the

00:04:55.410 --> 00:04:56.940
program, so what you got.

00:04:56.940 --> 00:04:58.570
If you're lucky you'll get a
program without too many

00:04:58.570 --> 00:05:01.160
dependences, but most probably
you'll get programs that have

00:05:01.160 --> 00:05:02.010
a lot of dependences.

00:05:02.010 --> 00:05:03.260
That's normal.

00:05:06.050 --> 00:05:08.170
There's a lot of importance
of the data dependence.

00:05:08.170 --> 00:05:10.120
It indicates the possibility
of these hazards, how these

00:05:10.120 --> 00:05:11.590
dependences have to work.

00:05:11.590 --> 00:05:14.020
And it determines the order in
which the results might be

00:05:14.020 --> 00:05:18.755
calculated, because if you need
the result of that to do

00:05:18.755 --> 00:05:21.700
the next, you have what you
call a dependency chain.

00:05:21.700 --> 00:05:23.730
And you have to excute
that in that order.

00:05:23.730 --> 00:05:26.930
And because of the dependency
chain, it sets an upper bound

00:05:26.930 --> 00:05:29.980
of how much parallelism that
can be possibly expected.

00:05:29.980 --> 00:05:32.230
If you can say in all your
program there's nothing

00:05:32.230 --> 00:05:35.190
dependent -- every instruction
just can go any time --

00:05:35.190 --> 00:05:38.390
then you can say the best
computer will get done in one

00:05:38.390 --> 00:05:40.180
cycle, because everything
can run.

00:05:40.180 --> 00:05:43.285
But if you say the next
instruction is dependent on

00:05:43.285 --> 00:05:44.760
the previous one, the next
instruction is dependent on

00:05:44.760 --> 00:05:46.610
the previous one, you
have a chain.

00:05:46.610 --> 00:05:48.620
And no matter how good the
hardware, you have to wait

00:05:48.620 --> 00:05:50.580
till that chain finishes.

00:05:50.580 --> 00:05:54.310
And you don't get that
much parallelism.

00:05:54.310 --> 00:05:57.150
So the goal is to exploit
parallelism by preserving the

00:05:57.150 --> 00:06:01.290
program order where it affects
the outcome of the program.

00:06:01.290 --> 00:06:04.190
So if we want to have a look and
feel like the program is

00:06:04.190 --> 00:06:07.620
run on a nice single-issue
machine that does one

00:06:07.620 --> 00:06:09.760
instruction after another after
another, that's the

00:06:09.760 --> 00:06:10.690
world we are looking in.

00:06:10.690 --> 00:06:13.730
And then we are doing all this
underneath to kind of get

00:06:13.730 --> 00:06:17.540
performance, but give
that abstraction.

00:06:17.540 --> 00:06:21.370
So there are other dependences
that we can do better.

00:06:21.370 --> 00:06:23.850
There are two types of
name dependences.

00:06:23.850 --> 00:06:29.450
That means there's no real
program use of data, but there

00:06:29.450 --> 00:06:31.340
are limited resources
in the program.

00:06:31.340 --> 00:06:33.130
And you have resource
contentions.

00:06:33.130 --> 00:06:38.430
So the two types of resources
are registers and memory.

00:06:38.430 --> 00:06:40.610
So linear resource
contentions.

00:06:40.610 --> 00:06:45.830
The first name dependence is
what we call anti-dependence.

00:06:45.830 --> 00:06:48.230
Anti-dependence means that --

00:06:54.840 --> 00:06:57.640
what I need to do is, I want
to write this register.

00:06:57.640 --> 00:06:59.180
But in the previous instruction
I'm actually

00:06:59.180 --> 00:07:02.110
reading the register.

00:07:02.110 --> 00:07:03.460
Because I'm writing the
next one, I'm not

00:07:03.460 --> 00:07:05.270
really using the value.

00:07:05.270 --> 00:07:08.220
But I cannot write it until
I have read that value.

00:07:08.220 --> 00:07:10.960
Because the minute I write it,
I lose the previous value.

00:07:10.960 --> 00:07:14.270
And if I haven't used
it, I'm out of luck.

00:07:14.270 --> 00:07:18.070
So there might be a case that
I have a register, that I'm

00:07:18.070 --> 00:07:20.740
reading the register and
rewriting it some new value.

00:07:20.740 --> 00:07:22.990
But I have to wait till the
reading is done before I do

00:07:22.990 --> 00:07:24.240
this new write.

00:07:24.240 --> 00:07:26.180
And that's called
anti-dependence.

00:07:26.180 --> 00:07:30.900
So what that means is we have
to wait to run this

00:07:30.900 --> 00:07:33.410
instruction until this is
all -- you can't do

00:07:33.410 --> 00:07:36.960
it all before that.

00:07:36.960 --> 00:07:41.230
So this is called a Write After
Read, as I said, in the

00:07:41.230 --> 00:07:42.460
architecture jargon.

00:07:42.460 --> 00:07:44.850
The other dependences have
what you call output

00:07:44.850 --> 00:07:46.470
dependence.

00:07:46.470 --> 00:07:50.550
Two guys are writing
the register, and

00:07:50.550 --> 00:07:51.720
then I'm reading it.

00:07:51.720 --> 00:07:55.710
So I want to read the value
the last guy wrote.

00:07:55.710 --> 00:07:59.080
So if I reorder that,
I get a wrong value.

00:07:59.080 --> 00:08:01.020
Actually you can even
do better in here.

00:08:01.020 --> 00:08:02.806
How can you do better in here?

00:08:02.806 --> 00:08:03.640
AUDIENCE: You can eliminate I.

00:08:03.640 --> 00:08:03.812
PROFESSOR: Yeah.

00:08:03.812 --> 00:08:05.580
You can elimiate the first one,
because nobody's using

00:08:05.580 --> 00:08:06.330
that value.

00:08:06.330 --> 00:08:09.740
So you can go even further
and further, but

00:08:09.740 --> 00:08:10.680
this is also a hazard.

00:08:10.680 --> 00:08:15.730
This is called a Write
After Write hazard.

00:08:15.730 --> 00:08:20.260
And the interesting thing is
by doing what you call

00:08:20.260 --> 00:08:23.650
register renaming, you can
eliminate these things.

00:08:23.650 --> 00:08:26.420
So why do both have to use
the same register?

00:08:26.420 --> 00:08:29.050
In these two, if I use a
different register I don't

00:08:29.050 --> 00:08:30.770
have that dependency.

00:08:30.770 --> 00:08:35.720
And so a lot of times in
software, and also in modern

00:08:35.720 --> 00:08:39.200
superscalar hardware, there's
this huge amount of hardware

00:08:39.200 --> 00:08:41.650
resources that actually
do register renaming.

00:08:41.650 --> 00:08:43.400
So they realized that
anti-dependence is output

00:08:43.400 --> 00:08:44.220
dependent, and said
-- "Wait minute.

00:08:44.220 --> 00:08:45.280
Why do I even have to do that?

00:08:45.280 --> 00:08:47.450
I can use a different
register." So even

00:08:47.450 --> 00:08:49.130
though you have --

00:08:49.130 --> 00:08:52.260
Intel basically [UNINTELLIGIBLE]

00:08:52.260 --> 00:08:54.340
accessory only have
eight registers.

00:08:54.340 --> 00:08:56.060
They are about 100
registers behind.

00:08:56.060 --> 00:08:58.190
Hardware registers just
basically let you do this

00:08:58.190 --> 00:09:01.120
reordering and renaming
-- register renaming.

00:09:03.670 --> 00:09:05.660
So the other type of depencence
is control

00:09:05.660 --> 00:09:07.170
dependence.

00:09:07.170 --> 00:09:11.150
So what that means is if you
have a program like this, you

00:09:11.150 --> 00:09:13.630
have to preserve the
program ordering.

00:09:13.630 --> 00:09:19.300
And what that means is S1 is
control dependent on p1.

00:09:19.300 --> 00:09:22.475
Because depending on what p1 is,
it will depend on this one

00:09:22.475 --> 00:09:23.870
to get excuted.

00:09:23.870 --> 00:09:27.370
S2 is control dependent
on p2, but not p1.

00:09:27.370 --> 00:09:32.550
So it doesn't matter what p1
does, S2 will execute only if

00:09:32.550 --> 00:09:34.800
p2 is true.

00:09:34.800 --> 00:09:36.260
So there's a control dependence
in there.

00:09:39.880 --> 00:09:42.900
Another interesting thing is
control dependence may -- you

00:09:42.900 --> 00:09:45.190
don't need to preserve
it all the time.

00:09:45.190 --> 00:09:48.250
You might be able to do things
out of this order.

00:09:48.250 --> 00:09:51.050
Basically, what you can do is if
you are willing to do more

00:09:51.050 --> 00:09:53.440
work, you can say -- "Well,
I will do this.

00:09:53.440 --> 00:09:55.590
I don't know that I really need
it, because I don't know

00:09:55.590 --> 00:09:56.950
whether the p2 is true or not.

00:09:56.950 --> 00:09:58.210
But I'll just keep doing it.

00:09:58.210 --> 00:10:02.800
And then if I really wanted,
I'll actually have the results

00:10:02.800 --> 00:10:07.170
ready for me." And that's called
speculative execution.

00:10:07.170 --> 00:10:08.550
So you can do speculation.

00:10:08.550 --> 00:10:10.220
You speculatively think
that you will need

00:10:10.220 --> 00:10:11.470
something, and go do it.

00:10:14.320 --> 00:10:18.000
Speculation provides you with
a lot of increased ILP,

00:10:18.000 --> 00:10:21.320
because it can overcome
control dependence by

00:10:21.320 --> 00:10:24.620
executing through branches,
before even you know where the

00:10:24.620 --> 00:10:25.700
branch is going.

00:10:25.700 --> 00:10:28.120
And a lot of times you can go
through both directions, and

00:10:28.120 --> 00:10:29.900
say -- "Wait a minute, I don't
know which way I'm going.

00:10:29.900 --> 00:10:33.210
I'll do both sides." And I know
at least one side you are

00:10:33.210 --> 00:10:34.230
going, and that will
be useful.

00:10:34.230 --> 00:10:37.090
And you can go more and more,
and soon you see that you are

00:10:37.090 --> 00:10:39.170
doing so much more work than
actually will be useful.

00:10:41.890 --> 00:10:45.710
So the first level of
speculation is -- speculation

00:10:45.710 --> 00:10:48.780
basically says, you go, you
fetch, issue, and execute

00:10:48.780 --> 00:10:49.240
everything.

00:10:49.240 --> 00:10:52.060
You do the end of the thing
without just committing your

00:10:52.060 --> 00:10:55.160
weight into the commit to make
sure that the right thing

00:10:55.160 --> 00:10:56.000
actually happened.

00:10:56.000 --> 00:10:58.800
So this is the full
speculation.

00:10:58.800 --> 00:11:02.140
There's a little bit of less
speculation called dynamic

00:11:02.140 --> 00:11:02.580
scheduling.

00:11:02.580 --> 00:11:04.760
If you look at a microprocessor,
one of the

00:11:04.760 --> 00:11:09.120
biggest problems is the pipeline
stall is a branch.

00:11:09.120 --> 00:11:12.430
You can't keep even a pipeline
going, even in a single-issue

00:11:12.430 --> 00:11:14.520
machine, if there's a branch,
because the branch condition

00:11:14.520 --> 00:11:15.470
gets resolved.

00:11:15.470 --> 00:11:18.750
Not after the next instruction
has to get fetched.

00:11:18.750 --> 00:11:21.100
So if you do a normal thing,
you just have to

00:11:21.100 --> 00:11:22.870
reinstall the pipeline.

00:11:22.870 --> 00:11:29.800
So what dynamic scheduling or
a branch predictor sometimes

00:11:29.800 --> 00:11:31.880
does is, it will say I
will predict where

00:11:31.880 --> 00:11:33.660
the branch is going.

00:11:33.660 --> 00:11:35.730
So I might not have fed board
direction, but I will

00:11:35.730 --> 00:11:38.340
speculatively go fetch down
one path, because it looks

00:11:38.340 --> 00:11:39.620
like it which it's going.

00:11:39.620 --> 00:11:42.890
For many times, like for example
in a loop, 99% of the

00:11:42.890 --> 00:11:44.935
time you are going in the
backage, because you don't go

00:11:44.935 --> 00:11:45.450
through that.

00:11:45.450 --> 00:11:46.750
And then if you predict
that you are mostly

00:11:46.750 --> 00:11:47.580
[UNINTELLIGIBLE].

00:11:47.580 --> 00:11:49.730
So the branch predictors are
pretty good at finding these

00:11:49.730 --> 00:11:50.870
kind of cases.

00:11:50.870 --> 00:11:53.710
There are very few branches
that are kind of 50-50.

00:11:53.710 --> 00:11:56.260
Most branches have
a preferred path.

00:11:56.260 --> 00:11:58.780
If you find the preferred path
you can go through that, and

00:11:58.780 --> 00:12:00.200
you don't pay any penalty.

00:12:00.200 --> 00:12:01.860
The penalty is if you made a
mistake, you had to kind of

00:12:01.860 --> 00:12:03.450
back up a few times.

00:12:03.450 --> 00:12:05.490
So you can at least do
in one direction.

00:12:05.490 --> 00:12:08.240
Most hardware do that, even the
simplest things do that.

00:12:08.240 --> 00:12:10.550
But if you do good speculation
you go both.

00:12:10.550 --> 00:12:13.150
You say -- "Eh, there's a chance
if I go down that path

00:12:13.150 --> 00:12:13.900
I'm going to lose a lot.

00:12:13.900 --> 00:12:18.920
So I'll do that, too." So that
does a lot of expensive stuff.

00:12:18.920 --> 00:12:23.080
And basically this is more
for data flow model.

00:12:23.080 --> 00:12:26.160
So as soon as data get available
you don't think too

00:12:26.160 --> 00:12:30.150
much about control, you
keep firing that.

00:12:30.150 --> 00:12:36.780
So today's superscalar
processors have huge amount of

00:12:36.780 --> 00:12:37.460
speculation.

00:12:37.460 --> 00:12:39.290
You speculate on everything.

00:12:39.290 --> 00:12:40.170
Branch prediction.

00:12:40.170 --> 00:12:42.690
You assume all the branches,
multilevel down you predict,

00:12:42.690 --> 00:12:43.470
and go that.

00:12:43.470 --> 00:12:44.360
Value prediction.

00:12:44.360 --> 00:12:45.960
You look at it and say -- "Hey,
I think it's going to be

00:12:45.960 --> 00:12:50.450
two." And in fact there's a
paper that says about 80% of

00:12:50.450 --> 00:12:51.700
program values are zero.

00:12:55.130 --> 00:12:56.060
And then you say -- "OK.

00:12:56.060 --> 00:12:57.510
I'll think it's zero,
and it'll go on.

00:12:57.510 --> 00:12:59.530
And if it is not zero, I'll
have to come back and do

00:12:59.530 --> 00:13:00.610
that." So things like that.

00:13:00.610 --> 00:13:02.437
AUDIENCE: Do you know what
percentage of the time it has

00:13:02.437 --> 00:13:03.870
to go back?

00:13:03.870 --> 00:13:08.350
PROFESSOR: A lot of times I
think it is probably an 80-20

00:13:08.350 --> 00:13:11.420
type thing, but if you do too
much you're always backing up.

00:13:11.420 --> 00:13:13.310
But you can at least do
a few things down

00:13:13.310 --> 00:13:14.650
assuming it's zero.

00:13:14.650 --> 00:13:16.260
So things like that.

00:13:16.260 --> 00:13:21.530
People, try to take advantage
of the statistical nature of

00:13:21.530 --> 00:13:24.690
programs. And you are
mining every day.

00:13:24.690 --> 00:13:29.160
So basically there's no --

00:13:29.160 --> 00:13:30.420
it's almost at the entropy.

00:13:30.420 --> 00:13:33.030
So every information is kind
of taken advantage in the

00:13:33.030 --> 00:13:37.370
program, but what that means
is you are wasting a lot of

00:13:37.370 --> 00:13:38.470
time cycles.

00:13:38.470 --> 00:13:40.740
So the conventional
wisdom was --

00:13:40.740 --> 00:13:42.610
"You have Moore's slope.

00:13:42.610 --> 00:13:43.920
You keep getting these
transistors.

00:13:43.920 --> 00:13:47.680
There's nothing to do with it,
so let me do more other work.

00:13:47.680 --> 00:13:50.080
We'll predicate, we'll do
additional work, we'll go

00:13:50.080 --> 00:13:52.560
through multipe branches, we'll
assume things are zero.

00:13:52.560 --> 00:13:54.110
Because what's wasted?

00:13:54.110 --> 00:13:57.580
Because it's extra work, if it
is wrong we just give it up."

00:13:57.580 --> 00:14:00.380
So that's the way it went, and
the thing is it's very

00:14:00.380 --> 00:14:00.895
inefficient.

00:14:00.895 --> 00:14:03.900
Because a lot of times you are
doing -- think about even a

00:14:03.900 --> 00:14:04.960
simple cache.

00:14:04.960 --> 00:14:07.580
If you have 4-way as a cache.

00:14:07.580 --> 00:14:09.700
Every cycle when you're doing
a memory fetch, you are

00:14:09.700 --> 00:14:14.140
fetching on all four, assuming
one of it will have hit.

00:14:14.140 --> 00:14:17.480
Even if you have a cache hit
where only one bank is hit,

00:14:17.480 --> 00:14:19.350
and all the other three
banks are not hit.

00:14:19.350 --> 00:14:21.750
So you are just doing a
lot more extra work

00:14:21.750 --> 00:14:23.340
just to get one thing.

00:14:23.340 --> 00:14:26.580
Of course because if you wait to
figure out which bank, it's

00:14:26.580 --> 00:14:28.000
going to add a little
bit more delay.

00:14:28.000 --> 00:14:28.710
So you won't do it parallelly.

00:14:28.710 --> 00:14:30.790
You know that's it's going to
be one of the lines, so you

00:14:30.790 --> 00:14:32.900
just go fetch everything
and then later decide

00:14:32.900 --> 00:14:33.840
which one you want.

00:14:33.840 --> 00:14:38.390
So things like that really
waste energy.

00:14:38.390 --> 00:14:41.560
And what has been happening in
the last 10 years is you

00:14:41.560 --> 00:14:44.320
double the amount of
transistors, and you add 5%

00:14:44.320 --> 00:14:46.060
more performance gain.

00:14:46.060 --> 00:14:49.470
Because statistically you have
mined most of the lower

00:14:49.470 --> 00:14:51.260
hanging fruit, there's
nothing much left.

00:14:51.260 --> 00:14:53.800
So you're getting to a point
that has a little bit of a

00:14:53.800 --> 00:14:56.280
statistical significance,
and you go after that.

00:14:56.280 --> 00:14:59.200
So of course, most of
the time it's wrong.

00:14:59.200 --> 00:15:03.060
So this leads to this chart that
actually yesterday I also

00:15:03.060 --> 00:15:03.730
pointed out.

00:15:03.730 --> 00:15:06.220
So you are going from hot plate
to nuclear reactor, to

00:15:06.220 --> 00:15:08.790
rocket nozzle.

00:15:08.790 --> 00:15:10.400
We tend to be going
in that direction.

00:15:10.400 --> 00:15:12.390
That is the path, because we
are just doing all these

00:15:12.390 --> 00:15:14.450
wasteful things.

00:15:14.450 --> 00:15:18.230
And right now, the power
consumption on processors is

00:15:18.230 --> 00:15:21.420
significant enough in both
things like laptops --

00:15:21.420 --> 00:15:24.110
because the battery's not
getting faster -- as well as

00:15:24.110 --> 00:15:25.220
things like Google.

00:15:25.220 --> 00:15:28.360
So doing this extra
useless work is

00:15:28.360 --> 00:15:29.610
actually starting to impact.

00:15:32.670 --> 00:15:34.980
So for example, if you look
at something like Pentium.

00:15:34.980 --> 00:15:40.310
You have 11 stages
of instructions.

00:15:40.310 --> 00:15:45.350
You can execute 3 x86
instructions per cycle.

00:15:45.350 --> 00:15:49.770
So you're doing this huge
superscalar thing, but

00:15:49.770 --> 00:15:52.750
something that had been creeping
in lately is also

00:15:52.750 --> 00:15:55.700
some amount of explicit
parallelism.

00:15:55.700 --> 00:15:58.780
So they introduced things like
MMX and SSE instructions.

00:15:58.780 --> 00:16:01.280
They are explicit parallelism,
visible to the user.

00:16:01.280 --> 00:16:03.670
So it's not hiding trying
to get parallelism.

00:16:03.670 --> 00:16:06.980
So we have been slowly moving to
this kind of model, saying

00:16:06.980 --> 00:16:09.670
if you want performance you have
to do something manual.

00:16:09.670 --> 00:16:11.580
So people who really cared
about performance had

00:16:11.580 --> 00:16:12.490
to deal with that.

00:16:12.490 --> 00:16:17.450
And of course, we put multiple
chips together to build a

00:16:17.450 --> 00:16:19.250
multiprocessor --

00:16:19.250 --> 00:16:22.120
it's not in a single chip --
that actually do parallel

00:16:22.120 --> 00:16:22.800
processing.

00:16:22.800 --> 00:16:28.270
So for about three, four years
if you buy a workstation it

00:16:28.270 --> 00:16:30.320
had two processors
sitting in there.

00:16:30.320 --> 00:16:32.650
So dual processor, quad
processor machines came about,

00:16:32.650 --> 00:16:33.820
and people started using that.

00:16:33.820 --> 00:16:37.240
So it's not like we are doing
this shift abruptly, we have

00:16:37.240 --> 00:16:39.770
been going that direction.

00:16:39.770 --> 00:16:41.880
For people who really cared
about performance, actually

00:16:41.880 --> 00:16:43.770
had to deal with that and were
actually using that.

00:16:46.960 --> 00:16:47.580
OK.

00:16:47.580 --> 00:16:49.380
So let's switch gears a little
bit and do explicit

00:16:49.380 --> 00:16:50.220
parallelism.

00:16:50.220 --> 00:16:51.980
So this is kind of
where we are --

00:16:51.980 --> 00:16:55.500
where we are today, where
we are switching.

00:16:55.500 --> 00:17:00.740
So basically, these are the
machines that parallelism

00:17:00.740 --> 00:17:02.410
exposed to software --
either compiler.

00:17:02.410 --> 00:17:05.890
So you might not see it as a
user, but it exposes some

00:17:05.890 --> 00:17:07.020
layer of software.

00:17:07.020 --> 00:17:09.210
And there are many different
forms of it.

00:17:09.210 --> 00:17:15.110
From very loosely coupled
multiprocessors sitting on a

00:17:15.110 --> 00:17:19.610
board, or even sitting on
multipe machines -- things

00:17:19.610 --> 00:17:22.460
like a cluster of
workstations --

00:17:22.460 --> 00:17:24.030
to very tightly coupled
machines.

00:17:24.030 --> 00:17:26.290
So we'll go through, and figure
out what are all the

00:17:26.290 --> 00:17:27.590
flavors of these things.

00:17:27.590 --> 00:17:28.625
AUDIENCE: Excuse me.

00:17:28.625 --> 00:17:29.142
PROFESSOR: Mhmm?

00:17:29.142 --> 00:17:31.830
AUDIENCE: So does it mean that
since there being the level

00:17:31.830 --> 00:17:35.900
parallelism, the processor can
exploit the fact that the

00:17:35.900 --> 00:17:37.740
compiler knows the higher
level instructions?

00:17:37.740 --> 00:17:38.950
Does that make any difference?

00:17:38.950 --> 00:17:40.410
PROFESSOR: It goes both ways.

00:17:40.410 --> 00:17:45.730
So what the processor knows is
it know values for everything.

00:17:45.730 --> 00:17:49.200
So it has full exact knowledge
of what's going on.

00:17:49.200 --> 00:17:51.620
Compiler is an abstraction.

00:17:51.620 --> 00:17:54.200
In that sense, processor
wins in those.

00:17:54.200 --> 00:17:56.730
On the other hand, compile
time doesn't

00:17:56.730 --> 00:17:58.160
affect the run time.

00:17:58.160 --> 00:18:00.670
So the compiler has a much
bigger view of the world.

00:18:03.440 --> 00:18:05.750
Even the most aggressive
processor can't look ahead

00:18:05.750 --> 00:18:07.940
more than 100 instructions.

00:18:07.940 --> 00:18:09.760
On the other hand, the compiler
sees ahead of

00:18:09.760 --> 00:18:11.600
millions of instructions.

00:18:11.600 --> 00:18:14.280
And so the compiler has the
ability to kind of get the big

00:18:14.280 --> 00:18:16.960
picture and do things --
global kind of things.

00:18:16.960 --> 00:18:19.360
But on the other hand, it loses
out when it doesn't have

00:18:19.360 --> 00:18:20.650
information.

00:18:20.650 --> 00:18:23.130
Whereas when you do the hardware
parallelism, you have

00:18:23.130 --> 00:18:23.870
full information.

00:18:23.870 --> 00:18:24.840
AUDIENCE: You don't have
to give up one at

00:18:24.840 --> 00:18:27.290
the loss of the other.

00:18:27.290 --> 00:18:29.490
PROFESSOR: The thing is, I don't
think we have a good way

00:18:29.490 --> 00:18:31.540
of combining both very well.

00:18:31.540 --> 00:18:34.350
Because the thing is, sometimes
global optimization

00:18:34.350 --> 00:18:36.140
needs local information,
and that's not

00:18:36.140 --> 00:18:38.150
available at run time.

00:18:38.150 --> 00:18:40.396
And global optimization is very
costly, so you can't say

00:18:40.396 --> 00:18:43.860
-- "OK, I'm going to do it any
time." So I think it's kind of

00:18:43.860 --> 00:18:45.720
even hybrid things.

00:18:45.720 --> 00:18:47.800
There's no nice mesh in there.

00:18:51.960 --> 00:18:55.540
So if you think a little bit
about parallelism, one

00:18:55.540 --> 00:18:58.100
interesting thing is
this Little's Law.

00:18:58.100 --> 00:19:05.500
Little's Law says parallelism
is a multiplication of

00:19:05.500 --> 00:19:07.020
throughput vs. latency.

00:19:09.840 --> 00:19:14.980
So the way to think about that
is the parallelism is dictated

00:19:14.980 --> 00:19:16.500
by the program in some sense.

00:19:16.500 --> 00:19:19.610
The program has a certain
amount of parallelism.

00:19:19.610 --> 00:19:22.735
So if you have a thing that has
a lot of latency to get to

00:19:22.735 --> 00:19:27.870
the result, what that means is
there's a certain amount of

00:19:27.870 --> 00:19:30.850
throughput you can satisfy.

00:19:30.850 --> 00:19:34.380
Whereas if you have a thing that
has a very low latency

00:19:34.380 --> 00:19:37.910
operation, you can
go much wider.

00:19:37.910 --> 00:19:40.460
So if you look at Intel
processors, what they have

00:19:40.460 --> 00:19:42.450
done is the superscalars --

00:19:42.450 --> 00:19:45.320
they have actually, to get
things faster they have a very

00:19:45.320 --> 00:19:46.380
long latency.

00:19:46.380 --> 00:19:48.210
Because they know they
couldn't go more than

00:19:48.210 --> 00:19:49.980
three or four wide.

00:19:49.980 --> 00:19:52.630
So they went like 55 the
pipeline, three wide.

00:19:55.130 --> 00:19:58.140
Because you can go fast,
so they assume the

00:19:58.140 --> 00:19:59.400
parallelism fits here.

00:19:59.400 --> 00:20:00.510
So still you need a lot
of parallelism.

00:20:00.510 --> 00:20:01.870
So you say -- "Three, why
should [UNINTELLIGIBLE]

00:20:01.870 --> 00:20:02.940
issue machine.

00:20:02.940 --> 00:20:05.600
[UNINTELLIGIBLE] three it's no
big deal." But no, if you have

00:20:05.600 --> 00:20:12.210
55 the pipeline you need to have
165 parallel instructions

00:20:12.210 --> 00:20:14.560
on the fly any given time.

00:20:14.560 --> 00:20:15.410
So that's the thing.

00:20:15.410 --> 00:20:17.960
Even in the moder machine,
there's about hundreds of

00:20:17.960 --> 00:20:19.180
instruction on the
fly, because the

00:20:19.180 --> 00:20:22.230
pipeline is so large.

00:20:22.230 --> 00:20:24.250
So if you said 3-issue,
it's not that.

00:20:24.250 --> 00:20:25.890
I mean this happens in there.

00:20:25.890 --> 00:20:29.280
So this gives designers a lot
of flexibiilty in where you

00:20:29.280 --> 00:20:30.540
are expanding.

00:20:30.540 --> 00:20:34.380
And in some ways you
can have a lot --

00:20:34.380 --> 00:20:36.930
there are some machines that
are a lot wider, but the

00:20:36.930 --> 00:20:38.070
latency is --

00:20:38.070 --> 00:20:41.020
For example, if you look
at an Itanium.

00:20:41.020 --> 00:20:46.290
It's clock cycle is about half
the time of the Pentium,

00:20:46.290 --> 00:20:51.160
because it has a lot less
latency but a lot wider.

00:20:51.160 --> 00:20:52.580
So you can do these
kind of tradeoffs.

00:20:55.240 --> 00:20:57.690
Types of parallelism.

00:20:57.690 --> 00:21:00.750
There are four categorizations
here.

00:21:00.750 --> 00:21:03.800
So one categorization is,
you have pipeline.

00:21:03.800 --> 00:21:07.620
You do the same thing in
a pipeline fashion.

00:21:07.620 --> 00:21:09.310
So you do the same
instruction.

00:21:09.310 --> 00:21:12.450
You do a little bit, and you
start another copy of another

00:21:12.450 --> 00:21:13.250
copy of another copy.

00:21:13.250 --> 00:21:15.710
So you kind of pipeline the
same thing down here.

00:21:15.710 --> 00:21:17.550
Kind of a vector machine --
we'll go through categories

00:21:17.550 --> 00:21:19.840
that kind of fit in here.

00:21:19.840 --> 00:21:22.390
Another category is data-level
parallelism.

00:21:22.390 --> 00:21:29.130
What that means is, in a given
cycle you do the same thing

00:21:29.130 --> 00:21:30.980
many many many many --

00:21:30.980 --> 00:21:33.610
the same instructions for
many many things.

00:21:33.610 --> 00:21:35.620
And then next cycle
you do something

00:21:35.620 --> 00:21:37.133
different, stuff like that.

00:21:37.133 --> 00:21:39.320
Thread-level parallelism breaks
in the other way.

00:21:39.320 --> 00:21:41.360
Thread-level parallelism
says --

00:21:41.360 --> 00:21:43.980
"I am not connecting the cycles,
they are independent.

00:21:43.980 --> 00:21:48.590
Each thread can go do something
different."

00:21:48.590 --> 00:21:50.470
And instruction-level
parallelism is kind of a

00:21:50.470 --> 00:21:51.280
combination.

00:21:51.280 --> 00:21:54.865
What you are doing is, you are
doing cycle by cycle -- they

00:21:54.865 --> 00:21:57.820
are connected -- and each cycle
you do some kind of a

00:21:57.820 --> 00:21:59.320
combination of operations.

00:21:59.320 --> 00:22:01.170
So if you look at
this closely.

00:22:01.170 --> 00:22:03.090
So pipelining hits here.

00:22:03.090 --> 00:22:05.590
Data parallel execution,
things like SIMD

00:22:05.590 --> 00:22:06.870
execution hits here.

00:22:06.870 --> 00:22:08.110
Thread-level parallelism.

00:22:08.110 --> 00:22:09.520
Instruction-level parallelism.

00:22:09.520 --> 00:22:11.530
So before a models of
parallelism, what software

00:22:11.530 --> 00:22:18.390
people see kind of fits also in
this architecture picture.

00:22:18.390 --> 00:22:21.440
So when you are designing a
parallel machine, what do you

00:22:21.440 --> 00:22:22.800
have to worry about?

00:22:22.800 --> 00:22:24.700
The first thing is
communication.

00:22:24.700 --> 00:22:26.060
That's the begin --

00:22:26.060 --> 00:22:27.140
the problem in here.

00:22:27.140 --> 00:22:30.930
How do parallel operations
communicate the data results?

00:22:30.930 --> 00:22:33.490
Because it's not only an
issue of bandwith,

00:22:33.490 --> 00:22:35.550
it's an issue of latency.

00:22:35.550 --> 00:22:38.300
The thing about bandwith is that
had been increasing by

00:22:38.300 --> 00:22:38.990
Moore's Law.

00:22:38.990 --> 00:22:40.600
Latency, speed of light.

00:22:40.600 --> 00:22:42.770
So as I pointed out, there's
no Moore's Law on speed of

00:22:42.770 --> 00:22:46.540
light, and you have
to deal with that.

00:22:46.540 --> 00:22:47.650
Synchronization.

00:22:47.650 --> 00:22:50.510
So when people do different
things, how do you synchronize

00:22:50.510 --> 00:22:50.990
at some point?

00:22:50.990 --> 00:22:53.550
Because you can't keep going
on different paths, at some

00:22:53.550 --> 00:22:54.680
point you have to
come together.

00:22:54.680 --> 00:22:56.270
What's the cost?

00:22:56.270 --> 00:22:57.700
What are the processes
of going it?

00:22:57.700 --> 00:23:01.670
Some stuff it's very
explicit --

00:23:01.670 --> 00:23:03.160
you have to deal with that.

00:23:03.160 --> 00:23:06.680
Some machines it's built in,
so every cycle you are

00:23:06.680 --> 00:23:08.840
synchronizing.

00:23:08.840 --> 00:23:14.300
So sometimes it makes it easier
for you, sometimes it

00:23:14.300 --> 00:23:15.940
makes it more inefficient.

00:23:15.940 --> 00:23:18.500
So you have to figure
out what is in here.

00:23:18.500 --> 00:23:20.932
Resource management.

00:23:20.932 --> 00:23:23.920
The thing about parallelism is
you have a lot of things going

00:23:23.920 --> 00:23:28.480
on, and managing that is
a very important issue.

00:23:28.480 --> 00:23:33.970
Because sometimes if you put
things in the wrong place, the

00:23:33.970 --> 00:23:36.260
cost of doing that might
be much higher.

00:23:36.260 --> 00:23:40.890
That really reduces the
benefit of doing that.

00:23:40.890 --> 00:23:43.010
And finally the scalability.

00:23:43.010 --> 00:23:48.070
How do you build process
that not only can do 2x

00:23:48.070 --> 00:23:50.110
parallelism, but can
do thousand?

00:23:50.110 --> 00:23:52.750
How can you keep growing
with Moore's Law.

00:23:52.750 --> 00:23:55.960
So there are some ways you can
get really good numbers, small

00:23:55.960 --> 00:23:58.240
numbers, but as you go bigger
and bigger you can't scale.

00:24:01.880 --> 00:24:02.850
So here's a classic

00:24:02.850 --> 00:24:04.340
classification of parallel machines.

00:24:04.340 --> 00:24:07.610
This has been [? divided ?]
up by Mike Flynn in 1966.

00:24:07.610 --> 00:24:10.310
So he came up with four ways
of classifying a machine.

00:24:10.310 --> 00:24:12.560
First he looked at how

00:24:12.560 --> 00:24:15.100
instruction and data is issued.

00:24:15.100 --> 00:24:18.240
So one thing is single
instruction, single data.

00:24:18.240 --> 00:24:21.080
So there's single instruction
given each cycle, and it

00:24:21.080 --> 00:24:22.040
affects single data.

00:24:22.040 --> 00:24:25.560
This is your conventional
uniprocessor.

00:24:25.560 --> 00:24:28.360
Then came a SIMD machine
-- single

00:24:28.360 --> 00:24:30.160
instruction, multiple data.

00:24:30.160 --> 00:24:32.520
So what that means is the given
instruction affects

00:24:32.520 --> 00:24:34.700
multiple data in here.

00:24:34.700 --> 00:24:38.640
So things like -- there are two
types, distributed memory

00:24:38.640 --> 00:24:39.390
and shared memory.

00:24:39.390 --> 00:24:41.270
I'll go to this distinction
later.

00:24:41.270 --> 00:24:43.120
So there are a bunch
of machines.

00:24:43.120 --> 00:24:46.010
In the good old times this was
a useful trick, because the

00:24:46.010 --> 00:24:48.620
sequencer -- or what ran the
instructions -- was a pretty

00:24:48.620 --> 00:24:50.930
substantial piece of hardware.

00:24:50.930 --> 00:24:54.780
So you build one of them and
use it for many, many data.

00:24:54.780 --> 00:24:57.030
Even today data in a Pentium
if you are doing a SIMD

00:24:57.030 --> 00:24:59.640
instruction, you just issue one
instruction, it affects

00:24:59.640 --> 00:25:04.400
multiple data, and you
can get a nice reuse

00:25:04.400 --> 00:25:06.190
of instruction decoding.

00:25:06.190 --> 00:25:10.920
Reduce the instruction bandwidth
by doing SIMD.

00:25:10.920 --> 00:25:13.600
Then you go to MIMD,
which is Multiple

00:25:13.600 --> 00:25:14.920
Instruction, Multiple Data.

00:25:14.920 --> 00:25:17.600
So we have multiple instruction
streams each

00:25:17.600 --> 00:25:20.510
affecting its own data.

00:25:20.510 --> 00:25:23.240
So each data streams,
instruction streams

00:25:23.240 --> 00:25:23.820
separately.

00:25:23.820 --> 00:25:27.250
So things like message passing
machines, coherent and

00:25:27.250 --> 00:25:28.390
non-coherent shared memory.

00:25:28.390 --> 00:25:30.060
I'll go into details
of coherence and

00:25:30.060 --> 00:25:31.180
non-coherence later.

00:25:31.180 --> 00:25:35.060
There are multiple categories
within that too.

00:25:35.060 --> 00:25:38.090
And then finally, there's kind
of a misnomer, MISD.

00:25:38.090 --> 00:25:39.520
There hasn't been a
single machine.

00:25:39.520 --> 00:25:41.595
It doesn't make sense to have
multiple instructions work on

00:25:41.595 --> 00:25:42.400
single data.

00:25:42.400 --> 00:25:46.000
So this classification,
right now -- question?

00:25:46.000 --> 00:25:49.070
AUDIENCE: I've heard
that [INAUDIBLE]

00:25:49.070 --> 00:25:51.040
PROFESSOR: Multiple instruction,
single data?

00:25:51.040 --> 00:25:53.140
I don't know.

00:25:53.140 --> 00:25:55.490
You can try to fit something
there just to have something,

00:25:55.490 --> 00:26:00.340
but it doesn't fit really well
into this kind of thinking.

00:26:00.340 --> 00:26:02.340
So I don't like that thinking.

00:26:02.340 --> 00:26:04.640
I was thinking how should I do
it, so I came up with a new

00:26:04.640 --> 00:26:05.830
way of classifying.

00:26:05.830 --> 00:26:09.390
So what my classification
is, what's the last

00:26:09.390 --> 00:26:10.350
thing you are sharing?

00:26:10.350 --> 00:26:13.440
Because when you are running
something, if it is some

00:26:13.440 --> 00:26:16.150
single machine, some thing has
to be shared, and some things

00:26:16.150 --> 00:26:17.170
have to be separated.

00:26:17.170 --> 00:26:19.830
So are you sharing instructions,
are you sharing

00:26:19.830 --> 00:26:22.740
the sequencer, are you sharing
the memory, are you sharing

00:26:22.740 --> 00:26:23.310
the network?

00:26:23.310 --> 00:26:27.290
So this kind of fits many things
nicely into this model.

00:26:27.290 --> 00:26:29.670
So let's go through
this model and see

00:26:29.670 --> 00:26:30.920
different things in there.

00:26:34.960 --> 00:26:38.630
So let's look at shared
instruction processors.

00:26:38.630 --> 00:26:43.130
So there had been a lot of work
in the good old days.

00:26:43.130 --> 00:26:48.290
Did anybody know Goodyear
actually made supercomputers?

00:26:48.290 --> 00:26:50.260
Not only did they make tires,
for a long time they were

00:26:50.260 --> 00:26:53.390
actually making processors.

00:26:53.390 --> 00:26:56.460
GE made processors,
stuff like that.

00:26:56.460 --> 00:27:00.550
And so a long time ago this
was a very interesting

00:27:00.550 --> 00:27:04.090
proposition, because there was a
huge amount of hardware that

00:27:04.090 --> 00:27:08.150
has to be dedicated to doing the
sequence and running the

00:27:08.150 --> 00:27:08.830
instruction.

00:27:08.830 --> 00:27:11.840
So just to share that was a
really interesting concept.

00:27:11.840 --> 00:27:14.470
So people built machines
that basically --

00:27:14.470 --> 00:27:17.090
single instruction
stream affecting

00:27:17.090 --> 00:27:18.340
multiple data in there.

00:27:18.340 --> 00:27:21.900
I think very well-known machines
are things like

00:27:21.900 --> 00:27:26.720
Thinking Machines CM-1,
Maspar MP-1 --

00:27:26.720 --> 00:27:31.100
which had 16,000 processors.

00:27:31.100 --> 00:27:32.310
Small processors --

00:27:32.310 --> 00:27:35.410
4-bit processors, you can only
do 4-bit computation.

00:27:35.410 --> 00:27:39.100
And then every cycle you can
do 16,000 of them, 4-bit

00:27:39.100 --> 00:27:40.640
things in here.

00:27:40.640 --> 00:27:43.250
It really fits in to the kind of
things they could build in

00:27:43.250 --> 00:27:45.430
hardware those days.

00:27:45.430 --> 00:27:47.400
And there's one controller
in there.

00:27:47.400 --> 00:27:49.230
So it is just a neat thing,
because you can do a lot of

00:27:49.230 --> 00:27:51.710
work if you actually can
match it in that form.

00:27:55.660 --> 00:27:58.760
So the way you look at that is,
to run this array you have

00:27:58.760 --> 00:28:00.570
this array controller.

00:28:00.570 --> 00:28:04.040
And then you have processing
elements, a

00:28:04.040 --> 00:28:04.750
huge amount of them.

00:28:04.750 --> 00:28:07.125
And you have each processor
mainly had distributed memory

00:28:07.125 --> 00:28:08.790
-- each has its own memory.

00:28:08.790 --> 00:28:12.150
And so given instruction,
everybody did the same thing

00:28:12.150 --> 00:28:15.310
to memory or arithmetic
in there.

00:28:15.310 --> 00:28:18.000
And then you had also
interconnect network, so you

00:28:18.000 --> 00:28:20.350
can actually send it.

00:28:20.350 --> 00:28:21.670
A lot of these things have
the [? near-enabled ?]

00:28:21.670 --> 00:28:22.580
communication.

00:28:22.580 --> 00:28:24.860
You can send data you near
enable, so everybody kind of

00:28:24.860 --> 00:28:29.900
shifts the 2-D or some kind
of torus mapping in there.

00:28:29.900 --> 00:28:33.240
And if you can program that,
you can get really good

00:28:33.240 --> 00:28:34.810
performance in there.

00:28:38.150 --> 00:28:39.880
And each cycle, it's
very synchronous.

00:28:39.880 --> 00:28:42.110
So each cycle everybody does
the same thing -- go to the

00:28:42.110 --> 00:28:43.360
next thing, do the same thing.

00:28:45.860 --> 00:28:49.840
So the next very interesting
machine is this Cray-1.

00:28:49.840 --> 00:28:51.710
I think this is one of
the first successful

00:28:51.710 --> 00:28:53.400
supercomputers out there.

00:28:53.400 --> 00:28:57.250
So here's the Cray-1, it is this
kind of round seat type

00:28:57.250 --> 00:28:59.340
thing sitting in here.

00:28:59.340 --> 00:29:01.914
Everybody know what was
under the seat?

00:29:01.914 --> 00:29:03.330
AUDIENCE: Cooling.

00:29:03.330 --> 00:29:04.030
PROFESSOR: Cooling.

00:29:04.030 --> 00:29:05.520
So here's a photo.

00:29:05.520 --> 00:29:08.120
I don't think you can see that
-- you can probably look at it

00:29:08.120 --> 00:29:09.700
when I put this on the
web -- was this

00:29:09.700 --> 00:29:10.880
entire cooling mechanism.

00:29:10.880 --> 00:29:14.350
In fact Seymour Cray at one
time said one of his most

00:29:14.350 --> 00:29:16.505
important innovations
in this machine is

00:29:16.505 --> 00:29:19.000
how to cool the thing.

00:29:19.000 --> 00:29:20.270
And this is a generation,
again, that

00:29:20.270 --> 00:29:22.420
power was a big thing.

00:29:22.420 --> 00:29:25.590
So each of these columns had
this huge amount of boards

00:29:25.590 --> 00:29:28.990
going, and in the middle had
all the wiring going.

00:29:28.990 --> 00:29:31.900
So we had this huge mess of
wiring in the middle --

00:29:31.900 --> 00:29:32.490
[UNINTELLIGIBLE]

00:29:32.490 --> 00:29:33.845
-- and then you had all
these boards in

00:29:33.845 --> 00:29:35.130
there in each of these.

00:29:35.130 --> 00:29:37.580
So this is the Cray-1
processor.

00:29:37.580 --> 00:29:39.190
AUDIENCE: Do you know
your little --

00:29:39.190 --> 00:29:43.630
your laptop is way faster
than that Cray --

00:29:43.630 --> 00:29:45.010
PROFESSOR: Yeah.

00:29:45.010 --> 00:29:48.655
Did you have the clock
speed in here?

00:29:48.655 --> 00:29:49.690
[INTERPOSING VOICES]

00:29:49.690 --> 00:29:51.930
AUDIENCE: 80 MHz.

00:29:51.930 --> 00:29:54.510
PROFESSOR: So, yeah.

00:29:54.510 --> 00:29:58.520
And that cost like $10 million
or something like

00:29:58.520 --> 00:30:01.470
that at that time.

00:30:01.470 --> 00:30:03.360
Moore's Law, it's
just amazing.

00:30:03.360 --> 00:30:05.640
If you think if you apply
Moore's Law to any other thing

00:30:05.640 --> 00:30:08.330
we have, it can't do
the comparison.

00:30:08.330 --> 00:30:11.290
We are very fortunate to be
part of that generation.

00:30:11.290 --> 00:30:13.040
AUDIENCE: But did it
have PowerPoint?

00:30:13.040 --> 00:30:16.580
PROFESSOR: So what it had,
was it had these

00:30:16.580 --> 00:30:17.690
three type of registers.

00:30:17.690 --> 00:30:19.550
It had scalar registers,
address

00:30:19.550 --> 00:30:21.470
registers, and vector registers.

00:30:21.470 --> 00:30:23.880
The key thing there is
the vector register.

00:30:23.880 --> 00:30:28.160
So if you want to do
things fast --

00:30:28.160 --> 00:30:29.250
no, fast is not the word.

00:30:29.250 --> 00:30:32.510
You can do a lot of computation
in a short amount

00:30:32.510 --> 00:30:35.840
of time by using the
vector registers.

00:30:35.840 --> 00:30:40.210
So the way to look at that is
normally when you go to the

00:30:40.210 --> 00:30:42.350
execute stage you
do one thing.

00:30:42.350 --> 00:30:44.670
In a vector register what
happens is it got pipelined.

00:30:44.670 --> 00:30:47.790
So execute state happened one
word next, next, next.

00:30:47.790 --> 00:30:51.660
You can do up to 64
or even bigger.

00:30:51.660 --> 00:30:53.380
I think it was 64 length,
length 64 things.

00:30:53.380 --> 00:30:55.360
So you can -- so that
instruction.

00:30:55.360 --> 00:30:58.640
So you do a few of these, and
then this state keeps going on

00:30:58.640 --> 00:31:00.590
and on and on, for 64.

00:31:00.590 --> 00:31:02.920
And then you can pipeline in
the way that you can start

00:31:02.920 --> 00:31:04.170
another one.

00:31:06.080 --> 00:31:08.220
Actually, this will use the same
executioner, so you have

00:31:08.220 --> 00:31:12.230
to wait till that finishes
to start.

00:31:12.230 --> 00:31:15.430
So you can pipeline to get a
huge amount of things going

00:31:15.430 --> 00:31:17.200
through the pipeline.

00:31:17.200 --> 00:31:20.750
And so each cycle you can
graduate many, many

00:31:20.750 --> 00:31:21.300
things going on.

00:31:21.300 --> 00:31:22.873
AUDIENCE: Can I ask you
a quick question?

00:31:22.873 --> 00:31:24.446
Something I'm trying to get
straight in my head.

00:31:24.446 --> 00:31:26.960
My notion -- and I don't think
I'm right on this, that's why

00:31:26.960 --> 00:31:31.305
I'm asking you -- is machines
like the Cray, I know you were

00:31:31.305 --> 00:31:34.285
talking about some of the vector
operations, those were

00:31:34.285 --> 00:31:36.980
by and large a relatively
small set of operations.

00:31:36.980 --> 00:31:39.840
Like dot products, and
vector time scalar.

00:31:39.840 --> 00:31:41.514
On the other hand, when you
look at the SIMD machines,

00:31:41.514 --> 00:31:43.860
they had a much richer
set of operations.

00:31:43.860 --> 00:31:49.110
PROFESSOR: I think with
scatter-gather and things like

00:31:49.110 --> 00:31:53.560
conditional execution, I think
vector machines could be a

00:31:53.560 --> 00:31:54.670
fairly large --

00:31:54.670 --> 00:31:58.298
I mean it's painful.

00:31:58.298 --> 00:32:01.230
AUDIENCE: [INAUDIBLE]

00:32:01.230 --> 00:32:03.660
PROFESSOR: The SIMD instruction
is Pentium.

00:32:03.660 --> 00:32:08.410
I think that is mainly targeting
single processing

00:32:08.410 --> 00:32:09.660
type stuff.

00:32:14.050 --> 00:32:15.260
They don't have real
scatter-gather.

00:32:15.260 --> 00:32:17.460
AUDIENCE: And the
Cell processor?

00:32:17.460 --> 00:32:20.370
PROFESSOR: Cell is distributed
memory.

00:32:20.370 --> 00:32:22.946
AUDIENCE: Yeah, but on one
the -- what do they

00:32:22.946 --> 00:32:23.490
call them, the --

00:32:23.490 --> 00:32:26.140
PROFESSOR: I don't think you
can scatter-gather either.

00:32:26.140 --> 00:32:31.260
It's just basically, you have
to align words in, word out.

00:32:31.260 --> 00:32:33.770
IBM is always about
doing align.

00:32:33.770 --> 00:32:37.030
So in even AltiVec, you can't
even do unaligned access.

00:32:37.030 --> 00:32:38.000
You had to do aligned access.

00:32:38.000 --> 00:32:40.795
So if there is no run align,
you had to pay a

00:32:40.795 --> 00:32:43.700
big penalty in there.

00:32:43.700 --> 00:32:46.140
So if you look at how
this happens.

00:32:46.140 --> 00:32:49.320
So you have this entire
pipeline thing.

00:32:49.320 --> 00:32:52.830
When things get started the
first value is at this point

00:32:52.830 --> 00:32:54.200
done in one clock cycle.

00:32:54.200 --> 00:32:56.250
The next value is halfway
through that.

00:32:56.250 --> 00:32:58.460
Another value is in
some part of a --

00:32:58.460 --> 00:33:00.550
is also pipelined, the
alias pipeline.

00:33:00.550 --> 00:33:03.840
And other values are kind of
feeding nicely into that.

00:33:03.840 --> 00:33:06.940
So if you have one -- this
is called one lane.

00:33:06.940 --> 00:33:10.520
You can have multiple lanes,
and then what you can do is

00:33:10.520 --> 00:33:13.230
each cycle you get 40
[UNINTELLIGIBLE]

00:33:13.230 --> 00:33:15.000
And the next ones are in
the middle of that,

00:33:15.000 --> 00:33:16.020
next ones are in middle.

00:33:16.020 --> 00:33:19.310
So what you have is a very
pipelined machine, so you can

00:33:19.310 --> 00:33:21.290
kind of pipeline things
in there.

00:33:21.290 --> 00:33:23.290
So you can have either one
lane, or multiple lanes

00:33:23.290 --> 00:33:25.090
pipeline coming out.

00:33:25.090 --> 00:33:27.720
So if you look at the
architecture, what you had is

00:33:27.720 --> 00:33:30.230
you have some kind of vector
registers feeding into these

00:33:30.230 --> 00:33:32.220
kind of functional units.

00:33:32.220 --> 00:33:34.910
So at a given time, in this one
you might be able to get

00:33:34.910 --> 00:33:38.030
eight results out, because
everything gets pipelined.

00:33:38.030 --> 00:33:42.330
But the same thing is
happening in there.

00:33:42.330 --> 00:33:44.720
Clear how vector
machines work?

00:33:44.720 --> 00:33:46.880
So it's not really parallelism,
it's basically --

00:33:46.880 --> 00:33:50.780
especially if you are one --
it's a superpipelined thing.

00:33:50.780 --> 00:33:53.740
But given one instruction, it
will crank out many, many,

00:33:53.740 --> 00:33:57.960
many things for that
instruction.

00:33:57.960 --> 00:34:00.220
And doing parallelism is easy in
here too, because it's the

00:34:00.220 --> 00:34:02.750
same thing happening to very
regular data sets.

00:34:02.750 --> 00:34:05.230
So there's no notion of
asynchronizations and all

00:34:05.230 --> 00:34:06.160
these weird things.

00:34:06.160 --> 00:34:08.980
It's just a very
simple pattern.

00:34:08.980 --> 00:34:13.030
So the next thing is the shared
sequencer processor.

00:34:13.030 --> 00:34:16.990
So here it's similar to the
vector machines because each

00:34:16.990 --> 00:34:20.840
cycle you issue a single
instruction.

00:34:20.840 --> 00:34:24.560
But the instruction is
a wide instruction.

00:34:24.560 --> 00:34:28.410
It had multiple operations in
these same instructions.

00:34:28.410 --> 00:34:29.490
So what it says is --

00:34:29.490 --> 00:34:32.190
"I have multiple execution
units, I have memory in a

00:34:32.190 --> 00:34:35.280
separate unit, and each
instruction I will tell each

00:34:35.280 --> 00:34:40.060
unit what to do." And so
something you might have --

00:34:40.060 --> 00:34:43.450
two integer units, two
memory/load store units, two

00:34:43.450 --> 00:34:44.360
floating-point units.

00:34:44.360 --> 00:34:47.190
Each cycle you tell each
of them what to do.

00:34:47.190 --> 00:34:49.210
So you just kind of keep issuing
an instruction that

00:34:49.210 --> 00:34:50.330
affects many of them.

00:34:50.330 --> 00:34:54.430
So sometimes what happens is if
this has latency of four,

00:34:54.430 --> 00:34:56.590
you might have to wait till this
is done to do the next

00:34:56.590 --> 00:34:56.940
instruction.

00:34:56.940 --> 00:34:59.560
So if one guy takes long,
everybody has to kind

00:34:59.560 --> 00:35:00.700
of wait till that.

00:35:00.700 --> 00:35:02.180
So it's very synchronous
going.

00:35:02.180 --> 00:35:04.150
So things like synchronization
stuff were

00:35:04.150 --> 00:35:05.401
not an issue in here.

00:35:09.250 --> 00:35:12.430
So if you look at a pipeline,
this is what happens.

00:35:12.430 --> 00:35:13.970
So you have this instruction.

00:35:13.970 --> 00:35:16.800
It's an instruction, but
you are fetching a wide

00:35:16.800 --> 00:35:17.120
instruction.

00:35:17.120 --> 00:35:18.430
You are not researching
a simple instruction.

00:35:18.430 --> 00:35:20.630
You decode the entire thing,
but you can decode it

00:35:20.630 --> 00:35:20.980
separately.

00:35:20.980 --> 00:35:23.984
And then you go execute on
each execution unit.

00:35:26.770 --> 00:35:28.980
One interesting problem
here was this

00:35:28.980 --> 00:35:31.410
was not really scalable.

00:35:31.410 --> 00:35:36.530
What happened here is each
functional unit, if you had

00:35:36.530 --> 00:35:40.020
one single register file, has
to access the register file.

00:35:40.020 --> 00:35:42.670
So each function would say --
"I am using register R1," "I

00:35:42.670 --> 00:35:46.060
am using R3," "I am using R5."
So what has to happen is the

00:35:46.060 --> 00:35:48.990
register file has to have --

00:35:48.990 --> 00:35:53.450
basically, if you have eight
functional units, 16 outports

00:35:53.450 --> 00:35:55.190
and 8 inports coming in.

00:35:55.190 --> 00:35:57.270
And then of course, when you
build a register file it has a

00:35:57.270 --> 00:36:01.880
scale, so it had huge
scalability issues.

00:36:01.880 --> 00:36:04.960
So it's a quadratically scalable
register function.

00:36:04.960 --> 00:36:05.476
Question?

00:36:05.476 --> 00:36:07.540
AUDIENCE: The sequencer
[INAUDIBLE PHRASE]

00:36:10.120 --> 00:36:11.370
PROFESSOR: Yeah.

00:36:13.270 --> 00:36:15.820
It's basically you had to wait
till everybody's done, there's

00:36:15.820 --> 00:36:17.820
nothing going out
of any order.

00:36:17.820 --> 00:36:19.150
And memory also.

00:36:19.150 --> 00:36:21.950
Since everybody's going to
memory, this is not scalable.

00:36:21.950 --> 00:36:26.880
So people try to build -- you
can do four, eight wide, but

00:36:26.880 --> 00:36:30.760
beyond that this register and
memory interconnect became a

00:36:30.760 --> 00:36:32.770
big mess to build.

00:36:32.770 --> 00:36:36.830
And so one kind of modification
thing people did

00:36:36.830 --> 00:36:39.690
was called Clustered VLIW.

00:36:39.690 --> 00:36:43.560
So what happens is you have a
very wide instruction in here.

00:36:43.560 --> 00:36:46.730
It goes to not one cluster,
but different clusters.

00:36:46.730 --> 00:36:49.940
Each cluster has its own
register file, its own kind of

00:36:49.940 --> 00:36:52.160
memory interconnect
going on there.

00:36:52.160 --> 00:36:55.750
And what that means is if you
want to do intercluster

00:36:55.750 --> 00:36:58.000
communication, you have to
go to a very special

00:36:58.000 --> 00:37:00.060
communication network.

00:37:00.060 --> 00:37:03.000
So you don't have this bandwidth
expansion register.

00:37:03.000 --> 00:37:06.180
So you only have, we'll say two
execution units, so you

00:37:06.180 --> 00:37:10.430
only have to have four
out and one in to the

00:37:10.430 --> 00:37:11.900
register filing cycle.

00:37:11.900 --> 00:37:15.030
And then if you want other
communication, you have a much

00:37:15.030 --> 00:37:17.600
lower bandwidth interconnect
that you'll have

00:37:17.600 --> 00:37:18.640
to go through that.

00:37:18.640 --> 00:37:23.070
So what this does is you kind
of expose more complexity to

00:37:23.070 --> 00:37:28.110
the compiler and software, and
the rationale here is most

00:37:28.110 --> 00:37:31.380
programs have locality.

00:37:31.380 --> 00:37:33.210
It's like everybody always wants
to to communicate with

00:37:33.210 --> 00:37:35.670
everybody else, so there are
some locality in here.

00:37:35.670 --> 00:37:38.610
So you can basically cluster
things that are local together

00:37:38.610 --> 00:37:41.360
and put it in here, and then
when other things have to be

00:37:41.360 --> 00:37:43.880
communicated you can use this
communication and go about

00:37:43.880 --> 00:37:44.210
doing that.

00:37:44.210 --> 00:37:48.540
So this is kind of the state of
the art in this technology.

00:37:48.540 --> 00:37:49.510
And something like --

00:37:49.510 --> 00:37:50.410
what I didn't put --

00:37:50.410 --> 00:37:52.710
Itanium kind of fits in here.

00:37:52.710 --> 00:37:55.830
Itanium processor.

00:37:55.830 --> 00:37:59.810
So then we go to
shared network.

00:37:59.810 --> 00:38:01.570
There has been a lot
of work in here.

00:38:01.570 --> 00:38:05.410
People have been building
multiprocessors for a long

00:38:05.410 --> 00:38:07.000
time, because it's a very
easy thing to build.

00:38:07.000 --> 00:38:09.870
So what you do is --

00:38:09.870 --> 00:38:13.490
if you look at it, you have a
processor unit that connects

00:38:13.490 --> 00:38:15.000
its own memory.

00:38:15.000 --> 00:38:16.340
And it's like a multiple
[UNINTELLIGIBLE]

00:38:16.340 --> 00:38:19.840
Then it has a very tightly
connected network interface

00:38:19.840 --> 00:38:21.820
that goes to interconnect
network.

00:38:21.820 --> 00:38:26.170
So we can even think about a
workstation farm as this type

00:38:26.170 --> 00:38:27.110
of a machine.

00:38:27.110 --> 00:38:33.200
But of course, the network is a
pretty slow one that requres

00:38:33.200 --> 00:38:34.180
an ethernet connector.

00:38:34.180 --> 00:38:35.930
But people build things
that have much

00:38:35.930 --> 00:38:39.060
faster networks in there.

00:38:39.060 --> 00:38:41.890
This was designed in a way you
can build hundreds and

00:38:41.890 --> 00:38:43.580
thousands of these things --

00:38:43.580 --> 00:38:44.610
nodes in here.

00:38:44.610 --> 00:38:48.760
So today if you look at the
top 500 supercomputers, a

00:38:48.760 --> 00:38:51.530
bunch of them fits into this
category because it's very

00:38:51.530 --> 00:38:54.510
easy to scale and build
very large.

00:38:54.510 --> 00:38:56.647
AUDIENCE: Are you doing
SMPs in this list,

00:38:56.647 --> 00:38:57.670
or some other place?

00:38:57.670 --> 00:39:00.020
PROFESSOR: SMP is mostly shared

00:39:00.020 --> 00:39:01.750
memory, so shared network.

00:39:01.750 --> 00:39:03.000
I'll do shared memory next.

00:39:06.500 --> 00:39:09.180
But there are problems
with it.

00:39:09.180 --> 00:39:12.860
All the data layout has to be
handled by software, or by the

00:39:12.860 --> 00:39:15.670
programmer basically.

00:39:15.670 --> 00:39:18.100
If you want something outside
your memory, you had to do

00:39:18.100 --> 00:39:19.310
very explicit communication.

00:39:19.310 --> 00:39:21.470
Not only you, the other guy who
has the data actually has

00:39:21.470 --> 00:39:23.420
to cooperate to send
it to you.

00:39:23.420 --> 00:39:26.320
And he needs to know that
now you have the data.

00:39:26.320 --> 00:39:29.480
All of that management
is your problem.

00:39:29.480 --> 00:39:34.020
And that makes programming
these kind of things very

00:39:34.020 --> 00:39:36.040
difficult, which you'll probably
figure out by the

00:39:36.040 --> 00:39:37.080
time you're done with Cell.

00:39:37.080 --> 00:39:41.930
So Cell has a lot of
these issues, too.

00:39:41.930 --> 00:39:45.980
The problem here is not dealing
with most of the data,

00:39:45.980 --> 00:39:48.200
but the kind of corner
cases that you don't

00:39:48.200 --> 00:39:49.520
know about that much.

00:39:49.520 --> 00:39:51.695
There's no nice safe way, of
saying -- "I don't know where,

00:39:51.695 --> 00:39:52.850
who's going to access it.

00:39:52.850 --> 00:39:54.430
I'll let the hardware take
care of it." There's no

00:39:54.430 --> 00:39:58.160
hardware, you have to
take of it yourself.

00:39:58.160 --> 00:40:02.060
And also message passing has
a very high overhead.

00:40:02.060 --> 00:40:04.980
Most of the time in order to do
message, you have to invoke

00:40:04.980 --> 00:40:06.130
some kind of a kernel thing.

00:40:06.130 --> 00:40:08.240
You have to actually do a kernel
switch that will call

00:40:08.240 --> 00:40:09.400
the network --

00:40:09.400 --> 00:40:11.990
it's operaing system involves a
process, basically, to get a

00:40:11.990 --> 00:40:13.850
message in there.

00:40:13.850 --> 00:40:16.250
And also when you get a message
out you have to do

00:40:16.250 --> 00:40:21.110
some kind of interrupt or
polling, and that's a bunch of

00:40:21.110 --> 00:40:22.140
copies out of kernel.

00:40:22.140 --> 00:40:25.040
And this became a pretty
expensive proposition.

00:40:25.040 --> 00:40:27.800
So you can't send messages the
size of one [UNINTELLIGIBLE]

00:40:27.800 --> 00:40:29.970
so you had to accumulate a huge
amount of things to send

00:40:29.970 --> 00:40:31.730
out to amortize the cost
of doing that.

00:40:37.430 --> 00:40:39.590
Sending can be somewhat
cheap, but receiving

00:40:39.590 --> 00:40:41.180
is a lot more expensive.

00:40:41.180 --> 00:40:42.690
Because receiving you
have to multiplex.

00:40:42.690 --> 00:40:44.280
You have no idea who
it's coming to.

00:40:44.280 --> 00:40:46.070
So you have to receive, you
have to figure out who is

00:40:46.070 --> 00:40:47.380
supposed to get it.

00:40:47.380 --> 00:40:49.455
Especially if you are running
multiple applications, it

00:40:49.455 --> 00:40:50.570
might be for someone's
application.

00:40:50.570 --> 00:40:51.810
You had to contact
[UNINTELLIGIBLE]

00:40:51.810 --> 00:40:53.060
So it's a big mess.

00:40:55.640 --> 00:40:58.800
That is where people went to
shared memory processors,

00:40:58.800 --> 00:41:02.040
because it became easier
message method to use.

00:41:02.040 --> 00:41:05.480
So that is basically the SMPs
Alan was talking about.

00:41:09.350 --> 00:41:12.160
The nice thing is it will work
with any data placement.

00:41:12.160 --> 00:41:15.390
It might work very slowly, but
at least it will work.

00:41:15.390 --> 00:41:18.860
So it makes it very easy to take
your existing application

00:41:18.860 --> 00:41:21.200
and first getting it working,
because it's

00:41:21.200 --> 00:41:22.880
just working there.

00:41:22.880 --> 00:41:25.700
You can choose to optimize
only critical sections.

00:41:25.700 --> 00:41:27.210
You can say -- "OK,
this section I

00:41:27.210 --> 00:41:28.290
know it's very important.

00:41:28.290 --> 00:41:30.380
I will do the right thing,
I will place it properly

00:41:30.380 --> 00:41:33.320
everything." And the rest of it
I can just leave alone, and

00:41:33.320 --> 00:41:35.730
it will go and get the
data and do it right.

00:41:35.730 --> 00:41:38.020
You can run sequentially, of
course, but at least the

00:41:38.020 --> 00:41:39.390
memory part I don't have
to deal with it.

00:41:39.390 --> 00:41:43.090
If some other memory just once
in a while accesses that data

00:41:43.090 --> 00:41:44.940
that you have actually
parallelized, it

00:41:44.940 --> 00:41:46.010
will actually work.

00:41:46.010 --> 00:41:47.690
So you only have to worry about
the [UNINTELLIGIBLE]

00:41:47.690 --> 00:41:48.940
that you are parallelizing.

00:41:51.130 --> 00:41:54.470
And you can communicate using
load store instructions.

00:41:54.470 --> 00:41:56.710
You don't have to get always
in order to do that.

00:41:56.710 --> 00:41:57.970
And it's a lot lower overhead.

00:41:57.970 --> 00:42:02.000
So 5 to 10 cycles, instead of
hundreds to thousands cycles

00:42:02.000 --> 00:42:03.030
to do that.

00:42:03.030 --> 00:42:05.840
And most of these messages
actually stoplight some

00:42:05.840 --> 00:42:08.230
instructions to do this
communication very fast.

00:42:08.230 --> 00:42:10.430
There's a thing called fetch&op,
and a thing called

00:42:10.430 --> 00:42:12.580
load linked/store conditional
operations.

00:42:12.580 --> 00:42:16.125
There are these very special
operations where if you are

00:42:16.125 --> 00:42:19.760
waiting for somebody else, you
can do it very fast. So if two

00:42:19.760 --> 00:42:21.430
people are communicating.

00:42:21.430 --> 00:42:24.550
So people came up with these
very fast operations that are

00:42:24.550 --> 00:42:26.320
low cost, as a last --

00:42:26.320 --> 00:42:28.230
if the data's available it
will happen very fast.

00:42:28.230 --> 00:42:29.480
Synchronization.

00:42:31.260 --> 00:42:34.820
And when you are starting to
build a large system, you can

00:42:34.820 --> 00:42:37.820
actually give a logically shared
view of memory, but the

00:42:37.820 --> 00:42:41.120
underlying hardware can be
still distributed memory.

00:42:41.120 --> 00:42:42.260
So there's a thing called --

00:42:42.260 --> 00:42:45.060
I will get into when you
do synchronization --

00:42:45.060 --> 00:42:46.290
directory-based cache
coherence.

00:42:46.290 --> 00:42:48.630
So you give a nice, simple
view of memory.

00:42:48.630 --> 00:42:50.250
But of course memory is
really disbributed.

00:42:50.250 --> 00:42:52.790
So that kind of gives the
best of both worlds.

00:42:52.790 --> 00:42:55.150
So you can keep scaling and
build large machines, but the

00:42:55.150 --> 00:42:59.450
view is a very simple
view of machines.

00:42:59.450 --> 00:43:00.920
So there are two categories
in here.

00:43:00.920 --> 00:43:03.660
One is non-cache coherent, and
then hardware cache coherence.

00:43:03.660 --> 00:43:08.450
So non-cache coherence kind of
gives a view of memory as a

00:43:08.450 --> 00:43:10.260
single address space.

00:43:10.260 --> 00:43:13.020
But you had to deal with that if
you write something to get

00:43:13.020 --> 00:43:14.510
there early to me, you had
to explicitly say --

00:43:14.510 --> 00:43:17.580
"Now send it to that person."
But we're still in a single

00:43:17.580 --> 00:43:19.380
address space.

00:43:19.380 --> 00:43:21.790
It doesn't give the
full benefits of a

00:43:21.790 --> 00:43:22.600
shared memory machine.

00:43:22.600 --> 00:43:24.610
It's kind of inbetween
distributed memory.

00:43:24.610 --> 00:43:26.100
In distributed memory basically
everybody's in a

00:43:26.100 --> 00:43:27.830
different address space,
so you had to map

00:43:27.830 --> 00:43:28.760
by sending a message.

00:43:28.760 --> 00:43:30.550
Here, you just say I have
to flush and send it

00:43:30.550 --> 00:43:31.800
to the other guy.

00:43:36.360 --> 00:43:39.080
Some of the early machines, as
well as some big machines,

00:43:39.080 --> 00:43:42.070
were no hardware cache
coherence.

00:43:42.070 --> 00:43:44.440
Things like supercomputers were
built in this way because

00:43:44.440 --> 00:43:45.980
it's very easy to build.

00:43:45.980 --> 00:43:49.900
And the nice thing here is if
you know your applications

00:43:49.900 --> 00:43:54.280
well, if you are running good
parallel large applications,

00:43:54.280 --> 00:43:55.980
and you are actually knowing
what the communication

00:43:55.980 --> 00:43:57.760
patterns are -- you can
actually do it.

00:43:57.760 --> 00:44:00.430
And you don't have to pay the
hardware overhead to have this

00:44:00.430 --> 00:44:02.470
nice hardware support
in there.

00:44:02.470 --> 00:44:07.230
However, a lot of small scale
machines -- for example, most

00:44:07.230 --> 00:44:12.360
people's workstations are
stuffy, it's probably now two

00:44:12.360 --> 00:44:14.240
Pentium Quad machines --

00:44:14.240 --> 00:44:15.430
actually add memory.

00:44:15.430 --> 00:44:20.430
Because if you are trying to
do the starting things it's

00:44:20.430 --> 00:44:21.540
much easier to do
shared memory.

00:44:21.540 --> 00:44:24.840
And also it's easier to bulid
small shared memory machines.

00:44:24.840 --> 00:44:32.480
And people talk about using a
bus-based machine, and also

00:44:32.480 --> 00:44:33.560
using a large scale

00:44:33.560 --> 00:44:34.818
directory-based machine in here.

00:44:38.170 --> 00:44:42.540
So for bus-based machines, how
do you do shared memory?

00:44:42.540 --> 00:44:46.880
So there's a protocol, what we
call a snoopy cache protocol.

00:44:46.880 --> 00:44:51.050
What that means is, every time
you modify your location

00:44:51.050 --> 00:44:54.120
somewhere -- so of course you
have that in your cache --

00:44:54.120 --> 00:44:57.070
you tell everybody in the world
who's using a busing, "I

00:44:57.070 --> 00:45:03.460
modified that." And then if
somebody else also has that

00:45:03.460 --> 00:45:04.470
memory location.

00:45:04.470 --> 00:45:06.390
That person says, "Oops, he
modified it." Either he

00:45:06.390 --> 00:45:09.160
invalidates it or gets
the modified copy.

00:45:09.160 --> 00:45:12.340
If you are using something new,
you have to go and snoop.

00:45:12.340 --> 00:45:15.040
And you can ask everybody and
say -- "Wait a minute, does

00:45:15.040 --> 00:45:19.160
anybody have a copy of this?"
And some more complicated

00:45:19.160 --> 00:45:22.680
protocols have saying -- "I
don't have any, I have a copy

00:45:22.680 --> 00:45:24.540
but it's only read-only.

00:45:24.540 --> 00:45:26.470
So I'm just reading it, I'm
not modifying it." Then

00:45:26.470 --> 00:45:28.940
multiple people can have the
same copy, because everybody's

00:45:28.940 --> 00:45:29.830
reading and it's OK.

00:45:29.830 --> 00:45:31.840
And then there's the next thing
-- "OK, I am actually

00:45:31.840 --> 00:45:33.550
trying to modify this thing."
And then only I

00:45:33.550 --> 00:45:35.080
can have the copy.

00:45:35.080 --> 00:45:37.830
So some data you can give to
multiple people as a read

00:45:37.830 --> 00:45:40.380
copy, and then when you are
trying to write everybody gets

00:45:40.380 --> 00:45:42.140
disinvited, only the person
who has write

00:45:42.140 --> 00:45:43.090
has access to it.

00:45:43.090 --> 00:45:45.315
And there are a lot of
complicated protocols how if

00:45:45.315 --> 00:45:46.870
you write it, and then somebody
else wants to write

00:45:46.870 --> 00:45:48.680
it, how do you get
to that person?

00:45:48.680 --> 00:45:50.990
And of course you have to keep
it consistent with memory.

00:45:50.990 --> 00:45:53.420
So there is a lot of work in
how to get these things all

00:45:53.420 --> 00:45:55.720
working, but that's the
kind of basic idea.

00:45:59.300 --> 00:46:01.730
So directory-based machines
are very different.

00:46:01.730 --> 00:46:05.060
In directory-based machines
mainly there's a

00:46:05.060 --> 00:46:06.820
notion of a home node.

00:46:06.820 --> 00:46:10.540
So everybody has local space in
memory, you keep some part

00:46:10.540 --> 00:46:10.820
of your memory.

00:46:10.820 --> 00:46:12.720
And of course you have
a cache also.

00:46:12.720 --> 00:46:16.130
So you have a notion that this
memory belongs to you.

00:46:16.130 --> 00:46:18.470
And every time I want to do
something with that memory I

00:46:18.470 --> 00:46:19.390
had to ask you.

00:46:19.390 --> 00:46:20.380
I had to get your permission.

00:46:20.380 --> 00:46:22.560
"I want that memory, can
you give it to me?"

00:46:22.560 --> 00:46:24.610
And so there are two things.

00:46:24.610 --> 00:46:26.670
That person has a directory
[UNINTELLIGIBLE] say -- "OK,

00:46:26.670 --> 00:46:28.150
this memory is in me.

00:46:28.150 --> 00:46:31.480
I am the one who right now owns
it, and I have the copy."

00:46:31.480 --> 00:46:32.420
Or it will say --

00:46:32.420 --> 00:46:36.120
"You want to copy that memory
to this other guy to write,

00:46:36.120 --> 00:46:38.380
and here is that person's
address or that machine's

00:46:38.380 --> 00:46:41.650
name." Or if multiple people
have taken this copy and are

00:46:41.650 --> 00:46:42.730
reading it.

00:46:42.730 --> 00:46:45.240
So when somebody asks
me for a copy --

00:46:45.240 --> 00:46:49.220
assume you ask to
read this copy.

00:46:49.220 --> 00:46:52.890
And if I have given it to nobody
to read, or if I have

00:46:52.890 --> 00:46:54.410
given it to other people
to read, so I say --

00:46:54.410 --> 00:46:55.330
"OK, here's a copy.

00:46:55.330 --> 00:46:58.610
Go read." And I add that person
is reading that, and I

00:46:58.610 --> 00:47:00.190
keep that in my directory.

00:47:00.190 --> 00:47:01.910
Or if somebody's writing that.

00:47:01.910 --> 00:47:04.010
I say sure, "I can't give it
to read because somebody's

00:47:04.010 --> 00:47:05.750
writing that." So I
can do two things.

00:47:05.750 --> 00:47:07.750
I can tell that person,
saying --

00:47:07.750 --> 00:47:11.350
"You have to get it from the
person who's writing.

00:47:11.350 --> 00:47:12.860
So go directly get
it from there.

00:47:12.860 --> 00:47:16.190
And I will mark that now you own
it as a read value." Or, I

00:47:16.190 --> 00:47:17.630
can tell the person
who's writing --

00:47:17.630 --> 00:47:19.400
"Look, you have to give up
your write privilege.

00:47:19.400 --> 00:47:21.990
If you have modified it, give
me the data back." And that

00:47:21.990 --> 00:47:23.950
person goes back to
the read or no

00:47:23.950 --> 00:47:25.330
privileges with that data.

00:47:25.330 --> 00:47:26.860
When I get that data, I'll
send it back to this

00:47:26.860 --> 00:47:27.240
person and say --

00:47:27.240 --> 00:47:29.600
"Here, you can read." And the
same thing if you ask for

00:47:29.600 --> 00:47:30.690
write permission.

00:47:30.690 --> 00:47:33.090
If anybody has [UNINTELLIGIBLE]

00:47:33.090 --> 00:47:34.010
I have to tell everybody --

00:47:34.010 --> 00:47:35.250
"Now you can't read
it anymore.

00:47:35.250 --> 00:47:37.760
Go invalidate, because
somebody's about to write."

00:47:37.760 --> 00:47:39.825
Get the invalidate request
coming back, and then when

00:47:39.825 --> 00:47:42.250
you've done that I say, "OK,
you can write that." So

00:47:42.250 --> 00:47:45.000
everybody keeps part of
the memory, and then

00:47:45.000 --> 00:47:45.720
all of that in there.

00:47:45.720 --> 00:47:48.762
So because of that you can
really scale this thing.

00:47:52.860 --> 00:47:54.700
So if you look at a
bus-based machine.

00:47:54.700 --> 00:47:55.930
This is the kind of
way it looks like.

00:47:55.930 --> 00:47:59.410
You have a cache in here,
microprocessor, central

00:47:59.410 --> 00:48:01.120
memory, and you have
a bus in here.

00:48:01.120 --> 00:48:04.560
And a lot of small machines,
including most people's

00:48:04.560 --> 00:48:06.770
desktops, basically fit
in this category.

00:48:06.770 --> 00:48:09.040
And you have a snoopy
bus in here.

00:48:09.040 --> 00:48:10.200
So a little bit of
a bigger machine,

00:48:10.200 --> 00:48:12.730
something like a Sun Starfire.

00:48:12.730 --> 00:48:17.230
Basically it had four processors
in the board, four

00:48:17.230 --> 00:48:20.250
caches, and had an interconnect
that actually has

00:48:20.250 --> 00:48:21.560
multiple buses going.

00:48:21.560 --> 00:48:23.450
So it can actually get a little
bit of scalability,

00:48:23.450 --> 00:48:24.290
because here's the bottleneck.

00:48:24.290 --> 00:48:25.780
The bus becomes the
bottleneck.

00:48:25.780 --> 00:48:27.400
Everybody has to go
through the bus.

00:48:27.400 --> 00:48:29.570
And so you actually get
multiple buses to get

00:48:29.570 --> 00:48:32.810
bottleneck, and it actually had
some distributed memory

00:48:32.810 --> 00:48:35.160
going through a crossbar here.

00:48:35.160 --> 00:48:36.583
So this cache coherent
protocol has

00:48:36.583 --> 00:48:38.400
to deal with that.

00:48:38.400 --> 00:48:41.100
And going to the other extreme,

00:48:41.100 --> 00:48:43.310
something like SGI Origin.

00:48:46.930 --> 00:48:50.170
In this machine there are two
processors, and it had

00:48:50.170 --> 00:48:52.090
actually a little bit of
processors and a lot of memory

00:48:52.090 --> 00:48:52.830
dealing with the directory.

00:48:52.830 --> 00:48:55.040
So you keep the data, and you
actually keep all the

00:48:55.040 --> 00:48:56.550
directory information
in there --

00:48:56.550 --> 00:48:57.070
in this.

00:48:57.070 --> 00:48:58.850
And then it goes --

00:48:58.850 --> 00:49:02.740
then after that it almost uses
a normal message passing type

00:49:02.740 --> 00:49:05.420
network to communicate
with that.

00:49:05.420 --> 00:49:07.520
And they use the crane to
connect networks, so we can

00:49:07.520 --> 00:49:09.660
have a very large machine
built out of that.

00:49:12.720 --> 00:49:14.450
So now let's switch to
multicore processors.

00:49:18.200 --> 00:49:21.930
If you look at the way we have
been dealing with VLSI, every

00:49:21.930 --> 00:49:24.920
generation we are getting more
and more transistors.

00:49:24.920 --> 00:49:27.470
So at the beginning when you
have enough transistors to

00:49:27.470 --> 00:49:29.860
deal with, people actually start
dealing with bit-level

00:49:29.860 --> 00:49:30.960
parallelism.

00:49:30.960 --> 00:49:35.270
So you didn't have -- you can
do 16-bit, 32-bit machines.

00:49:35.270 --> 00:49:36.990
You can do wider machines,
because you have enough

00:49:36.990 --> 00:49:37.850
transistors.

00:49:37.850 --> 00:49:39.610
Because at the beginning you
have like 8-bit processors,

00:49:39.610 --> 00:49:41.110
16-bit, 32-bit.

00:49:41.110 --> 00:49:43.790
And then at some point that I
have still more transistors, I

00:49:43.790 --> 00:49:47.660
start doing instruction-level
parallelism in a die.

00:49:47.660 --> 00:49:50.080
So even in a bit-level
parallelism, in order to get

00:49:50.080 --> 00:49:53.830
64-bit you had to actually
have multiple chips.

00:49:53.830 --> 00:49:57.135
So in this regime in order to
get parallelism, you need to

00:49:57.135 --> 00:49:58.150
have multiple processors --

00:49:58.150 --> 00:49:59.370
multiprocessors.

00:49:59.370 --> 00:50:02.860
So in the good old days you
actually built a processsor,

00:50:02.860 --> 00:50:03.950
things like a minicomputer.

00:50:03.950 --> 00:50:06.620
Basically you had one
processor dealing

00:50:06.620 --> 00:50:07.380
with a 1-bit slice.

00:50:07.380 --> 00:50:10.700
So in the 4-bit slice, dealing
with that amount, you could

00:50:10.700 --> 00:50:12.230
fit in a chip.

00:50:12.230 --> 00:50:14.550
And a multichip made
a single processor.

00:50:14.550 --> 00:50:17.870
Here a multichip made
a multiprocessor.

00:50:17.870 --> 00:50:20.510
We are hitting a regime
where a multichip --

00:50:20.510 --> 00:50:22.870
what [? it ?] will be
multiprocessor -- now fits in

00:50:22.870 --> 00:50:26.030
one piece of silicon, because
you have more transistors.

00:50:26.030 --> 00:50:29.560
So we are going into a time
where multicore is basically

00:50:29.560 --> 00:50:31.630
multiple processors
on a die --

00:50:31.630 --> 00:50:33.790
on a chip.

00:50:33.790 --> 00:50:35.140
So I showed this slide.

00:50:35.140 --> 00:50:39.650
We are getting there, and it's
getting pretty fast. You had

00:50:39.650 --> 00:50:41.450
something like this, and
suddenly we accelerated.

00:50:41.450 --> 00:50:46.530
We added more and more
cores on a die.

00:50:46.530 --> 00:50:50.000
So I categorized multicores
also the way I categorized

00:50:50.000 --> 00:50:51.020
them previously.

00:50:51.020 --> 00:50:54.850
There are shared memory
multicores.

00:50:54.850 --> 00:50:56.180
Here are some examples.

00:50:56.180 --> 00:50:59.100
Then there are shared
network multicores.

00:50:59.100 --> 00:51:01.930
Cell processor is one,
and at MIT we are

00:51:01.930 --> 00:51:04.440
building also Raw processor.

00:51:04.440 --> 00:51:07.700
And there is another part, what
they call crippled or

00:51:07.700 --> 00:51:08.550
mini-cores.

00:51:08.550 --> 00:51:15.000
So the reason in this graph you
can have 512, is because

00:51:15.000 --> 00:51:17.130
it's not Pentium sized things
sitting in there.

00:51:17.130 --> 00:51:20.940
You are putting very simple
small cores, and a

00:51:20.940 --> 00:51:21.940
huge amount of them.

00:51:21.940 --> 00:51:24.890
So for some class replication,
that's also useful.

00:51:24.890 --> 00:51:29.120
So if you look at shared memory
multicores, basically

00:51:29.120 --> 00:51:32.730
this is an evolution path
for current processors.

00:51:32.730 --> 00:51:35.890
So if you look at it, what they
did was they took their

00:51:35.890 --> 00:51:38.160
years' worth of and billions
of dollars worth of

00:51:38.160 --> 00:51:42.880
engineering building a single
superscalar processor.

00:51:42.880 --> 00:51:45.456
Then they slapped a few of them
on the same die, and said

00:51:45.456 --> 00:51:48.390
-- "Hey, we've got a multicore."
And of course they

00:51:48.390 --> 00:51:54.450
were always doing shared memory
at the network level.

00:51:54.450 --> 00:51:56.220
They said -- "OK, I'll put the
shared memory bus also into

00:51:56.220 --> 00:51:58.340
the same die, and I got a
multicore." So this is

00:51:58.340 --> 00:52:00.440
basically what all these
things are all about.

00:52:00.440 --> 00:52:03.170
So this is kind of gluing these
things together, it's a

00:52:03.170 --> 00:52:04.240
first generation.

00:52:04.240 --> 00:52:07.740
However, you didn't build a core
completely from scratch.

00:52:07.740 --> 00:52:11.330
You just kind of integrated what
we had in multiple chips

00:52:11.330 --> 00:52:15.880
into one chip, and basically
got that.

00:52:15.880 --> 00:52:19.640
So to go a little bit beyond,
I think you can do better.

00:52:19.640 --> 00:52:24.260
So for example, this
AMD multicore.

00:52:24.260 --> 00:52:31.240
Basically you have CPUs in
there, actually have a full

00:52:31.240 --> 00:52:34.400
snoopy controller in there,
and can have some other

00:52:34.400 --> 00:52:35.280
interface with that.

00:52:35.280 --> 00:52:38.900
So you can actually start
building more and more uni

00:52:38.900 --> 00:52:41.440
CPU, thinking that you're
building a multicore.

00:52:41.440 --> 00:52:43.745
Instead of saying, "I had this
thing in my shelf, I'm going

00:52:43.745 --> 00:52:45.480
to plop it here, and then
kind of [INAUDIBLE]

00:52:45.480 --> 00:52:46.950
And you'll see, I
think, a lot of

00:52:46.950 --> 00:52:48.100
interesting things happening.

00:52:48.100 --> 00:52:52.310
Because now as they're connected
closely in the same

00:52:52.310 --> 00:52:56.170
die, you can do more things than
what you could do in a

00:52:56.170 --> 00:52:57.000
multiprocessor.

00:52:57.000 --> 00:52:59.300
So in the last lecture we talked
a little bit about what

00:52:59.300 --> 00:53:01.530
the future could be in
this kind of regime.

00:53:10.040 --> 00:53:11.290
Come on.

00:53:13.930 --> 00:53:14.500
OK.

00:53:14.500 --> 00:53:18.560
So one thing we have been doing
at MIT for -- now this

00:53:18.560 --> 00:53:23.190
practice is ended, we started
about eight years ago -- is to

00:53:23.190 --> 00:53:28.050
figure out when you have all the
silicon, how can you build

00:53:28.050 --> 00:53:30.460
a multicore if you to
start from scratch.

00:53:30.460 --> 00:53:33.120
So we built this Raw processor
where each --

00:53:33.120 --> 00:53:37.100
we have 16, these small cores,
identical ones in here.

00:53:37.100 --> 00:53:40.260
And the interesting thing is
what we said was, we have all

00:53:40.260 --> 00:53:41.500
this bandwidth.

00:53:41.500 --> 00:53:44.060
It's not just going from pins
to memory, we have all this

00:53:44.060 --> 00:53:45.580
bandwidth sitting next
to each other.

00:53:45.580 --> 00:53:48.990
So can we really take advantage
of that to do a lot

00:53:48.990 --> 00:53:50.240
of communication?

00:53:50.240 --> 00:53:52.300
And also the other thing is that
to build something like a

00:53:52.300 --> 00:53:54.850
bus, you need a lot
of long wires.

00:53:54.850 --> 00:53:56.940
And it's really hard to
build long wires.

00:53:56.940 --> 00:54:00.770
So in Raw processor it's
something like each chip, a

00:54:00.770 --> 00:54:05.430
large amount of part, is into
this eight 32-bit buses.

00:54:05.430 --> 00:54:06.940
So you have a huge amount
of communication

00:54:06.940 --> 00:54:07.950
next to each other.

00:54:07.950 --> 00:54:10.320
And we don't have any kind of
global memory because that

00:54:10.320 --> 00:54:12.400
requires, right now, either do
a directory, which you didn't

00:54:12.400 --> 00:54:15.750
want to build, or have a bus,
which will require long wires.

00:54:15.750 --> 00:54:19.570
So we did in a way that all
wires -- no wires longer than

00:54:19.570 --> 00:54:22.830
one of the cores.

00:54:22.830 --> 00:54:25.980
So we can do short wires, but
we came up with a lot of

00:54:25.980 --> 00:54:29.380
communications for each of
these, what we called tile

00:54:29.380 --> 00:54:32.170
those days, are very
tightly coupled.

00:54:32.170 --> 00:54:35.730
So this is kind of a direction
where people perhaps might go,

00:54:35.730 --> 00:54:39.580
because now we have all this
bandwidth in here.

00:54:39.580 --> 00:54:41.260
And how would you take advantage
of that bandwidth?

00:54:41.260 --> 00:54:43.720
So this is a different way
of looking at that.

00:54:43.720 --> 00:54:47.970
And in some sense the Cell fits
somewhere in this regime.

00:54:47.970 --> 00:54:51.070
Because what Cell did was
instead of -- it says, "I'm

00:54:51.070 --> 00:54:52.300
not building a bus,
I am actually

00:54:52.300 --> 00:54:53.750
building a ring network.

00:54:53.750 --> 00:54:57.000
I'm keeping distributed memory,
and provide to Cell a

00:54:57.000 --> 00:54:58.910
ring." I'm not going to go
through Cell, because actually

00:54:58.910 --> 00:55:03.457
you had a full lecture the day
before yesterday on this.

00:55:03.457 --> 00:55:04.888
AUDIENCE: Saman, can I
ask you a question?

00:55:04.888 --> 00:55:07.325
Is there a conclusion that I
should be reaching in that I

00:55:07.325 --> 00:55:09.405
look at the multicores you can
buy today are still by and

00:55:09.405 --> 00:55:11.085
large two and four processors.

00:55:11.085 --> 00:55:12.280
There are people that
have done more.

00:55:12.280 --> 00:55:15.480
The Verano has 16 and
the Dell has 8.

00:55:15.480 --> 00:55:19.530
And the conclusion that I want
to reach is that as an

00:55:19.530 --> 00:55:21.635
engineering tradeoff, if you
throw away the shared memory

00:55:21.635 --> 00:55:23.070
you can add processors.

00:55:23.070 --> 00:55:24.120
Is that a straightforward
tradeoff?

00:55:24.120 --> 00:55:26.140
PROFESSOR: I don't think
it's a shared memory.

00:55:26.140 --> 00:55:29.600
You can still have things
like directory-based

00:55:29.600 --> 00:55:32.200
cache coherent things.

00:55:32.200 --> 00:55:34.940
What's missing right now is what
people have done is just

00:55:34.940 --> 00:55:37.570
basically took parts in their
shelves, and kind of put it

00:55:37.570 --> 00:55:39.230
into the chip.

00:55:39.230 --> 00:55:43.830
If you look at it, if you put
two chips next to each other

00:55:43.830 --> 00:55:46.370
on a board, there's a certain
amount of communication

00:55:46.370 --> 00:55:48.020
bandwidth going here.

00:55:48.020 --> 00:55:51.640
And if you put those things
into the same die, there's

00:55:51.640 --> 00:55:55.430
about five orders of magnitude
possibility to communicate.

00:55:55.430 --> 00:55:58.080
We haven't figured out how to
take advantage of that.

00:55:58.080 --> 00:56:00.770
In some sense, we can almost say
I want to copy the entire

00:56:00.770 --> 00:56:04.180
cache from this machine to
another machine in the cycle.

00:56:04.180 --> 00:56:06.440
I don't think you even would
want to do that, but you can

00:56:06.440 --> 00:56:09.280
have that level of huge amount
of communication.

00:56:09.280 --> 00:56:11.530
We are still kind of doing
this evolutionary path in

00:56:11.530 --> 00:56:15.600
there [UNINTELLIGIBLE] but I
don't think we know what cool

00:56:15.600 --> 00:56:16.660
things we can do with that.

00:56:16.660 --> 00:56:19.050
There's a lot of opportunity
in that in some sense.

00:56:19.050 --> 00:56:20.760
AUDIENCE: [INAUDIBLE]

00:56:20.760 --> 00:56:23.240
PROFESSOR: Yeah, because the
interesting thing is --

00:56:23.240 --> 00:56:26.920
the way I would say it is,
in the good old days

00:56:26.920 --> 00:56:29.190
parallelization sometimes
was a scary prospect.

00:56:29.190 --> 00:56:31.510
Because the minute you
distribute data, if you don't

00:56:31.510 --> 00:56:35.610
do it right it's a lot slower
than sequential execution.

00:56:35.610 --> 00:56:39.100
Because your access time becomes
so large, and you're

00:56:39.100 --> 00:56:40.540
basically dead in water.

00:56:40.540 --> 00:56:42.610
In this kind of machine
you don't have to.

00:56:42.610 --> 00:56:44.950
There's so much bandwidth
in here.

00:56:44.950 --> 00:56:47.130
Latency was still -- latency
would be better than going to

00:56:47.130 --> 00:56:49.800
the outside memory.

00:56:49.800 --> 00:56:51.610
And we don't know how
to take advantage of

00:56:51.610 --> 00:56:53.040
that bandwidth yet.

00:56:53.040 --> 00:56:57.310
And my feeling is as we go about
trying to rebuild from

00:56:57.310 --> 00:57:02.440
scratch multicore processors,
we'll try to figure out

00:57:02.440 --> 00:57:03.060
different ways.

00:57:03.060 --> 00:57:10.510
So for example, people are
coming up with much more rich

00:57:10.510 --> 00:57:14.860
semantics for speculation and
stuff like that, and we can

00:57:14.860 --> 00:57:16.580
take advantage of that.

00:57:16.580 --> 00:57:20.980
So I think there's a lot of
interesting hardware,

00:57:20.980 --> 00:57:24.910
microprocessor, and then kind
of programming research now.

00:57:24.910 --> 00:57:27.770
Because I don't think anybody
had anything in there saying ,

00:57:27.770 --> 00:57:30.130
"Here's how we would take it
down to this bandwidth." I

00:57:30.130 --> 00:57:31.810
think that'll happen.

00:57:31.810 --> 00:57:35.480
Now the next [? thing ?]
is these mini-cores.

00:57:35.480 --> 00:57:38.070
So for example, this PicoChip
has array of

00:57:38.070 --> 00:57:39.720
322 processing elements.

00:57:39.720 --> 00:57:43.010
They have 16-bit RISC, so
it's not even a 32-bit.

00:57:43.010 --> 00:57:44.950
Piddling little things,
3-way issue in.

00:57:44.950 --> 00:57:48.980
And they had like
240 standard --

00:57:48.980 --> 00:57:50.370
basically, nothing
more than just a

00:57:50.370 --> 00:57:52.850
multiplier, and add in there.

00:57:52.850 --> 00:57:56.880
64 memory tiles, full control,
and 14 some special

00:57:56.880 --> 00:57:58.480
[UNINTELLIGIBLE] function
accelerator.

00:57:58.480 --> 00:58:03.240
So this is kind of what people
call heterogeneous systems.

00:58:03.240 --> 00:58:05.505
Where what this is -- you have
all these cores, why do you

00:58:05.505 --> 00:58:07.160
make everything the same?

00:58:07.160 --> 00:58:09.450
I can make something that's good
doing graphics, something

00:58:09.450 --> 00:58:11.110
that's good doing networking.

00:58:11.110 --> 00:58:13.540
So I can kind of customize
in these things.

00:58:13.540 --> 00:58:15.350
Because what we have in
excess is silicon.

00:58:15.350 --> 00:58:17.080
We don't have power in excess.

00:58:17.080 --> 00:58:21.250
So in the future you can't
assume everything is working

00:58:21.250 --> 00:58:22.600
all the time, because
that will still

00:58:22.600 --> 00:58:24.310
create too much heat.

00:58:24.310 --> 00:58:27.710
So you kind of say -- the best
efficiencies, for each type of

00:58:27.710 --> 00:58:30.170
computation you have some few
special purpose units.

00:58:30.170 --> 00:58:34.680
So we kind of say if I'm doing
graphics, I fit to my graphics

00:58:34.680 --> 00:58:35.500
optimized code.

00:58:35.500 --> 00:58:36.190
So I will do that.

00:58:36.190 --> 00:58:38.570
And the minute I want to do a
little bit of arithmetic I'll

00:58:38.570 --> 00:58:39.620
switch to that.

00:58:39.620 --> 00:58:43.190
And sometimes I am doing TCP,
I'll switch to my TCP offload.

00:58:43.190 --> 00:58:43.770
Stuff like that.

00:58:43.770 --> 00:58:46.040
Can you do some kind
of mixed in there?

00:58:46.040 --> 00:58:48.880
The problem there is you need to
understand what the mix is.

00:58:48.880 --> 00:58:50.600
So we need to have a good
understanding of

00:58:50.600 --> 00:58:51.880
what that mix is.

00:58:51.880 --> 00:58:54.360
The advantage is it will be a
lot more memory efficient.

00:58:54.360 --> 00:58:56.930
So this is kind of going
in that direction.

00:58:56.930 --> 00:59:00.550
And so in some sense, if you
want to communicate you have

00:59:00.550 --> 00:59:03.280
these special communication
elements.

00:59:03.280 --> 00:59:04.280
You have to go through that.

00:59:04.280 --> 00:59:06.540
And the processor can do some
work, and there are some

00:59:06.540 --> 00:59:07.340
memory elements.

00:59:07.340 --> 00:59:08.630
So far and so forth.

00:59:08.630 --> 00:59:11.950
So that's one push, people are
pushing more for embedded very

00:59:11.950 --> 00:59:13.120
low power in.

00:59:13.120 --> 00:59:15.770
AUDIENCE: Is this starting to
look more and more like FPGA,

00:59:15.770 --> 00:59:16.830
which is [UNINTELLIGIBLE]

00:59:16.830 --> 00:59:20.660
PROFESSOR: Yeah, it's a
kind of a combination.

00:59:20.660 --> 00:59:25.300
Because the thing about FPGA is,
it's just done 1-bit lot.

00:59:25.300 --> 00:59:27.950
That doesn't make sense
to do any arithmetic.

00:59:27.950 --> 00:59:30.550
So this is saying -- "Ok,
instead of 1 bit I am doing 16

00:59:30.550 --> 00:59:34.660
bits." Because then I can
very efficiently build

00:59:34.660 --> 00:59:35.760
[UNINTELLIGIBLE]

00:59:35.760 --> 00:59:36.960
Because I don't have to
build [UNINTELLIGIBLE]

00:59:36.960 --> 00:59:38.890
out of scratch.

00:59:38.890 --> 00:59:42.140
So I think that an interesting
convergence is happening.

00:59:42.140 --> 00:59:45.930
Because what happened, I think,
for a long time was

00:59:45.930 --> 00:59:47.860
things like architecture and
programming languages, and

00:59:47.860 --> 00:59:50.220
stuff like that, kind of
got stuck in a rut.

00:59:50.220 --> 00:59:52.320
Because things there are
so very efficiently and

00:59:52.320 --> 00:59:56.270
incremental -- it's like doing
research in airplanes.

00:59:56.270 --> 00:59:58.760
Things are so efficient,
so complex.

00:59:58.760 --> 01:00:05.020
Here AeroAstro can't build an
airplane, because it's a $9

01:00:05.020 --> 01:00:10.000
billion job to build a good
airplane in there.

01:00:10.000 --> 01:00:11.380
And it became like that.

01:00:11.380 --> 01:00:13.350
Universities could not build
it because if you want to

01:00:13.350 --> 01:00:16.610
build a superscalar it's,
again, a $9 billion type

01:00:16.610 --> 01:00:19.130
endeavor to do that -- thousands
of people, was very,

01:00:19.130 --> 01:00:20.020
very customized.

01:00:20.020 --> 01:00:22.670
But now it's kind of hitting
the end of the road.

01:00:22.670 --> 01:00:24.562
Everbody's going back and saying
-- "Jeez, what's the

01:00:24.562 --> 01:00:26.090
new thing?" And I think there's
a lot of opportunity

01:00:26.090 --> 01:00:29.270
to kind of figure out is there
some radically different thing

01:00:29.270 --> 01:00:30.340
you can do.

01:00:30.340 --> 01:00:33.640
So this is what I have
for my first lecture.

01:00:33.640 --> 01:00:35.130
Some conclusions basically.

01:00:35.130 --> 01:00:38.530
I think for a lot of people who
are programmers, there was

01:00:38.530 --> 01:00:42.210
a time that you never cared
about what's under the hood.

01:00:42.210 --> 01:00:44.200
You knew it was going to
go fast, and in the

01:00:44.200 --> 01:00:45.290
air it will go faster.

01:00:45.290 --> 01:00:47.420
I think that's kind of
coming to an end.

01:00:47.420 --> 01:00:49.480
And there's a lot of
variations/choices in

01:00:49.480 --> 01:00:51.900
hardware, and I think software
people should understand and

01:00:51.900 --> 01:00:54.970
know what they can
choose in here.

01:00:54.970 --> 01:00:57.630
And many have performance
implications.

01:00:57.630 --> 01:01:01.710
And if you know these things you
will be able to get high

01:01:01.710 --> 01:01:03.070
performance of software
built easy.

01:01:03.070 --> 01:01:05.570
You can't do high performance
software without knowing what

01:01:05.570 --> 01:01:07.190
it's running on.

01:01:07.190 --> 01:01:09.860
However, there's a
note of caution.

01:01:09.860 --> 01:01:13.550
If you become too much attached
to your hardware, we

01:01:13.550 --> 01:01:16.270
go back to the old days of
assembly language programming.

01:01:16.270 --> 01:01:19.910
So you say -- "I got every
performance out of a -- now

01:01:19.910 --> 01:01:24.090
the Cell says you have seven
SPEs." So in two years, they

01:01:24.090 --> 01:01:25.290
come with 16 SPEs.

01:01:25.290 --> 01:01:26.080
And what's going to happen?

01:01:26.080 --> 01:01:28.920
Your thing is still working on
seven SPEs very well, but it

01:01:28.920 --> 01:01:31.020
might not work on 16 SPEs,
even with that.

01:01:31.020 --> 01:01:33.700
But of course, you really
customize for Cell too.

01:01:33.700 --> 01:01:36.780
And I guarantee it will not
run good with the Intel --

01:01:36.780 --> 01:01:39.670
probably Quad, Xeon processor
-- because it will be doing

01:01:39.670 --> 01:01:41.040
something very different.

01:01:41.040 --> 01:01:44.950
And so there's this tension
that's coming back again.

01:01:44.950 --> 01:01:48.540
How to do something that is
general, portable, malleable,

01:01:48.540 --> 01:01:52.255
and at the same time get good
performance with hardware

01:01:52.255 --> 01:01:52.770
being exposed.

01:01:52.770 --> 01:01:54.020
I don't think there's
an answer for that.

01:01:54.020 --> 01:01:55.870
And in this class we are going
to go to one extreme.

01:01:55.870 --> 01:01:58.710
We are going to go low level
and really understand the

01:01:58.710 --> 01:02:01.540
hardware, and take advantage
of that.

01:02:01.540 --> 01:02:04.340
But at some point we have to
probably come out of that and

01:02:04.340 --> 01:02:06.420
figure out how to be,
again, high level.

01:02:06.420 --> 01:02:09.137
And I think that these
are open questions.

01:02:09.137 --> 01:02:10.965
AUDIENCE: Do you have any
thoughts, and this may be

01:02:10.965 --> 01:02:15.620
unanswerable, but how could
Cell really [INAUDIBLE].

01:02:15.620 --> 01:02:18.970
And not Cell only, but some of
these other ones that are out

01:02:18.970 --> 01:02:22.870
there today, given how hard
they are to program.

01:02:22.870 --> 01:02:25.200
PROFESSOR: So I have
this talk that I'm

01:02:25.200 --> 01:02:25.860
giving at all the places.

01:02:25.860 --> 01:02:28.320
I said the third software
crisis is due

01:02:28.320 --> 01:02:30.340
to multicore menace.

01:02:30.340 --> 01:02:35.090
I termed it a menace, because it
will create this thing that

01:02:35.090 --> 01:02:36.000
people will have to change.

01:02:36.000 --> 01:02:38.410
Something has to change,
something has to give.

01:02:38.410 --> 01:02:40.300
I don't know who's
going to give.

01:02:40.300 --> 01:02:42.560
Either people will say -- "This
is too complicated, I am

01:02:42.560 --> 01:02:44.050
happy with the current
performance.

01:02:44.050 --> 01:02:46.550
I will live for the next 20
years at today's level of

01:02:46.550 --> 01:02:51.070
performance." I doubt
that will happen.

01:02:51.070 --> 01:02:53.290
The other end is saying --
"Jeez, you know I am going to

01:02:53.290 --> 01:02:56.410
learn parallel programming, and
I will deal with locks and

01:02:56.410 --> 01:02:58.060
semaphores, and all
those things.

01:02:58.060 --> 01:03:00.080
And I am going to jump in
there." That's not going to

01:03:00.080 --> 01:03:01.040
happen either.

01:03:01.040 --> 01:03:02.790
So there has to be something
in the middle.

01:03:02.790 --> 01:03:04.380
And the neat thing is,
I don't think anybody

01:03:04.380 --> 01:03:07.650
knows what it is.

01:03:07.650 --> 01:03:12.120
Being in industry, it makes them
terrified, because they

01:03:12.120 --> 01:03:13.190
have no idea what's happening.

01:03:13.190 --> 01:03:14.360
But in a university,
it's a fun time.

01:03:14.360 --> 01:03:17.220
[LAUGHTER]

01:03:17.220 --> 01:03:18.650
AUDIENCE: Good question.

01:03:18.650 --> 01:03:18.890
PROFESSOR: OK.

01:03:18.890 --> 01:03:21.850
So we'll take about a five
minutes break, and switch

01:03:21.850 --> 01:03:24.490
gears into concurrent
programming.