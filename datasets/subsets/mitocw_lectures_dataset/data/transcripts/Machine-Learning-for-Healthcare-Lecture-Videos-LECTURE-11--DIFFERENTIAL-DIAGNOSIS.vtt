WEBVTT

00:00:15.730 --> 00:00:16.840
PETER SZOLOVITS: OK.

00:00:16.840 --> 00:00:20.810
Today's topic is
differential diagnosis.

00:00:20.810 --> 00:00:23.230
And so I'm just
quoting Wikipedia here.

00:00:23.230 --> 00:00:27.340
Diagnosis is the identification
of the nature and cause

00:00:27.340 --> 00:00:29.230
of a certain phenomenon.

00:00:29.230 --> 00:00:33.070
And differential diagnosis
is the distinguishing

00:00:33.070 --> 00:00:35.800
of a particular
disease or condition

00:00:35.800 --> 00:00:39.490
from others that present
similar clinical features.

00:00:39.490 --> 00:00:43.720
So doctors typically talk
about differential diagnosis

00:00:43.720 --> 00:00:46.090
when they're faced
with a patient

00:00:46.090 --> 00:00:49.510
and they make list of what
are the things that might

00:00:49.510 --> 00:00:51.650
be wrong with this patient.

00:00:51.650 --> 00:00:53.200
And then they go
through the process

00:00:53.200 --> 00:00:56.560
of trying to figure out
which one it actually is.

00:00:56.560 --> 00:00:59.900
So that's what we're
going to focus on today.

00:00:59.900 --> 00:01:05.530
Now, just to scare you,
here's a lovely model

00:01:05.530 --> 00:01:09.400
of human circulatory physiology.

00:01:09.400 --> 00:01:15.700
So this is from Guyton's
textbook of cardiology.

00:01:15.700 --> 00:01:20.170
And I'm not going to hold
you responsible for all

00:01:20.170 --> 00:01:24.520
of the details of this
model, but it's interesting,

00:01:24.520 --> 00:01:29.020
because this is, at least
as of maybe 20 years ago,

00:01:29.020 --> 00:01:32.890
the state of the art of how
people understood what happens

00:01:32.890 --> 00:01:35.120
in the circulatory system.

00:01:35.120 --> 00:01:38.590
And it has various
control inputs

00:01:38.590 --> 00:01:44.200
that determine things like
how your hormone levels change

00:01:44.200 --> 00:01:47.800
various aspects of the
cardiovascular system

00:01:47.800 --> 00:01:52.060
and how the interactions
between different components

00:01:52.060 --> 00:01:57.220
of the cardiovascular
system affect each other.

00:01:57.220 --> 00:02:02.620
And so in principle, if I
could tune this model to me,

00:02:02.620 --> 00:02:06.100
then I could make all kinds
of pretty good predictions

00:02:06.100 --> 00:02:11.560
that say if I increase my
systemic vascular resistance,

00:02:11.560 --> 00:02:13.510
then here's what's
going to happen

00:02:13.510 --> 00:02:16.570
as the rest of the
system adjusts.

00:02:16.570 --> 00:02:20.590
And if I get a blockage
in a coronary artery,

00:02:20.590 --> 00:02:24.340
then here's what's going to
happen to my cardiac output

00:02:24.340 --> 00:02:27.080
and various other things.

00:02:27.080 --> 00:02:28.610
So this would be terrific.

00:02:28.610 --> 00:02:32.170
And if we had this
kind of model for not

00:02:32.170 --> 00:02:36.310
just the cardiovascular system,
but the entire body, then we'd

00:02:36.310 --> 00:02:39.550
say, OK, we've solved medicine.

00:02:39.550 --> 00:02:44.500
Well, we don't have this kind
of model for most systems.

00:02:44.500 --> 00:02:47.230
And also, there's
this minor problem

00:02:47.230 --> 00:02:51.310
that if I give you this
model and say, "How does this

00:02:51.310 --> 00:02:55.030
relate to a
particular patient?",

00:02:55.030 --> 00:02:57.250
how would you figure that out?

00:02:57.250 --> 00:02:59.740
This has hundreds of
differential equations

00:02:59.740 --> 00:03:02.590
that are being represented
by this diagram.

00:03:02.590 --> 00:03:05.830
And they have many
hundreds of parameters.

00:03:05.830 --> 00:03:11.920
And so we were joking when we
started working with this model

00:03:11.920 --> 00:03:14.800
that you'd really have to
kill the patient in order

00:03:14.800 --> 00:03:21.070
to do enough measurements to
be able to tune this model

00:03:21.070 --> 00:03:23.620
to their particular physiology.

00:03:23.620 --> 00:03:28.030
And of course, that's probably
not a good practical approach.

00:03:28.030 --> 00:03:31.990
We're getting a little better
by developing more non-invasive

00:03:31.990 --> 00:03:36.520
ways of measuring these things.

00:03:36.520 --> 00:03:38.830
But that's moving
along very slowly.

00:03:38.830 --> 00:03:43.030
And I don't expect that I
or maybe even any of you

00:03:43.030 --> 00:03:47.620
will live long enough that
sort of this approach to doing

00:03:47.620 --> 00:03:50.410
medical reasoning
and medical diagnosis

00:03:50.410 --> 00:03:54.110
is actually going to happen.

00:03:54.110 --> 00:03:57.670
So what we're going
to look at today

00:03:57.670 --> 00:04:04.660
is what simpler models are
there for diagnostic reasoning.

00:04:04.660 --> 00:04:08.260
And I'm going to take
the liberty of inflicting

00:04:08.260 --> 00:04:12.970
a bit of history on you, because
I think it's interesting where

00:04:12.970 --> 00:04:16.168
a lot of these ideas came from.

00:04:16.168 --> 00:04:20.540
So the first idea was
to build flowcharts.

00:04:20.540 --> 00:04:25.810
Oh, and by the way,
the signs and symptoms,

00:04:25.810 --> 00:04:29.120
I've forgotten if we've talked
about that in the class.

00:04:29.120 --> 00:04:33.680
So a sign is something
that a doctor sees,

00:04:33.680 --> 00:04:37.980
and a symptom is something
that the patient experiences.

00:04:37.980 --> 00:04:39.750
So a sign is objective.

00:04:39.750 --> 00:04:42.950
It's something that can
be told outside your body.

00:04:42.950 --> 00:04:45.950
A symptom is something
that you feel.

00:04:45.950 --> 00:04:49.070
So if you're feeling
dizzy, then that's

00:04:49.070 --> 00:04:53.840
a symptom, because it's not
obvious to somebody outside you

00:04:53.840 --> 00:04:58.640
that you're dizzy, or that you
have a pain, or such things.

00:04:58.640 --> 00:05:01.970
Normally, we talk about
manifestations or findings,

00:05:01.970 --> 00:05:05.270
which is sort of a super
category of all the things

00:05:05.270 --> 00:05:09.590
that are determinable
about a patient.

00:05:09.590 --> 00:05:14.060
So we'll talk about
flowcharts, models

00:05:14.060 --> 00:05:17.180
based on associations
between diseases

00:05:17.180 --> 00:05:19.392
and these manifestations.

00:05:19.392 --> 00:05:21.350
Then there are some issues
about whether you're

00:05:21.350 --> 00:05:25.040
trying to diagnose a single
disease or a multiplicity

00:05:25.040 --> 00:05:29.330
of diseases, which makes the
models much more complicated

00:05:29.330 --> 00:05:32.540
whether you're trying to
do probabilistic diagnosis

00:05:32.540 --> 00:05:35.600
or definitive or categorical.

00:05:35.600 --> 00:05:39.440
And then we'll talk about some
utility theoretic methods.

00:05:39.440 --> 00:05:43.040
And I'll just mention some
rule-based and pattern-matching

00:05:43.040 --> 00:05:45.030
kinds of approaches.

00:05:45.030 --> 00:05:47.660
So this is kind of cute.

00:05:47.660 --> 00:05:50.540
This is from 1973.

00:05:50.540 --> 00:05:54.920
And if you were a woman and
walked into the MIT Health

00:05:54.920 --> 00:05:58.460
Center and complained
of potentially

00:05:58.460 --> 00:06:02.210
a urinary tract infection,
they would take out

00:06:02.210 --> 00:06:05.720
this sheet of paper, which
was nicely color-coded,

00:06:05.720 --> 00:06:08.120
and they would check
a bunch of boxes.

00:06:08.120 --> 00:06:13.080
And if you hit a red box,
that represented a conclusion.

00:06:13.080 --> 00:06:15.950
And otherwise, it
gave you suggestions

00:06:15.950 --> 00:06:18.590
about what further tests to do.

00:06:18.590 --> 00:06:21.320
And this was essentially
a triage instrument.

00:06:21.320 --> 00:06:25.040
It said, does this woman
have a problem that

00:06:25.040 --> 00:06:26.960
requires immediate attention?

00:06:26.960 --> 00:06:30.560
And so we should either
call an ambulance

00:06:30.560 --> 00:06:35.810
and take them to a hospital,
or is it something where

00:06:35.810 --> 00:06:37.970
we can just tell them to
come back the next day

00:06:37.970 --> 00:06:40.640
and see a doctor,
or is it in fact

00:06:40.640 --> 00:06:44.480
some self-limited thing where
we say, take two aspirin,

00:06:44.480 --> 00:06:46.170
and it'll go away.

00:06:46.170 --> 00:06:48.860
So that was the attempt here.

00:06:48.860 --> 00:06:51.110
Now, interestingly, if
you look at the history

00:06:51.110 --> 00:06:54.770
of this project between the
Beth Israel Hospital and Lincoln

00:06:54.770 --> 00:06:59.060
Laboratories, it started
off as a computer aid.

00:06:59.060 --> 00:07:02.420
So they were building
a computer system

00:07:02.420 --> 00:07:05.040
that was supposed to do this.

00:07:05.040 --> 00:07:09.180
And then in-- but you can
imagine, in the late 1960s,

00:07:09.180 --> 00:07:13.970
early 1970s, computers
were pretty clunky.

00:07:13.970 --> 00:07:15.900
PCs hadn't been invented yet.

00:07:15.900 --> 00:07:19.110
So this was like mainframe
kinds of operations.

00:07:19.110 --> 00:07:21.680
It was very hard to use.

00:07:21.680 --> 00:07:25.610
And so they said, well, this
is a small enough program

00:07:25.610 --> 00:07:28.870
that we can reduce it to
about 20 flow sheets--

00:07:28.870 --> 00:07:34.100
20 sheets like this, which
they proceeded to print up.

00:07:34.100 --> 00:07:37.040
And I was amused,
because in the--

00:07:37.040 --> 00:07:40.930
around 1980, I was working
in my office one night.

00:07:40.930 --> 00:07:43.130
And I got this
splitting headache.

00:07:43.130 --> 00:07:45.530
And I went over to MIT medical.

00:07:45.530 --> 00:07:47.960
And sure enough,
the nurse pulled out

00:07:47.960 --> 00:07:50.480
one of these sheets
for headaches

00:07:50.480 --> 00:07:52.760
and went through it
with me and decided

00:07:52.760 --> 00:07:56.420
that a couple of
Tylenols should fix me.

00:07:56.420 --> 00:07:57.770
But it was interesting.

00:07:57.770 --> 00:08:00.740
So this was really
in use for a while.

00:08:00.740 --> 00:08:04.640
Now, the difficulty with
approaches like this,

00:08:04.640 --> 00:08:06.450
of which there have
been many, many,

00:08:06.450 --> 00:08:11.180
many in the medical world,
is that they're very fragile.

00:08:11.180 --> 00:08:13.620
They're very specific.

00:08:13.620 --> 00:08:18.410
They don't take account
of unusual cases.

00:08:18.410 --> 00:08:22.100
And there's a lot of effort
in coming to consensus

00:08:22.100 --> 00:08:23.730
to build these things.

00:08:23.730 --> 00:08:27.080
And then they're not necessarily
useful for a long time.

00:08:27.080 --> 00:08:30.800
So MIT actually stopped
using them shortly

00:08:30.800 --> 00:08:33.530
after my headache experience.

00:08:33.530 --> 00:08:36.710
But if you go over
to a hospital and you

00:08:36.710 --> 00:08:41.900
look on the bookshelf
of a junior doctor,

00:08:41.900 --> 00:08:44.600
you will still find
manuals that look

00:08:44.600 --> 00:08:47.960
kind of like this
that say, how do we

00:08:47.960 --> 00:08:50.960
deal with tropical diseases?

00:08:50.960 --> 00:08:53.450
So you ask a bunch of
questions, and then

00:08:53.450 --> 00:08:56.990
depending on the branching
logic of the flowchart,

00:08:56.990 --> 00:09:00.000
it'll tell you whether
this is serious or not.

00:09:00.000 --> 00:09:02.690
And the reason is because if
you do your medical training

00:09:02.690 --> 00:09:04.580
in Boston, you're
not going to see

00:09:04.580 --> 00:09:06.800
very many tropical diseases.

00:09:06.800 --> 00:09:10.220
And so you don't have
a base of experience

00:09:10.220 --> 00:09:13.370
on the basis of which
you can learn and become

00:09:13.370 --> 00:09:14.630
an expert at doing it.

00:09:14.630 --> 00:09:16.970
And so they use this as
a kind of cheat sheet.

00:09:20.070 --> 00:09:25.830
I mentioned that the association
between diseases and symptoms

00:09:25.830 --> 00:09:29.850
is another important
way of doing diagnosis.

00:09:29.850 --> 00:09:35.100
And I swear to you, there was
a paper in the 1960s, I think,

00:09:35.100 --> 00:09:37.200
that actually proposed this.

00:09:37.200 --> 00:09:41.790
So if any of you have hung
around ancient libraries,

00:09:41.790 --> 00:09:44.910
libraries used to have
card catalogs that

00:09:44.910 --> 00:09:48.780
were physical pieces
of paper, cardboard.

00:09:48.780 --> 00:09:55.170
And one of the things they
did with these was each card

00:09:55.170 --> 00:09:57.270
would be a book.

00:09:57.270 --> 00:10:01.330
And then around the edges
were a bunch of holes,

00:10:01.330 --> 00:10:04.770
and depending on
categorizations of the book

00:10:04.770 --> 00:10:09.000
along various dimensions,
like its Dewey decimal number,

00:10:09.000 --> 00:10:13.380
or the top digits of
its Library of Congress

00:10:13.380 --> 00:10:19.030
number or something, they would
punch out holes in the borders.

00:10:19.030 --> 00:10:21.090
And this allowed
you to do a kind

00:10:21.090 --> 00:10:24.130
of easy sorting of these books.

00:10:24.130 --> 00:10:26.100
So if you've got
a bunch of cards

00:10:26.100 --> 00:10:29.100
together when people were
returning their books

00:10:29.100 --> 00:10:31.440
and you pulled a bunch of cards.

00:10:31.440 --> 00:10:34.320
And you wanted to find
all the math books.

00:10:34.320 --> 00:10:38.040
So what you would do is you'd
stick a needle through the hole

00:10:38.040 --> 00:10:42.300
that represented math books,
and then you shake the pile,

00:10:42.300 --> 00:10:44.670
and all the math
books would fall out

00:10:44.670 --> 00:10:46.890
because they had punched.

00:10:46.890 --> 00:10:52.120
So somebody seriously proposed
this as a diagnostic algorithm.

00:10:52.120 --> 00:10:54.660
And in fact, implemented it.

00:10:54.660 --> 00:10:59.010
And was trying to
even make money on it.

00:10:59.010 --> 00:11:03.390
I think this was an attempt at
a commercial venture, where they

00:11:03.390 --> 00:11:07.440
were going to provide doctors
with these library cards that

00:11:07.440 --> 00:11:09.390
represented diseases.

00:11:09.390 --> 00:11:12.390
And the holes now
represented not mathematics

00:11:12.390 --> 00:11:15.570
versus literature,
but they represented

00:11:15.570 --> 00:11:18.750
shortness of breath versus
pain in the left ankle

00:11:18.750 --> 00:11:20.490
versus whatever.

00:11:20.490 --> 00:11:22.980
And again, as people
came in and complained

00:11:22.980 --> 00:11:25.500
about some condition,
you'd stick a needle

00:11:25.500 --> 00:11:27.820
through that condition
and you'd shake,

00:11:27.820 --> 00:11:33.000
and up would come the cards that
had that condition in common.

00:11:33.000 --> 00:11:37.500
So one of the obvious
problems with this approach

00:11:37.500 --> 00:11:42.150
is that if you had two
things wrong with you, then

00:11:42.150 --> 00:11:47.970
you would wind up with no cards
very quickly, because nothing

00:11:47.970 --> 00:11:49.980
would fall out of the pile.

00:11:49.980 --> 00:11:51.350
So this didn't go anywhere.

00:11:51.350 --> 00:11:56.890
But interestingly,
even in the late 1980s,

00:11:56.890 --> 00:12:01.560
I remember being asked by the
board of directors of the New

00:12:01.560 --> 00:12:04.560
England Journal of Medicine
to come to a meeting

00:12:04.560 --> 00:12:08.700
where they had gotten a pitch
from somebody who was proposing

00:12:08.700 --> 00:12:11.730
essentially exactly
this diagnostic model,

00:12:11.730 --> 00:12:16.680
except implemented in a computer
now and not in these library

00:12:16.680 --> 00:12:17.670
cards.

00:12:17.670 --> 00:12:20.070
And they wanted to know
whether this was something

00:12:20.070 --> 00:12:22.740
that they ought to get
behind and invest in.

00:12:22.740 --> 00:12:24.600
And I and a bunch
of my colleagues

00:12:24.600 --> 00:12:27.857
assured them that this was
probably not a great idea

00:12:27.857 --> 00:12:29.940
and they should stay away
from it, which they did.

00:12:32.940 --> 00:12:36.900
Well, a more sophisticated model
is something like a Naive Bayes

00:12:36.900 --> 00:12:40.065
model that says if
you have a disease--

00:12:43.450 --> 00:12:44.390
where is my cursor?

00:12:44.390 --> 00:12:48.330
If you have a disease, and you
have a bunch of manifestations

00:12:48.330 --> 00:12:50.850
that can be caused
by the disease,

00:12:50.850 --> 00:12:53.700
we can make some
simplifying assumptions

00:12:53.700 --> 00:12:55.860
that say that you
will only ever have

00:12:55.860 --> 00:12:59.250
one disease at a
time, which means

00:12:59.250 --> 00:13:05.400
that the values of that node D
form an exhaustive and mutually

00:13:05.400 --> 00:13:08.460
exclusive set of values.

00:13:08.460 --> 00:13:10.830
And we can assume that
the manifestations are

00:13:10.830 --> 00:13:14.970
conditionally independent
observables that depend only

00:13:14.970 --> 00:13:17.970
on the disease that you
have, but not on each other

00:13:17.970 --> 00:13:21.760
or not on any other factors.

00:13:21.760 --> 00:13:23.790
And if you make that
assumption, then you

00:13:23.790 --> 00:13:27.960
can apply good old
Thomas Bayes's rule.

00:13:27.960 --> 00:13:31.540
This, by the way, is
the Reverend Bayes.

00:13:31.540 --> 00:13:35.310
Do you guys know his history?

00:13:35.310 --> 00:13:41.410
So he was a nonconformist
minister in England.

00:13:41.410 --> 00:13:47.200
And he was not a
mathematician, except I mean,

00:13:47.200 --> 00:13:49.360
he was an amateur mathematician.

00:13:49.360 --> 00:13:51.580
But he decided that
he wanted to prove

00:13:51.580 --> 00:13:54.830
to people that God existed.

00:13:54.830 --> 00:13:57.070
And so he developed
Bayesian reasoning

00:13:57.070 --> 00:13:59.470
in order to make this proof.

00:13:59.470 --> 00:14:02.310
And so his argument
was, well, suppose

00:14:02.310 --> 00:14:04.330
you're completely in doubt.

00:14:04.330 --> 00:14:08.140
So you have 50/50
odds that God exists.

00:14:08.140 --> 00:14:12.220
And then you say,
let's look at miracles.

00:14:12.220 --> 00:14:15.160
And let's ask, what's the
likelihood of this miracle

00:14:15.160 --> 00:14:20.560
having occurred if God exists
versus if God doesn't exist?

00:14:20.560 --> 00:14:23.560
And so by racking up
a bunch of miracles,

00:14:23.560 --> 00:14:28.060
you can convince people more
and more that God must exist,

00:14:28.060 --> 00:14:29.860
because otherwise
all these miracles

00:14:29.860 --> 00:14:31.750
couldn't have happened.

00:14:31.750 --> 00:14:35.500
So he never publish this in his
lifetime, but after his death

00:14:35.500 --> 00:14:38.110
one of his colleagues
actually presented this

00:14:38.110 --> 00:14:42.580
as a paper at the Royal
Society in the UK.

00:14:42.580 --> 00:14:46.930
And so Bayes became
famous as the originator

00:14:46.930 --> 00:14:52.060
of this notion of how to do
probabilistic reasoning about

00:14:52.060 --> 00:14:54.520
at least fairly
simple situations,

00:14:54.520 --> 00:14:58.600
like in his case, the existence
or nonexistence of God.

00:14:58.600 --> 00:15:01.840
Or in our case, the
cause of some disease,

00:15:01.840 --> 00:15:04.340
the nature of some disease.

00:15:04.340 --> 00:15:05.980
And so you can draw these trees.

00:15:05.980 --> 00:15:08.420
And Bayes's rule is very simple.

00:15:08.420 --> 00:15:10.870
I'm sure you've all seen it.

00:15:10.870 --> 00:15:15.050
One thing that, again,
makes contact with medicine

00:15:15.050 --> 00:15:19.040
is that a lot of times,
you're not just interested

00:15:19.040 --> 00:15:22.670
in the impact of one
observable on your probability

00:15:22.670 --> 00:15:25.520
distribution, but you're
interested in the impact

00:15:25.520 --> 00:15:28.140
of a sequence of observations.

00:15:28.140 --> 00:15:31.320
And so one thing you
can do is you can say,

00:15:31.320 --> 00:15:34.680
well, here is my
general population.

00:15:34.680 --> 00:15:40.310
So let's say disease 2 has
37% prevalence and disease 1

00:15:40.310 --> 00:15:43.100
has 12%, et cetera.

00:15:43.100 --> 00:15:45.470
And now I make some observation.

00:15:45.470 --> 00:15:47.210
I apply Bayes's rule.

00:15:47.210 --> 00:15:50.980
And I revise my
probability distribution.

00:15:50.980 --> 00:15:55.560
So this is the equivalent of
finding a smaller population

00:15:55.560 --> 00:15:58.290
of patients who have
all had whatever

00:15:58.290 --> 00:16:00.960
answer I got for symptom 1.

00:16:00.960 --> 00:16:03.100
And then I just keep doing that.

00:16:03.100 --> 00:16:06.660
And so this is the sequential
application of Bayes's rule.

00:16:06.660 --> 00:16:10.410
And of course, it does depend
on the conditional independence

00:16:10.410 --> 00:16:13.030
of all these symptoms.

00:16:13.030 --> 00:16:15.820
But in medicine,
people don't like

00:16:15.820 --> 00:16:19.600
to do math, even
arithmetic much.

00:16:19.600 --> 00:16:23.530
And they prefer doing addition
rather than multiplication,

00:16:23.530 --> 00:16:25.400
because it's easier.

00:16:25.400 --> 00:16:27.590
And so what they've
done is they said,

00:16:27.590 --> 00:16:30.820
well, instead of
representing all this data

00:16:30.820 --> 00:16:36.550
in a probabilistic framework,
let's represent it as odds.

00:16:36.550 --> 00:16:45.470
And if you represent it
as odds, then the odds

00:16:45.470 --> 00:16:48.320
of some disease given
a bunch of symptoms,

00:16:48.320 --> 00:16:50.520
given the independence
assumption,

00:16:50.520 --> 00:16:53.720
is just the prior
odds of the disease

00:16:53.720 --> 00:16:57.110
times the conditional
odds, the likelihood

00:16:57.110 --> 00:17:00.560
ratio of each of the symptoms
that you've observed.

00:17:00.560 --> 00:17:03.410
So you've just got to
multiply these together.

00:17:03.410 --> 00:17:06.230
And then because they like
adding more than multiplying,

00:17:06.230 --> 00:17:10.250
they said, let's take
the log of both sides.

00:17:10.250 --> 00:17:12.650
And then you can just add them.

00:17:12.650 --> 00:17:18.109
And so if you remember when I
was talking about medical data,

00:17:18.109 --> 00:17:20.540
there are things
like the Glasgow Coma

00:17:20.540 --> 00:17:24.829
score, or the APACHE
score, or various measures

00:17:24.829 --> 00:17:29.300
of how badly or well a patient
is doing that often involve

00:17:29.300 --> 00:17:33.170
adding up numbers corresponding
to different conditions

00:17:33.170 --> 00:17:34.590
that they have.

00:17:34.590 --> 00:17:37.160
And what they're
doing is exactly this.

00:17:37.160 --> 00:17:40.730
They're applying
sequentially Bayes's rule

00:17:40.730 --> 00:17:45.050
with these independence
assumptions in the form of logs

00:17:45.050 --> 00:17:48.457
rather than
multiplications, log odds,

00:17:48.457 --> 00:17:49.790
and that's how they're doing it.

00:17:52.460 --> 00:17:55.250
Very quickly.

00:17:55.250 --> 00:18:00.680
Somebody in a previous lecture
was wondering about receiver

00:18:00.680 --> 00:18:02.990
operator characteristic curves.

00:18:02.990 --> 00:18:05.880
And I just wanted to give you a
little bit of insight on those.

00:18:05.880 --> 00:18:10.880
So if you do a test on two
populations of patients--

00:18:10.880 --> 00:18:12.620
the red ones are sick patients.

00:18:12.620 --> 00:18:16.130
The blue ones are
not sick patients.

00:18:16.130 --> 00:18:17.150
You do some test.

00:18:17.150 --> 00:18:20.750
What you expect is that
the result of that test

00:18:20.750 --> 00:18:23.420
will be some continuous
number, and it'll

00:18:23.420 --> 00:18:26.270
be distributed something
like the blue distribution

00:18:26.270 --> 00:18:28.550
for the well patients
and something

00:18:28.550 --> 00:18:31.550
like the red distribution
for the ill patients.

00:18:31.550 --> 00:18:35.190
And typically, we
choose some threshold.

00:18:35.190 --> 00:18:39.110
And we say, well,
if you choose this

00:18:39.110 --> 00:18:43.040
to be the threshold between
a prediction of sick

00:18:43.040 --> 00:18:46.430
or well, then what
you're going to get

00:18:46.430 --> 00:18:49.130
is that the part of the
blue distribution that

00:18:49.130 --> 00:18:52.820
lies to the right is
the false positives

00:18:52.820 --> 00:18:54.830
and the part of the
red distribution that

00:18:54.830 --> 00:18:59.240
lies to the left is
the false negatives.

00:18:59.240 --> 00:19:03.500
And often people will choose the
lowest point at which these two

00:19:03.500 --> 00:19:07.010
curves intersect as the
threshold, but that, of course,

00:19:07.010 --> 00:19:09.440
isn't necessarily the case.

00:19:09.440 --> 00:19:11.630
Now, if I give
you a better test,

00:19:11.630 --> 00:19:16.040
one like this, that's terrific,
because there is essentially no

00:19:16.040 --> 00:19:17.180
overlap.

00:19:17.180 --> 00:19:21.860
Very small false negative
and false positive rates.

00:19:21.860 --> 00:19:23.690
And as I said, you
can choose to put

00:19:23.690 --> 00:19:26.090
the threshold in
different places,

00:19:26.090 --> 00:19:27.830
depending on how you
want to trade off

00:19:27.830 --> 00:19:30.380
sensitivity and specificity.

00:19:30.380 --> 00:19:33.350
And we measure this by
this receiver operator

00:19:33.350 --> 00:19:38.000
characteristics curve,
which has the general form

00:19:38.000 --> 00:19:42.170
that if you get a
curve like this,

00:19:42.170 --> 00:19:45.470
that means that there's an
exact trade-off for sensitivity

00:19:45.470 --> 00:19:50.220
and specificity, which is the
case if you're flipping coins.

00:19:50.220 --> 00:19:52.070
So it's random.

00:19:52.070 --> 00:19:56.400
And of course, if you manage
to hit the top corner up there,

00:19:56.400 --> 00:19:58.430
that means that there
would be no overlap

00:19:58.430 --> 00:20:01.190
whatsoever between
the two distributions,

00:20:01.190 --> 00:20:03.350
and you would get
a perfect result.

00:20:03.350 --> 00:20:06.570
And so typically you get
something in between.

00:20:06.570 --> 00:20:09.110
And so normally,
if you do a study

00:20:09.110 --> 00:20:13.760
and your AUC, the area
under this receiver operator

00:20:13.760 --> 00:20:17.930
characteristics curve,
is barely over a half,

00:20:17.930 --> 00:20:20.150
you're pretty
close to worthless,

00:20:20.150 --> 00:20:22.160
whereas if it's
close to 1, then you

00:20:22.160 --> 00:20:25.880
have a really good
method for distinguishing

00:20:25.880 --> 00:20:27.935
these categories of patients.

00:20:31.040 --> 00:20:32.395
Next topic.

00:20:32.395 --> 00:20:33.770
What does it mean
to be rational?

00:20:38.084 --> 00:20:40.320
I should have a
philosophy course here.

00:20:43.164 --> 00:20:44.810
AUDIENCE: Are you
talking about pi?

00:20:44.810 --> 00:20:45.200
PETER SZOLOVITS: Sorry.

00:20:45.200 --> 00:20:46.658
AUDIENCE: Are you
talking about pi?

00:20:46.658 --> 00:20:47.870
Pi is--

00:20:47.870 --> 00:20:49.910
PETER SZOLOVITS:
Pi is irrational,

00:20:49.910 --> 00:20:51.560
but that's not what
I'm talking about.

00:20:54.170 --> 00:20:56.870
Well, so there is this
principle of rationality

00:20:56.870 --> 00:21:01.940
that says that what you want
to do is to act in such a way

00:21:01.940 --> 00:21:04.950
as to maximize your
expected utility.

00:21:04.950 --> 00:21:07.100
So for example, if
you're a gambler

00:21:07.100 --> 00:21:11.270
and you have a choice of various
ways of betting in some poker

00:21:11.270 --> 00:21:15.200
game or something, then if
you were a perfect calculator

00:21:15.200 --> 00:21:20.960
of the odds of getting a
queen on your next draw,

00:21:20.960 --> 00:21:23.360
then you could make
some rational decision

00:21:23.360 --> 00:21:26.180
about whether to
bet more or less,

00:21:26.180 --> 00:21:31.550
but you'd also have to take into
account things like, "How could

00:21:31.550 --> 00:21:38.480
I convince my opponent that I am
not bluffing if I am bluffing?"

00:21:38.480 --> 00:21:42.080
and "How could I convince them
that I'm bluffing if I'm not

00:21:42.080 --> 00:21:44.040
bluffing?" and so on.

00:21:44.040 --> 00:21:47.130
So there is a
complicated model there.

00:21:47.130 --> 00:21:49.100
But nevertheless,
the idea is that you

00:21:49.100 --> 00:21:51.440
should behave in a
way that will give you

00:21:51.440 --> 00:21:54.450
the best expected outcome.

00:21:54.450 --> 00:21:58.490
And so people joke that
this is Homo economicus,

00:21:58.490 --> 00:22:01.550
because economists make
the assumption that this

00:22:01.550 --> 00:22:03.680
is how people behave.

00:22:03.680 --> 00:22:07.140
And we now know that that's
not really how people behave.

00:22:07.140 --> 00:22:10.310
But it's a pretty common
model of their behavior,

00:22:10.310 --> 00:22:12.530
because it's easy
to compute, and it

00:22:12.530 --> 00:22:16.520
has some appropriate
characteristics.

00:22:16.520 --> 00:22:20.180
So as I mentioned,
every action has a cost.

00:22:20.180 --> 00:22:23.540
And utility measures the
value or the goodness

00:22:23.540 --> 00:22:27.260
of some outcome, which is the
amount of money you've won,

00:22:27.260 --> 00:22:31.550
or whether you live or die, or
quality adjusted life years,

00:22:31.550 --> 00:22:35.780
or various other
measures of utility--

00:22:35.780 --> 00:22:39.370
how much it costs for
your hospitalization.

00:22:39.370 --> 00:22:40.770
So let me give you an example.

00:22:40.770 --> 00:22:45.200
This actually comes from a
decision analysis service

00:22:45.200 --> 00:22:47.930
at New England
Medical Center Tufts

00:22:47.930 --> 00:22:50.580
Hospital in the late 1970s.

00:22:50.580 --> 00:22:53.300
So this was an elderly
Chinese gentleman

00:22:53.300 --> 00:22:55.460
whose foot had gangrene.

00:22:55.460 --> 00:23:00.170
So gangrene is an infection
that usually people

00:23:00.170 --> 00:23:04.010
who have bad circulation
can get these.

00:23:04.010 --> 00:23:06.590
And what he was
facing was a choice

00:23:06.590 --> 00:23:09.680
of whether to amputate
his foot or to try

00:23:09.680 --> 00:23:12.260
to treat him medically.

00:23:12.260 --> 00:23:16.130
To treat him medically
means injecting antibiotics

00:23:16.130 --> 00:23:20.090
into his system and hoping that
his circulation is good enough

00:23:20.090 --> 00:23:23.490
to get them to the
infected areas.

00:23:23.490 --> 00:23:28.520
And so the choice becomes
a little more complicated,

00:23:28.520 --> 00:23:31.970
because if the medical treatment
fails, then, of course,

00:23:31.970 --> 00:23:35.360
the patient may
die, a bad outcome.

00:23:35.360 --> 00:23:38.420
Or you may have to now
amputate the whole leg,

00:23:38.420 --> 00:23:42.560
because the gangrene has spread
from his foot up the foot,

00:23:42.560 --> 00:23:45.240
and now you're
cutting off his leg.

00:23:45.240 --> 00:23:46.400
So what should you do?

00:23:46.400 --> 00:23:49.040
And how should you
reason about this?

00:23:49.040 --> 00:23:55.370
So Pauker's staff came up
with this decision tree.

00:23:55.370 --> 00:23:59.360
By the way, decision tree in
this literature means something

00:23:59.360 --> 00:24:05.120
different from decision
tree in like C4.5.

00:24:05.120 --> 00:24:08.510
So your choices here
are to amputate the foot

00:24:08.510 --> 00:24:11.430
or start with medical care.

00:24:11.430 --> 00:24:13.520
And if you amputate
the foot, let's

00:24:13.520 --> 00:24:17.730
say there is a 99% chance
that the patient will live.

00:24:17.730 --> 00:24:23.120
There's a 1% chance that maybe
the anesthesia will kill him.

00:24:23.120 --> 00:24:27.290
And if we treat
him medically, they

00:24:27.290 --> 00:24:30.830
estimated that there is a
70% chance of full recovery,

00:24:30.830 --> 00:24:35.180
a 25% chance that he'd
get worse, a 5% chance

00:24:35.180 --> 00:24:37.730
that he would die.

00:24:37.730 --> 00:24:42.020
If he got worse, you're now
faced with another decision,

00:24:42.020 --> 00:24:44.330
which is, do we
amputate the whole leg

00:24:44.330 --> 00:24:47.360
or continue pushing medicine?

00:24:47.360 --> 00:24:50.570
And again, there are various
outcomes with various estimated

00:24:50.570 --> 00:24:53.130
probabilities.

00:24:53.130 --> 00:24:57.470
Now, the critical thing here
that this group was pushing

00:24:57.470 --> 00:24:59.900
was the idea that
these decisions

00:24:59.900 --> 00:25:04.190
shouldn't be based on what the
doctor thinks is good for you.

00:25:04.190 --> 00:25:07.580
They should be based on what
you think is good for you.

00:25:07.580 --> 00:25:09.860
And so they worked
very hard to try

00:25:09.860 --> 00:25:15.870
to elicit individualized
utilities from patients.

00:25:15.870 --> 00:25:22.280
So for example, this guy said
that having your foot amputated

00:25:22.280 --> 00:25:27.470
was worth 850 points on a scale
of 1,000 where being healthy

00:25:27.470 --> 00:25:33.050
was 1 and being dead was 0.

00:25:33.050 --> 00:25:35.180
Now, you could imagine
that that number

00:25:35.180 --> 00:25:38.630
would be very different
for different individuals.

00:25:38.630 --> 00:25:41.240
If you asked LeBron
James how bad would it

00:25:41.240 --> 00:25:43.710
be to have your
foot amputated, he

00:25:43.710 --> 00:25:46.420
might think that it's
much worse than I would,

00:25:46.420 --> 00:25:51.150
because it would be a pain
to have my foot amputated,

00:25:51.150 --> 00:25:53.310
but I could still do
most of the things

00:25:53.310 --> 00:25:57.660
that I do professionally,
whereas he probably couldn't

00:25:57.660 --> 00:26:02.170
as a star basketball player.

00:26:02.170 --> 00:26:04.100
So how do you solve
a problem like this?

00:26:04.100 --> 00:26:07.680
Well, you say, OK,
at every chance node

00:26:07.680 --> 00:26:12.550
I can calculate the expected
value of what happens here.

00:26:12.550 --> 00:26:17.400
So here at it's 0.6
times 995, 0.4 times 0.

00:26:17.400 --> 00:26:20.100
That gets me a value
for this decision.

00:26:20.100 --> 00:26:21.900
Do the same thing here.

00:26:21.900 --> 00:26:26.040
I compare the values here
and choose the best one.

00:26:26.040 --> 00:26:28.630
That gives me a value
for this decision.

00:26:28.630 --> 00:26:32.880
And so I fold back
this decision tree.

00:26:32.880 --> 00:26:36.920
And my next slide should have--

00:26:36.920 --> 00:26:39.840
yeah, so these are the
numbers that you get.

00:26:39.840 --> 00:26:42.870
And what you discover
is that the utility

00:26:42.870 --> 00:26:45.840
of trying medical treatment
is somewhat higher

00:26:45.840 --> 00:26:49.320
than the utility of
immediately amputating the foot

00:26:49.320 --> 00:26:53.430
if you believe these
numbers and those utilities,

00:26:53.430 --> 00:26:57.000
these probabilities
and those utilities.

00:26:57.000 --> 00:27:01.560
Now, the difficulty is that
these numbers are fickle.

00:27:01.560 --> 00:27:05.400
And so you'd like to do some
sort of sensitivity analysis.

00:27:05.400 --> 00:27:08.340
And you say, for example,
what if this gentleman

00:27:08.340 --> 00:27:12.420
valued his living
with an amputated foot

00:27:12.420 --> 00:27:15.390
at 900 rather than 850.

00:27:15.390 --> 00:27:19.020
And now you discover
that amputating the foot

00:27:19.020 --> 00:27:24.450
looks like a slightly better
decision than the other.

00:27:24.450 --> 00:27:30.630
So this is actually applied
in clinical medicine.

00:27:30.630 --> 00:27:33.450
And there are now
thousands of doctors

00:27:33.450 --> 00:27:35.880
who have been trained
in these techniques

00:27:35.880 --> 00:27:39.720
and really try to work through
this with individual patients.

00:27:39.720 --> 00:27:43.530
Of course, it's used much more
on an epidemiological basis

00:27:43.530 --> 00:27:46.123
when people look at
large populations.

00:27:46.123 --> 00:27:47.290
AUDIENCE: I have a question.

00:27:47.290 --> 00:27:48.207
PETER SZOLOVITS: Yeah.

00:27:48.207 --> 00:27:50.560
AUDIENCE: How are the
probabilities assessed?

00:27:55.260 --> 00:27:56.760
PETER SZOLOVITS:
So the service that

00:27:56.760 --> 00:27:59.610
did this study would
read the literature,

00:27:59.610 --> 00:28:01.530
and they would
look in databases.

00:28:01.530 --> 00:28:04.410
And they would try to
estimate those probabilities.

00:28:04.410 --> 00:28:08.520
We can do a lot better today
than they could at that time,

00:28:08.520 --> 00:28:12.030
because we have a lot more
data that you can look in.

00:28:12.030 --> 00:28:15.530
But you could say,
OK, for people--

00:28:15.530 --> 00:28:20.130
men of this age who have
gangrenous feet, what

00:28:20.130 --> 00:28:23.910
fraction of them have
the following experience?

00:28:23.910 --> 00:28:26.910
And that's how
these are estimated.

00:28:26.910 --> 00:28:30.720
Some of it feels like 5%.

00:28:38.220 --> 00:28:38.720
OK.

00:28:38.720 --> 00:28:42.205
So I just said this.

00:28:42.205 --> 00:28:43.580
And then the
question of where do

00:28:43.580 --> 00:28:47.640
you get these utilities
is a tricky one.

00:28:47.640 --> 00:28:51.770
So one way is to do the
standard gamble, which

00:28:51.770 --> 00:29:00.320
says, OK, Mr. Szolovits,
we're going to play this game.

00:29:00.320 --> 00:29:07.490
We're going to roll a fair die
or something that will come up

00:29:07.490 --> 00:29:11.570
with some continuous
number between 0 and 1,

00:29:11.570 --> 00:29:18.710
and then I'm going to play the
game where either I chop off

00:29:18.710 --> 00:29:23.330
your foot, or I roll
this die and if it

00:29:23.330 --> 00:29:29.300
exceeds some threshold,
then I kill you.

00:29:29.300 --> 00:29:32.300
Nice game.

00:29:32.300 --> 00:29:37.790
So now if you find the point
at which I'm indifferent,

00:29:37.790 --> 00:29:45.460
if I say, well, 0.8, that's
a 20% chance of dying.

00:29:45.460 --> 00:29:47.780
It seems like a lot.

00:29:47.780 --> 00:29:51.920
But maybe I'll go
for 0.9, right?

00:29:51.920 --> 00:29:53.720
Now you've said,
OK, well, that means

00:29:53.720 --> 00:29:56.870
that you value
living without a foot

00:29:56.870 --> 00:30:01.710
at 0.9 of the value
of being healthy.

00:30:01.710 --> 00:30:03.660
So this is a way of doing it.

00:30:03.660 --> 00:30:06.070
And this is typically done.

00:30:06.070 --> 00:30:10.445
Unfortunately, of course,
it's difficult to ascertain

00:30:10.445 --> 00:30:11.810
the problem.

00:30:11.810 --> 00:30:13.260
And it's also not stable.

00:30:13.260 --> 00:30:17.210
So people have done experiments
where they get somebody

00:30:17.210 --> 00:30:20.690
to give them this kind of
number as a hypothetical,

00:30:20.690 --> 00:30:23.390
and then when that
person winds up actually

00:30:23.390 --> 00:30:26.570
faced with such a
decision, they no longer

00:30:26.570 --> 00:30:29.060
will abide with that number.

00:30:29.060 --> 00:30:34.012
So they've changed their mind
when the situation is real.

00:30:34.012 --> 00:30:36.470
AUDIENCE: But it's nice, because
there are two feet, right?

00:30:36.470 --> 00:30:39.707
So you could run this
experiment and see.

00:30:39.707 --> 00:30:41.540
PETER SZOLOVITS: They
didn't actually do it.

00:30:41.540 --> 00:30:43.810
It was hypothetical.

00:30:46.640 --> 00:30:48.020
OK.

00:30:48.020 --> 00:30:50.900
Next program I want to
tell you about, again,

00:30:50.900 --> 00:30:54.770
the technique for this was
developed as a PhD thesis

00:30:54.770 --> 00:30:57.290
here at MIT in 1967.

00:30:57.290 --> 00:30:59.970
So this is hot off the presses.

00:30:59.970 --> 00:31:04.190
But it's still used,
this type of idea.

00:31:04.190 --> 00:31:08.780
And so this was a program that
was published in the American

00:31:08.780 --> 00:31:12.710
Journal of Medicine, which is
a high impact medical journal.

00:31:12.710 --> 00:31:14.750
I think this was
actually the first sort

00:31:14.750 --> 00:31:18.680
of computational program
that journal had ever

00:31:18.680 --> 00:31:21.980
published as a medical journal.

00:31:21.980 --> 00:31:24.260
And it was addressed
at the problem

00:31:24.260 --> 00:31:28.650
of the diagnosis of acute
oliguric renal failure.

00:31:28.650 --> 00:31:32.030
Oliguric means you're
not peeing enough.

00:31:32.030 --> 00:31:33.870
Renal is your kidney.

00:31:33.870 --> 00:31:36.650
So this is something's gone
wrong with your kidney,

00:31:36.650 --> 00:31:40.160
and you're not
producing enough urine.

00:31:40.160 --> 00:31:45.890
Now, this is a good problem to
address with these techniques,

00:31:45.890 --> 00:31:49.970
because if something
happens to you suddenly,

00:31:49.970 --> 00:31:54.950
it's very likely that
there is one cause for it.

00:31:54.950 --> 00:31:59.000
If you are 85 years old
and you have a little heart

00:31:59.000 --> 00:32:01.730
disease and a little
kidney disease

00:32:01.730 --> 00:32:05.660
and a little liver disease
and a little lung disease,

00:32:05.660 --> 00:32:08.750
there's no guarantee that there
was one thing that went wrong

00:32:08.750 --> 00:32:11.310
with you that caused all these.

00:32:11.310 --> 00:32:15.290
But if you were OK yesterday
and then you stopped peeing,

00:32:15.290 --> 00:32:18.790
it's pretty likely that there's
one thing that's gone wrong.

00:32:18.790 --> 00:32:22.260
So it's a good
application of this model.

00:32:22.260 --> 00:32:25.520
So what they said is there
are 14 potential causes.

00:32:25.520 --> 00:32:28.830
And these are exhaustive
and mutually exclusive.

00:32:28.830 --> 00:32:33.080
There are 27 tests or
questions or observations

00:32:33.080 --> 00:32:35.570
that are relevant
to the differential.

00:32:35.570 --> 00:32:37.520
These are cheap
tests, so they didn't

00:32:37.520 --> 00:32:41.150
involve doing anything
either expensive or dangerous

00:32:41.150 --> 00:32:41.960
to the patient.

00:32:41.960 --> 00:32:44.210
It was measuring
something in the lab

00:32:44.210 --> 00:32:47.930
or asking questions
of the patient.

00:32:47.930 --> 00:32:50.750
But they didn't want to
have to ask all of them,

00:32:50.750 --> 00:32:53.060
because that's pretty tedious.

00:32:53.060 --> 00:32:55.070
And so they were
trying to minimize

00:32:55.070 --> 00:32:56.600
the amount of
information that they

00:32:56.600 --> 00:32:59.090
needed to gather
in order to come up

00:32:59.090 --> 00:33:01.610
with an appropriate decision.

00:33:01.610 --> 00:33:05.360
Now, the real problem, there
were three invasive tests

00:33:05.360 --> 00:33:07.940
that are dangerous
and expensive,

00:33:07.940 --> 00:33:09.710
and then eight
different treatments

00:33:09.710 --> 00:33:11.360
that could be applied.

00:33:11.360 --> 00:33:14.390
And I'm only going to tell
you about the first part

00:33:14.390 --> 00:33:16.730
of this problem.

00:33:16.730 --> 00:33:21.750
This 1973 article shows you
what the program looked like.

00:33:21.750 --> 00:33:25.430
It was a computer terminal
where it gave you choices,

00:33:25.430 --> 00:33:27.980
and you would type in an answer.

00:33:27.980 --> 00:33:32.330
And so that was the state
of the art at the time.

00:33:32.330 --> 00:33:35.390
But what I'm going to
do is, god willing,

00:33:35.390 --> 00:33:39.080
I'm going to demonstrate
a reconstruction

00:33:39.080 --> 00:33:40.505
that I made of this program.

00:33:43.440 --> 00:33:51.310
So these guys are the potential
causes of stopping to pee--

00:33:51.310 --> 00:33:55.700
acute tubular necrosis,
functional acute renal failure,

00:33:55.700 --> 00:34:00.920
urinary tract obstruction, acute
glomerulonephritis, et cetera.

00:34:00.920 --> 00:34:04.137
And these are the
prior probabilities.

00:34:04.137 --> 00:34:05.720
Now, I have to warn
you, these numbers

00:34:05.720 --> 00:34:09.110
were, in fact, estimated by
people sticking their finger

00:34:09.110 --> 00:34:12.949
in the air and figuring out
which way the wind was blowing,

00:34:12.949 --> 00:34:18.440
because in 1973, there were not
great databases that you could

00:34:18.440 --> 00:34:20.360
turn to.

00:34:20.360 --> 00:34:23.060
And then these are
the questions that

00:34:23.060 --> 00:34:25.730
were available to be asked.

00:34:25.730 --> 00:34:27.620
And what you see in
the first column,

00:34:27.620 --> 00:34:31.790
at least if you're sitting
close to the screen,

00:34:31.790 --> 00:34:37.219
is the expected entropy of
the probability distribution

00:34:37.219 --> 00:34:41.010
if you answered this question.

00:34:41.010 --> 00:34:45.170
So this is basically saying, if
I ask this question, how likely

00:34:45.170 --> 00:34:49.670
is each of the possible answers,
given my disease distribution

00:34:49.670 --> 00:34:51.290
probabilities?

00:34:51.290 --> 00:34:53.340
And then for each
of those answers,

00:34:53.340 --> 00:34:57.530
I do a Bayesian revision,
then I weight the entropy

00:34:57.530 --> 00:35:03.170
of that resulting distribution
by the probability of getting

00:35:03.170 --> 00:35:04.310
that answer.

00:35:04.310 --> 00:35:06.350
And that gets me
the expected entropy

00:35:06.350 --> 00:35:08.300
for asking that question.

00:35:08.300 --> 00:35:11.150
And the idea is that the
lower the expected entropy,

00:35:11.150 --> 00:35:14.040
the more valuable the question.

00:35:14.040 --> 00:35:15.420
Makes sense.

00:35:15.420 --> 00:35:19.640
So if we look, for example,
the most valuable question

00:35:19.640 --> 00:35:25.190
is, what was the blood pressure
at the onset of oliguria?

00:35:25.190 --> 00:35:35.630
And I can click on this
and say it was, let's say,

00:35:35.630 --> 00:35:37.900
moderately elevated.

00:35:37.900 --> 00:35:39.920
And what this little
colorful graph

00:35:39.920 --> 00:35:45.110
is showing you is that if you
look at the initial probability

00:35:45.110 --> 00:35:51.050
distribution, acute tubular
necrosis was about 25%,

00:35:51.050 --> 00:35:54.260
and has gone down to
a very small amount,

00:35:54.260 --> 00:35:56.630
whereas some of these
others have grown

00:35:56.630 --> 00:35:58.220
in importance considerably.

00:36:02.950 --> 00:36:07.150
So we can answer more
questions, we can say--

00:36:07.150 --> 00:36:08.620
let's see.

00:36:08.620 --> 00:36:10.840
What is the degree--

00:36:10.840 --> 00:36:12.160
is there proteinuria?

00:36:12.160 --> 00:36:14.630
Is there protein in the urine?

00:36:14.630 --> 00:36:18.206
And we say, no, there isn't.

00:36:18.206 --> 00:36:20.320
I think we say, no, there isn't.

00:36:20.320 --> 00:36:22.600
0.

00:36:22.600 --> 00:36:25.770
And that revises the
probability distribution.

00:36:25.770 --> 00:36:30.460
And then it says the next most
important thing is kidney size.

00:36:30.460 --> 00:36:34.040
And we say-- let's say
the kidney size is normal.

00:36:34.040 --> 00:36:38.060
So now all of a sudden
functional acute renal failure,

00:36:38.060 --> 00:36:40.390
which, by the way, is one
of these funny medical care

00:36:40.390 --> 00:36:43.150
categories that says
it doesn't work well,

00:36:43.150 --> 00:36:46.210
doesn't explain to why
it doesn't work well,

00:36:46.210 --> 00:36:49.280
but it's sort of
a generic thing.

00:36:49.280 --> 00:36:51.080
And sure enough.

00:36:51.080 --> 00:36:54.460
We can keep answering
questions about,

00:36:54.460 --> 00:36:58.360
are you producing
less than 50 ccs

00:36:58.360 --> 00:37:02.200
of urine, which is a tiny
amount, or somewhere between 50

00:37:02.200 --> 00:37:04.090
and 400?

00:37:04.090 --> 00:37:07.060
Remember, this is for people
who are not producing enough.

00:37:07.060 --> 00:37:10.120
So normally you'd be over 400.

00:37:10.120 --> 00:37:12.190
So these are the only choices.

00:37:12.190 --> 00:37:13.510
So let's say it's moderate.

00:37:13.510 --> 00:37:17.500
And so you see the probability
distribution keeps changing.

00:37:17.500 --> 00:37:20.470
And what happened in
the original program

00:37:20.470 --> 00:37:22.720
is they had an
arbitrary threshold that

00:37:22.720 --> 00:37:28.240
said when the probability of one
of these causes of the disease

00:37:28.240 --> 00:37:33.940
reaches 95%, then we
switch to a different mode,

00:37:33.940 --> 00:37:36.910
where now we're actually
willing to contemplate

00:37:36.910 --> 00:37:43.240
doing the expensive tests and
doing the expensive treatments.

00:37:43.240 --> 00:37:45.490
And we build the
decision tree, as we

00:37:45.490 --> 00:37:48.370
saw in the case of the
gangrenous foot, that

00:37:48.370 --> 00:37:52.280
figures out which of those
is the optimal approach.

00:37:52.280 --> 00:37:55.600
So the idea here was
because building a decision

00:37:55.600 --> 00:38:02.590
tree with 27 potential questions
becomes enormously bushy,

00:38:02.590 --> 00:38:04.840
we're using a
heuristic that says

00:38:04.840 --> 00:38:09.460
information maximization
or entropy reduction

00:38:09.460 --> 00:38:13.240
is a reasonable way of
focusing in on what's

00:38:13.240 --> 00:38:14.990
wrong with this patient.

00:38:14.990 --> 00:38:17.470
And then once we focused
in pretty well, then

00:38:17.470 --> 00:38:20.620
we can begin to do
more detailed analysis

00:38:20.620 --> 00:38:25.090
on the remaining more
consequential and more costly

00:38:25.090 --> 00:38:27.940
tests that are available.

00:38:27.940 --> 00:38:31.510
Now, this program didn't
work terribly well,

00:38:31.510 --> 00:38:36.850
because the numbers
were badly estimated,

00:38:36.850 --> 00:38:44.590
and also because of the
utility model that they

00:38:44.590 --> 00:38:50.820
had for the decision analytic
part was particularly terrible.

00:38:50.820 --> 00:38:54.640
It didn't really reflect
anything in the real world.

00:38:54.640 --> 00:38:58.420
They had an incremental utility
model that said the patient

00:38:58.420 --> 00:39:01.900
either got better, or stayed
the same, or got worse.

00:39:01.900 --> 00:39:06.070
And obviously in that
order of utilities,

00:39:06.070 --> 00:39:09.460
but they didn't correspond
to how much better he got

00:39:09.460 --> 00:39:10.960
or how much worse he got.

00:39:10.960 --> 00:39:14.020
And so it wasn't
terribly useful.

00:39:14.020 --> 00:39:19.450
So nevertheless, in the 1990s,
I was teaching a tutorial

00:39:19.450 --> 00:39:21.720
at a Medical
Informatics conference,

00:39:21.720 --> 00:39:23.910
and there were a bunch of
doctors in the audience.

00:39:23.910 --> 00:39:25.840
And I showed them this program.

00:39:25.840 --> 00:39:28.870
And one of the doctors came
up afterwards and said, wow,

00:39:28.870 --> 00:39:30.430
it thinks just the way I do.

00:39:33.220 --> 00:39:34.780
And I said, really?

00:39:37.990 --> 00:39:39.760
I don't think so.

00:39:39.760 --> 00:39:43.630
But clearly, it
was doing something

00:39:43.630 --> 00:39:45.760
that corresponded
to the way that he

00:39:45.760 --> 00:39:48.550
thought about these cases.

00:39:48.550 --> 00:39:51.730
So I thought that
was a good thing.

00:39:51.730 --> 00:39:52.360
All right.

00:39:52.360 --> 00:39:55.300
Well, what happens
if we can't assume

00:39:55.300 --> 00:39:58.210
that there's just a single
disease underlying the person's

00:39:58.210 --> 00:40:00.220
problems?

00:40:00.220 --> 00:40:03.730
If there are
multiple diseases, we

00:40:03.730 --> 00:40:06.760
can build this kind
of bipartite model

00:40:06.760 --> 00:40:08.810
that says we have
a list of diseases

00:40:08.810 --> 00:40:10.630
and we have a list
of manifestations.

00:40:10.630 --> 00:40:13.570
And some subset of
the diseases can

00:40:13.570 --> 00:40:16.240
cause some subset
of the symptoms,

00:40:16.240 --> 00:40:18.790
of the manifestations.

00:40:18.790 --> 00:40:21.250
And so the manifestations
depend only

00:40:21.250 --> 00:40:24.550
on the diseases that are
present, not on each other.

00:40:24.550 --> 00:40:27.440
And therefore, we have
conditional independence.

00:40:27.440 --> 00:40:30.460
And this is a type of
Bayesian network, which

00:40:30.460 --> 00:40:36.190
can't be solved exactly
because of the computational

00:40:36.190 --> 00:40:37.390
complexity.

00:40:37.390 --> 00:40:39.640
So a program I'll
show you in a minute

00:40:39.640 --> 00:40:46.360
had 400 or 500 diseases and
thousands of manifestations.

00:40:46.360 --> 00:40:50.080
And the computational complexity
of exact solution techniques

00:40:50.080 --> 00:40:53.260
for these networks tends
to go exponentially

00:40:53.260 --> 00:40:56.590
with the number of undirected
cycles in the network.

00:40:56.590 --> 00:40:59.530
And of course, there are
plenty of undirected cycles

00:40:59.530 --> 00:41:02.960
in a network like that.

00:41:02.960 --> 00:41:07.390
So there was a program
developed originally

00:41:07.390 --> 00:41:12.490
in the early 1970s
called Dialog.

00:41:12.490 --> 00:41:15.590
And then they got sued, because
somebody owned that name.

00:41:15.590 --> 00:41:17.170
And then they
called it Internist,

00:41:17.170 --> 00:41:21.530
and they got sued because
somebody owned that name.

00:41:21.530 --> 00:41:23.800
And then they
called it QMR, which

00:41:23.800 --> 00:41:25.750
stands for Quick
Medical Reference,

00:41:25.750 --> 00:41:28.150
and nobody owned that name.

00:41:28.150 --> 00:41:33.430
So around 1982, this program
had about 500 diseases,

00:41:33.430 --> 00:41:37.840
which they estimated
represented about 70% to 75%

00:41:37.840 --> 00:41:41.020
of major diagnoses in
internal medicine, about

00:41:41.020 --> 00:41:43.060
3,500 manifestations.

00:41:43.060 --> 00:41:47.950
And it took about 15 man
years of manual effort

00:41:47.950 --> 00:41:51.850
to sit there and read medical
textbooks and journal articles

00:41:51.850 --> 00:41:56.440
and look at records of
patients in their hospital.

00:41:56.440 --> 00:42:00.370
The effort was led by
a computer scientist

00:42:00.370 --> 00:42:04.270
at the University of Pittsburgh
and the chief of medicine

00:42:04.270 --> 00:42:09.010
at UPMC, the University of
Pittsburgh Medical Center, who

00:42:09.010 --> 00:42:10.480
was just a fanatic.

00:42:10.480 --> 00:42:14.260
And he got all the
medical school trainees

00:42:14.260 --> 00:42:19.480
to spend hours and hours
coming up with these databases.

00:42:19.480 --> 00:42:23.350
By 1997, they had
commercialized it

00:42:23.350 --> 00:42:27.170
through a company that had
bought the rights to it.

00:42:27.170 --> 00:42:29.620
And they had-- that
company had expanded it

00:42:29.620 --> 00:42:35.870
to about 750 diagnoses and
about 5,500 manifestations.

00:42:35.870 --> 00:42:38.740
So they made it
considerably larger.

00:42:38.740 --> 00:42:43.820
Details are-- I've tried to put
references on all the slides.

00:42:43.820 --> 00:42:47.590
So here's what data
in QMR looks like.

00:42:47.590 --> 00:42:50.440
For each diagnosis,
there is a list

00:42:50.440 --> 00:42:54.040
of associated
manifestations with evoking

00:42:54.040 --> 00:42:56.030
strengths and frequencies.

00:42:56.030 --> 00:42:58.900
So I'll explain
that in a minute.

00:42:58.900 --> 00:43:04.270
On average, there are about
75 manifestations per disease.

00:43:04.270 --> 00:43:05.950
And for each disease--

00:43:05.950 --> 00:43:09.130
for each manifestation
in addition to the data

00:43:09.130 --> 00:43:12.610
you see here, there is
also an important measure

00:43:12.610 --> 00:43:17.220
that says how critical is it to
explain this particular symptom

00:43:17.220 --> 00:43:21.280
or sign or lab value
in the final diagnosis.

00:43:21.280 --> 00:43:24.190
So for example, if
you have a headache,

00:43:24.190 --> 00:43:25.990
that could be
incidental and it's not

00:43:25.990 --> 00:43:28.330
that important to explain it.

00:43:28.330 --> 00:43:33.760
If you're bleeding from your
gastrointestinal system,

00:43:33.760 --> 00:43:36.280
that's really
important to explain.

00:43:36.280 --> 00:43:38.710
And you wouldn't
expect a diagnosis

00:43:38.710 --> 00:43:41.440
of that patient that
doesn't explain to you why

00:43:41.440 --> 00:43:43.180
they have that symptom.

00:43:43.180 --> 00:43:49.030
And then here is an example
of alcoholic hepatitis.

00:43:49.030 --> 00:43:53.800
And the two numbers here are
a so-called evoking strength

00:43:53.800 --> 00:43:55.210
and a frequency.

00:43:55.210 --> 00:43:57.010
These are both on scales--

00:43:57.010 --> 00:44:01.090
well, evoking strength
is on a scale of 0 to 5,

00:44:01.090 --> 00:44:04.552
and frequency is on
a scale of 1 to 5.

00:44:04.552 --> 00:44:07.840
And I'll show you what
those are supposed to mean.

00:44:07.840 --> 00:44:10.330
And so, for example,
what this says

00:44:10.330 --> 00:44:14.680
is that if you're
anorexic, that should not

00:44:14.680 --> 00:44:20.480
make you think about alcoholic
hepatitis as a disease.

00:44:20.480 --> 00:44:23.530
But you should expect
that if somebody

00:44:23.530 --> 00:44:28.880
has alcoholic hepatitis, they're
very likely to have anorexia.

00:44:28.880 --> 00:44:30.790
So that's the frequency number.

00:44:30.790 --> 00:44:32.980
This is the evoking
strength number.

00:44:32.980 --> 00:44:35.890
And you see that there
is a variety of those.

00:44:35.890 --> 00:44:40.360
So much of that many,
many years of effort

00:44:40.360 --> 00:44:42.550
went into coming
up with these lists

00:44:42.550 --> 00:44:45.130
and coming up with
those numbers.

00:44:45.130 --> 00:44:46.540
Here are the scales.

00:44:46.540 --> 00:44:49.440
So the evoking strength--

00:44:49.440 --> 00:44:51.460
0 means nonspecific.

00:44:51.460 --> 00:44:53.380
5 means its pathognomonic.

00:44:53.380 --> 00:44:55.810
In other words, just
seeing the symptom

00:44:55.810 --> 00:44:58.750
is enough to convince
you that the patient must

00:44:58.750 --> 00:45:00.890
have this disease.

00:45:00.890 --> 00:45:05.680
Similarly, frequency 1
means it occurs rarely,

00:45:05.680 --> 00:45:09.490
and 5 means that it occurs
in essentially all cases

00:45:09.490 --> 00:45:12.460
with scaled values in between.

00:45:12.460 --> 00:45:15.400
And these are kind
of like odds ratios.

00:45:15.400 --> 00:45:19.750
And they add them kind of as
if they were log likelihood

00:45:19.750 --> 00:45:20.470
ratios.

00:45:20.470 --> 00:45:23.290
And so there's been
a big literature

00:45:23.290 --> 00:45:27.310
on trying to figure out exactly
what these numbers mean,

00:45:27.310 --> 00:45:30.640
because there's no formal
definition in terms of you

00:45:30.640 --> 00:45:34.060
count the number of this and
divide by the number of that,

00:45:34.060 --> 00:45:36.530
and that gives you
the right answer.

00:45:36.530 --> 00:45:39.730
These were sort of the
impressionistic kinds

00:45:39.730 --> 00:45:40.345
of numbers.

00:45:42.860 --> 00:45:50.560
So the logic in the system
was that you would come to it

00:45:50.560 --> 00:45:54.820
and give it a list of the
manifestations of a case.

00:45:54.820 --> 00:45:59.090
And to their credit, they went
after very complicated cases.

00:45:59.090 --> 00:46:02.750
So they took clinical
pathologic conference cases

00:46:02.750 --> 00:46:05.030
from The New England
Journal of Medicine.

00:46:05.030 --> 00:46:09.140
These are cases selected to be
difficult enough that doctors

00:46:09.140 --> 00:46:11.000
are willing to read these.

00:46:11.000 --> 00:46:15.200
And they're typically presented
at Grand Rounds at MGH

00:46:15.200 --> 00:46:18.470
by somebody who is often
stumped by the case.

00:46:18.470 --> 00:46:22.430
So it's an opportunity to watch
people reason interactively

00:46:22.430 --> 00:46:25.050
about these things.

00:46:25.050 --> 00:46:28.820
And so you evoke
the diagnoses that

00:46:28.820 --> 00:46:32.870
have a high evoking strength
from the giving manifestations.

00:46:32.870 --> 00:46:35.390
And then you do a
scoring calculation

00:46:35.390 --> 00:46:37.340
based on those numbers.

00:46:37.340 --> 00:46:39.920
The details of this
are probably all wrong,

00:46:39.920 --> 00:46:42.950
but that's the way
they went about it.

00:46:42.950 --> 00:46:45.770
And then you form a
differential around the highest

00:46:45.770 --> 00:46:47.660
scoring diagnosis.

00:46:47.660 --> 00:46:50.460
Now, this is actually
an interesting idea.

00:46:50.460 --> 00:46:54.600
It's a heuristic idea, but it's
one that worked pretty well.

00:46:54.600 --> 00:46:57.210
So suppose I have two diseases.

00:46:57.210 --> 00:47:01.250
D1 can cause
manifestations 1 through 4.

00:47:01.250 --> 00:47:03.365
And D2 can cause 3 through 6.

00:47:05.990 --> 00:47:12.110
So are these competing to
explain the same case or could

00:47:12.110 --> 00:47:14.870
they be complementary?

00:47:14.870 --> 00:47:17.270
Well, until we know what
symptoms the patient

00:47:17.270 --> 00:47:19.790
actually has, we don't know.

00:47:19.790 --> 00:47:21.060
But let's trace through this.

00:47:21.060 --> 00:47:25.010
So suppose I tell you that
the patient has manifestations

00:47:25.010 --> 00:47:27.280
3 and 4.

00:47:27.280 --> 00:47:28.460
OK.

00:47:28.460 --> 00:47:31.820
Well, you would say,
there is no reason

00:47:31.820 --> 00:47:35.600
to think that the patient
may have both diseases,

00:47:35.600 --> 00:47:39.690
because either of them can
explain those manifestations,

00:47:39.690 --> 00:47:40.310
right?

00:47:40.310 --> 00:47:42.680
So you would consider
them to be competitors.

00:47:45.860 --> 00:47:49.100
What about if I add M1?

00:47:49.100 --> 00:47:52.310
So here, it's getting
a little dicier.

00:47:52.310 --> 00:47:56.430
Now you're more likely
to think that it's D1.

00:47:56.430 --> 00:48:00.440
But if it's D1, that could
explain all the manifestations,

00:48:00.440 --> 00:48:05.210
and D2 is still viewable
as a competitor.

00:48:05.210 --> 00:48:08.900
On the other hand,
if I also add M6,

00:48:08.900 --> 00:48:12.890
now neither disease can
explain all the manifestations.

00:48:12.890 --> 00:48:17.570
And so it's more likely,
somewhat more likely,

00:48:17.570 --> 00:48:21.050
that there may be
two diseases present.

00:48:21.050 --> 00:48:25.320
So what Internist had was
this interesting heuristic,

00:48:25.320 --> 00:48:31.490
which said that when you get
that complementary situation,

00:48:31.490 --> 00:48:36.030
you form a differential around
the top ranked hypothesis.

00:48:36.030 --> 00:48:40.430
In other words, you retain
all those diseases that

00:48:40.430 --> 00:48:43.830
compete with that hypothesis.

00:48:43.830 --> 00:48:46.460
And that defines
a subproblem that

00:48:46.460 --> 00:48:49.700
looks like the acute
renal failure problem,

00:48:49.700 --> 00:48:53.030
because now you have
one set of factors

00:48:53.030 --> 00:48:56.900
that you're trying to
explain by one disease.

00:48:56.900 --> 00:49:00.680
And you set aside all of
the other manifestations

00:49:00.680 --> 00:49:02.390
and all of the
other diseases that

00:49:02.390 --> 00:49:04.670
are potentially complementary.

00:49:04.670 --> 00:49:06.890
And you don't worry about
them for the moment.

00:49:06.890 --> 00:49:10.670
Just focus on this
cluster of things

00:49:10.670 --> 00:49:13.940
that are competitors
to explain some subset

00:49:13.940 --> 00:49:16.610
of the manifestations.

00:49:16.610 --> 00:49:21.560
And then there are different
questioning strategies.

00:49:21.560 --> 00:49:25.160
So depending on the scores
within these things,

00:49:25.160 --> 00:49:28.070
if one of those diseases
has a very high score

00:49:28.070 --> 00:49:30.890
and the others have
relatively low scores,

00:49:30.890 --> 00:49:34.890
you would choose a pursue
strategy that says,

00:49:34.890 --> 00:49:37.730
OK, I'm interested
in asking questions

00:49:37.730 --> 00:49:40.280
that will more
likely convince me

00:49:40.280 --> 00:49:44.850
of the correctness of
that leading hypothesis.

00:49:44.850 --> 00:49:48.500
So you look for the things
that it predicts strongly.

00:49:48.500 --> 00:49:53.210
If you have a very large
list in the differential,

00:49:53.210 --> 00:49:55.550
you might say, I'm
going to try to reduce

00:49:55.550 --> 00:49:59.360
the size of the differential
by looking for things that

00:49:59.360 --> 00:50:03.890
are likely in some of the
less likely hypotheses

00:50:03.890 --> 00:50:07.910
so that I can rule them out
if that thing is not present.

00:50:07.910 --> 00:50:09.470
So different strategies.

00:50:09.470 --> 00:50:13.030
And I'll come back to
that in a few minutes.

00:50:13.030 --> 00:50:18.710
So their test, of course,
based on their own evaluation

00:50:18.710 --> 00:50:19.550
was terrific.

00:50:19.550 --> 00:50:21.590
It did wonderfully well.

00:50:21.590 --> 00:50:23.870
The paper got published
in The New England Journal

00:50:23.870 --> 00:50:27.080
of Medicine, which was an
unbelievable breakthrough

00:50:27.080 --> 00:50:30.350
to have an AI program that
the editors of The New England

00:50:30.350 --> 00:50:34.490
Journal considered interesting.

00:50:34.490 --> 00:50:39.080
Now, unfortunately, it
didn't hold up very well.

00:50:39.080 --> 00:50:42.380
And so there was this
paper by Eta Berner

00:50:42.380 --> 00:50:46.530
and her colleagues in
1994 where they evaluated

00:50:46.530 --> 00:50:49.260
QMR and three other programs.

00:50:49.260 --> 00:50:53.020
DXplain is very similar
in structure to QMR.

00:50:53.020 --> 00:50:57.070
Iliad and Meditel
are Bayesian network,

00:50:57.070 --> 00:51:01.670
or almost naive
Bayesian types of models

00:51:01.670 --> 00:51:03.740
developed by other groups.

00:51:03.740 --> 00:51:07.890
And they looked for
results, which is coverage.

00:51:07.890 --> 00:51:14.300
So what fraction of the real
diagnoses in these 105 cases

00:51:14.300 --> 00:51:18.800
that they chose to test on could
any of these programs actually

00:51:18.800 --> 00:51:19.860
diagnose?

00:51:19.860 --> 00:51:23.150
So if the program didn't
know about a certain disease,

00:51:23.150 --> 00:51:26.750
then obviously it wasn't
going to get it right.

00:51:26.750 --> 00:51:30.170
And then they said, OK, of
the program's diagnoses,

00:51:30.170 --> 00:51:33.560
what fraction were considered
correct by the experts?

00:51:33.560 --> 00:51:36.680
What was the rank order
of that correct diagnosis

00:51:36.680 --> 00:51:39.620
among the list of diagnoses
that the program gave?

00:51:42.500 --> 00:51:47.000
The experts were asked to list
all the plausible diagnoses

00:51:47.000 --> 00:51:49.010
from these cases.

00:51:49.010 --> 00:51:53.570
What fraction of those showed
up in the program's top 20?

00:51:53.570 --> 00:51:58.520
And then did the program have
any value added by coming up

00:51:58.520 --> 00:52:01.610
with things that the experts
had not thought about,

00:52:01.610 --> 00:52:03.410
but that they
agreed when they saw

00:52:03.410 --> 00:52:08.180
them were reasonable
explanations for this case?

00:52:08.180 --> 00:52:10.130
So here are the results.

00:52:10.130 --> 00:52:18.890
And what you see is that the
diagnoses in these 105 test

00:52:18.890 --> 00:52:25.460
cases, 91% of them appeared
in the DXplain program,

00:52:25.460 --> 00:52:30.310
but, for example, only 73%
of them in the QMR program.

00:52:30.310 --> 00:52:32.110
So that means that
right off the bat

00:52:32.110 --> 00:52:36.620
it's missing about a quarter
of the possible cases.

00:52:36.620 --> 00:52:39.160
And then if you look
at correct diagnosis,

00:52:39.160 --> 00:52:45.370
you're seeing numbers like
0.69, 0.61, 0.71, et cetera.

00:52:45.370 --> 00:52:52.120
So these are-- it's like the
dog who sings, but badly, right?

00:52:52.120 --> 00:52:54.670
It's remarkable that
it can sing at all,

00:52:54.670 --> 00:52:57.250
but it's not something
you want to listen to.

00:53:00.220 --> 00:53:06.550
And then rank of the correct
diagnosis in the program

00:53:06.550 --> 00:53:10.400
is at like 12 or
10 or 13 or so on.

00:53:10.400 --> 00:53:15.740
So it is in the top 20, but it's
not at the top of the top 20.

00:53:15.740 --> 00:53:19.870
So the results were
a bit disappointing.

00:53:19.870 --> 00:53:24.310
And depending on where
you put the cut off,

00:53:24.310 --> 00:53:29.920
you get the proportion of cases
where a correct diagnosis is

00:53:29.920 --> 00:53:31.810
within the top end.

00:53:31.810 --> 00:53:37.060
And you see that at 20,
you're up at a little over 0.5

00:53:37.060 --> 00:53:39.280
for most of these programs.

00:53:39.280 --> 00:53:43.870
And it gets better if you extend
the list to longer and longer.

00:53:43.870 --> 00:53:47.320
Of course, if you
extended the list to 100,

00:53:47.320 --> 00:53:51.190
then you reach 100%, but it
wouldn't be practically very

00:53:51.190 --> 00:53:52.013
useful.

00:53:52.013 --> 00:53:54.089
AUDIENCE: Why didn't
they somehow compare it

00:53:54.089 --> 00:53:55.586
to the human decision?

00:53:58.275 --> 00:53:59.900
PETER SZOLOVITS:
Well, so first of all,

00:53:59.900 --> 00:54:03.260
they assumed that their
experts were perfect.

00:54:03.260 --> 00:54:06.150
So they were the gold standard.

00:54:06.150 --> 00:54:09.965
So they were comparing
it to a human in a way.

00:54:09.965 --> 00:54:10.590
AUDIENCE: Yeah.

00:54:14.597 --> 00:54:15.430
PETER SZOLOVITS: OK.

00:54:15.430 --> 00:54:20.350
So the bottom line is that
although the sensitivity

00:54:20.350 --> 00:54:23.190
and specificity
were not impressive,

00:54:23.190 --> 00:54:25.780
the programs were
potentially useful,

00:54:25.780 --> 00:54:29.650
because they had interactive
displays of signs and symptoms

00:54:29.650 --> 00:54:31.830
associated with diseases.

00:54:31.830 --> 00:54:33.790
They could give you
the relative likelihood

00:54:33.790 --> 00:54:36.200
of various diagnoses.

00:54:36.200 --> 00:54:39.700
And they concluded
that they needed

00:54:39.700 --> 00:54:43.510
to study the effects of
whether a program like this

00:54:43.510 --> 00:54:47.680
actually helped a doctor
perform medicine better.

00:54:47.680 --> 00:54:50.750
So just here's an example.

00:54:50.750 --> 00:54:52.970
I did a reconstruction
of this program.

00:54:52.970 --> 00:54:55.810
This is the kind of
exploration you could say.

00:54:55.810 --> 00:55:00.430
So if you click on
angina pectoris,

00:55:00.430 --> 00:55:02.950
here are the findings that
are associated with it.

00:55:02.950 --> 00:55:05.590
So you can browse
through its database.

00:55:05.590 --> 00:55:10.360
You can type in an example
case, or select an example case.

00:55:10.360 --> 00:55:14.050
So this is one of those
clinical pathological conference

00:55:14.050 --> 00:55:17.920
cases, and then
the manifestations

00:55:17.920 --> 00:55:20.620
that are present and
absent, and then you

00:55:20.620 --> 00:55:23.600
can get an
interpretation that says,

00:55:23.600 --> 00:55:26.920
OK, this is our differential.

00:55:26.920 --> 00:55:30.760
And these are the
complementary hypotheses.

00:55:30.760 --> 00:55:35.350
And therefore these
are the manifestations

00:55:35.350 --> 00:55:40.990
that we set aside, whereas
these are the ones explained

00:55:40.990 --> 00:55:42.580
by that set of diseases.

00:55:42.580 --> 00:55:49.210
And so you could watch how the
program does its reasoning.

00:55:49.210 --> 00:55:53.050
Well, then a group at
Stanford came along

00:55:53.050 --> 00:55:56.140
when belief networks
or Bayesian networks

00:55:56.140 --> 00:55:59.110
were created, and
said, hey, why don't we

00:55:59.110 --> 00:56:04.960
treat this database as if
it were a Bayesian network

00:56:04.960 --> 00:56:08.650
and see if we can
evaluate things that way?

00:56:08.650 --> 00:56:11.630
So they had to fill
in a lot of details.

00:56:11.630 --> 00:56:17.200
They wound up using
the QMR database

00:56:17.200 --> 00:56:19.560
with a binary interpretation.

00:56:19.560 --> 00:56:21.980
So a disease was
present or absent.

00:56:21.980 --> 00:56:25.510
The manifestation was
present or absent.

00:56:25.510 --> 00:56:28.900
They used causal independence,
or a leaky noisy-OR,

00:56:28.900 --> 00:56:32.140
which I think you've
seen in other contexts.

00:56:32.140 --> 00:56:37.510
So this just says if there are
multiple independent causes

00:56:37.510 --> 00:56:41.350
of something, how likely is it
to happen depending on which

00:56:41.350 --> 00:56:43.970
of those is present or not.

00:56:43.970 --> 00:56:49.060
And there is a simplified way
of doing that calculation, which

00:56:49.060 --> 00:56:52.300
corresponds to sort
of causal independence

00:56:52.300 --> 00:56:56.980
and is computationally
reasonably fast to do.

00:56:56.980 --> 00:57:03.250
And then they also estimated
priors on the various diagnoses

00:57:03.250 --> 00:57:06.970
from national health statistics,
because the original data

00:57:06.970 --> 00:57:08.540
did not have prior data--

00:57:08.540 --> 00:57:10.330
priors.

00:57:10.330 --> 00:57:14.110
They wound up not using
the evoking strengths,

00:57:14.110 --> 00:57:17.560
because they were doing a
pretty straight Bayesian

00:57:17.560 --> 00:57:21.730
model where all you need is the
priors and the conditionals.

00:57:21.730 --> 00:57:26.620
They took the frequency as a
kind of scaled conditional,

00:57:26.620 --> 00:57:29.230
and then built a
system based on that.

00:57:29.230 --> 00:57:31.520
And I'll just show
you the results.

00:57:31.520 --> 00:57:35.830
So they took a bunch of
Scientific American medicine

00:57:35.830 --> 00:57:42.880
cases and said, what are the
ranks assigned to the reference

00:57:42.880 --> 00:57:46.150
diagnoses of these 23 cases?

00:57:46.150 --> 00:57:49.030
And you see that like
in case number one,

00:57:49.030 --> 00:57:53.920
QMR ranked the correct
solution as number six,

00:57:53.920 --> 00:57:59.320
but their two methods,
TB and iterative TB

00:57:59.320 --> 00:58:01.180
ranked it as number one.

00:58:01.180 --> 00:58:07.030
And then these are attempts to
do a kind of ablation analysis

00:58:07.030 --> 00:58:10.750
to see how well the program
works if you take away

00:58:10.750 --> 00:58:13.670
various of its clever features.

00:58:13.670 --> 00:58:17.170
But what you see is that
it works reasonably well,

00:58:17.170 --> 00:58:19.210
except for a few cases.

00:58:19.210 --> 00:58:26.390
So case number 23, all variants
of the program did badly.

00:58:26.390 --> 00:58:28.510
And then they excused
themselves and said,

00:58:28.510 --> 00:58:31.210
well, there's actually
a generalization

00:58:31.210 --> 00:58:34.750
of the disease that was in the
Scientific American medicine

00:58:34.750 --> 00:58:38.580
conclusion, which the
programs did find,

00:58:38.580 --> 00:58:41.980
and so that would have been
number one across the board.

00:58:41.980 --> 00:58:45.070
So they can sort of make a
kind of handwavy argument

00:58:45.070 --> 00:58:47.840
that it really got
that one right.

00:58:47.840 --> 00:58:50.290
And so these were pretty good.

00:58:50.290 --> 00:58:58.030
And so this validated the
idea of using this model

00:58:58.030 --> 00:58:59.740
in that way.

00:58:59.740 --> 00:59:05.960
Now, today you can go out and
go to your favorite Google App

00:59:05.960 --> 00:59:11.080
store or Apple's app store
or anybody's app store

00:59:11.080 --> 00:59:16.640
and download tons and tons
and tons of symptom checkers.

00:59:16.640 --> 00:59:23.620
So I wanted to give you a demo
of one of these if it works.

00:59:23.620 --> 00:59:24.680
OK.

00:59:24.680 --> 00:59:28.220
So I was playing earlier
with having abdominal pain

00:59:28.220 --> 00:59:29.690
and headache.

00:59:29.690 --> 00:59:33.980
So let's start a new one.

00:59:33.980 --> 00:59:37.070
So type in how
you're feeling today.

00:59:37.070 --> 00:59:40.910
Should we have a cough, or runny
nose, abdominal pain, fever,

00:59:40.910 --> 00:59:44.085
sore throat, headache, back
pain, fatigue, diarrhea,

00:59:44.085 --> 00:59:44.585
or phlegm?

00:59:47.120 --> 00:59:47.630
Phlegm?

00:59:47.630 --> 00:59:48.680
Phlegm is the winner.

00:59:51.530 --> 00:59:54.703
Phlegm is like coughing
up crap in your throat.

00:59:54.703 --> 00:59:58.960
AUDIENCE: Oh, luckily,
they visualize it.

00:59:58.960 --> 01:00:00.380
PETER SZOLOVITS: Right.

01:00:00.380 --> 01:00:02.590
So tell me about your phlegm.

01:00:02.590 --> 01:00:04.658
When did it start?

01:00:04.658 --> 01:00:05.875
AUDIENCE: Last week.

01:00:05.875 --> 01:00:07.000
PETER SZOLOVITS: Last week?

01:00:07.000 --> 01:00:07.500
OK.

01:00:14.870 --> 01:00:17.810
I signed in as Paul,
because I didn't

01:00:17.810 --> 01:00:21.830
want to be associated
with any of this data.

01:00:21.830 --> 01:00:25.790
So was the phlegm bloody
or pus-like or watery

01:00:25.790 --> 01:00:26.720
or none of the above?

01:00:29.558 --> 01:00:30.980
AUDIENCE: None of the above.

01:00:30.980 --> 01:00:32.438
PETER SZOLOVITS:
None of the above.

01:00:32.438 --> 01:00:33.672
So what was it like?

01:00:33.672 --> 01:00:34.630
AUDIENCE: I don't know.

01:00:34.630 --> 01:00:36.920
Paul?

01:00:36.920 --> 01:00:38.756
PETER SZOLOVITS: Is it
any of these colors?

01:00:38.756 --> 01:00:40.890
AUDIENCE: Green.

01:00:40.890 --> 01:00:44.230
PETER SZOLOVITS: I think
I'll make it yellow.

01:00:44.230 --> 01:00:44.730
Next.

01:00:47.520 --> 01:00:50.340
Does it happen in the morning,
midday, evening, nighttime,

01:00:50.340 --> 01:00:51.930
or a specific time of year?

01:00:51.930 --> 01:00:53.680
AUDIENCE: Specific time of year.

01:00:53.680 --> 01:00:54.305
AUDIENCE: Yeah.

01:00:54.305 --> 01:00:54.780
Specific time of year.

01:00:54.780 --> 01:00:56.405
PETER SZOLOVITS:
Specific time of year.

01:01:00.840 --> 01:01:03.996
And does lying down or physical
activity make it worse?

01:01:03.996 --> 01:01:05.704
AUDIENCE: Well, it's
generally not worse.

01:01:05.704 --> 01:01:08.030
So that's physical activity.

01:01:08.030 --> 01:01:09.488
PETER SZOLOVITS:
Physical activity.

01:01:13.240 --> 01:01:16.073
How often is this a problem?

01:01:16.073 --> 01:01:16.615
I don't know.

01:01:16.615 --> 01:01:18.110
A couple times a week maybe.

01:01:22.150 --> 01:01:26.368
Did eating suspect food
trigger your phlegm?

01:01:26.368 --> 01:01:27.340
AUDIENCE: No.

01:01:27.340 --> 01:01:28.030
PETER SZOLOVITS: I don't know.

01:01:28.030 --> 01:01:29.620
I don't know what
a suspect food is.

01:01:29.620 --> 01:01:31.493
AUDIENCE: [INAUDIBLE] food.

01:01:31.493 --> 01:01:32.410
PETER SZOLOVITS: Yeah.

01:01:32.410 --> 01:01:36.358
This is going to
kill most of my time.

01:01:36.358 --> 01:01:39.945
AUDIENCE: Is it getting better?

01:01:39.945 --> 01:01:41.320
PETER SZOLOVITS:
Is it improving?

01:01:41.320 --> 01:01:42.985
Sure, it's improving.

01:01:46.450 --> 01:01:48.640
Can I think of another
related symptom?

01:01:48.640 --> 01:01:49.780
No.

01:01:49.780 --> 01:01:54.595
I'm comparing your case
to men aged 66 to 72.

01:01:54.595 --> 01:01:58.030
A number of similar
cases gets more refined.

01:01:58.030 --> 01:02:00.220
Do I have shortness of breath?

01:02:00.220 --> 01:02:02.470
No.

01:02:02.470 --> 01:02:03.200
That's good.

01:02:03.200 --> 01:02:03.700
All right.

01:02:03.700 --> 01:02:04.990
Do I have a runny nose?

01:02:04.990 --> 01:02:05.620
Yeah, sure.

01:02:05.620 --> 01:02:06.670
I have a runny nose.

01:02:10.900 --> 01:02:13.435
It's-- I don't know--
a watery, runny nose.

01:02:17.482 --> 01:02:20.862
AUDIENCE: Does it say you've
got to call [INAUDIBLE]??

01:02:20.862 --> 01:02:22.570
PETER SZOLOVITS: Well,
I'm going to stop,

01:02:22.570 --> 01:02:23.950
because it will just take--

01:02:23.950 --> 01:02:27.830
it takes too long to go through
this, but you get the idea.

01:02:27.830 --> 01:02:29.770
So what this is
doing is actually

01:02:29.770 --> 01:02:32.200
running an algorithm
that is a cousin

01:02:32.200 --> 01:02:36.010
of the acute renal failure
algorithm that I showed you.

01:02:36.010 --> 01:02:40.030
So it's trying to optimize the
questions that it's asking,

01:02:40.030 --> 01:02:45.400
and it's trying to come up
with a diagnostic conclusion.

01:02:45.400 --> 01:02:47.470
Now, in order not
to get in trouble

01:02:47.470 --> 01:02:49.660
with things like
the FDA, it winds up

01:02:49.660 --> 01:02:52.330
wimping out at the
end, and it says,

01:02:52.330 --> 01:02:56.740
if you're feeling really
bad, go see a doctor.

01:02:56.740 --> 01:03:01.660
But nevertheless, these kinds
of things are now becoming real,

01:03:01.660 --> 01:03:03.760
and they're getting
better because they're

01:03:03.760 --> 01:03:06.180
based on more and more data.

01:03:06.180 --> 01:03:06.750
Yeah.

01:03:06.750 --> 01:03:09.413
AUDIENCE: [INAUDIBLE]

01:03:09.413 --> 01:03:11.330
PETER SZOLOVITS: Well,
I can't get to the end,

01:03:11.330 --> 01:03:12.720
because we're only at 36%.

01:03:12.720 --> 01:03:14.910
[INTERPOSING VOICES]

01:03:14.910 --> 01:03:15.410
Yeah.

01:03:15.410 --> 01:03:18.050
Here.

01:03:18.050 --> 01:03:19.280
All right.

01:03:19.280 --> 01:03:21.156
Somebody--

01:03:21.156 --> 01:03:22.890
AUDIENCE: Oh, I think
I need your finger.

01:03:22.890 --> 01:03:25.600
PETER SZOLOVITS: Oh.

01:03:25.600 --> 01:03:27.380
OK.

01:03:27.380 --> 01:03:28.885
Just don't drain
my bank account.

01:03:33.510 --> 01:03:37.040
So The British Medical
Journal did a test

01:03:37.040 --> 01:03:40.430
of a bunch of symptom checkers,
of 23 symptom checkers

01:03:40.430 --> 01:03:43.170
like this about four years ago.

01:03:43.170 --> 01:03:49.610
And they said, well, can it
on 45 standardized patient

01:03:49.610 --> 01:03:53.960
vignettes can it find at least
the right level of urgency

01:03:53.960 --> 01:03:57.560
to recommend whether you should
go to the emergency room,

01:03:57.560 --> 01:04:01.980
get other kinds of care, or
just take care of yourself.

01:04:01.980 --> 01:04:06.320
And then the goals were
that if the diagnosis is

01:04:06.320 --> 01:04:10.140
given by the program, it should
be in the top 20 of the list

01:04:10.140 --> 01:04:11.060
that it gives you.

01:04:11.060 --> 01:04:13.490
And if triage is
given, then it should

01:04:13.490 --> 01:04:15.800
be the right level of urgency.

01:04:15.800 --> 01:04:20.540
The correct diagnosis was
first in 34% of the cases.

01:04:20.540 --> 01:04:25.160
It was within the top
20 in 58% of the cases.

01:04:25.160 --> 01:04:30.230
And the correct triage
was 57% accurate.

01:04:30.230 --> 01:04:33.830
But notice it was more accurate
in the emergent cases, which

01:04:33.830 --> 01:04:38.180
is good, because those are the
ones where you really care.

01:04:38.180 --> 01:04:40.430
So we have--

01:04:40.430 --> 01:04:41.480
OK.

01:04:41.480 --> 01:04:46.220
So based on what
he said about me,

01:04:46.220 --> 01:04:51.770
I have an upper respiratory
infection with 50% likelihood.

01:04:51.770 --> 01:04:57.440
And I can ask what to do next.

01:05:00.050 --> 01:05:03.770
Watch for symptoms like
sore throat and fever.

01:05:03.770 --> 01:05:07.160
Physicians often
perform a physical exam,

01:05:07.160 --> 01:05:11.000
explore other treatment
options, and recovery

01:05:11.000 --> 01:05:16.790
for most cases like this is
a matter of days to weeks.

01:05:16.790 --> 01:05:20.780
And I can go back and
say, I might have the flu,

01:05:20.780 --> 01:05:24.560
or I might have
allergic rhinitis.

01:05:24.560 --> 01:05:27.080
So that's actually reasonable.

01:05:27.080 --> 01:05:30.425
I don't know exactly
what you put in about me.

01:05:30.425 --> 01:05:31.950
AUDIENCE: What is
the less than 50?

01:05:31.950 --> 01:05:33.200
PETER SZOLOVITS: What is what?

01:05:33.200 --> 01:05:35.980
AUDIENCE: The less than 50.

01:05:35.980 --> 01:05:37.970
[INTERPOSING VOICES]

01:05:37.970 --> 01:05:40.743
AUDIENCE: Patients have to
be the same demographics.

01:05:40.743 --> 01:05:41.660
PETER SZOLOVITS: Yeah.

01:05:41.660 --> 01:05:44.430
I don't know what the less
than 50 is supposed to mean.

01:05:44.430 --> 01:05:48.280
AUDIENCE: It started
with 200,000 or so.

01:05:48.280 --> 01:05:50.150
PETER SZOLOVITS:
Oh, so this is based

01:05:50.150 --> 01:05:52.470
on a small number of patients.

01:05:52.470 --> 01:05:55.430
So what happens, of course,
is as you slice and dice

01:05:55.430 --> 01:05:58.600
a population, it gets
smaller and smaller.

01:05:58.600 --> 01:06:01.440
So that's what we're seeing.

01:06:01.440 --> 01:06:02.300
OK.

01:06:02.300 --> 01:06:03.740
Thank you.

01:06:03.740 --> 01:06:05.780
OK.

01:06:05.780 --> 01:06:09.890
So two more topics I'm
going to rush through.

01:06:09.890 --> 01:06:18.530
One is that-- as I mentioned in
one of the much earlier slides,

01:06:18.530 --> 01:06:21.550
every action has a cost.

01:06:21.550 --> 01:06:23.580
It at least takes time.

01:06:23.580 --> 01:06:27.230
And sometimes it induces
potentially bad things

01:06:27.230 --> 01:06:29.730
to happen to a patient.

01:06:29.730 --> 01:06:34.490
And so people began studying
a long time ago what does

01:06:34.490 --> 01:06:37.280
it mean to be rational
under resource constraints

01:06:37.280 --> 01:06:41.720
rather than rational just in
this Home economicus model.

01:06:41.720 --> 01:06:46.290
And so Eric Horvitz, who's
now a big cheese guy,

01:06:46.290 --> 01:06:50.600
he's head of Microsoft
Research, but used

01:06:50.600 --> 01:06:54.100
to be just a lowly graduate
student at Stanford

01:06:54.100 --> 01:06:58.806
when he started doing this work.

01:06:58.806 --> 01:07:01.730
He said, well,
utility comes not only

01:07:01.730 --> 01:07:03.410
from what happens
to the patient,

01:07:03.410 --> 01:07:05.990
but also from the
reasoning process

01:07:05.990 --> 01:07:08.750
from the computational
process itself.

01:07:08.750 --> 01:07:12.030
And so consider-- do
you guys watch MacGyver?

01:07:12.030 --> 01:07:14.630
This is way out of date.

01:07:14.630 --> 01:07:19.640
So if MacGyver is defusing
some bomb that's ticking down

01:07:19.640 --> 01:07:24.890
to zero and he runs out of
time, then his utilities

01:07:24.890 --> 01:07:28.290
take a very sharp
drop at that point.

01:07:28.290 --> 01:07:33.650
So that's what this work is
really about, saying, well,

01:07:33.650 --> 01:07:36.860
what can we do when we don't
have all the time in the world

01:07:36.860 --> 01:07:41.930
to do the computation as well
as having to try to maximize

01:07:41.930 --> 01:07:44.000
utility to the patient?

01:07:44.000 --> 01:07:47.150
And Daniel Kahneman, who won
the Nobel Prize a few years ago

01:07:47.150 --> 01:07:52.230
in economics for this notion
of bounded rationality

01:07:52.230 --> 01:07:56.270
that says that the way we
would like to be rational

01:07:56.270 --> 01:07:59.150
is not actually the
way we behave, and he

01:07:59.150 --> 01:08:01.850
wrote this popular
book that I really like

01:08:01.850 --> 01:08:05.000
called Thinking,
Fast and Slow that

01:08:05.000 --> 01:08:10.040
says that if you're trying to
figure out which house to buy,

01:08:10.040 --> 01:08:14.240
you have a lot of time to do it,
so you can deliberate and list

01:08:14.240 --> 01:08:17.689
all the advantages and
disadvantages and costs and so

01:08:17.689 --> 01:08:22.700
on of different houses and take
your time making a decision.

01:08:22.700 --> 01:08:26.870
If you see a car barreling
toward you as you are crossing

01:08:26.870 --> 01:08:30.120
in a crosswalk, you
don't stop and say, well,

01:08:30.120 --> 01:08:34.200
let me figure out the pluses and
minuses of moving to the left

01:08:34.200 --> 01:08:39.410
or moving to the right, because
by the time you figure it out,

01:08:39.410 --> 01:08:40.580
you're dead.

01:08:40.580 --> 01:08:45.890
And so he claims that
human beings have evolved

01:08:45.890 --> 01:08:51.890
in a way where we have a kind of
instinctual very fast response,

01:08:51.890 --> 01:08:55.729
and that the deliberative
process is only

01:08:55.729 --> 01:08:57.890
invoked relatively rarely.

01:08:57.890 --> 01:09:00.229
Now, he bemoans this
fact, because he

01:09:00.229 --> 01:09:03.890
claims that people make too
many decisions that they ought

01:09:03.890 --> 01:09:08.420
to be deliberative about based
on these sort of gut instincts.

01:09:08.420 --> 01:09:11.720
For example, our
current president.

01:09:11.720 --> 01:09:15.859
But never mind.

01:09:15.859 --> 01:09:20.240
So what Eric and his
colleagues were doing

01:09:20.240 --> 01:09:25.399
was trying really to look at
how this kind of meta level

01:09:25.399 --> 01:09:28.370
reasoning about how
much reasoning and what

01:09:28.370 --> 01:09:33.109
kind of reasoning is worth doing
plays into the decision making

01:09:33.109 --> 01:09:34.319
process.

01:09:34.319 --> 01:09:36.260
So the expected
value of computation

01:09:36.260 --> 01:09:38.930
as a fundamental
component of reflection

01:09:38.930 --> 01:09:41.580
about alternative
inference strategies.

01:09:41.580 --> 01:09:44.720
So for example, I
mentioned that QMR

01:09:44.720 --> 01:09:47.600
had these alternative
questioning methods

01:09:47.600 --> 01:09:50.689
depending on the length
of the differential

01:09:50.689 --> 01:09:52.350
that it was working on.

01:09:52.350 --> 01:09:55.910
So that's an example of a kind
of meta level reasoning that

01:09:55.910 --> 01:09:59.330
says that it may be
more effective to do

01:09:59.330 --> 01:10:03.620
one kind of question asking
strategy than another.

01:10:03.620 --> 01:10:05.690
The degree of
refinement, people talk

01:10:05.690 --> 01:10:08.510
about things like
just-in-time algorithms,

01:10:08.510 --> 01:10:13.160
where if you run out of time
to think more deliberately,

01:10:13.160 --> 01:10:16.250
you can just take the best
answer that's available to you

01:10:16.250 --> 01:10:17.360
now.

01:10:17.360 --> 01:10:20.540
And so taking the
value of information,

01:10:20.540 --> 01:10:24.290
the value of computation, and
the value of experimentation

01:10:24.290 --> 01:10:27.860
into account in doing
this meta level reasoning

01:10:27.860 --> 01:10:31.940
is important to come up with
the most effective strategies.

01:10:31.940 --> 01:10:36.470
So he gives an example
of a time pressure

01:10:36.470 --> 01:10:42.890
decision problem where you have
a patient, a 75-year-old woman

01:10:42.890 --> 01:10:47.870
in the ICU, and she develops
sudden breathing difficulties.

01:10:47.870 --> 01:10:48.620
So what do you do?

01:10:52.820 --> 01:10:55.370
Well, it's a challenge, right?

01:10:55.370 --> 01:10:58.160
You could be very
deliberative, but the problem

01:10:58.160 --> 01:11:02.510
is that she may die because
she's not breathing well,

01:11:02.510 --> 01:11:04.610
or you could
impulsively say, well,

01:11:04.610 --> 01:11:06.620
let's put her on a
mechanical ventilator,

01:11:06.620 --> 01:11:09.170
because we know that
that will prevent her

01:11:09.170 --> 01:11:11.570
from dying in the
short term, but that

01:11:11.570 --> 01:11:14.630
may be the wrong decision,
because that has bad side

01:11:14.630 --> 01:11:16.370
effects.

01:11:16.370 --> 01:11:20.210
She may get an infection, get
pneumonia, and die that way.

01:11:20.210 --> 01:11:23.210
And you certainly don't want
to subject her to that risk

01:11:23.210 --> 01:11:26.990
if she didn't need
to take that risk.

01:11:26.990 --> 01:11:31.640
So they designed an
architecture that

01:11:31.640 --> 01:11:35.150
says, well, this is the decision
that you're trying to make,

01:11:35.150 --> 01:11:37.700
which they're modeling
by an influence diagram.

01:11:37.700 --> 01:11:42.110
So this is a Bayesian network
with the addition of decision

01:11:42.110 --> 01:11:44.060
nodes and value nodes.

01:11:44.060 --> 01:11:48.110
But you use Bayesian network
techniques to calculate

01:11:48.110 --> 01:11:50.150
optimal decisions here.

01:11:50.150 --> 01:11:52.910
And then this is kind of
the background knowledge

01:11:52.910 --> 01:11:56.180
of what we understand
about the relationships

01:11:56.180 --> 01:11:59.270
among different things in
the intensive care unit.

01:11:59.270 --> 01:12:03.470
And this is a representation
of the meta reasoning

01:12:03.470 --> 01:12:07.590
that says, which utility
model should we use?

01:12:07.590 --> 01:12:09.560
Which reasoning
technique should we use?

01:12:09.560 --> 01:12:10.670
And so on.

01:12:10.670 --> 01:12:13.370
And they built an
architecture that integrates

01:12:13.370 --> 01:12:17.180
these various approaches.

01:12:17.180 --> 01:12:20.420
And then in my last
2 minutes, I just

01:12:20.420 --> 01:12:24.470
want to tell you about
an interesting-- this

01:12:24.470 --> 01:12:26.720
is a modern view,
not historical.

01:12:26.720 --> 01:12:31.910
So this was a paper presented at
the last NeurIPS meeting, which

01:12:31.910 --> 01:12:36.890
said the kinds of problems
that we've been talking about,

01:12:36.890 --> 01:12:39.170
like the acute renal
failure problem

01:12:39.170 --> 01:12:44.390
or like any of these others,
we can reformulate this

01:12:44.390 --> 01:12:48.690
as a reinforcement
learning problem.

01:12:48.690 --> 01:12:57.140
So the idea is that if you
treat all activities, including

01:12:57.140 --> 01:13:00.380
putting somebody on a
ventilator or concluding

01:13:00.380 --> 01:13:05.060
a diagnostic conclusion or
asking a question or any

01:13:05.060 --> 01:13:07.520
of the other things
that we've contemplated,

01:13:07.520 --> 01:13:10.070
if you treat those
all in a uniform way

01:13:10.070 --> 01:13:14.540
and say these are
actions, we then

01:13:14.540 --> 01:13:19.310
model the universe as a
Markov decision process,

01:13:19.310 --> 01:13:22.770
where every time that you
take one of these actions,

01:13:22.770 --> 01:13:26.360
it changes the state
of the patient,

01:13:26.360 --> 01:13:30.110
or the state of our
knowledge about the patient.

01:13:30.110 --> 01:13:32.180
And then you do
reinforcement learning

01:13:32.180 --> 01:13:35.120
to figure out what
is the optimal policy

01:13:35.120 --> 01:13:38.360
to apply under all
possible states

01:13:38.360 --> 01:13:42.750
in order to maximize
the expected outcome.

01:13:42.750 --> 01:13:47.360
So that's exactly the
approach that they're taking.

01:13:47.360 --> 01:13:50.810
The state space is the set of
positive and negative findings.

01:13:50.810 --> 01:13:53.900
The action space is
to ask about a finding

01:13:53.900 --> 01:13:56.000
or conclude a diagnosis.

01:13:56.000 --> 01:14:00.140
The reward is the correct or
incorrect single diagnosis.

01:14:00.140 --> 01:14:03.170
So once you reach a
diagnosis, the process stops,

01:14:03.170 --> 01:14:05.170
and you get your reward.

01:14:05.170 --> 01:14:08.150
It's finite horizon
because they impose a limit

01:14:08.150 --> 01:14:09.920
on the number of questions.

01:14:09.920 --> 01:14:13.070
If you don't get an
answer by then, you lose.

01:14:13.070 --> 01:14:15.650
You get a minus one reward.

01:14:15.650 --> 01:14:21.830
There is a discount factor so
that the further away a reward

01:14:21.830 --> 01:14:26.690
is, the less value it has to you
at any point, which encourages

01:14:26.690 --> 01:14:29.000
shorter question sequences.

01:14:29.000 --> 01:14:32.540
And they use a pretty standard Q
learning framework, or at least

01:14:32.540 --> 01:14:36.860
a modern Q learning framework
using a double deep neural

01:14:36.860 --> 01:14:38.840
network strategy.

01:14:38.840 --> 01:14:41.750
And then there are two
pieces of magic sauce

01:14:41.750 --> 01:14:44.550
that make this work better.

01:14:44.550 --> 01:14:47.120
And one of them
is that they want

01:14:47.120 --> 01:14:50.030
to encourage asking
questions that

01:14:50.030 --> 01:14:52.640
are likely to have
positive answers rather

01:14:52.640 --> 01:14:54.440
than negative answers.

01:14:54.440 --> 01:14:57.200
And the reason is
because in their world,

01:14:57.200 --> 01:15:00.020
there are hundreds and
hundreds of questions.

01:15:00.020 --> 01:15:02.270
And of course,
most patients don't

01:15:02.270 --> 01:15:05.030
have most of those findings.

01:15:05.030 --> 01:15:07.880
And so you don't want to ask
a whole bunch of questions

01:15:07.880 --> 01:15:12.320
to which the answer is no, no,
no, no, no, no, no, no, no,

01:15:12.320 --> 01:15:14.600
because that doesn't give
you very much guidance.

01:15:14.600 --> 01:15:17.420
You want to ask questions
where the answer is yes,

01:15:17.420 --> 01:15:22.950
because that helps you clue
in on what's really going on.

01:15:22.950 --> 01:15:26.990
So they actually
have a nice proof

01:15:26.990 --> 01:15:30.680
that they do this thing
they call reward shaping,

01:15:30.680 --> 01:15:35.540
which basically adds
some incremental reward

01:15:35.540 --> 01:15:39.650
for asking questions that
will have a positive answer.

01:15:39.650 --> 01:15:43.460
But they can prove that
an optimal policy learned

01:15:43.460 --> 01:15:46.130
from that reward
function is also

01:15:46.130 --> 01:15:50.090
optimal for the reward function
that would not include it.

01:15:50.090 --> 01:15:52.510
So that's kind of cool.

01:15:52.510 --> 01:15:54.930
And then the other
thing they do is

01:15:54.930 --> 01:15:59.490
to try to identify a
reduced space of findings

01:15:59.490 --> 01:16:02.310
by what they call
feature rebuilding.

01:16:02.310 --> 01:16:05.910
And this is essentially a
dimension reduction technique

01:16:05.910 --> 01:16:09.060
where they're co-training.

01:16:09.060 --> 01:16:11.510
In this dual network
architecture,

01:16:11.510 --> 01:16:15.690
they're co-training
the policy model.

01:16:15.690 --> 01:16:17.940
It's, of course, the
neural network model,

01:16:17.940 --> 01:16:21.480
this being that 2010s.

01:16:21.480 --> 01:16:26.640
And so they're generating a
sequence, a deep layered set

01:16:26.640 --> 01:16:30.750
of neural networks that
generate an output, which

01:16:30.750 --> 01:16:35.490
is the m questions and the n
conclusions that can be made.

01:16:35.490 --> 01:16:37.860
And I think there's
a soft max over these

01:16:37.860 --> 01:16:43.330
to come up with the right policy
for any particular situation.

01:16:43.330 --> 01:16:48.000
But at the same time, they
co-train it in order to predict

01:16:48.000 --> 01:16:51.870
a number of--

01:16:51.870 --> 01:16:56.610
all of the manifestations from
what they've observed before.

01:16:56.610 --> 01:17:00.390
So it's using-- it's learning
a probabilistic model that

01:17:00.390 --> 01:17:03.240
says if you've answered
the following questions

01:17:03.240 --> 01:17:07.050
in the following ways, here
are the likely answers that you

01:17:07.050 --> 01:17:10.560
would give to the
remaining manifestations.

01:17:10.560 --> 01:17:12.550
And the reason they
can do that, of course,

01:17:12.550 --> 01:17:14.850
is because they really
are not independent.

01:17:14.850 --> 01:17:18.570
They're very often co-varying.

01:17:18.570 --> 01:17:20.700
And so they learn
that covariance,

01:17:20.700 --> 01:17:24.690
and therefore can predict which
answers are going to get yes

01:17:24.690 --> 01:17:28.380
answers, which questions are
going to get yes answers.

01:17:28.380 --> 01:17:33.330
And therefore, they can bias
the learning toward doing that.

01:17:33.330 --> 01:17:35.835
So last slide.

01:17:38.400 --> 01:17:41.220
So this system is called REFUEL.

01:17:41.220 --> 01:17:43.870
It's been tested
on a simulated data

01:17:43.870 --> 01:17:49.560
set of 650 diseases
and 375 symptoms.

01:17:49.560 --> 01:17:56.160
And what they show is that the
red line is their algorithm.

01:17:56.160 --> 01:18:01.220
The yellow line uses only
this reward reshaping.

01:18:01.220 --> 01:18:04.590
And the blue line is just
a straight reinforcement

01:18:04.590 --> 01:18:05.910
learning approach.

01:18:05.910 --> 01:18:07.980
And you can see
that they're doing

01:18:07.980 --> 01:18:11.780
much better after many
fewer epochs of training

01:18:11.780 --> 01:18:12.990
in doing this.

01:18:12.990 --> 01:18:17.310
Now, take this with a grain of
salt. This is all fake data.

01:18:17.310 --> 01:18:20.790
So they didn't have real
data sets to test this on.

01:18:20.790 --> 01:18:25.770
They got statistics on
what diseases are common

01:18:25.770 --> 01:18:28.860
and what symptoms are
common in those diseases.

01:18:28.860 --> 01:18:31.830
And then they had
a generative model

01:18:31.830 --> 01:18:34.420
that generated this fake data.

01:18:34.420 --> 01:18:37.350
And then they learned from
that generative model.

01:18:37.350 --> 01:18:40.680
So of course it would be
really important to redo

01:18:40.680 --> 01:18:44.370
the study with real data,
but they've not done that.

01:18:44.370 --> 01:18:47.110
This was just published
a few months ago.

01:18:47.110 --> 01:18:50.470
So that's sort of where we
are at the moment in diagnosis

01:18:50.470 --> 01:18:53.040
and in differential diagnosis.

01:18:53.040 --> 01:18:56.850
And I wanted to
start by introducing

01:18:56.850 --> 01:19:00.360
these ideas in a kind
of historical framework.

01:19:00.360 --> 01:19:03.210
But it means that there are a
tremendous number of papers,

01:19:03.210 --> 01:19:08.160
as you can imagine, that have
been written since the 1990s

01:19:08.160 --> 01:19:11.220
and '80s that I was
showing you that

01:19:11.220 --> 01:19:15.090
are essentially elaborations
on the same themes.

01:19:15.090 --> 01:19:18.690
And it's only in the
past decade of the advent

01:19:18.690 --> 01:19:21.690
of these neural network
models that people

01:19:21.690 --> 01:19:25.440
have changed strategy,
so that instead

01:19:25.440 --> 01:19:28.440
of learning explicit
probabilities, for example,

01:19:28.440 --> 01:19:31.920
like you do in a Bayesian
network, you just say,

01:19:31.920 --> 01:19:36.030
well, this is simply
a prediction task.

01:19:36.030 --> 01:19:38.130
And so we'll predict
the way we predict

01:19:38.130 --> 01:19:40.980
everything else with neural
network models, which

01:19:40.980 --> 01:19:45.330
is we build a CNN, or an RNN,
or some combination of things,

01:19:45.330 --> 01:19:48.660
or some attention
model, or something.

01:19:48.660 --> 01:19:49.980
And we throw that at it.

01:19:49.980 --> 01:19:52.380
And it does typically
a slightly better job

01:19:52.380 --> 01:19:54.420
than any of the previous
learning methods

01:19:54.420 --> 01:19:57.780
that we've used
typically, but not always.

01:19:57.780 --> 01:19:58.320
OK.

01:19:58.320 --> 01:19:59.870
Peace.