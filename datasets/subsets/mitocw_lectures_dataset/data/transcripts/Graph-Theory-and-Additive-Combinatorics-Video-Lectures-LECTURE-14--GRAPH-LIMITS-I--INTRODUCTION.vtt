WEBVTT

00:00:18.265 --> 00:00:19.640
YUFEI ZHAO: So
today we are going

00:00:19.640 --> 00:00:22.320
to start a new chapter
on graph limits.

00:00:32.020 --> 00:00:37.380
So graph limits is a relatively
new subject in graph theory.

00:00:37.380 --> 00:00:39.190
So as the name
suggests, we're looking

00:00:39.190 --> 00:00:44.140
at some kind of an analytic
limit of graphs, which

00:00:44.140 --> 00:00:46.120
sounds kind of
like a strange idea

00:00:46.120 --> 00:00:49.990
because you think of graphs as
fundamentally discrete objects.

00:00:49.990 --> 00:00:54.390
But let me begin with
an example to motivate,

00:00:54.390 --> 00:00:57.700
at least pure mathematical
motivation for graph limits.

00:00:57.700 --> 00:01:01.180
There are several other ways
you can motivate graphs limits,

00:01:01.180 --> 00:01:03.910
especially coming from
more applied perspectives.

00:01:03.910 --> 00:01:09.070
But let me stick with
the following story.

00:01:09.070 --> 00:01:12.880
So suppose you lived
in ancient Greece

00:01:12.880 --> 00:01:15.280
and you only knew
rational numbers.

00:01:15.280 --> 00:01:17.490
You didn't know
about real numbers.

00:01:17.490 --> 00:01:20.290
But you understand
perfectly rational numbers.

00:01:20.290 --> 00:01:22.450
And we wish to maximize.

00:01:26.360 --> 00:01:31.630
So we wish to then minimize
the following polynomial,

00:01:31.630 --> 00:01:39.880
x cubed minus x, let's
say for x between 0 and 1.

00:01:44.990 --> 00:01:47.090
So you can do this.

00:01:47.090 --> 00:01:49.990
And suppose also the
Greeks knew calculus

00:01:49.990 --> 00:01:52.510
and take the derivative
and all of that.

00:01:52.510 --> 00:01:54.370
So you find that.

00:01:54.370 --> 00:01:58.870
You have a problem
because we know--

00:01:58.870 --> 00:02:01.540
so given our advanced
state of mathematics,

00:02:01.540 --> 00:02:04.780
we know that the maximum--

00:02:04.780 --> 00:02:11.470
so the minimizer is at x
equals to 1 over root 3.

00:02:11.470 --> 00:02:15.880
But that number doesn't
exist in the real numbers.

00:02:15.880 --> 00:02:20.840
So how might a civilization
that only knew rational numbers

00:02:20.840 --> 00:02:23.580
express this answer?

00:02:23.580 --> 00:02:29.670
They could say, the minimum
occurs not in Q. So there's

00:02:29.670 --> 00:02:30.960
not minimized in Q--

00:02:35.210 --> 00:02:39.695
but not minimized by a single
number, but by a sequence.

00:02:47.710 --> 00:02:52.230
And this is a sequence that
a more advanced civilization

00:02:52.230 --> 00:02:59.100
would know, a sequence that
converges to 1 over root of 3.

00:02:59.100 --> 00:03:02.220
But I can give you this sequence
through some other means.

00:03:02.220 --> 00:03:06.260
And this is one of the ways
of defining the complete set

00:03:06.260 --> 00:03:07.970
of real numbers, for instance.

00:03:07.970 --> 00:03:10.720
But you can define explicitly
a sequence of real numbers

00:03:10.720 --> 00:03:13.818
that converges.

00:03:13.818 --> 00:03:15.610
But of course, this is
all quite cumbersome

00:03:15.610 --> 00:03:16.870
if you have to
actually write down

00:03:16.870 --> 00:03:19.078
this sequence of real numbers
to express this answer.

00:03:19.078 --> 00:03:21.560
It will be much better if
we knew the real numbers.

00:03:21.560 --> 00:03:22.400
And we do.

00:03:22.400 --> 00:03:25.300
And the real numbers,
in some sense,

00:03:25.300 --> 00:03:27.610
in the very rigorous
sense, is a completion

00:03:27.610 --> 00:03:29.320
of the rational numbers.

00:03:33.730 --> 00:03:38.120
That's the story that
we're all familiar with.

00:03:38.120 --> 00:03:41.090
But now let's think
about graphs which

00:03:41.090 --> 00:03:43.760
are some kind of a
discrete set of objects,

00:03:43.760 --> 00:03:47.050
akin to the rational numbers.

00:03:47.050 --> 00:03:51.170
And the story now
is, among graphs,

00:03:51.170 --> 00:03:58.250
suppose I have a fixed
p between 0 and 1.

00:03:58.250 --> 00:04:02.140
And the problem
now is to minimize

00:04:02.140 --> 00:04:15.529
the 4-cycle density among graphs
with density, with edge density

00:04:15.529 --> 00:04:16.029
p.

00:04:24.750 --> 00:04:27.450
So this is some kind of
optimization problem.

00:04:27.450 --> 00:04:29.360
So I don't restrict
the number of vertices.

00:04:29.360 --> 00:04:31.730
You can use as many
vertices as you like.

00:04:31.730 --> 00:04:36.004
And I would like to minimize
the 4-cycle density.

00:04:36.004 --> 00:04:41.500
Now, we saw a few lectures
ago this inequality

00:04:41.500 --> 00:04:44.590
that tells us that--

00:04:44.590 --> 00:04:50.730
so we saw a few lectures ago
that this density is always

00:04:50.730 --> 00:04:53.320
at least p to the fourth.

00:04:53.320 --> 00:04:55.420
So in the lecture on
quasirandomness, so

00:04:55.420 --> 00:04:57.570
we saw this inequality.

00:04:57.570 --> 00:05:02.140
And we also saw
that this minimum

00:05:02.140 --> 00:05:11.860
is approached by a sequence
of quasirandom graphs.

00:05:20.270 --> 00:05:24.260
And in some sense, that-- so
the answer is p to the fourth.

00:05:24.260 --> 00:05:25.820
And there's not
a specific graph.

00:05:25.820 --> 00:05:27.830
There's no one graph
that minimizes.

00:05:27.830 --> 00:05:31.700
This 4-cycle density is
minimized by a sequence.

00:05:31.700 --> 00:05:34.430
And just like in the story
with the rational numbers

00:05:34.430 --> 00:05:36.920
and the real numbers,
it would be nice

00:05:36.920 --> 00:05:39.620
if we didn't have to
write out the answer

00:05:39.620 --> 00:05:42.050
in this cumbersome,
sequential way,

00:05:42.050 --> 00:05:45.770
but just have a single
graphical-like object that

00:05:45.770 --> 00:05:48.900
depicts what the
minimizer should be.

00:05:48.900 --> 00:05:51.630
And graph limits provides a
language for us to do this.

00:05:54.640 --> 00:05:59.160
So one of the goals
of the graph limits--

00:06:01.870 --> 00:06:11.890
this gives us a single
object for this minimizer

00:06:11.890 --> 00:06:15.200
instead of taking a sequence.

00:06:15.200 --> 00:06:20.050
So roughly that is the idea that
you have a sequence of graphs.

00:06:20.050 --> 00:06:22.750
And I would like
some analytic object

00:06:22.750 --> 00:06:27.190
to capture the behavior of
the sequence in the limit.

00:06:27.190 --> 00:06:29.680
And these graph limits
can be written actually

00:06:29.680 --> 00:06:31.955
in a fairly concrete form.

00:06:31.955 --> 00:06:34.360
And so now let me begin
with some definitions.

00:06:37.160 --> 00:06:39.250
The main object
that we'll look at

00:06:39.250 --> 00:06:41.160
is something called a graphon.

00:06:45.990 --> 00:06:48.515
So it merges the two
words graph, function.

00:06:53.273 --> 00:07:07.520
A graphon is by definition a
symmetric, measurable function,

00:07:07.520 --> 00:07:12.560
often denoted by the letter
W from the unit squared

00:07:12.560 --> 00:07:16.240
to the 0, 1 interval.

00:07:16.240 --> 00:07:23.540
And here being symmetric means
that if you exchange the two

00:07:23.540 --> 00:07:26.390
argument variables, this
function remains the same.

00:07:30.120 --> 00:07:30.830
So that's it.

00:07:30.830 --> 00:07:32.750
So that's the
definition of a graphon.

00:07:32.750 --> 00:07:35.450
And these are the objects that
will play the role of limits

00:07:35.450 --> 00:07:37.400
for sequences of graphs.

00:07:37.400 --> 00:07:40.430
And I will give you lots
of examples in a second.

00:07:40.430 --> 00:07:42.018
So that's the definition.

00:07:44.710 --> 00:07:46.840
This is the form of
the graphons that we'll

00:07:46.840 --> 00:07:48.060
be looking at mostly.

00:07:48.060 --> 00:07:51.730
But just to mention
a few remarks,

00:07:51.730 --> 00:07:59.440
that the domain can
be instead any product

00:07:59.440 --> 00:08:03.130
of any square of a
probability measure space--

00:08:10.300 --> 00:08:12.240
so instead of taking
the 0, 1 interval,

00:08:12.240 --> 00:08:14.560
I could also use any
probability measure space.

00:08:14.560 --> 00:08:16.590
So it's only slightly
more general.

00:08:16.590 --> 00:08:19.380
So there are some general
theorems in measure theory

00:08:19.380 --> 00:08:22.680
that tells us that most
probability measure

00:08:22.680 --> 00:08:24.420
spaces, if they're
nice enough, they

00:08:24.420 --> 00:08:26.160
are in some sense
equivalent or can

00:08:26.160 --> 00:08:28.480
be captured by this interval.

00:08:28.480 --> 00:08:32.007
So I don't want you to worry
too much about all the measure

00:08:32.007 --> 00:08:33.299
where there are technicalities.

00:08:33.299 --> 00:08:36.210
I think they are not so
important for the discussion

00:08:36.210 --> 00:08:37.140
of graph limits.

00:08:37.140 --> 00:08:43.620
But there are some subtle issues
like that just lurking behind.

00:08:43.620 --> 00:08:46.017
But I just don't want to
really talk about them.

00:08:46.017 --> 00:08:47.600
So for the most part,
we'll be looking

00:08:47.600 --> 00:08:49.280
at graphons of this one.

00:08:49.280 --> 00:08:57.060
And also the-- so instead of
the domain, so the values--

00:08:57.060 --> 00:09:04.830
so instead of 0, 1
interval, you could also

00:09:04.830 --> 00:09:09.750
take a more general space,
for example, the real numbers

00:09:09.750 --> 00:09:12.000
or even the complex numbers.

00:09:12.000 --> 00:09:13.680
I'm going to use
the word graphon

00:09:13.680 --> 00:09:21.220
to reserve this word for when
the values are between 0 and 1.

00:09:21.220 --> 00:09:25.950
And if it's in R, let me
call this just a kernel,

00:09:25.950 --> 00:09:29.100
although that will
not come up so much.

00:09:29.100 --> 00:09:31.320
So when I say
graphon, I just mean

00:09:31.320 --> 00:09:32.850
the values between 0 and 1.

00:09:32.850 --> 00:09:36.450
Although if you do look up
papers in the literature,

00:09:36.450 --> 00:09:39.180
sometimes they don't use
these words so consistently.

00:09:39.180 --> 00:09:41.480
So be careful what
they mean by a graphon.

00:09:44.678 --> 00:09:45.720
So that's the definition.

00:09:45.720 --> 00:09:47.555
But now let me give
you some examples

00:09:47.555 --> 00:09:50.310
on how do we think of
graphons and what do they

00:09:50.310 --> 00:09:52.780
have to do with graphs.

00:09:52.780 --> 00:09:56.820
So if we start with a
graph, I want to show you

00:09:56.820 --> 00:09:58.730
how to turn it into a graphon.

00:10:03.390 --> 00:10:06.590
So let's start with this graph,
which you've seen before.

00:10:06.590 --> 00:10:07.720
This is the half graph.

00:10:16.480 --> 00:10:22.650
So from this graph, I
can label the vertices

00:10:22.650 --> 00:10:35.570
and form an adjacency
matrix of this graph, where

00:10:35.570 --> 00:10:41.940
I label the rows and
columns by the vertices

00:10:41.940 --> 00:10:44.670
and put in zeros
and ones according

00:10:44.670 --> 00:10:48.810
to whether the
edges are adjacent.

00:10:53.100 --> 00:10:54.860
So that's the adjacency matrix.

00:10:58.230 --> 00:11:01.650
And now I want you
to view this matrix

00:11:01.650 --> 00:11:03.900
as a black and white picture.

00:11:03.900 --> 00:11:07.410
So think one of these
pixelated images, where I

00:11:07.410 --> 00:11:12.435
turn the ones into black boxes.

00:11:21.100 --> 00:11:23.620
Of course, on the blackboard,
black is white and white

00:11:23.620 --> 00:11:26.310
is black.

00:11:26.310 --> 00:11:29.370
So I turn the ones
into black boxes.

00:11:29.370 --> 00:11:34.750
And I leave the zeros
as empty white space.

00:11:34.750 --> 00:11:36.790
So I get this image.

00:11:36.790 --> 00:11:41.670
And I think of this
image as a function.

00:11:41.670 --> 00:11:53.190
And this is the function
going from 0, 1, squared to 0,

00:11:53.190 --> 00:11:55.920
1, interval, taking
only 0 and 1 values.

00:12:05.820 --> 00:12:10.120
So that's a function
on the square.

00:12:10.120 --> 00:12:12.770
But now, so this
is a single graph.

00:12:12.770 --> 00:12:14.530
So for any specific
graph, I can turn it

00:12:14.530 --> 00:12:16.950
into a graphon like this.

00:12:16.950 --> 00:12:18.880
But now imagine you have
a sequence of graphs.

00:12:18.880 --> 00:12:21.940
And in particular, consider
a sequence of half graphs.

00:12:28.480 --> 00:12:31.520
So here is H3.

00:12:31.520 --> 00:12:34.540
So Hn is the general half graph.

00:12:34.540 --> 00:12:38.320
And you can imagine
that, as n gets large,

00:12:38.320 --> 00:12:40.690
this picture looks like--

00:12:44.280 --> 00:12:46.080
instead of the
staircase you just

00:12:46.080 --> 00:12:49.170
have a straight line
connecting the two ends.

00:13:00.800 --> 00:13:04.510
And indeed, this function
here, this graphon,

00:13:04.510 --> 00:13:11.400
is the limit of the
sequence of half graphs

00:13:11.400 --> 00:13:13.553
as n goes to infinity.

00:13:16.730 --> 00:13:18.790
So one way you can
think about graphons

00:13:18.790 --> 00:13:20.710
is you have a
sequence of graphs.

00:13:20.710 --> 00:13:23.440
You look at their
adjacency matrix.

00:13:23.440 --> 00:13:25.832
You view it as a picture,
a pixelated image,

00:13:25.832 --> 00:13:27.790
black and white according
to the zeros and ones

00:13:27.790 --> 00:13:30.160
in its adjacency matrix.

00:13:30.160 --> 00:13:35.140
And as you take a sequence,
you make your eyes a little bit

00:13:35.140 --> 00:13:35.750
blurry.

00:13:35.750 --> 00:13:38.260
And then you think about
what the sequence of images

00:13:38.260 --> 00:13:40.120
converges to.

00:13:40.120 --> 00:13:44.110
So the resulting
limit is the limit

00:13:44.110 --> 00:13:47.120
of this sequence of graphs.

00:13:47.120 --> 00:13:50.440
So that's an
informal explanation.

00:13:50.440 --> 00:13:52.408
So I haven't done
anything precisely.

00:13:52.408 --> 00:13:53.950
And in fact, one
needs to be somewhat

00:13:53.950 --> 00:13:56.620
careful with this
depiction because let

00:13:56.620 --> 00:13:57.925
me give you another example.

00:14:01.150 --> 00:14:16.080
Suppose I have a sequence of
random or quasirandom graphs

00:14:16.080 --> 00:14:18.860
with edge density 1/2.

00:14:25.270 --> 00:14:28.230
So what does this look like?

00:14:28.230 --> 00:14:29.860
And I have this picture here.

00:14:29.860 --> 00:14:31.630
And I have a lot of--

00:14:34.280 --> 00:14:35.450
so I have a lot of--

00:14:37.990 --> 00:14:40.810
one-half of the
pixels are black.

00:14:40.810 --> 00:14:44.010
And the other half
pixels are white.

00:14:44.010 --> 00:14:45.930
And you can think,
from far away,

00:14:45.930 --> 00:14:48.570
I cannot distinguish necessarily
which ones are black and which

00:14:48.570 --> 00:14:49.610
ones are white.

00:14:49.610 --> 00:14:52.470
And in the limit, it looks
like a grayscale image,

00:14:52.470 --> 00:14:57.030
with a grayscale being
one-half density.

00:14:57.030 --> 00:15:01.330
And indeed, it converges to
the constant function, 1/2.

00:15:10.760 --> 00:15:15.190
So the limits represented
by this problem up here

00:15:15.190 --> 00:15:20.120
is the constant graphon
with the constant value p.

00:15:20.120 --> 00:15:22.370
But now let me give you
a different example.

00:15:25.350 --> 00:15:27.001
Consider a checkerboard.

00:15:36.340 --> 00:15:53.280
So here is a checkerboard, where
I color the squares according

00:15:53.280 --> 00:15:56.070
to, in this alternating
black and white manner,

00:15:56.070 --> 00:15:58.162
according to a
usual checkerboard.

00:16:02.890 --> 00:16:07.690
And as the number of
squares goes to infinity,

00:16:07.690 --> 00:16:10.330
what should this converge to?

00:16:13.210 --> 00:16:14.960
By the story I
just told you, you

00:16:14.960 --> 00:16:18.020
might think that
if you zoom out,

00:16:18.020 --> 00:16:21.030
everything looks density 1/2.

00:16:21.030 --> 00:16:24.210
And so you might guess
that the image, the limit,

00:16:24.210 --> 00:16:27.820
is the 1/2 constant.

00:16:27.820 --> 00:16:28.870
But what is this graph?

00:16:32.570 --> 00:16:34.090
It's a complete bipartite graph.

00:16:38.390 --> 00:16:44.053
It is a complete bipartite
graph between all the even rows.

00:16:44.053 --> 00:16:46.470
And there's a different way
to draw the complete bipartite

00:16:46.470 --> 00:16:48.590
graph--

00:16:48.590 --> 00:16:59.740
namely, that picture, just by
permuting the rows and columns.

00:16:59.740 --> 00:17:04.000
And it's much more
reasonable that this

00:17:04.000 --> 00:17:07.030
is the limit of the sequence
of complete bipartite

00:17:07.030 --> 00:17:08.470
graphs with equal parts.

00:17:11.010 --> 00:17:12.569
So one needs to be very careful.

00:17:12.569 --> 00:17:17.810
And so it's not necessarily
an intuitive definition.

00:17:17.810 --> 00:17:20.520
The idea that you
just squint your eyes

00:17:20.520 --> 00:17:22.770
and think about what
the image becomes,

00:17:22.770 --> 00:17:26.010
that works fine for
intuition for some examples,

00:17:26.010 --> 00:17:27.147
but not for others.

00:17:27.147 --> 00:17:28.980
So we do really need
to be careful in giving

00:17:28.980 --> 00:17:30.390
a precise definition.

00:17:30.390 --> 00:17:33.750
And here the rearrangement
of the rows and columns

00:17:33.750 --> 00:17:34.950
needs to be taken care of.

00:17:46.777 --> 00:17:47.860
So let me be more precise.

00:17:50.700 --> 00:17:57.680
Starting with a graph G, I can--

00:17:57.680 --> 00:18:03.280
so let me label the
vertices by 1 through n.

00:18:03.280 --> 00:18:15.490
I can denote by W sub G
this function, this graphon,

00:18:15.490 --> 00:18:20.292
obtained by the
following procedure.

00:18:24.630 --> 00:18:30.820
First, you partition
the interval

00:18:30.820 --> 00:18:39.090
into intervals of
length exactly 1 over n.

00:18:44.680 --> 00:18:54.330
And you set W of x
comma y to be basically

00:18:54.330 --> 00:18:57.780
what happened in
the procedure above.

00:18:57.780 --> 00:19:04.530
If x and y lie in the box
I sub I cross I sub J,

00:19:04.530 --> 00:19:11.300
then I put in 1 if I is
adjacent to J and 0 otherwise--

00:19:15.800 --> 00:19:18.390
so this picture,
where we obtained

00:19:18.390 --> 00:19:21.720
by taking the adjacency
matrix and transforming it

00:19:21.720 --> 00:19:22.980
into a pixelated image.

00:19:28.342 --> 00:19:29.800
What are some of
the things that we

00:19:29.800 --> 00:19:32.437
would like to do with graph
limits or graphs in general?

00:19:32.437 --> 00:19:32.937
Yeah?

00:19:32.937 --> 00:19:37.273
AUDIENCE: Is the range
also 0, 1, squared or 0, 1?

00:19:37.273 --> 00:19:38.190
YUFEI ZHAO: Thank you.

00:19:38.190 --> 00:19:39.060
The range is 0, 1.

00:19:42.730 --> 00:19:45.850
So here are some quantities
we are interested in when

00:19:45.850 --> 00:19:48.170
considering graph limits.

00:19:48.170 --> 00:19:58.954
So given two graphs, G and H, we
say that a graph homomorphism--

00:20:04.770 --> 00:20:13.390
so a graph homomorphism
between from G to H

00:20:13.390 --> 00:20:26.137
is a map of their vertexes such
that the edges are preserved.

00:20:30.910 --> 00:20:40.865
So you have-- and so
whenever uv is an edge of H,

00:20:40.865 --> 00:20:48.880
your image vertices get
mapped to an edge of G.

00:20:48.880 --> 00:20:52.810
And we are interested in the
number of graph homomorphisms.

00:20:52.810 --> 00:21:04.400
So often I use uppercase to
denote a set of homomorphisms G

00:21:04.400 --> 00:21:05.390
to H--

00:21:05.390 --> 00:21:07.580
and lowercase to
denote the number.

00:21:16.690 --> 00:21:21.480
So for example, the
number of homomorphisms

00:21:21.480 --> 00:21:24.570
from a single vertex--

00:21:24.570 --> 00:21:30.093
so a single vertex with no edge
to a graph G, that's just the--

00:21:30.093 --> 00:21:31.010
what is this quantity?

00:21:34.572 --> 00:21:36.530
So some number of
vertices of G--

00:21:40.730 --> 00:21:48.740
what about homomorphisms
from an edge to G?

00:21:48.740 --> 00:21:51.180
AUDIENCE: The number of edges?

00:21:51.180 --> 00:21:53.790
YUFEI ZHAO: Not quite the
number of edges, but twice

00:21:53.790 --> 00:21:54.690
the number of edges.

00:21:59.730 --> 00:22:02.810
What about the number
of homomorphisms

00:22:02.810 --> 00:22:04.310
from a triangle to G?

00:22:10.420 --> 00:22:12.170
AUDIENCE: 6 times the
number of triangles.

00:22:12.170 --> 00:22:14.570
YUFEI ZHAO: So yeah, you got
the idea-- so the 6 times

00:22:14.570 --> 00:22:16.088
the number of triangles.

00:22:22.172 --> 00:22:26.740
So now let me ask a slightly
more interesting question.

00:22:26.740 --> 00:22:28.890
What about the number
of homomorphisms from H

00:22:28.890 --> 00:22:29.665
to a triangle?

00:22:34.990 --> 00:22:37.010
What's a different name
for this quantity here?

00:22:45.630 --> 00:22:47.550
It's the number of
proper three colorings.

00:22:53.820 --> 00:22:57.380
So it's the number of
proper three colorings,

00:22:57.380 --> 00:23:05.160
the number of proper
colorings of H with three

00:23:05.160 --> 00:23:07.020
labeled colors, red,
green, and blue.

00:23:09.940 --> 00:23:11.640
So think about the
three vertices.

00:23:11.640 --> 00:23:13.510
That's red, green, and blue.

00:23:13.510 --> 00:23:16.790
And whichever vertex
of H can map to red,

00:23:16.790 --> 00:23:18.610
color that vertex red.

00:23:18.610 --> 00:23:21.150
So you see that there is a
one-to-one correspondence

00:23:21.150 --> 00:23:25.680
between such homomorphisms
and proper colorings.

00:23:25.680 --> 00:23:28.740
So many important graph
parameters, graph quantities,

00:23:28.740 --> 00:23:33.420
can be encoded in terms
of graph homomorphisms.

00:23:33.420 --> 00:23:35.100
And these are the
ones that we're

00:23:35.100 --> 00:23:36.920
going to be looking
at most of the time.

00:23:39.780 --> 00:23:42.490
When we're thinking
about very large graphs,

00:23:42.490 --> 00:23:45.490
often it's not the number of
homomorphisms that concern us,

00:23:45.490 --> 00:23:48.980
but the density
of homomorphisms.

00:23:48.980 --> 00:23:52.730
And the difference
between homomorphisms

00:23:52.730 --> 00:23:59.450
on one hand and subgraphs is
that the homomorphisms are not

00:23:59.450 --> 00:24:03.320
quite the same as
subgraphs, other

00:24:03.320 --> 00:24:06.620
than this multiplicity,
because you might have

00:24:06.620 --> 00:24:12.220
non-injective homomorphisms.

00:24:12.220 --> 00:24:14.770
But these non-injective
homomorphisms

00:24:14.770 --> 00:24:18.700
do not end up contributing
very much because they only

00:24:18.700 --> 00:24:25.540
have n to the number of
vertices of H minus 1

00:24:25.540 --> 00:24:30.070
on that border where I think
of n as the number of vertices

00:24:30.070 --> 00:24:33.690
of G. n is supposed to be large.

00:24:33.690 --> 00:24:37.170
So in terms of graph
limits when n gets large,

00:24:37.170 --> 00:24:40.350
I don't need to distinguish
so much between homomorphisms

00:24:40.350 --> 00:24:44.020
and subgraphs.

00:24:44.020 --> 00:24:57.380
We define the homomorphism
density, denoted by the letter

00:24:57.380 --> 00:25:04.060
t, from H to G, by--

00:25:04.060 --> 00:25:08.810
define it to be the
fraction of all vertex maps

00:25:08.810 --> 00:25:11.690
that are homomorphisms.

00:25:17.890 --> 00:25:22.720
So this is also equivalent to
be defined as the probability

00:25:22.720 --> 00:25:31.780
that a uniform random
map from the vertex set

00:25:31.780 --> 00:25:40.930
of H to the vertex set of G
is a homomorphism from H to G.

00:25:40.930 --> 00:25:43.460
So it's a graph homomorphism.

00:25:43.460 --> 00:25:45.680
And this quantity turns
out to be quite important.

00:25:45.680 --> 00:25:48.740
So we're going to be
seeing this a lot.

00:25:48.740 --> 00:25:55.170
And because of this remark
over here, in the limit,

00:25:55.170 --> 00:26:02.600
this quantity of graph
homomorphism densities

00:26:02.600 --> 00:26:06.960
in the limit as the number of
vertices G goes to infinity

00:26:06.960 --> 00:26:16.470
and H fixed, the
homomorphism densities

00:26:16.470 --> 00:26:20.467
approaches the same limit
as subgraph densities.

00:26:29.132 --> 00:26:30.840
So you should regard
these two quantities

00:26:30.840 --> 00:26:32.070
as basically the same thing.

00:26:35.575 --> 00:26:36.450
Any questions so far?

00:26:41.100 --> 00:26:45.480
So all of these quantities
so far defined are for--

00:26:45.480 --> 00:26:49.290
so everything is defined so
far for graphs, so what happens

00:26:49.290 --> 00:26:50.820
between graphs and graphs.

00:26:50.820 --> 00:26:53.590
So what about for graphons?

00:26:53.590 --> 00:26:56.730
I'll give you this limit
object, this analytic object.

00:26:56.730 --> 00:27:01.500
I can still define
densities by integrals now.

00:27:01.500 --> 00:27:06.560
So suppose I start with a
symmetric measurable function.

00:27:14.480 --> 00:27:19.250
So tell me, for
example, a graphon.

00:27:19.250 --> 00:27:23.780
But I can let my range
be even more generous.

00:27:23.780 --> 00:27:29.390
Starting with such a function,
I define the graph homomorphism

00:27:29.390 --> 00:27:35.030
density from a fixed graph
H to this graphon or kernel,

00:27:35.030 --> 00:27:45.223
more generally, to be the
following integral, where I'm--

00:27:45.223 --> 00:27:46.640
before writing
down the full form,

00:27:46.640 --> 00:27:48.015
let me first give
you an example.

00:27:48.015 --> 00:27:49.530
I think it will be more helpful.

00:27:49.530 --> 00:27:57.690
So if I'm looking at a triangle
going to W, what I would like

00:27:57.690 --> 00:28:01.725
is the integral that captures
the triangle density.

00:28:08.410 --> 00:28:19.120
So this quantity here, if I let
x, y, and z vary over 0 and 1,

00:28:19.120 --> 00:28:21.670
0 through 1, independently
and uniformly,

00:28:21.670 --> 00:28:27.540
then this quantity here captures
the triangle density in W.

00:28:27.540 --> 00:28:30.660
In fact, and I'll state this
more precisely in a second--

00:28:30.660 --> 00:28:33.180
if you look at the
translation from graph

00:28:33.180 --> 00:28:37.160
to graphon and combine
that translation

00:28:37.160 --> 00:28:41.240
with this definition here, you
recover the triangle density.

00:28:45.830 --> 00:28:51.500
More generally, for H
instead of a triangle,

00:28:51.500 --> 00:28:57.110
the H density in a graphon is
defined to be the integral of--

00:28:57.110 --> 00:29:01.010
instead of this
product here, I take

00:29:01.010 --> 00:29:05.880
a product corresponding to
the graph structure of H

00:29:05.880 --> 00:29:14.360
with one factor
for each edge of H.

00:29:14.360 --> 00:29:28.910
And the variables go
over the vertex set of H.

00:29:28.910 --> 00:29:33.000
So this is the definition
of homomorphism densities,

00:29:33.000 --> 00:29:37.250
not for graphs, but for
symmetric measurable functions,

00:29:37.250 --> 00:29:39.300
in particular, for graphons.

00:29:39.300 --> 00:29:40.970
And we define it
this way because--

00:29:40.970 --> 00:29:43.900
and we use the same symbols
because these two definitions

00:29:43.900 --> 00:29:44.400
agree.

00:29:48.970 --> 00:29:52.780
If you start with a graph and
look at the H density in G,

00:29:52.780 --> 00:29:58.780
then this quantity here
is equal to the H density

00:29:58.780 --> 00:30:02.590
in the graphon associated
to the graph G constructed

00:30:02.590 --> 00:30:03.520
as we did just now.

00:30:06.710 --> 00:30:09.110
So make sure you
understand why this is true

00:30:09.110 --> 00:30:11.030
and why we defined the
densities this way.

00:30:13.905 --> 00:30:14.780
Any questions so far?

00:30:24.990 --> 00:30:31.310
So we've given the definition
of graph homomorphism density.

00:30:31.310 --> 00:30:34.260
And we've defined these
objects, these graphons.

00:30:40.550 --> 00:30:43.800
And I mentioned even something
about the idea of a limit.

00:30:43.800 --> 00:30:46.980
But in what sense can we
have a limit of graphs?

00:31:04.430 --> 00:31:06.250
So here is an
important definition

00:31:06.250 --> 00:31:08.650
on the convergence of graphs.

00:31:08.650 --> 00:31:14.140
So in what sense can we say that
a sequence of graphs converge?

00:31:14.140 --> 00:31:28.680
So we say that a sequence
of graphs G sub n--

00:31:32.800 --> 00:31:36.310
graphs or graphons, so
these two definitions

00:31:36.310 --> 00:31:42.610
are interchangeable for what I'm
about to say regarding limits

00:31:42.610 --> 00:31:44.470
for graphons, in
which case I'm going

00:31:44.470 --> 00:31:46.450
to denote them by W sub n.

00:31:49.830 --> 00:31:57.370
So we say the
sequence is convergent

00:31:57.370 --> 00:32:06.590
if the sequence of
subgraph densities--

00:32:06.590 --> 00:32:08.390
of course, if you are
looking at graphons,

00:32:08.390 --> 00:32:14.600
then you should look at the
graphon, the subgraph density

00:32:14.600 --> 00:32:15.790
in--

00:32:15.790 --> 00:32:20.570
homomorphism density in
graphons if this sequence

00:32:20.570 --> 00:32:41.540
converges as n goes to
infinity for every graph H.

00:32:41.540 --> 00:32:43.460
So that's the
definition of what it

00:32:43.460 --> 00:32:46.220
means for a sequence
of graphs to converge,

00:32:46.220 --> 00:32:48.860
which so far looks actually
quite different from what

00:32:48.860 --> 00:32:50.810
we discussed intuitively.

00:32:50.810 --> 00:32:52.790
But I will state some
theorems towards the end

00:32:52.790 --> 00:32:56.450
of this lecture explaining
what the connections are.

00:32:56.450 --> 00:32:58.538
So intuitively
what I said earlier

00:32:58.538 --> 00:33:00.080
is that you have a
sequence of graphs

00:33:00.080 --> 00:33:05.600
that are convergent if you
have some vague notion of one

00:33:05.600 --> 00:33:09.260
image morphing into a
sequence of images morphing

00:33:09.260 --> 00:33:11.288
into this final image.

00:33:11.288 --> 00:33:12.830
Still hold that
thought in your mind.

00:33:12.830 --> 00:33:15.050
But that's not a
rigorous definition yet.

00:33:15.050 --> 00:33:18.470
The definition we will
use for convergence

00:33:18.470 --> 00:33:20.780
is if all the subgraph--

00:33:20.780 --> 00:33:23.480
all the homomorphism densities
were equivalently subgraph

00:33:23.480 --> 00:33:24.660
densities, they converge.

00:33:27.250 --> 00:33:28.960
So this is the definition.

00:33:28.960 --> 00:33:30.380
It's not required.

00:33:30.380 --> 00:33:33.910
So this is basically
rigorous as stated.

00:33:33.910 --> 00:33:38.890
Just as a remark,
it's not required

00:33:38.890 --> 00:33:46.350
that the number of
vertices goes to infinity,

00:33:46.350 --> 00:33:50.220
although you really should
think that that is the case.

00:33:57.140 --> 00:33:59.000
So just to put it
out there-- so I

00:33:59.000 --> 00:34:00.620
can have a sequence
of constant graphs

00:34:00.620 --> 00:34:02.037
and they will still
be convergent.

00:34:02.037 --> 00:34:03.283
And that's still OK.

00:34:03.283 --> 00:34:04.700
But you should
think of the number

00:34:04.700 --> 00:34:05.980
of vertices going to infinity.

00:34:05.980 --> 00:34:06.874
Yeah?

00:34:06.874 --> 00:34:09.560
AUDIENCE: What is F
in the definition?

00:34:09.560 --> 00:34:10.820
YUFEI ZHAO: F is H. Thank you.

00:34:14.670 --> 00:34:15.868
Any other questions?

00:34:32.380 --> 00:34:35.280
So there are some questions
that we'd like to discuss.

00:34:35.280 --> 00:34:37.980
And this will occupy
the next few lectures

00:34:37.980 --> 00:34:41.080
in terms of proving the
following statements.

00:34:41.080 --> 00:34:44.699
One is do you always
have graph limits?

00:34:44.699 --> 00:34:48.699
If you have a convergent
sequence of graphs,

00:34:48.699 --> 00:34:53.129
do they always approach a limit?

00:34:53.129 --> 00:34:55.440
And just because
something is convergent

00:34:55.440 --> 00:34:58.670
doesn't mean you can represent
the limit necessarily.

00:34:58.670 --> 00:35:01.080
So it turns out
the answer is yes.

00:35:01.080 --> 00:35:03.200
It turns out that--
and this makes

00:35:03.200 --> 00:35:05.400
it a good theory, a
good, useful theory,

00:35:05.400 --> 00:35:07.870
and an easy theory to use,
that there is always a limit

00:35:07.870 --> 00:35:11.530
object whenever you
have convergence.

00:35:11.530 --> 00:35:14.580
And the other
question is while we

00:35:14.580 --> 00:35:18.880
have described intuitively
one notion of convergence

00:35:18.880 --> 00:35:23.440
and also defined more
rigorously another definition

00:35:23.440 --> 00:35:27.370
of convergence, are these
two notions compatible?

00:35:27.370 --> 00:35:29.410
And what does this
even mean, this idea

00:35:29.410 --> 00:35:32.720
of image becoming closer
and closer to a final image?

00:35:32.720 --> 00:35:34.303
What does that even mean?

00:35:34.303 --> 00:35:35.720
So these are some
of the questions

00:35:35.720 --> 00:35:37.290
that I would like to address.

00:35:37.290 --> 00:35:42.440
So in the next few things
that I would like to discuss,

00:35:42.440 --> 00:35:44.870
first, I want to
give you a definition

00:35:44.870 --> 00:35:50.390
of a distance between two
graphons or two graphs.

00:35:50.390 --> 00:35:54.980
If I give you two graphs,
how similar or dissimilar

00:35:54.980 --> 00:35:56.860
are they--

00:35:56.860 --> 00:35:58.770
so that we have this metric.

00:35:58.770 --> 00:36:01.460
And then we can talk about
convergence in metric spaces.

00:36:04.040 --> 00:36:06.150
So let's take a quick break.

00:36:09.610 --> 00:36:11.700
So given this notion
of convergence,

00:36:11.700 --> 00:36:14.620
I would like to define
the notion of distance

00:36:14.620 --> 00:36:18.520
between graphs so that
convergence corresponds

00:36:18.520 --> 00:36:21.220
to convergence in
the metric space

00:36:21.220 --> 00:36:23.980
sense of distance going to 0.

00:36:23.980 --> 00:36:25.240
So how can we define distance?

00:36:36.742 --> 00:36:38.825
First, let me tell you
that there's a trivial way.

00:36:38.825 --> 00:36:41.200
And so there's a way in which
you look at that definition

00:36:41.200 --> 00:36:42.830
and produce a distance out.

00:36:42.830 --> 00:36:45.800
And here's what you can do.

00:36:45.800 --> 00:36:50.000
I can convert that
definition to a metric

00:36:50.000 --> 00:37:00.310
by setting the
distance between two

00:37:00.310 --> 00:37:10.150
graphs G and G prime to be the
following quantity, obtained

00:37:10.150 --> 00:37:11.920
by--

00:37:11.920 --> 00:37:12.920
what would I like to do?

00:37:12.920 --> 00:37:15.350
I would like to say the
distance goes to 0 if and only

00:37:15.350 --> 00:37:21.010
if the homomorphism
densities, they

00:37:21.010 --> 00:37:22.660
are all close to each other.

00:37:22.660 --> 00:37:30.290
And so I can sum up all
the homomorphism densities

00:37:30.290 --> 00:37:37.220
and look at their differences
between G and G prime.

00:37:37.220 --> 00:37:42.980
And I simply enumerate the
list of all possible graphs.

00:37:55.450 --> 00:37:57.950
I want to be just slightly more
careful with this definition

00:37:57.950 --> 00:38:00.390
here because I want
something which--

00:38:00.390 --> 00:38:03.380
so when I write
this, this number

00:38:03.380 --> 00:38:06.770
might be infinite for
all pairs G and G prime.

00:38:06.770 --> 00:38:12.400
So if I just add a scaling
factor here, then--

00:38:12.400 --> 00:38:15.140
and this is some distance.

00:38:15.140 --> 00:38:16.340
So this is some distance.

00:38:16.340 --> 00:38:18.890
And you see that it matches
the definition up there.

00:38:18.890 --> 00:38:21.050
But it's completely useless.

00:38:21.050 --> 00:38:22.040
It might as well--

00:38:22.040 --> 00:38:23.540
might as well not
have said anything

00:38:23.540 --> 00:38:29.080
because it's tautologically the
same as what happened up there.

00:38:29.080 --> 00:38:31.490
And if I give you two
graphs, it doesn't really

00:38:31.490 --> 00:38:32.930
tell you all that
much information

00:38:32.930 --> 00:38:35.360
except to encapsulate
that definition

00:38:35.360 --> 00:38:37.590
into a single number.

00:38:37.590 --> 00:38:38.090
Great.

00:38:38.090 --> 00:38:40.730
So I'm just-- the point of
this is just to tell you

00:38:40.730 --> 00:38:45.470
that there is always a trivial
way to define distance.

00:38:45.470 --> 00:38:49.160
But we want some more
interesting ways.

00:38:49.160 --> 00:38:51.920
So what can we do?

00:38:51.920 --> 00:38:55.310
So here is an attempt, which
is that of an edit distance.

00:38:58.690 --> 00:39:01.460
So we have seen this before when
we discussed removal lemmas.

00:39:01.460 --> 00:39:04.250
The edit distance is
the number of edges

00:39:04.250 --> 00:39:09.780
you need to change to go from
one graph to the other graph.

00:39:09.780 --> 00:39:12.060
And this seems like a pretty
reasonable thing to do.

00:39:12.060 --> 00:39:16.240
And it is an important
quantity for many applications,

00:39:16.240 --> 00:39:19.970
but turns out not the right
one for all application.

00:39:19.970 --> 00:39:21.840
And here is the reason.

00:39:21.840 --> 00:39:25.960
So this is why the
edit distance is--

00:39:25.960 --> 00:39:30.180
by edit distance, I
mean 1 over the number

00:39:30.180 --> 00:39:38.470
of vertex squared times the
number of edge changes needed.

00:39:43.148 --> 00:39:45.440
So there's normalization so
that the distance is always

00:39:45.440 --> 00:39:47.310
between 0 and 1.

00:39:47.310 --> 00:39:48.890
But this is not a
very good notion

00:39:48.890 --> 00:39:51.420
for the following reason.

00:39:51.420 --> 00:39:58.970
If I take two copies of the
Erdos-Reyni random graph G, n,

00:39:58.970 --> 00:40:04.180
1/2, what do you think is
the edit distance between two

00:40:04.180 --> 00:40:05.320
such random graphs?

00:40:10.440 --> 00:40:11.080
How many edges?

00:40:11.080 --> 00:40:11.580
Yeah?

00:40:11.580 --> 00:40:14.045
AUDIENCE: Isn't it roughly
one-half of the number of edges

00:40:14.045 --> 00:40:15.712
because there's like
a one-half probably

00:40:15.712 --> 00:40:21.440
that won't be there or
not be there [INAUDIBLE]??

00:40:21.440 --> 00:40:23.607
YUFEI ZHAO: So yeah, so
let me try to rephrase

00:40:23.607 --> 00:40:24.440
what you are saying.

00:40:24.440 --> 00:40:29.680
So suppose I have
this G and G prime

00:40:29.680 --> 00:40:37.850
both sitting on top
of the vertex set n.

00:40:37.850 --> 00:40:40.790
So if I'm not allowed to
rearrange the vertices,

00:40:40.790 --> 00:40:45.860
how many edge changes do I need
to go from one to the other?

00:40:45.860 --> 00:40:47.700
I need about 1/2.

00:41:01.890 --> 00:41:05.910
So one-half the time, I'm going
to have a wrong edge there.

00:41:05.910 --> 00:41:09.020
Now you can make this
number just slightly smaller

00:41:09.020 --> 00:41:11.330
by permuting the vertices.

00:41:11.330 --> 00:41:13.490
But actually you will
not improve that much.

00:41:13.490 --> 00:41:17.060
It is still going to be roughly
that edit distance, which

00:41:17.060 --> 00:41:18.560
is quite large.

00:41:18.560 --> 00:41:22.520
This is almost as large
as you can possibly get

00:41:22.520 --> 00:41:25.960
between two arbitrary graphs.

00:41:25.960 --> 00:41:30.510
So if we want to say
that random graphs,

00:41:30.510 --> 00:41:33.660
they approach a
limit, a single limit,

00:41:33.660 --> 00:41:36.630
then this is not a very
good notion because they are

00:41:36.630 --> 00:41:38.240
quite far apart for every n.

00:41:41.120 --> 00:41:45.110
So this is the reason why
the more obvious suggestion

00:41:45.110 --> 00:41:49.590
of an edit distance might
not be such a great idea.

00:41:49.590 --> 00:41:51.700
So what should we use instead?

00:41:51.700 --> 00:41:54.650
So we should take
inspiration from what we

00:41:54.650 --> 00:41:56.595
discussed in quasirandomness.

00:41:56.595 --> 00:41:58.416
You have a question.

00:41:58.416 --> 00:42:01.017
AUDIENCE: Is the edit
distance only for two graphs

00:42:01.017 --> 00:42:02.665
of the same vertex set?

00:42:02.665 --> 00:42:05.040
YUFEI ZHAO: So the question
is, is the edit distance only

00:42:05.040 --> 00:42:07.480
for two graphs with
the same vertex set?

00:42:07.480 --> 00:42:08.280
Let's say yes.

00:42:08.280 --> 00:42:10.500
So we'll see later
on, you can also

00:42:10.500 --> 00:42:15.580
compare graphs with
different number of vertices.

00:42:15.580 --> 00:42:18.540
So hold onto that thought.

00:42:18.540 --> 00:42:21.480
So I would like to come up
with a notion of distance

00:42:21.480 --> 00:42:24.660
between graphs that is
inspired by our discussion

00:42:24.660 --> 00:42:28.120
of quasirandomness earlier.

00:42:28.120 --> 00:42:34.940
So think about the discussion of
quasirandomness or quasirandom

00:42:34.940 --> 00:42:35.440
graphs.

00:42:42.920 --> 00:42:56.660
In what sense can G be close
to a constant, let's say p?

00:42:59.300 --> 00:43:03.050
And so this was the
Chung-Graham-Wilson theorem

00:43:03.050 --> 00:43:05.460
that we proved a
few lectures ago.

00:43:05.460 --> 00:43:07.790
So in what sense
can G be close to p?

00:43:07.790 --> 00:43:10.874
And one of those
definitions was discrepancy.

00:43:16.570 --> 00:43:25.840
And discrepancy says that
if the following quantity

00:43:25.840 --> 00:43:31.540
is small for all
subsets x and y, which

00:43:31.540 --> 00:43:34.370
are subsets of vertices of G--

00:43:34.370 --> 00:43:37.390
so you remember, all of
you remember, this part,

00:43:37.390 --> 00:43:41.550
the discrepancy hypothesis
for quasirandomness.

00:43:41.550 --> 00:43:43.240
And this is a kind
of definition that we

00:43:43.240 --> 00:43:45.590
would like to describe
when two graphs are

00:43:45.590 --> 00:43:50.750
similar to each other, when they
are close in this discrepancy

00:43:50.750 --> 00:43:52.270
sense.

00:43:52.270 --> 00:43:56.410
So now, instead of a
graph and a number,

00:43:56.410 --> 00:43:59.280
what if now I have two graphs?

00:43:59.280 --> 00:44:05.820
I'll give you two
graphs of G and G prime.

00:44:05.820 --> 00:44:11.750
And what I would like to
say is that, if for now,

00:44:11.750 --> 00:44:21.490
so if they have the
same vertex set,

00:44:21.490 --> 00:44:32.460
I want to say that there
are close if I have

00:44:32.460 --> 00:44:37.170
that the number of edges
between x and y in G

00:44:37.170 --> 00:44:42.390
is very close to
the number of edges

00:44:42.390 --> 00:44:46.860
between x and y in G prime.

00:44:46.860 --> 00:44:55.245
And I normalize by the
number of vertices squared,

00:44:55.245 --> 00:44:58.040
so n this number of vertices.

00:44:58.040 --> 00:45:01.800
And I would like to find out
the worst possible scenario, so

00:45:01.800 --> 00:45:07.130
overall, x and y subsets
of the vertex set.

00:45:07.130 --> 00:45:12.408
If this quantity
is small, then I

00:45:12.408 --> 00:45:14.950
would like to say that G and G
prime are close to each other.

00:45:18.240 --> 00:45:20.810
So this is inspired by
this discrepancy notion.

00:45:24.850 --> 00:45:28.142
Can you see anything wrong
with this definition here?

00:45:28.142 --> 00:45:28.642
Yeah?

00:45:28.642 --> 00:45:30.620
AUDIENCE: [INAUDIBLE]

00:45:30.620 --> 00:45:33.230
YUFEI ZHAO: So
permutations are vertices.

00:45:33.230 --> 00:45:37.140
So just like in the checkerboard
example we saw earlier,

00:45:37.140 --> 00:45:38.460
you have two graphs.

00:45:38.460 --> 00:45:40.860
And if they are
indeed labeled graphs

00:45:40.860 --> 00:45:43.920
in the same labeled
vertex set, then this

00:45:43.920 --> 00:45:46.470
is the definition more
or less what we used.

00:45:46.470 --> 00:45:48.600
I will define it more
precisely in a second.

00:45:48.600 --> 00:45:51.690
But if they are
unlabeled vertices,

00:45:51.690 --> 00:46:01.480
we need to possibly
optimize permutations

00:46:01.480 --> 00:46:12.470
over rearrangements of vertices,
which actually turns out

00:46:12.470 --> 00:46:13.630
to be quite subtle.

00:46:13.630 --> 00:46:16.160
So I'm going to give precise
definitions in a second.

00:46:16.160 --> 00:46:19.133
But this one here, so think
about permuting vertices.

00:46:19.133 --> 00:46:21.050
But it's actually a bit
more subtle than that.

00:46:26.680 --> 00:46:30.750
So here are some
actual definitions.

00:46:30.750 --> 00:46:34.630
I'm going to define this
quantity called a cut norm.

00:46:39.490 --> 00:46:43.360
So this chapter is all going to
be somewhat functional analytic

00:46:43.360 --> 00:46:44.570
in nature.

00:46:44.570 --> 00:46:47.680
So get used to the
analytic language.

00:46:47.680 --> 00:46:57.560
So the cut norm of W is defined
to be the following quantity

00:46:57.560 --> 00:47:02.510
denoted by this norm with a
box in the subscript, which

00:47:02.510 --> 00:47:04.860
is defined to be--

00:47:04.860 --> 00:47:11.910
if I look at this W, and
I integrate it over a box,

00:47:11.910 --> 00:47:15.420
and I would like to
maximize this quantity here

00:47:15.420 --> 00:47:19.590
over choices of
boxes S and T, they

00:47:19.590 --> 00:47:22.545
are subsets of the interval
measurable subsets.

00:47:27.390 --> 00:47:31.950
So choose your-- so over
all possible choices

00:47:31.950 --> 00:47:36.630
of measurable
subsets S and T, if I

00:47:36.630 --> 00:47:39.630
integrate W over
S cross T, what is

00:47:39.630 --> 00:47:41.760
the furthest I can get from 0?

00:47:47.117 --> 00:47:48.700
So this is the
definition of cut norm.

00:47:48.700 --> 00:47:51.550
And you can already see that
it has some relations to what

00:47:51.550 --> 00:47:53.100
were discussed up there.

00:47:53.100 --> 00:47:54.920
But while we're
talking about norms,

00:47:54.920 --> 00:47:56.690
let me just mention a
few other norms that

00:47:56.690 --> 00:48:00.950
might come up later on when
we discuss graph limits.

00:48:00.950 --> 00:48:03.920
So there will be a lot
of norms throughout.

00:48:03.920 --> 00:48:09.440
So in particular, the lp norm is
going to play a frequent role.

00:48:09.440 --> 00:48:15.170
So lp norm is defined by
looking at the peak norm

00:48:15.170 --> 00:48:17.690
of the absolute value,
integrated and then

00:48:17.690 --> 00:48:20.960
raised to 1 over p.

00:48:20.960 --> 00:48:26.500
And so the infinity norm--

00:48:26.500 --> 00:48:30.610
so this is almost, but not
quite the same as the sup--

00:48:34.940 --> 00:48:37.520
so almost the same
as the supremum,

00:48:37.520 --> 00:48:44.210
but not quite because I need
to ignore subsets of measure 0.

00:48:54.940 --> 00:48:57.760
So I can write down a formal
definition in a second.

00:48:57.760 --> 00:48:59.470
But I need to--

00:48:59.470 --> 00:49:01.447
if I change W on the
subset of the measure 0,

00:49:01.447 --> 00:49:03.030
I shouldn't change
any of these norms.

00:49:03.030 --> 00:49:07.090
And so the one way to define
this essential supremum--

00:49:07.090 --> 00:49:09.630
it's called an essential sup--

00:49:09.630 --> 00:49:14.360
is that it is the largest--

00:49:14.360 --> 00:49:20.920
so it is the smallest
lambda such that--

00:49:20.920 --> 00:49:25.210
so the smallest number
m such that the measure

00:49:25.210 --> 00:49:40.700
of the set taking value bigger
than m this set has measure 0.

00:49:40.700 --> 00:49:46.140
So it's the threshold
above which you--

00:49:46.140 --> 00:49:47.670
this, it has measure 0.

00:49:50.890 --> 00:49:53.983
And the l2 norm will play a
particularly special role.

00:49:53.983 --> 00:49:55.400
And for the l2
norm, you're really

00:49:55.400 --> 00:49:57.350
in the Hilbert
space, in which case

00:49:57.350 --> 00:50:00.770
we are going to
have inner products.

00:50:00.770 --> 00:50:04.380
And we denote inner
products using the square--

00:50:04.380 --> 00:50:06.430
using these brackets.

00:50:06.430 --> 00:50:07.590
So everything is real.

00:50:07.590 --> 00:50:10.231
I don't have to worry
about complex conjugates.

00:50:16.080 --> 00:50:17.910
So comparing with the
discussion up there,

00:50:17.910 --> 00:50:21.535
we see that a sequence of--

00:50:21.535 --> 00:50:32.328
so sequence Gn of
quasirandom graphs

00:50:32.328 --> 00:50:40.050
has a property that the
associated graphons converge

00:50:40.050 --> 00:50:42.470
to p in the cut norm.

00:50:55.610 --> 00:50:58.580
For quasirandom graphs,
there is no issue

00:50:58.580 --> 00:51:02.510
having to do with permutations
because the target is

00:51:02.510 --> 00:51:04.670
invariant upon permutations.

00:51:04.670 --> 00:51:06.410
But if I give you
two different graphs,

00:51:06.410 --> 00:51:09.110
then I need to think
about their permutations.

00:51:09.110 --> 00:51:12.530
And to study
permutations of vertices,

00:51:12.530 --> 00:51:15.290
the right way to do
this is to consider

00:51:15.290 --> 00:51:19.640
measure-preserving
transformations.

00:51:19.640 --> 00:51:27.870
So we say that phi from the
interval to the interval

00:51:27.870 --> 00:51:36.790
is measure-preserving
because first of all,

00:51:36.790 --> 00:51:38.040
it has to be a measurable map.

00:51:38.040 --> 00:51:40.500
And everything I'm going to
talk about are measurable.

00:51:40.500 --> 00:51:43.290
So sometimes I will
even omit mentioning it.

00:51:43.290 --> 00:51:53.480
So it is measure-preserving
if, for all measurable subsets

00:51:53.480 --> 00:52:07.160
A of this interval, one
has that the pullback of A

00:52:07.160 --> 00:52:09.200
has the same
measure as A itself.

00:52:13.720 --> 00:52:15.210
Let me give you an example.

00:52:15.210 --> 00:52:17.710
So you have to be also slightly
careful with this definition

00:52:17.710 --> 00:52:22.090
if you think about the
pushforward that's false.

00:52:22.090 --> 00:52:23.650
It has to be the pullback.

00:52:23.650 --> 00:52:30.930
So for example, the
map which sends--

00:52:30.930 --> 00:52:39.860
so an easy example, the map
which sends x to x plus 1/2--

00:52:39.860 --> 00:52:43.720
so think about a
circle as your space.

00:52:43.720 --> 00:52:47.900
And here I am just rotating the
circle by one-half rotation.

00:52:47.900 --> 00:52:49.625
So it's obviously
measure-preserving.

00:52:49.625 --> 00:52:53.350
I am not changing any measures.

00:52:53.350 --> 00:52:56.020
Slightly more
interesting example,

00:52:56.020 --> 00:53:00.340
quite a bit more interesting
example is setting x to 2x.

00:53:00.340 --> 00:53:02.350
This is also measure-preserving.

00:53:02.350 --> 00:53:04.940
And you might be
puzzled for a second why

00:53:04.940 --> 00:53:07.680
it's measure-preserving because
it sounds like it's dilating

00:53:07.680 --> 00:53:10.262
everything by a factor of 2.

00:53:10.262 --> 00:53:12.220
But if you look at the
definition-- and so here

00:53:12.220 --> 00:53:15.660
is again mod 1.

00:53:15.660 --> 00:53:25.790
If you look at the
definition, if you look at,

00:53:25.790 --> 00:53:32.300
let's say, a subset
A, which is--

00:53:35.020 --> 00:53:36.770
so what should I think?

00:53:40.018 --> 00:53:46.660
For example, so if that is my
A, so what's the inverse of A?

00:53:51.460 --> 00:53:54.250
So it's this set.

00:53:59.420 --> 00:54:03.105
So the measure is preserved
upon this pullback.

00:54:03.105 --> 00:54:05.360
And so if you
pushforward, then you

00:54:05.360 --> 00:54:06.680
might dilate by a factor of 2.

00:54:06.680 --> 00:54:08.857
But when you pullback, the
measure gets preserved.

00:54:15.400 --> 00:54:17.920
So these measure-preserving
transformations

00:54:17.920 --> 00:54:23.260
are going to play role of
permutations of vertices.

00:54:23.260 --> 00:54:25.900
So it turns out that these
things are actually-- they

00:54:25.900 --> 00:54:28.420
are quite subtle technically.

00:54:28.420 --> 00:54:31.510
And I am going to,
as much as I can,

00:54:31.510 --> 00:54:35.340
ignore some of the measure
theoretic technicalities.

00:54:35.340 --> 00:54:37.690
But they are quite subtle.

00:54:37.690 --> 00:54:40.180
So for example, so
now let me give you

00:54:40.180 --> 00:54:45.240
a definition for the distance
between two graphons.

00:54:45.240 --> 00:54:51.290
I write, starting with a
symmetric measurable function

00:54:51.290 --> 00:55:07.290
W, so I write W superscript phi
to denote the function obtained

00:55:07.290 --> 00:55:07.980
as follows.

00:55:07.980 --> 00:55:10.770
So I think of this as relabeling
the vertices of a graph.

00:55:14.480 --> 00:55:20.390
And now I define this distance.

00:55:20.390 --> 00:55:31.680
So this is going to be called
the cut distance between two

00:55:31.680 --> 00:55:34.860
symmetric measurable
functions, U and W,

00:55:34.860 --> 00:55:48.520
to be the infimum over all
measure-preserving bijections.

00:56:02.310 --> 00:56:04.340
So this is the definition
for the distance

00:56:04.340 --> 00:56:05.410
between two graphons.

00:56:09.648 --> 00:56:13.020
To take the optimal--

00:56:13.020 --> 00:56:15.440
and my question does it--

00:56:15.440 --> 00:56:16.930
I am looking at nth.

00:56:16.930 --> 00:56:18.700
So I haven't told
you yet whether you

00:56:18.700 --> 00:56:19.710
can take a single one.

00:56:19.710 --> 00:56:21.335
And it turns out
that's a subtle issue.

00:56:21.335 --> 00:56:23.050
And generally it doesn't exist.

00:56:23.050 --> 00:56:26.320
But I look over all
measure-preserving bijections

00:56:26.320 --> 00:56:27.010
phi.

00:56:27.010 --> 00:56:31.450
And I look at the
distance between W and Wv,

00:56:31.450 --> 00:56:35.061
optimized over the best possible
measure-preserving bijection.

00:56:39.680 --> 00:56:41.571
So this nth is really an nth.

00:56:45.128 --> 00:56:46.170
It's not always obtained.

00:56:49.270 --> 00:56:54.210
And actually, this example
here is a great example for--

00:56:54.210 --> 00:56:58.360
you can create an example for
why nth is not always obtained

00:56:58.360 --> 00:57:00.580
from the discussion over here.

00:57:00.580 --> 00:57:08.390
For example, if U is
the function x times y,

00:57:08.390 --> 00:57:18.020
this is a graphon
xy and W is Uv,

00:57:18.020 --> 00:57:28.793
where v is the map distance
x to 2x, then in your mind,

00:57:28.793 --> 00:57:31.210
you should think of these two
as really the same graphons.

00:57:31.210 --> 00:57:32.410
You are applying the
measure-preserving

00:57:32.410 --> 00:57:33.035
transformation.

00:57:33.035 --> 00:57:35.010
It's like doing a permutation.

00:57:35.010 --> 00:57:39.550
But because phi
is not bijective,

00:57:39.550 --> 00:57:43.980
you cannot putting phi here
to get these two things to be

00:57:43.980 --> 00:57:44.480
the same.

00:57:48.812 --> 00:57:50.020
So there are some subtleties.

00:57:50.020 --> 00:57:52.330
So this is really
an example just

00:57:52.330 --> 00:57:54.370
to highlight there's
some subtleties here,

00:57:54.370 --> 00:57:58.420
which I am going to try to
ignore as much as possible.

00:57:58.420 --> 00:58:02.067
But I will always give
you correct definitions.

00:58:02.067 --> 00:58:02.650
Any questions?

00:58:02.650 --> 00:58:03.150
Yeah?

00:58:06.533 --> 00:58:07.033
Yeah?

00:58:07.033 --> 00:58:09.575
AUDIENCE: So can we expect the
cut distance between these two

00:58:09.575 --> 00:58:10.733
sets to be 0 [INAUDIBLE]?

00:58:10.733 --> 00:58:12.150
YUFEI ZHAO: So the
question, do we

00:58:12.150 --> 00:58:14.640
expect the cut distance
between these two to be 0?

00:58:14.640 --> 00:58:16.280
And the answer is yes.

00:58:16.280 --> 00:58:19.510
So we do expect them to be 0.

00:58:19.510 --> 00:58:22.100
And they are 0.

00:58:22.100 --> 00:58:24.620
They are equal to 0.

00:58:24.620 --> 00:58:27.890
And let me just tell you
one something that is new.

00:58:27.890 --> 00:58:31.190
And this is one of
those statements

00:58:31.190 --> 00:58:34.250
that has a lot of measure
theoretic technicalities.

00:58:34.250 --> 00:58:42.650
For all graphons U and W, it
turns out that there exist

00:58:42.650 --> 00:58:43.940
measure-preserving maps--

00:58:52.020 --> 00:58:57.460
so not necessarily bijections,
but measure-preserving maps

00:58:57.460 --> 00:59:02.720
from 0, 1 interval to itself,
such that the distance between

00:59:02.720 --> 00:59:09.530
U and W, the cut distance,
is obtained by the cut norm

00:59:09.530 --> 00:59:10.730
difference between--

00:59:10.730 --> 00:59:14.600
the difference between
U phi and W psi.

00:59:27.314 --> 00:59:29.770
So don't worry about it.

00:59:40.840 --> 00:59:43.500
So far, we have defined
this notion of a cut

00:59:43.500 --> 00:59:47.170
distance between two graphons.

00:59:47.170 --> 00:59:49.750
But now I'll give
you two graphs.

00:59:49.750 --> 00:59:53.855
So what do you do
for two graphs?

00:59:53.855 --> 00:59:55.396
Or I can-- yeah?

00:59:55.396 --> 00:59:57.438
AUDIENCE: You can take
the graphon associated it.

00:59:57.438 --> 00:59:58.188
YUFEI ZHAO: Great.

00:59:58.188 --> 01:00:00.550
So take the graphon
associated with these graphs

01:00:00.550 --> 01:00:03.020
and consider their cut distance.

01:00:03.020 --> 01:00:12.310
So for graphs G and G
prime, and potentially even

01:00:12.310 --> 01:00:19.570
a different number
of vertices, I

01:00:19.570 --> 01:00:25.540
can define the distance, the
cut distance between these two

01:00:25.540 --> 01:00:34.650
graphs to be the distance
between the associated

01:00:34.650 --> 01:00:35.150
graphons.

01:00:40.100 --> 01:00:47.338
And similarly, if I have
a graph and a graphon,

01:00:47.338 --> 01:00:48.755
I can also compare
their distance.

01:01:01.660 --> 01:01:03.340
So what does this actually mean?

01:01:03.340 --> 01:01:05.190
So if I give you
two graphs, even

01:01:05.190 --> 01:01:08.240
with the same
number of vertices,

01:01:08.240 --> 01:01:12.620
it's not quite the same thing
as a permutation of vertices.

01:01:12.620 --> 01:01:14.930
It's a bit more subtle.

01:01:14.930 --> 01:01:17.520
Now why is it more subtle than
just permuting the vertices?

01:01:20.650 --> 01:01:23.180
So here we are using
measure-preserving

01:01:23.180 --> 01:01:27.020
transformations, which doesn't
see your atomic vertices.

01:01:27.020 --> 01:01:30.120
So we might split
up your vertices.

01:01:30.120 --> 01:01:33.510
So you might take a
vertex and chop it in half

01:01:33.510 --> 01:01:35.880
and send one half
somewhere and another half

01:01:35.880 --> 01:01:38.080
somewhere else
because these guys,

01:01:38.080 --> 01:01:41.060
they don't care about
your vertices anymore.

01:01:41.060 --> 01:01:47.130
So it's not quite the same
as permuting vertices.

01:01:52.450 --> 01:01:54.010
But it's some kind of--

01:01:54.010 --> 01:01:59.530
so you allow some kind of
splitting and rearrangement

01:01:59.530 --> 01:02:01.760
and overlays.

01:02:01.760 --> 01:02:04.510
So you can write
out this distance

01:02:04.510 --> 01:02:08.230
in this format, find out the
best way to split and overlay

01:02:08.230 --> 01:02:09.760
and to rearrange that way.

01:02:09.760 --> 01:02:13.250
But it's much cleaner to
define it in terms of graphons.

01:02:13.250 --> 01:02:13.750
Yes?

01:02:13.750 --> 01:02:15.792
AUDIENCE: Is this why we
take bijections up there

01:02:15.792 --> 01:02:16.972
[INAUDIBLE]?

01:02:16.972 --> 01:02:19.430
YUFEI ZHAO: The question is,
is that why we take bijections

01:02:19.430 --> 01:02:19.930
up there?

01:02:19.930 --> 01:02:22.370
And no, so up there,
if I wrote instead

01:02:22.370 --> 01:02:26.270
measure-preserving maps, it's
still a correct definition

01:02:26.270 --> 01:02:28.740
and it's the same definition.

01:02:28.740 --> 01:02:30.930
And the fact that these
two are equivalent

01:02:30.930 --> 01:02:35.660
goes to some measure
theory, which I will not--

01:02:35.660 --> 01:02:44.295
do not want to indulge yo Great.

01:02:44.295 --> 01:02:46.420
But the moral of the story
is you take two graphons

01:02:46.420 --> 01:02:50.620
and rearrange the vertices
in some way, in the best way,

01:02:50.620 --> 01:02:53.603
overlay them on top of each
other and take the difference

01:02:53.603 --> 01:02:54.645
and look at the cut norm.

01:02:54.645 --> 01:02:55.810
And so that's the distance.

01:02:59.180 --> 01:03:04.570
So I want to finish by
stating the main theorems

01:03:04.570 --> 01:03:07.890
that form graph limit theory.

01:03:07.890 --> 01:03:10.620
And these address the
questions I mentioned right

01:03:10.620 --> 01:03:11.940
before the break.

01:03:11.940 --> 01:03:14.960
So do there exist limits?

01:03:14.960 --> 01:03:18.620
And do these two
different notions of one

01:03:18.620 --> 01:03:20.900
having to do with
distance and another

01:03:20.900 --> 01:03:24.410
having to do with
homomorphism densities,

01:03:24.410 --> 01:03:26.400
how do they relate
to each other?

01:03:26.400 --> 01:03:27.590
Are they consistent?

01:03:50.070 --> 01:03:54.420
So the first theorem,
Theorem 1, has

01:03:54.420 --> 01:04:07.480
to do with the equivalence
of the convergence,

01:04:07.480 --> 01:04:22.800
namely, that if you have a
sequence of graphs or graphons,

01:04:22.800 --> 01:04:30.540
the sequence is convergent
in the sense, up there, if

01:04:30.540 --> 01:04:38.380
and only if they are
convergent in the sense

01:04:38.380 --> 01:04:40.440
of-- in this metric space.

01:04:40.440 --> 01:04:43.360
So remember what convergence
means in the metric space

01:04:43.360 --> 01:04:46.310
is that of a Cauchy sequence--

01:04:46.310 --> 01:04:55.600
so if and only if it is a
Cauchy sequence with respect

01:04:55.600 --> 01:05:02.470
to this cut distance.

01:05:02.470 --> 01:05:05.360
So it's just-- maybe
for many of you,

01:05:05.360 --> 01:05:07.140
it's been a while
since you took 18-100.

01:05:07.140 --> 01:05:10.800
So let remind you a Cauchy
sequence, in this case,

01:05:10.800 --> 01:05:20.310
it means that, if I look at the
distance between two graphs,

01:05:20.310 --> 01:05:22.920
if I look far enough
out, then I can

01:05:22.920 --> 01:05:28.500
contain the rest of the sequence
in an arbitrarily small ball.

01:05:28.500 --> 01:05:33.150
So the sup positive
m of this guy here,

01:05:33.150 --> 01:05:37.986
goes to 0 as n goes to infinity.

01:05:37.986 --> 01:05:42.173
But because we don't know
yet whether the limit exists,

01:05:42.173 --> 01:05:44.340
so I can't talk about them
getting closer and closer

01:05:44.340 --> 01:05:45.060
to a limit.

01:05:45.060 --> 01:05:47.110
But they mutually get
closer to each other.

01:05:50.320 --> 01:05:54.350
So Theorem 1 tells us that
these two notions, one having

01:05:54.350 --> 01:05:57.980
to do with homomorphism
densities, is consistent

01:05:57.980 --> 01:06:02.360
and in fact equivalent
to the appropriate notion

01:06:02.360 --> 01:06:03.450
in the metric space.

01:06:11.200 --> 01:06:16.380
So let's use a symbol.

01:06:16.380 --> 01:06:27.250
So we say that G sub n
converges to W, or in the case

01:06:27.250 --> 01:06:29.020
of a sequence of graphons.

01:06:29.020 --> 01:06:30.520
So we can do that as well.

01:06:34.630 --> 01:06:43.440
So here we say that G
sub n converges with W,

01:06:43.440 --> 01:06:48.960
if whenever you look at
the F density in G sub n,

01:06:48.960 --> 01:06:56.610
this sequence converges to the
corresponding f density in W

01:06:56.610 --> 01:07:01.360
for every f, and
similarly, if you have

01:07:01.360 --> 01:07:04.480
a graphon instead of a graph.

01:07:04.480 --> 01:07:08.160
So that definition was
just whether a sequence

01:07:08.160 --> 01:07:09.140
is convergent.

01:07:09.140 --> 01:07:16.190
Here it converges
to this graphon W.

01:07:16.190 --> 01:07:21.380
And the question is, if you
give me a convergent sequence,

01:07:21.380 --> 01:07:23.150
is there a limit?

01:07:23.150 --> 01:07:25.560
Does it converge to some limit?

01:07:25.560 --> 01:07:27.430
And the answer is yes.

01:07:27.430 --> 01:07:34.070
And that's the
second theorem, which

01:07:34.070 --> 01:07:37.840
tells us the existence of a
limit, of the limit object.

01:07:41.990 --> 01:07:49.830
So the statement is that
every convergent sequence

01:07:49.830 --> 01:08:04.650
of graph or graphons
has a limit graphon.

01:08:18.346 --> 01:08:22.930
So now I want you to imagine
this space of graphons.

01:08:22.930 --> 01:08:25.960
So we'll have this space
containing all the graphons.

01:08:25.960 --> 01:08:30.149
And let me denote this
space by this curly W0.

01:08:30.149 --> 01:08:32.279
So this, the 0 is--
don't worry about it.

01:08:32.279 --> 01:08:34.080
It's more just convention.

01:08:34.080 --> 01:08:37.600
But let me also put a tilde on
top for the following reason.

01:08:37.600 --> 01:08:51.630
Let this be the space of
graphons where we identify

01:08:51.630 --> 01:08:53.250
graphons with distance 0.

01:09:09.640 --> 01:09:17.290
So then the space combined with
this metric is a metric space.

01:09:22.149 --> 01:09:23.380
It is the space of graphons.

01:09:26.060 --> 01:09:32.859
And so the third
theorem is that it's

01:09:32.859 --> 01:09:35.529
the compactness of
the space of graphons,

01:09:35.529 --> 01:09:37.345
namely, that this
space is compact.

01:09:50.970 --> 01:09:53.819
Because we're in
the metric space,

01:09:53.819 --> 01:09:59.340
compactness in the usual
sense of every open cover has

01:09:59.340 --> 01:10:04.260
a finite subcover is equivalent
to the slightly more intuitive

01:10:04.260 --> 01:10:06.550
notion of sequential
compactness--

01:10:06.550 --> 01:10:10.170
every sequence has a
convergent subsequence.

01:10:10.170 --> 01:10:13.530
And then it's also,
if you have a limit,

01:10:13.530 --> 01:10:15.710
so it converges to some limit.

01:10:18.330 --> 01:10:20.670
So how should you
think of Theorem 3?

01:10:20.670 --> 01:10:24.720
So it's about compactness
and some tautological notion.

01:10:24.720 --> 01:10:27.720
But intuitively, you
should think of compactness

01:10:27.720 --> 01:10:29.170
as saying--

01:10:29.170 --> 01:10:31.050
and the English word,
the English meaning

01:10:31.050 --> 01:10:33.330
of the word compact is small.

01:10:33.330 --> 01:10:37.050
You should think of this
space as being quite small,

01:10:37.050 --> 01:10:40.140
which is rather
counterintuitive because we're

01:10:40.140 --> 01:10:43.970
looking at the space of
graphons, certainly at least as

01:10:43.970 --> 01:10:48.150
large as the space of graphs,
but really all functions

01:10:48.150 --> 01:10:49.875
from the square to the interval.

01:10:49.875 --> 01:10:53.010
This seems like a
pretty large space.

01:10:53.010 --> 01:10:55.290
But this theorem here says
that, in fact, that space

01:10:55.290 --> 01:10:57.950
is quite small.

01:10:57.950 --> 01:11:01.880
And where have we also seen
that philosophy before?

01:11:06.120 --> 01:11:09.920
So in Szemeredi's
Graph Regularity Lemma,

01:11:09.920 --> 01:11:12.350
the underlying
philosophy there is

01:11:12.350 --> 01:11:15.560
that, even though the
possibilities, the space

01:11:15.560 --> 01:11:19.400
of possibilities for
graph is quite large,

01:11:19.400 --> 01:11:22.360
once you apply Szemeredi's
Regularity Lemma,

01:11:22.360 --> 01:11:26.400
and once you are OK with
some epsilon approximations,

01:11:26.400 --> 01:11:29.510
there is only a
small description,

01:11:29.510 --> 01:11:32.280
this bounded
description, of a graph.

01:11:32.280 --> 01:11:35.060
And you can work with
that description.

01:11:35.060 --> 01:11:37.820
And these two philosophies,
it's no coincidence

01:11:37.820 --> 01:11:41.270
that they are consistent
with each other

01:11:41.270 --> 01:11:46.790
because we will use Szemeredi's
Regularity Lemma to prove

01:11:46.790 --> 01:11:49.710
this compactness.

01:11:49.710 --> 01:11:51.900
In fact, we will use a
slightly weaker version

01:11:51.900 --> 01:11:55.560
of Szemeredi's Regularity
Lemma to prove compactness.

01:11:55.560 --> 01:11:58.890
And then you will see
that, from the compactness,

01:11:58.890 --> 01:12:03.010
one can use properties
of the compactness

01:12:03.010 --> 01:12:07.750
to boost to a stronger
version of regularity.

01:12:07.750 --> 01:12:09.890
But the underlying
philosophy here

01:12:09.890 --> 01:12:19.570
is that this compactness is
in some sense a quantity.

01:12:19.570 --> 01:12:29.530
It's a qualitative
reformulation,

01:12:29.530 --> 01:12:38.770
analytic reformulation of
Szemeredi's Graph Regularity

01:12:38.770 --> 01:12:39.270
Lemma.

01:12:51.030 --> 01:12:53.780
OK, so--

01:12:53.780 --> 01:12:55.560
So this topic,
this graph limits,

01:12:55.560 --> 01:12:57.840
which we'll explore for
the next few lecturers,

01:12:57.840 --> 01:13:01.620
including giving a proof of all
three of these main theorems,

01:13:01.620 --> 01:13:05.250
nicely encapsulates the past
couple of topics we have done.

01:13:05.250 --> 01:13:08.040
So on one hand, Szemeredi's
Regularity Lemma,

01:13:08.040 --> 01:13:09.930
or some version of
that, will be used

01:13:09.930 --> 01:13:14.520
in proving the existence of the
limit and also the compactness.

01:13:14.520 --> 01:13:17.370
And also it's philosophically
and in some sense

01:13:17.370 --> 01:13:19.830
related and very much
equivalent in some sense

01:13:19.830 --> 01:13:22.830
and related to these notions.

01:13:22.830 --> 01:13:25.710
It is also related
to quasirandomness--

01:13:25.710 --> 01:13:28.320
in particular,
quasirandom graphs

01:13:28.320 --> 01:13:31.680
that we did a few lectures ago,
where in quasirandom graphs,

01:13:31.680 --> 01:13:35.030
we are really looking
at the constant graphon

01:13:35.030 --> 01:13:36.890
in this language.

01:13:36.890 --> 01:13:39.340
And now we expand our horizons.

01:13:39.340 --> 01:13:43.570
And instead of just looking
at the constant graphon,

01:13:43.570 --> 01:13:47.840
we can now consider
arbitrary graphons.

01:13:47.840 --> 01:13:51.970
They are also this model
for a very large graph.

01:13:54.530 --> 01:13:56.110
Any questions?

01:13:56.110 --> 01:13:56.610
Yeah?

01:13:56.610 --> 01:14:00.530
AUDIENCE: Can we prove the
theorem analytically and then

01:14:00.530 --> 01:14:02.490
deduce the Regularity
Lemma with it?

01:14:02.490 --> 01:14:03.950
YUFEI ZHAO: The
question is, can we

01:14:03.950 --> 01:14:07.520
prove Theorem 3 analytically
and deduce the Regularity Lemma?

01:14:07.520 --> 01:14:09.990
So you will see once
you see the proof.

01:14:09.990 --> 01:14:11.600
It depends on what you mean.

01:14:11.600 --> 01:14:12.990
But roughly, the answer is yes.

01:14:12.990 --> 01:14:14.840
But there's a very
important caveat.

01:14:14.840 --> 01:14:17.930
It's that, because we
are using compactness,

01:14:17.930 --> 01:14:20.390
any argument
involving compactness

01:14:20.390 --> 01:14:23.750
gives no quantitative bounds.

01:14:23.750 --> 01:14:28.940
So you will have a proof of
the Szemeredi Regularity Lemma

01:14:28.940 --> 01:14:33.020
that tells you there is
a bound for each epsilon.

01:14:33.020 --> 01:14:37.305
But it doesn't tell
you what the bound is.

01:14:37.305 --> 01:14:38.778
Yeah?

01:14:38.778 --> 01:14:40.742
AUDIENCE: Doesn't
Theorem 3 imply Theorem 1

01:14:40.742 --> 01:14:43.197
because of the [INAUDIBLE]?

01:14:46.150 --> 01:14:48.280
YUFEI ZHAO: Does Code
Theorem 3 imply Theorem 1?

01:14:48.280 --> 01:14:50.800
And the answer is no
because in Theorem 1,

01:14:50.800 --> 01:14:55.800
the notion of convergence is
about homomorphism densities.

01:14:55.800 --> 01:14:59.000
So Theorem 1 is about
these two different notions

01:14:59.000 --> 01:15:02.730
of convergence and that they
are equivalent to each other.

01:15:02.730 --> 01:15:05.420
Theorem 3 is just
about the metric.

01:15:05.420 --> 01:15:07.506
It's about the cut metric.

01:15:07.506 --> 01:15:10.340
And so Theorem 1 is-- the point
of Theorem 1 is that you have

01:15:10.340 --> 01:15:11.470
these two--

01:15:11.470 --> 01:15:13.670
you have these two
notions of convergence,

01:15:13.670 --> 01:15:16.340
one having to do with subgraph
densities and the other

01:15:16.340 --> 01:15:18.052
having to do with
a cut distance.

01:15:18.052 --> 01:15:19.760
And in fact, they are
equivalent notions.

01:15:23.140 --> 01:15:24.870
So all great questions--

01:15:24.870 --> 01:15:25.959
any others?

01:15:25.959 --> 01:15:27.875
AUDIENCE: And for
that F, is F a graphon

01:15:27.875 --> 01:15:30.270
because the [INAUDIBLE]?

01:15:30.270 --> 01:15:32.838
Is F a graphon or a graph?

01:15:32.838 --> 01:15:35.130
YUFEI ZHAO: The question is,
is F a graph or a graphon?

01:15:35.130 --> 01:15:37.300
F is always a graph.

01:15:37.300 --> 01:15:45.880
So in t F, W, I do not define
this quantity for graphon F.

01:15:45.880 --> 01:15:48.000
So this quantity
here, I have only

01:15:48.000 --> 01:15:50.580
allowed the second
argument to be a graphon.

01:15:50.580 --> 01:15:52.740
The first argument is not
allowed to be a graphon.

01:15:52.740 --> 01:15:53.657
It doesn't make sense.

01:15:56.277 --> 01:15:56.777
Yeah?

01:15:56.777 --> 01:16:00.810
AUDIENCE: Doesn't Theorem 1
and 2 together imply Theorem 3?

01:16:00.810 --> 01:16:03.310
YUFEI ZHAO: The question is,
doesn't Theorem 1 and Theorem 2

01:16:03.310 --> 01:16:05.240
together imply Theorem 3?

01:16:05.240 --> 01:16:07.780
So first of all,
Theorem 1 is really--

01:16:07.780 --> 01:16:10.090
it's not about compactness.

01:16:10.090 --> 01:16:11.590
So it's really about
the equivalence

01:16:11.590 --> 01:16:13.452
of two different
notions of convergence.

01:16:13.452 --> 01:16:15.160
It's like you have
two different metrics.

01:16:15.160 --> 01:16:16.618
I am showing that
these two metrics

01:16:16.618 --> 01:16:18.300
are equivalent to each other.

01:16:18.300 --> 01:16:21.430
Theorem 2 and Theorem 3 are
quite intimately related.

01:16:21.430 --> 01:16:22.530
So Theorem 2 is about--

01:16:25.590 --> 01:16:27.788
Theorem 2, so they
are quite related.

01:16:27.788 --> 01:16:29.080
But they're not quite the same.

01:16:29.080 --> 01:16:31.290
So let me just give you
the real line analogy,

01:16:31.290 --> 01:16:33.140
going back to what we
said in the beginning.

01:16:33.140 --> 01:16:37.410
So Theorem 2 is kind of like
saying that the real numbers is

01:16:37.410 --> 01:16:38.490
complete.

01:16:38.490 --> 01:16:40.870
Every convergent
sequence has a limit,

01:16:40.870 --> 01:16:43.060
whereas Theorem 3
is more than that.

01:16:43.060 --> 01:16:45.295
It's also bounded in some sense.

01:16:45.295 --> 01:16:48.118
But here, there is
no notion of bounded.

01:16:48.118 --> 01:16:48.660
It's compact.

01:16:51.570 --> 01:16:54.390
But the main-- you
should think of these two

01:16:54.390 --> 01:16:55.890
are very much related
to each other.

01:16:55.890 --> 01:16:59.347
But here it's-- but
they are not equivalent.

01:17:02.647 --> 01:17:03.230
Anything else?

01:17:05.910 --> 01:17:06.410
Great.

01:17:06.410 --> 01:17:08.410
So that's all for today.