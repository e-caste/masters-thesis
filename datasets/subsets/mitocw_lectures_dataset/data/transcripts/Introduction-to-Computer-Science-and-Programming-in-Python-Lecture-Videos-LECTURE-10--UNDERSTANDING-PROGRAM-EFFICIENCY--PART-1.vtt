WEBVTT

00:00:00.790 --> 00:00:03.190
The following content is
provided under a Creative

00:00:03.190 --> 00:00:04.730
Commons license.

00:00:04.730 --> 00:00:07.030
Your support will help
MIT OpenCourseWare

00:00:07.030 --> 00:00:11.390
continue to offer high quality
educational resources for free.

00:00:11.390 --> 00:00:13.990
To make a donation or
view additional materials

00:00:13.990 --> 00:00:16.450
from hundreds of
MIT courses, visit

00:00:16.450 --> 00:00:18.480
MITOpenCourseWare@ocw.mit.edu.

00:00:24.840 --> 00:00:27.550
PROFESSOR: OK, folks.

00:00:27.550 --> 00:00:28.930
Welcome back.

00:00:28.930 --> 00:00:31.000
Hope you had a nice long
weekend with no classes.

00:00:31.000 --> 00:00:33.000
You got caught up on all
those problem sets that

00:00:33.000 --> 00:00:34.550
have been sneaking up on you.

00:00:34.550 --> 00:00:36.540
You enjoyed watching
the Patriots and Tom

00:00:36.540 --> 00:00:37.320
Brady come back.

00:00:37.320 --> 00:00:40.720
Oh, sorry, I'm
showing my local bias.

00:00:40.720 --> 00:00:44.030
Before we talk
about today's topic,

00:00:44.030 --> 00:00:45.940
I want to take a second
to set the stage.

00:00:45.940 --> 00:00:48.440
And I want you to stop and
think about what you've

00:00:48.440 --> 00:00:50.322
seen so far in this course.

00:00:50.322 --> 00:00:52.280
We're coming up on the
end of the first section

00:00:52.280 --> 00:00:53.120
of the course.

00:00:53.120 --> 00:00:55.100
And you've already seen a lot.

00:00:55.100 --> 00:00:58.120
You've certainly learned about
fundamentals of computation.

00:00:58.120 --> 00:01:00.760
You've seen different
kinds of data structures,

00:01:00.760 --> 00:01:04.180
both mutable and immutable, so
tuples and lists, dictionaries,

00:01:04.180 --> 00:01:07.150
different ways of
pulling things together.

00:01:07.150 --> 00:01:08.530
You've seen a
range of algorithms

00:01:08.530 --> 00:01:12.730
from simple linear code
to loops, fors and whiles.

00:01:12.730 --> 00:01:14.380
You've seen
iterative algorithms.

00:01:14.380 --> 00:01:16.150
You've seen
recursive algorithms.

00:01:16.150 --> 00:01:19.630
You've seen classes
of algorithms.

00:01:19.630 --> 00:01:21.280
Divide and conquer.

00:01:21.280 --> 00:01:22.630
Greedy algorithms.

00:01:22.630 --> 00:01:24.490
Bisection search.

00:01:24.490 --> 00:01:25.270
A range of things.

00:01:25.270 --> 00:01:27.520
And then most recently,
you start pulling things

00:01:27.520 --> 00:01:30.430
together with classes--
a way to group together

00:01:30.430 --> 00:01:34.570
data that belongs together along
with methods or procedures that

00:01:34.570 --> 00:01:37.120
are designed to
manipulate that data.

00:01:37.120 --> 00:01:39.520
So you've had actually
a fairly good coverage

00:01:39.520 --> 00:01:43.181
already of a lot of the
fundamentals of computation.

00:01:43.181 --> 00:01:44.680
And you're starting
to get geared up

00:01:44.680 --> 00:01:49.180
to be able to tackle a pretty
interesting range of problems.

00:01:49.180 --> 00:01:51.880
Today and Monday,
we're going to take

00:01:51.880 --> 00:01:54.280
a little bit of a different
look at computation.

00:01:54.280 --> 00:01:57.280
Because now that you've got
the tools to start building up

00:01:57.280 --> 00:02:01.390
your own personal
armamentarium of tools,

00:02:01.390 --> 00:02:04.330
we'd like to ask a couple
of important questions.

00:02:04.330 --> 00:02:09.840
The primary one of which is how
efficient are my algorithms?

00:02:09.840 --> 00:02:13.060
And by efficiency, we'll see it
refers both to space and time,

00:02:13.060 --> 00:02:14.610
but primarily to time.

00:02:14.610 --> 00:02:18.460
And we'd like to know both how
fast are my algorithms going

00:02:18.460 --> 00:02:21.771
to run and how could I reason
about past performance.

00:02:21.771 --> 00:02:24.020
And that's what we're going
to do with today's topics.

00:02:24.020 --> 00:02:26.020
We're going to talk
about orders of growth.

00:02:26.020 --> 00:02:27.980
We'll define what that
means in a few minutes.

00:02:27.980 --> 00:02:30.970
We're going to talk about what's
called the big O notation.

00:02:30.970 --> 00:02:33.730
And we're going to begin to
explore different classes

00:02:33.730 --> 00:02:36.160
of algorithms.

00:02:36.160 --> 00:02:39.685
Before we do that though,
let's talk about why.

00:02:39.685 --> 00:02:43.740
And I want to suggest to you
there are two reasons this

00:02:43.740 --> 00:02:47.230
is important to be considering.

00:02:47.230 --> 00:02:49.930
First question is how could
we reason about an algorithm

00:02:49.930 --> 00:02:53.950
something you write in order
to predict how much time is

00:02:53.950 --> 00:02:57.920
it going to need to solve a
problem of a particular size?

00:02:57.920 --> 00:03:00.960
I might be testing my code
on small scale examples.

00:03:00.960 --> 00:03:03.390
And I want to know if I'd run
it on a really big one, how

00:03:03.390 --> 00:03:04.431
long is it going to take?

00:03:04.431 --> 00:03:05.940
Can I predict that?

00:03:05.940 --> 00:03:09.030
Can I make guesses
about how much time

00:03:09.030 --> 00:03:11.170
I'm going to need to
solve this problem?

00:03:11.170 --> 00:03:12.750
Especially if it's
in a real world

00:03:12.750 --> 00:03:16.120
circumstance where time
is going to be crucial.

00:03:16.120 --> 00:03:20.110
Equally important is
going the other direction.

00:03:20.110 --> 00:03:23.380
We want you to begin to
reason about the algorithms

00:03:23.380 --> 00:03:25.480
you write to be
able to say how do

00:03:25.480 --> 00:03:30.040
certain choices in a design
of an algorithm influence

00:03:30.040 --> 00:03:32.050
how much time it's
going to take.

00:03:32.050 --> 00:03:33.602
If I choose to do
this recursively,

00:03:33.602 --> 00:03:35.560
is that going to be
different than iteratively?

00:03:35.560 --> 00:03:38.620
If I choose to do this with a
particular kind of structure

00:03:38.620 --> 00:03:41.890
in my algorithm, what does
that say about the amount

00:03:41.890 --> 00:03:43.240
of time I'm going to need?

00:03:43.240 --> 00:03:45.760
And you're going to see
there's a nice association

00:03:45.760 --> 00:03:48.910
between classes of algorithms
and the interior structure

00:03:48.910 --> 00:03:50.050
of them.

00:03:50.050 --> 00:03:54.580
And in particular, we want to
ask some fundamental questions.

00:03:54.580 --> 00:03:57.759
Are there fundamental
limits to how much time

00:03:57.759 --> 00:03:59.800
it's going to take to
solve a particular problem,

00:03:59.800 --> 00:04:02.649
no matter what kind of
algorithm I design around this?

00:04:02.649 --> 00:04:04.690
And we'll see that there
are some nice challenges

00:04:04.690 --> 00:04:05.950
about that.

00:04:05.950 --> 00:04:09.380
So that's what we're going
to do over the next two days.

00:04:09.380 --> 00:04:11.850
Before we do though, let's
maybe ask the obvious question--

00:04:11.850 --> 00:04:14.210
why should we care?

00:04:14.210 --> 00:04:18.110
Could be on a quiz,
might matter to you.

00:04:18.110 --> 00:04:22.630
Better choice is because it
actually makes a difference.

00:04:22.630 --> 00:04:25.100
And I say that because it
may not be as obvious to you

00:04:25.100 --> 00:04:26.660
as it was in an
earlier generation.

00:04:26.660 --> 00:04:29.390
So people with my gray hair
or what's left of my gray hair

00:04:29.390 --> 00:04:31.200
like to tell stories.

00:04:31.200 --> 00:04:33.460
I'll make it short.

00:04:33.460 --> 00:04:37.700
But I started programming
41 years ago-- no,

00:04:37.700 --> 00:04:40.541
sorry, 45 years
ago-- on punch cards.

00:04:40.541 --> 00:04:43.040
You don't know what those are
unless you've been to a museum

00:04:43.040 --> 00:04:45.440
on a machine that
filled a half a room

00:04:45.440 --> 00:04:47.750
and that took about
five minutes to execute

00:04:47.750 --> 00:04:50.970
what you can do in a fraction
of a second on your phone.

00:04:50.970 --> 00:04:51.470
Right.

00:04:51.470 --> 00:04:53.660
This is to tell you're
living in a great time, not

00:04:53.660 --> 00:04:57.320
independent of what's going
to happen on November 8th.

00:04:57.320 --> 00:04:57.820
All right.

00:04:57.820 --> 00:05:00.860
We'll stay away from those
topics as well, won't we?

00:05:00.860 --> 00:05:03.230
My point is yeah,
I tell old stories.

00:05:03.230 --> 00:05:04.532
I'm an old guy.

00:05:04.532 --> 00:05:05.990
But you might argue
look, computers

00:05:05.990 --> 00:05:07.400
are getting so much faster.

00:05:07.400 --> 00:05:08.480
Does it really matter?

00:05:08.480 --> 00:05:10.730
And I want to say to you--
maybe it's obvious to you--

00:05:10.730 --> 00:05:13.040
yes, absolutely it does.

00:05:13.040 --> 00:05:16.190
Because in conjunction with
us getting faster computers,

00:05:16.190 --> 00:05:17.990
we're increasing the
sizes of the problems.

00:05:17.990 --> 00:05:21.790
The data sets we want to
analyze are getting massive.

00:05:21.790 --> 00:05:23.830
And I'll give you an example.

00:05:23.830 --> 00:05:26.990
I just pulled this off
of Google, of course.

00:05:26.990 --> 00:05:29.240
In 2014-- I don't have
more recent numbers--

00:05:29.240 --> 00:05:31.910
Google served-- I think I
have that number right--

00:05:31.910 --> 00:05:37.110
30 trillion pages on the web.

00:05:37.110 --> 00:05:39.540
It's either 30 trillionaire
or 30 quadrillion.

00:05:39.540 --> 00:05:41.370
I can't count that
many zeros there.

00:05:41.370 --> 00:05:45.586
It covered 100 million
gigabytes of data.

00:05:45.586 --> 00:05:48.210
And I suggest to you if you want
to find a piece of information

00:05:48.210 --> 00:05:51.402
on the web, can you write a
simple little search algorithm

00:05:51.402 --> 00:05:53.610
that's going to sequentially
go through all the pages

00:05:53.610 --> 00:05:57.090
and find anything in any
reasonable amount of time?

00:05:57.090 --> 00:05:57.820
Probably not.

00:05:57.820 --> 00:05:58.320
Right?

00:05:58.320 --> 00:06:00.840
It's just growing way too fast.

00:06:00.840 --> 00:06:03.870
This, by the way, is of course,
why Google makes a lot of money

00:06:03.870 --> 00:06:06.480
off of their map
reduced algorithm

00:06:06.480 --> 00:06:08.760
for searching the web,
written by the way,

00:06:08.760 --> 00:06:12.540
or co-written by an MIT grad
and the parent of a current MIT

00:06:12.540 --> 00:06:13.040
student.

00:06:13.040 --> 00:06:14.610
So there's a nice
hook in there, not

00:06:14.610 --> 00:06:17.040
that Google pays MIT royalties
for that wonderful thing,

00:06:17.040 --> 00:06:18.490
by the way.

00:06:18.490 --> 00:06:19.230
All right.

00:06:19.230 --> 00:06:23.070
Bad jokes aside, searching
Google-- ton of time.

00:06:23.070 --> 00:06:26.590
Searching a genomics
data set-- ton of time.

00:06:26.590 --> 00:06:29.100
The data sets are
growing so fast.

00:06:29.100 --> 00:06:30.750
You're working for
the US government.

00:06:30.750 --> 00:06:33.960
You want to track terrorists
using image surveillance

00:06:33.960 --> 00:06:37.140
from around the world,
growing incredibly rapidly.

00:06:37.140 --> 00:06:37.960
Pick a problem.

00:06:37.960 --> 00:06:41.100
The data sets grow
so quickly that even

00:06:41.100 --> 00:06:42.750
if the computers
speed up, you still

00:06:42.750 --> 00:06:45.690
need to think about how to
come up with efficient ways

00:06:45.690 --> 00:06:47.680
to solve those problems.

00:06:47.680 --> 00:06:50.010
So I want to suggest
to you while sometimes

00:06:50.010 --> 00:06:52.570
simple solutions are great,
they are the easy ones to rate--

00:06:52.570 --> 00:06:53.130
too write.

00:06:53.130 --> 00:06:53.910
Sorry.

00:06:53.910 --> 00:06:56.070
At times, you need to
be more sophisticated.

00:06:56.070 --> 00:06:59.010
Therefore, we want
to reason about

00:06:59.010 --> 00:07:01.440
how do we measure
efficiency and how do we

00:07:01.440 --> 00:07:05.340
relate algorithm design
choices to the cost that's

00:07:05.340 --> 00:07:08.170
going to be associated with it?

00:07:08.170 --> 00:07:09.669
OK.

00:07:09.669 --> 00:07:11.710
Even when we do that,
we've got a choice to make.

00:07:11.710 --> 00:07:15.850
Because we could talk about
both efficiency in terms of time

00:07:15.850 --> 00:07:18.700
or in terms of space,
meaning how much storage

00:07:18.700 --> 00:07:20.920
do I have inside the computer?

00:07:20.920 --> 00:07:22.390
And the reason
that's relevant is

00:07:22.390 --> 00:07:25.955
there's actually in many cases
a trade-off between those two.

00:07:25.955 --> 00:07:28.330
And you've actually seen an
example, which you may or may

00:07:28.330 --> 00:07:29.830
not remember.

00:07:29.830 --> 00:07:31.810
You may recall when we
introduced dictionaries,

00:07:31.810 --> 00:07:33.550
I showed you a
variation where you

00:07:33.550 --> 00:07:37.060
could compute Fibonacci
using the dictionary to keep

00:07:37.060 --> 00:07:39.760
track of intermediate values.

00:07:39.760 --> 00:07:42.130
And we'll see in next week
that it actually tremendously

00:07:42.130 --> 00:07:43.960
reduces the time complexity.

00:07:43.960 --> 00:07:46.330
That's called a trade-off,
in the sense that sometimes I

00:07:46.330 --> 00:07:49.710
can pre-compute
portions of the answer,

00:07:49.710 --> 00:07:51.220
store them away,
so that when I've

00:07:51.220 --> 00:07:52.844
tried to a bigger
version of the answer

00:07:52.844 --> 00:07:54.910
I can just look
up those portions.

00:07:54.910 --> 00:07:56.890
So there's going to
be a trade-off here.

00:07:56.890 --> 00:07:59.020
We're going to
focus, for purposes

00:07:59.020 --> 00:08:01.910
of this lecture and the next
one, on time efficiency.

00:08:01.910 --> 00:08:03.910
How much time is it going
to take our algorithms

00:08:03.910 --> 00:08:07.060
to solve a problem?

00:08:07.060 --> 00:08:08.612
OK.

00:08:08.612 --> 00:08:10.570
What are the challenges
in doing that before we

00:08:10.570 --> 00:08:11.710
look at the actual tools?

00:08:11.710 --> 00:08:15.230
And in fact, this is going
to lead into the tools.

00:08:15.230 --> 00:08:19.490
The first one is even if
I've decided on an algorithm,

00:08:19.490 --> 00:08:21.930
there are lots of ways
to implement that.

00:08:21.930 --> 00:08:24.570
A while loop and a for loop
might have slightly different

00:08:24.570 --> 00:08:25.650
behavior.

00:08:25.650 --> 00:08:28.170
I could choose to do it
with temporary variables

00:08:28.170 --> 00:08:29.530
or using direct substitution.

00:08:29.530 --> 00:08:31.290
There's lots of little choices.

00:08:31.290 --> 00:08:32.940
So an algorithm
could be implemented

00:08:32.940 --> 00:08:34.210
many different ways.

00:08:34.210 --> 00:08:38.179
How do I measure the actual
efficiency of the algorithm?

00:08:38.179 --> 00:08:40.730
Second one is I might,
for a given problem,

00:08:40.730 --> 00:08:43.460
have different
choices of algorithm.

00:08:43.460 --> 00:08:46.490
A recursive solution
versus an iterative one.

00:08:46.490 --> 00:08:49.562
Using divide and conquer
versus straightforward search.

00:08:49.562 --> 00:08:51.270
We're going to see
some examples of that.

00:08:51.270 --> 00:08:54.320
So I've got to somehow
separate those pieces out.

00:08:54.320 --> 00:08:56.480
And in particular, I'd
like to separate out

00:08:56.480 --> 00:09:00.110
the choice of implementation
from the choice of algorithm.

00:09:00.110 --> 00:09:02.010
I want to measure how
hard is the algorithm,

00:09:02.010 --> 00:09:04.430
not can I come up
with a slightly more

00:09:04.430 --> 00:09:08.650
efficient way of coming
up with an implementation.

00:09:08.650 --> 00:09:10.950
So here are three
ways I might do it.

00:09:10.950 --> 00:09:13.770
And we're going to look at
each one of them very briefly.

00:09:13.770 --> 00:09:17.750
The obvious one is we could
be scientists-- time it.

00:09:17.750 --> 00:09:20.390
Write the code, run a bunch
of test case, run a timer,

00:09:20.390 --> 00:09:25.160
use that to try and come up with
a way of estimating efficiency.

00:09:25.160 --> 00:09:27.610
We'll see some
challenges with that.

00:09:27.610 --> 00:09:32.590
Slightly more abstractly,
we could count operations.

00:09:32.590 --> 00:09:35.260
We could say here are the set
of fundamental operations--

00:09:35.260 --> 00:09:38.890
mathematical operations,
comparisons, setting values,

00:09:38.890 --> 00:09:40.480
retrieving values.

00:09:40.480 --> 00:09:42.760
And simply say how many
of those operations

00:09:42.760 --> 00:09:45.220
do I use in my
algorithm as a function

00:09:45.220 --> 00:09:46.930
of the size of the input?

00:09:46.930 --> 00:09:50.490
And that could be used to
give us a sense of efficiency.

00:09:50.490 --> 00:09:53.480
We're going to see both of
those are flawed somewhat more

00:09:53.480 --> 00:09:55.570
in the first case
than the second one.

00:09:55.570 --> 00:09:57.720
And so we're going to
abstract that second one

00:09:57.720 --> 00:09:59.520
to a more abstract
notion of something

00:09:59.520 --> 00:10:01.760
we call an order of growth.

00:10:01.760 --> 00:10:04.970
And I'll come back to that
in a couple of minutes.

00:10:04.970 --> 00:10:07.320
This is the one we're
going to focus on.

00:10:07.320 --> 00:10:09.065
It's one that computer
scientists use.

00:10:09.065 --> 00:10:11.180
It leads to what we
call complexity classes.

00:10:11.180 --> 00:10:13.430
So order of growth
or big O notation

00:10:13.430 --> 00:10:16.190
is a way of
abstractly describing

00:10:16.190 --> 00:10:18.620
the behavior of an
algorithm, and especially

00:10:18.620 --> 00:10:21.880
the equivalences of
different algorithms.

00:10:21.880 --> 00:10:23.300
But let's look at those.

00:10:23.300 --> 00:10:25.497
Timing.

00:10:25.497 --> 00:10:26.830
Python provides a timer for you.

00:10:26.830 --> 00:10:28.771
You could import
the time module.

00:10:28.771 --> 00:10:31.020
And then you can call, as
you can see right down here.

00:10:31.020 --> 00:10:33.820
I might have defined a really
simple little function--

00:10:33.820 --> 00:10:35.920
convert Celsius to Fahrenheit.

00:10:35.920 --> 00:10:39.130
And in particular, I could
invoke the clock method

00:10:39.130 --> 00:10:41.500
from the time module.

00:10:41.500 --> 00:10:42.880
And what that does
is it gives me

00:10:42.880 --> 00:10:46.040
a number as the number of some
fractions of a second currently

00:10:46.040 --> 00:10:46.870
there.

00:10:46.870 --> 00:10:49.280
Having done that I
could call the function.

00:10:49.280 --> 00:10:52.270
And then I could call the clock
again, and take the difference

00:10:52.270 --> 00:10:54.630
to tell me how much time
it took to execute this.

00:10:54.630 --> 00:10:56.620
It's going to be a
tiny amount of time.

00:10:56.620 --> 00:11:00.022
And then I could certainly
print out some statistics.

00:11:00.022 --> 00:11:01.480
I could do that
over a large number

00:11:01.480 --> 00:11:03.700
of runs-- different
sizes of the input--

00:11:03.700 --> 00:11:08.470
and come up with a sense of
how much time does it take.

00:11:08.470 --> 00:11:10.760
Here's the problem with that.

00:11:10.760 --> 00:11:12.580
Not a bad idea.

00:11:12.580 --> 00:11:15.920
But again, my goal is
to evaluate algorithms.

00:11:15.920 --> 00:11:17.840
Do different algorithms
have different amounts

00:11:17.840 --> 00:11:20.066
of time associated with them?

00:11:20.066 --> 00:11:22.190
The good news is is that
if I measure running time,

00:11:22.190 --> 00:11:25.640
it will certainly vary
as the algorithm changes.

00:11:25.640 --> 00:11:27.050
Just what I want to measure.

00:11:27.050 --> 00:11:28.640
Sorry.

00:11:28.640 --> 00:11:30.890
But one of the problems
is that it will also

00:11:30.890 --> 00:11:34.330
vary as a function of
the implementation.

00:11:34.330 --> 00:11:34.830
Right?

00:11:34.830 --> 00:11:37.260
If I use a loop that's got a
couple of more steps inside

00:11:37.260 --> 00:11:38.761
of it in one algorithm
than another,

00:11:38.761 --> 00:11:40.010
it's going to change the time.

00:11:40.010 --> 00:11:42.090
And I don't really care
about that difference.

00:11:42.090 --> 00:11:45.690
So I'm confounding or conflating
implementation influence

00:11:45.690 --> 00:11:49.530
on time with algorithm
influence on time.

00:11:49.530 --> 00:11:51.450
Not so good.

00:11:51.450 --> 00:11:55.890
Worse, timing will
depend on the computer.

00:11:55.890 --> 00:11:57.139
My Mac here is pretty old.

00:11:57.139 --> 00:11:58.680
Well, at least for
computer versions.

00:11:58.680 --> 00:11:59.820
It's about five years old.

00:11:59.820 --> 00:12:02.370
I'm sure some of you have
much more recent Macs

00:12:02.370 --> 00:12:04.195
or other kinds of machines.

00:12:04.195 --> 00:12:05.820
Your speeds may be
different from mine.

00:12:05.820 --> 00:12:08.490
That's not going to help me
in trying to measure this.

00:12:08.490 --> 00:12:11.730
And even if I could measure
it on small sized problems,

00:12:11.730 --> 00:12:13.710
it doesn't necessarily
predict what

00:12:13.710 --> 00:12:16.350
happens when I go to a
really large sized problems,

00:12:16.350 --> 00:12:18.000
because of issues
like the time it

00:12:18.000 --> 00:12:20.340
takes to get things
out of memory

00:12:20.340 --> 00:12:22.840
and bring them back
in to the computer.

00:12:22.840 --> 00:12:26.280
So what it says is
that timing does

00:12:26.280 --> 00:12:28.650
vary based on what
I'd like to measure,

00:12:28.650 --> 00:12:30.520
but it varies on a
lot of other factors.

00:12:30.520 --> 00:12:33.630
And it's really not
all that valuable.

00:12:33.630 --> 00:12:34.230
OK.

00:12:34.230 --> 00:12:36.265
Got rid of the first one.

00:12:36.265 --> 00:12:39.320
Let's abstract that.

00:12:39.320 --> 00:12:41.730
By abstract, I'm going to
make the following assumption.

00:12:41.730 --> 00:12:46.026
I'm going to identify a set
of primitive operations.

00:12:46.026 --> 00:12:47.400
Kind of get to
say what they are,

00:12:47.400 --> 00:12:48.816
but the obvious
one is to say what

00:12:48.816 --> 00:12:51.450
does the machine do
for me automatically.

00:12:51.450 --> 00:12:53.490
That would be things
like arithmetic

00:12:53.490 --> 00:12:56.940
or mathematical operations,
multiplication, division,

00:12:56.940 --> 00:13:00.492
subtraction,
comparisons, something

00:13:00.492 --> 00:13:02.450
equal to another thing,
something greater than,

00:13:02.450 --> 00:13:05.620
something less
than, assignments,

00:13:05.620 --> 00:13:09.190
set a name to a value,
and retrieval from memory.

00:13:09.190 --> 00:13:12.580
I'm going to assume that
all of these operations

00:13:12.580 --> 00:13:16.764
take about the same amount
of time inside my machine.

00:13:16.764 --> 00:13:18.180
Nice thing here
is then it doesn't

00:13:18.180 --> 00:13:19.471
matter which machine I'm using.

00:13:19.471 --> 00:13:21.870
I'm measuring how long
does the algorithm take

00:13:21.870 --> 00:13:25.200
by counting how many
operations of this type

00:13:25.200 --> 00:13:28.130
are done inside
of the algorithm.

00:13:28.130 --> 00:13:29.880
And I'm going to use
that count to come up

00:13:29.880 --> 00:13:31.650
with a number of
operations executed

00:13:31.650 --> 00:13:33.996
as a function of the
size of the input.

00:13:33.996 --> 00:13:35.370
And if I'm lucky,
that'll give me

00:13:35.370 --> 00:13:38.770
a sense of what's the
efficiency of the algorithm.

00:13:38.770 --> 00:13:40.910
So this one's pretty boring.

00:13:40.910 --> 00:13:41.920
It's got three steps.

00:13:41.920 --> 00:13:42.420
Right?

00:13:42.420 --> 00:13:44.770
A multiplication, a division,
and an addition-- four,

00:13:44.770 --> 00:13:46.570
if you count the return.

00:13:46.570 --> 00:13:48.580
But if I had a little
thing here that added up

00:13:48.580 --> 00:13:51.651
the integers from
0 up to x, I've

00:13:51.651 --> 00:13:52.900
got a little loop inside here.

00:13:52.900 --> 00:13:55.150
And I could count operations.

00:13:55.150 --> 00:13:57.550
So in this case, it's just,
as I said, three operations.

00:13:57.550 --> 00:14:01.050
Here, I've got one operation.

00:14:01.050 --> 00:14:03.270
I'm doing an assignment.

00:14:03.270 --> 00:14:05.550
And then inside
here, in essence,

00:14:05.550 --> 00:14:10.630
there's one operation to set i
to a value from that iterator.

00:14:10.630 --> 00:14:11.880
Initially, it's going to be 0.

00:14:11.880 --> 00:14:13.046
And then it's going to be 1.

00:14:13.046 --> 00:14:14.840
And you get the idea.

00:14:14.840 --> 00:14:19.500
And here, that's
actually two operations.

00:14:19.500 --> 00:14:20.806
It's nice Python shorthand.

00:14:20.806 --> 00:14:21.930
But what is that operation?

00:14:21.930 --> 00:14:24.810
It says take the value of
total and the value of i,

00:14:24.810 --> 00:14:26.850
add them together--
it's one operation--

00:14:26.850 --> 00:14:30.030
and then set that value,
or rather, set the name

00:14:30.030 --> 00:14:31.210
total to that new value.

00:14:31.210 --> 00:14:32.580
So a second operation.

00:14:32.580 --> 00:14:36.100
So you can see in here,
I've got three operations.

00:14:36.100 --> 00:14:37.390
And what else do I have?

00:14:37.390 --> 00:14:41.530
Well, I'm going to go
through this loop x times.

00:14:41.530 --> 00:14:42.030
Right?

00:14:42.030 --> 00:14:43.350
I do it for i equals 0.

00:14:43.350 --> 00:14:44.850
And therefore, i
equal 1, and so on.

00:14:44.850 --> 00:14:48.480
So I'm going to run
through that loop x times.

00:14:48.480 --> 00:14:54.210
And if I put that together, I
get a nice little expression.

00:14:54.210 --> 00:14:56.744
1 plus 3x.

00:14:56.744 --> 00:14:58.160
Actually, I probably
cheated here.

00:14:58.160 --> 00:14:58.820
I shouldn't say cheated.

00:14:58.820 --> 00:15:00.528
I probably should have
counted the return

00:15:00.528 --> 00:15:03.680
as one more operation, so that
would be 1 plus 3x plus 1,

00:15:03.680 --> 00:15:07.480
or 3x plus 2 operations.

00:15:07.480 --> 00:15:09.557
Why should you care?

00:15:09.557 --> 00:15:11.140
It's a little closer
to what I'd like.

00:15:11.140 --> 00:15:12.760
Because now I've
got an expression

00:15:12.760 --> 00:15:15.160
that tells me something
about how much time

00:15:15.160 --> 00:15:20.180
is this going to take as I
change the size of the problem.

00:15:20.180 --> 00:15:23.060
If x is equal to 10, it's
going to take me 32 operations.

00:15:23.060 --> 00:15:25.790
If x is equal to
100, 302 operations.

00:15:25.790 --> 00:15:29.061
If x is equal to 1,000,
3,002 operations.

00:15:29.061 --> 00:15:30.560
And if I wanted the
actual time, I'd

00:15:30.560 --> 00:15:33.394
just multiply that by whatever
that constant amount of time

00:15:33.394 --> 00:15:34.310
is for each operation.

00:15:34.310 --> 00:15:37.420
I've got a good
estimate of that.

00:15:37.420 --> 00:15:39.230
Sounds pretty good.

00:15:39.230 --> 00:15:43.060
Not quite what we
want, but it's close.

00:15:43.060 --> 00:15:46.190
So if I was counting operations,
what could I say about it?

00:15:46.190 --> 00:15:49.930
First of all, it certainly
depends on the algorithm.

00:15:49.930 --> 00:15:51.250
That's great.

00:15:51.250 --> 00:15:53.560
Number of operations is
going to directly relate

00:15:53.560 --> 00:15:55.240
to the algorithm I'm
trying to measure,

00:15:55.240 --> 00:15:57.550
which is what I'm after.

00:15:57.550 --> 00:16:01.060
Unfortunately, it still
depends a little bit

00:16:01.060 --> 00:16:02.932
on the implementation.

00:16:02.932 --> 00:16:04.390
Let me show you
what I mean by that

00:16:04.390 --> 00:16:07.880
by backing up for a second.

00:16:07.880 --> 00:16:12.460
Suppose I were to change this
for loop to a while loop.

00:16:12.460 --> 00:16:14.780
I'll set i equal to 0
outside of the loop.

00:16:14.780 --> 00:16:18.930
And then while i is
less than x plus 1,

00:16:18.930 --> 00:16:20.910
I'll do the things
inside of that.

00:16:20.910 --> 00:16:23.500
That would actually
add one more operation

00:16:23.500 --> 00:16:28.130
inside the loop, because I
both have to set the value of i

00:16:28.130 --> 00:16:30.650
and I have to test
the value of i,

00:16:30.650 --> 00:16:33.710
as well as doing the other
operations down here.

00:16:33.710 --> 00:16:38.760
And so rather than getting 3x
plus 1, I would get 4x plus 1.

00:16:38.760 --> 00:16:40.135
Eh.

00:16:40.135 --> 00:16:42.010
As the government says,
what's the difference

00:16:42.010 --> 00:16:43.720
between three and for
when you're talking

00:16:43.720 --> 00:16:46.200
about really big numbers?

00:16:46.200 --> 00:16:49.220
Problem is in terms of
counting, it does depend.

00:16:49.220 --> 00:16:50.970
And I want to get rid
of that in a second,

00:16:50.970 --> 00:16:53.370
so it still depends a little
bit on the implementation.

00:16:53.370 --> 00:16:55.740
I remind you, I
wanted to measure

00:16:55.740 --> 00:16:57.961
impact of the algorithm.

00:16:57.961 --> 00:16:59.630
But the other good
news is the count

00:16:59.630 --> 00:17:02.700
is independent of which
computer I run on.

00:17:02.700 --> 00:17:05.180
As long as all my computers
come with the same set

00:17:05.180 --> 00:17:07.339
of basic operations,
I don't care

00:17:07.339 --> 00:17:09.410
what the time of my
computer is versus yours

00:17:09.410 --> 00:17:11.839
to do those operations
on measuring

00:17:11.839 --> 00:17:13.921
how much time it would take.

00:17:13.921 --> 00:17:15.359
And I should say,
by the way, one

00:17:15.359 --> 00:17:17.275
of the reasons I want
to do it is last to know

00:17:17.275 --> 00:17:20.609
is it going to take 37.42
femtoseconds or not,

00:17:20.609 --> 00:17:22.560
but rather to say if
this algorithm has

00:17:22.560 --> 00:17:26.700
a particular behavior, if I
double the size of the input,

00:17:26.700 --> 00:17:28.590
does that double the
amount of time I need?

00:17:28.590 --> 00:17:30.510
Does that quadruple the
amount of time I need?

00:17:30.510 --> 00:17:32.970
Does it increase it
by a factor of 10?

00:17:32.970 --> 00:17:36.670
And here, what matters isn't
the speed of the computer.

00:17:36.670 --> 00:17:39.550
It's the number of operations.

00:17:39.550 --> 00:17:41.620
The last one I'm not going
to really worry about.

00:17:41.620 --> 00:17:43.600
But we'd have to
really think about what

00:17:43.600 --> 00:17:46.300
are the operations
we want to count.

00:17:46.300 --> 00:17:48.310
I made an assumption
that the amount

00:17:48.310 --> 00:17:50.440
of time it takes to retrieve
something from memory

00:17:50.440 --> 00:17:51.814
is the same as
the amount of time

00:17:51.814 --> 00:17:54.010
it takes to do a
numerical computation.

00:17:54.010 --> 00:17:55.780
That may not be accurate.

00:17:55.780 --> 00:17:57.370
But this one could
probably be dealt

00:17:57.370 --> 00:18:00.010
with by just agreeing on what
are the common operations

00:18:00.010 --> 00:18:02.330
and then doing the measurement.

00:18:02.330 --> 00:18:04.640
So this is closer.

00:18:04.640 --> 00:18:05.400
Excuse me.

00:18:05.400 --> 00:18:08.840
And certainly, that count
varies for different inputs.

00:18:08.840 --> 00:18:11.090
And we can use it to come
up with a relationship

00:18:11.090 --> 00:18:13.690
between the inputs
and the count.

00:18:13.690 --> 00:18:16.640
And for the most part, it
reflects the algorithm, not

00:18:16.640 --> 00:18:18.024
the implementation.

00:18:18.024 --> 00:18:19.940
But it's still got that
last piece left there,

00:18:19.940 --> 00:18:22.710
so I need to get rid
of the last piece.

00:18:22.710 --> 00:18:23.710
So what can we say here?

00:18:23.710 --> 00:18:27.760
Timing and counting do evaluate
or reflect implementations?

00:18:27.760 --> 00:18:28.510
I don't want that.

00:18:28.510 --> 00:18:31.480
Timing also evaluates
the machines.

00:18:31.480 --> 00:18:34.790
What I want to do is just
evaluate the algorithm.

00:18:34.790 --> 00:18:38.480
And especially, I want to
understand how does it scale?

00:18:38.480 --> 00:18:41.180
I'm going to say what I said
a few minutes ago again.

00:18:41.180 --> 00:18:43.730
If I were to take
an algorithm, and I

00:18:43.730 --> 00:18:46.910
say I know what its
complexity is, my question is

00:18:46.910 --> 00:18:49.567
if I double the
size of the input,

00:18:49.567 --> 00:18:50.900
what does that say to the speed?

00:18:50.900 --> 00:18:52.550
Because that's going to tell me
something about the algorithm.

00:18:52.550 --> 00:18:54.300
I want to say what
happens when I scale it?

00:18:54.300 --> 00:18:55.966
And in particular, I
want to relate that

00:18:55.966 --> 00:18:58.845
to the size of the input.

00:18:58.845 --> 00:19:00.220
So here's what
we're going to do.

00:19:00.220 --> 00:19:01.970
We're going to introduce
orders of growth.

00:19:01.970 --> 00:19:03.946
It's a wonderful tool
in computer science.

00:19:03.946 --> 00:19:05.820
And what we're going to
focus on is that idea

00:19:05.820 --> 00:19:07.980
of counting operations.

00:19:07.980 --> 00:19:11.100
But we're not going to worry
about small variations,

00:19:11.100 --> 00:19:13.267
whether it's three or four
steps inside of the loop.

00:19:13.267 --> 00:19:15.141
We're going to show that
that doesn't matter.

00:19:15.141 --> 00:19:17.580
And if you think about my
statement of does it double

00:19:17.580 --> 00:19:20.250
in terms of size or speed
or not-- or I'm sorry-- time

00:19:20.250 --> 00:19:24.090
or not, whether it goes from
three to six or four to eight,

00:19:24.090 --> 00:19:25.100
it's still a doubling.

00:19:25.100 --> 00:19:28.170
So I don't care about
those pieces inside.

00:19:28.170 --> 00:19:29.730
I'm going to focus
on what happens

00:19:29.730 --> 00:19:34.000
when the size of the problem
gets arbitrarily large.

00:19:34.000 --> 00:19:35.710
I don't care about
counting things from 0

00:19:35.710 --> 00:19:37.300
up to x when x is 10 or 20.

00:19:37.300 --> 00:19:38.950
What happens when
it's a million?

00:19:38.950 --> 00:19:40.190
100 million?

00:19:40.190 --> 00:19:42.610
What's the asymptotic
behavior of this?

00:19:42.610 --> 00:19:45.190
And I want to relate
that time needed

00:19:45.190 --> 00:19:47.650
against the size of the
input, so I can make

00:19:47.650 --> 00:19:50.370
that comparison I suggested.

00:19:50.370 --> 00:19:50.870
OK.

00:19:50.870 --> 00:19:53.280
So to do that, we've got
to do a couple of things.

00:19:53.280 --> 00:19:55.290
We have to decide what
are we going to measure?

00:19:55.290 --> 00:19:56.998
And then we have to
think about how do we

00:19:56.998 --> 00:20:00.750
count without worrying about
implementation details.

00:20:00.750 --> 00:20:03.110
So we're going to
express efficiency

00:20:03.110 --> 00:20:05.570
in terms of size of input.

00:20:05.570 --> 00:20:08.190
And usually, this is
going to be obvious.

00:20:08.190 --> 00:20:12.310
If I've got a procedure that
takes one argument that's

00:20:12.310 --> 00:20:14.227
an integer, the
size of the integer

00:20:14.227 --> 00:20:16.060
is the thing I'm going
to measure things in.

00:20:16.060 --> 00:20:17.920
If I double the size
of that integer,

00:20:17.920 --> 00:20:20.270
what happens to the computation?

00:20:20.270 --> 00:20:22.700
If I'm computing
something over a list,

00:20:22.700 --> 00:20:25.130
typically the length
of the list is

00:20:25.130 --> 00:20:27.680
going to be the thing I'm
going to use to characterize

00:20:27.680 --> 00:20:29.660
the size of the problem.

00:20:29.660 --> 00:20:32.360
If I've got-- and we'll see this
in a second-- a function that

00:20:32.360 --> 00:20:34.490
takes more than
one argument, I get

00:20:34.490 --> 00:20:36.500
to decide what's the
parameter I want to use.

00:20:36.500 --> 00:20:39.071
If I'm searching to see is
this element in that list,

00:20:39.071 --> 00:20:40.820
typically, I'm going
to worry about what's

00:20:40.820 --> 00:20:43.910
the size of the list, not
what's the size of the element.

00:20:43.910 --> 00:20:46.135
But we have to specify what
is that we're measuring.

00:20:46.135 --> 00:20:48.510
And we're going to see examples
of that in just a second.

00:20:51.500 --> 00:20:52.990
OK.

00:20:52.990 --> 00:20:56.440
So now, we start thinking
about that sounds great.

00:20:56.440 --> 00:20:58.930
Certainly fun computing
something numeric.

00:20:58.930 --> 00:21:00.835
Sum of integers from 0 up to x.

00:21:00.835 --> 00:21:03.284
It's kind of obvious x is
the size of my problem.

00:21:03.284 --> 00:21:04.450
How many steps does it take?

00:21:04.450 --> 00:21:06.170
I can count that.

00:21:06.170 --> 00:21:09.020
But in some cases, the
amount of time the code takes

00:21:09.020 --> 00:21:12.030
is going to depend on the input.

00:21:12.030 --> 00:21:14.660
So let's take this little
piece of code here.

00:21:14.660 --> 00:21:17.381
And I do hope by now, even
though we flash up code,

00:21:17.381 --> 00:21:19.630
you're already beginning to
recognize what does it do.

00:21:19.630 --> 00:21:23.600
Not the least of which, by
the clever name that we chose.

00:21:23.600 --> 00:21:25.700
But this is obviously
just a little function.

00:21:25.700 --> 00:21:29.480
It runs through a loop--
sorry, a for loop that takes i

00:21:29.480 --> 00:21:32.090
for each element in
a list L. And it's

00:21:32.090 --> 00:21:35.522
checking to see is i equal
to the element I've provided.

00:21:35.522 --> 00:21:37.230
And when it is, I'm
going to return true.

00:21:37.230 --> 00:21:39.860
If I get all the way through
the loop and I didn't find it,

00:21:39.860 --> 00:21:40.943
I'm going to return false.

00:21:40.943 --> 00:21:45.730
It's just saying is
e in my input list L?

00:21:45.730 --> 00:21:48.779
How many steps is
this going to take?

00:21:48.779 --> 00:21:51.320
Well, we can certainly count
the number of steps in the loop.

00:21:51.320 --> 00:21:51.820
Right?

00:21:51.820 --> 00:21:52.770
We've got a set i.

00:21:52.770 --> 00:21:55.140
We've got to compare i
and potentially we've

00:21:55.140 --> 00:21:55.790
got to return.

00:21:55.790 --> 00:21:59.322
So there's at most three
steps inside the loop.

00:21:59.322 --> 00:22:03.070
But depends on how
lucky I'm feeling.

00:22:03.070 --> 00:22:04.857
Right?

00:22:04.857 --> 00:22:06.940
If e happens to be the
first element in the list--

00:22:06.940 --> 00:22:09.040
it goes through the
loop once-- I'm done.

00:22:09.040 --> 00:22:11.430
Great.

00:22:11.430 --> 00:22:13.700
I'm not always that lucky.

00:22:13.700 --> 00:22:16.420
If e is not in the
list, then it will

00:22:16.420 --> 00:22:18.370
go through this
entire loop until it

00:22:18.370 --> 00:22:20.770
gets all the way through
the elements of L

00:22:20.770 --> 00:22:23.010
before saying false.

00:22:23.010 --> 00:22:26.740
So this-- sort of a
best case scenario.

00:22:26.740 --> 00:22:29.002
This is the worst case scenario.

00:22:29.002 --> 00:22:31.470
Again, if I'm assigned and say
well, let's run some trials.

00:22:31.470 --> 00:22:33.709
Let's do a bunch of examples
and see how many steps

00:22:33.709 --> 00:22:34.500
does it go through.

00:22:34.500 --> 00:22:36.360
And that would be
the average case.

00:22:36.360 --> 00:22:39.600
On average, I'm likely to
look at half the elements

00:22:39.600 --> 00:22:42.171
in the list before I find it.

00:22:42.171 --> 00:22:42.670
Right?

00:22:42.670 --> 00:22:43.836
If I'm lucky, it's early on.

00:22:43.836 --> 00:22:46.070
If I'm not so lucky,
it's later on.

00:22:46.070 --> 00:22:48.120
Which one do I use?

00:22:48.120 --> 00:22:52.210
Well, we're going to
focus on this one.

00:22:52.210 --> 00:22:54.869
Because that gives you an upper
bound on the amount of time

00:22:54.869 --> 00:22:55.660
it's going to take.

00:22:55.660 --> 00:22:58.900
What happens in the
worst case scenario?

00:22:58.900 --> 00:23:01.360
We will find at times
it's valuable to look

00:23:01.360 --> 00:23:03.760
at the average case to give
us a rough sense of what's

00:23:03.760 --> 00:23:05.719
going to happen on average.

00:23:05.719 --> 00:23:07.510
But usually, when we
talk about complexity,

00:23:07.510 --> 00:23:11.839
we're going to focus on
the worst case behavior.

00:23:11.839 --> 00:23:13.630
So to say it in a little
bit different way,

00:23:13.630 --> 00:23:14.796
let's go back to my example.

00:23:14.796 --> 00:23:17.620
Suppose you gave it a
list L of some length.

00:23:17.620 --> 00:23:20.050
Length of L, you can call
that len if you like.

00:23:20.050 --> 00:23:24.095
Then my best case would be
the minimum running type.

00:23:24.095 --> 00:23:26.720
And in this case, it will be for
the first element in the list.

00:23:26.720 --> 00:23:30.200
And notice in that case,
the number of steps I take

00:23:30.200 --> 00:23:32.776
would be independent of the
length of L. That's great.

00:23:32.776 --> 00:23:34.400
It doesn't matter
how long the list is.

00:23:34.400 --> 00:23:37.860
If I'm always going to find
the first element, I'm done.

00:23:37.860 --> 00:23:39.870
The average case
would be the average

00:23:39.870 --> 00:23:42.557
over the number of
steps I take, depending

00:23:42.557 --> 00:23:43.640
on the length of the list.

00:23:43.640 --> 00:23:45.973
It's going to grow linearly
with the length of the list.

00:23:45.973 --> 00:23:47.850
It's a good practical measure.

00:23:47.850 --> 00:23:52.400
But the one I want to focus
on will be the worst case.

00:23:52.400 --> 00:23:54.282
And here, the amount
of time as we're

00:23:54.282 --> 00:23:55.740
going to see in a
couple of slides,

00:23:55.740 --> 00:23:59.660
is linear in the
size of the problem.

00:23:59.660 --> 00:24:04.550
Meaning if I double the length
of the list in the worst case,

00:24:04.550 --> 00:24:07.220
it's going to take
me twice as much time

00:24:07.220 --> 00:24:09.140
to find that it's not there.

00:24:09.140 --> 00:24:11.824
If I increase the length in
the list by a factor of 10,

00:24:11.824 --> 00:24:13.490
in the worst case,
it's going to take me

00:24:13.490 --> 00:24:16.310
10 times as much time as
it did in the earlier case

00:24:16.310 --> 00:24:18.320
to find out that the
problem's not there.

00:24:18.320 --> 00:24:23.170
And that linear relationship
is what I want to capture.

00:24:23.170 --> 00:24:24.570
So I'm going to focus on that.

00:24:24.570 --> 00:24:26.937
What's the worst case behavior?

00:24:26.937 --> 00:24:29.520
And we're about ready to start
talking about orders of growth,

00:24:29.520 --> 00:24:31.380
but here then is
what orders of growth

00:24:31.380 --> 00:24:32.546
are going to provide for me.

00:24:32.546 --> 00:24:36.390
I want to evaluate
efficiency, particularly when

00:24:36.390 --> 00:24:37.470
the input is very large.

00:24:37.470 --> 00:24:39.906
What happens when I
really scale this up?

00:24:39.906 --> 00:24:44.260
I want to express the growth
of the program's runtime

00:24:44.260 --> 00:24:46.450
as that input grows.

00:24:46.450 --> 00:24:50.830
Not the exact runtime, but
that notion of if I doubled it,

00:24:50.830 --> 00:24:52.060
how much longer does it take?

00:24:52.060 --> 00:24:54.310
What's the relationship
between increasing

00:24:54.310 --> 00:24:56.620
the size of the input
and the increase

00:24:56.620 --> 00:24:59.640
in the amount of time
it takes to solve it?

00:24:59.640 --> 00:25:02.889
We're going to put an
upper bound on that growth.

00:25:02.889 --> 00:25:04.430
And if you haven't
seen this in math,

00:25:04.430 --> 00:25:07.830
it basically says I want to
come up with a description that

00:25:07.830 --> 00:25:11.850
is at least as big as--
sorry-- as big as or bigger

00:25:11.850 --> 00:25:15.517
than the actual amount of
time it's going to take.

00:25:15.517 --> 00:25:17.475
And I'm going to not
worry about being precise.

00:25:17.475 --> 00:25:20.770
We're going to talk about the
order of rather than exact.

00:25:20.770 --> 00:25:23.340
I don't need to know to the
femtosecond how long this

00:25:23.340 --> 00:25:26.065
is going to take, or to exactly
one operation how long this

00:25:26.065 --> 00:25:26.880
is going to take.

00:25:26.880 --> 00:25:31.057
But I want to say things like
this is going to grow linearly.

00:25:31.057 --> 00:25:33.640
I double the size of the input,
it doubles the amount of time.

00:25:33.640 --> 00:25:35.750
Or this is going to
grow quadratically.

00:25:35.750 --> 00:25:37.750
I double the size
of the input, it's

00:25:37.750 --> 00:25:40.750
going to take four times
as much time to solve it.

00:25:40.750 --> 00:25:43.756
Or if I'm really lucky, this is
going to have constant growth.

00:25:43.756 --> 00:25:45.130
No matter how I
change the input,

00:25:45.130 --> 00:25:47.590
it's not going to
take any more time.

00:25:47.590 --> 00:25:49.820
To do that, we're going
to look at the largest

00:25:49.820 --> 00:25:50.960
factors in the runtime.

00:25:50.960 --> 00:25:53.864
Which piece of the program
takes the most time?

00:25:53.864 --> 00:25:55.280
And so in orders
of growth, we are

00:25:55.280 --> 00:25:57.770
going to look for
as tight as possible

00:25:57.770 --> 00:26:00.650
an upper bound on the growth
as a function of the size

00:26:00.650 --> 00:26:03.730
of the input in the worst case.

00:26:03.730 --> 00:26:07.530
Nice long definition.

00:26:07.530 --> 00:26:09.250
Almost ready to look
at some examples.

00:26:09.250 --> 00:26:11.660
So here's the notation
we're going to use.

00:26:11.660 --> 00:26:13.419
It's called Big O notation.

00:26:13.419 --> 00:26:15.210
I have to admit-- and
John's not here today

00:26:15.210 --> 00:26:17.460
to remind me the history--
I think it comes because we

00:26:17.460 --> 00:26:19.530
used Omicron-- God knows why.

00:26:19.530 --> 00:26:21.690
Sounds like something
from Futurama.

00:26:21.690 --> 00:26:26.152
But we used Omicron as
our symbol to define this.

00:26:26.152 --> 00:26:28.110
I'm having such good luck
with bad jokes today.

00:26:28.110 --> 00:26:30.405
You're not even wincing when
I throw those things out.

00:26:30.405 --> 00:26:31.740
But that's OK.

00:26:31.740 --> 00:26:33.044
It's called Big O notation.

00:26:33.044 --> 00:26:33.960
We're going to use it.

00:26:33.960 --> 00:26:35.280
We're going to describe
the rules of it.

00:26:35.280 --> 00:26:36.690
Is this the tradition of it?

00:26:36.690 --> 00:26:39.030
It describes the worst
case, because it's often

00:26:39.030 --> 00:26:40.309
the bottleneck we're after.

00:26:40.309 --> 00:26:41.850
And as we said, it's
going to express

00:26:41.850 --> 00:26:46.468
the growth of the program
relative to the input size.

00:26:46.468 --> 00:26:47.452
OK.

00:26:47.452 --> 00:26:50.070
Let's see how we go
from counting operations

00:26:50.070 --> 00:26:51.790
to getting to orders of growth.

00:26:51.790 --> 00:26:54.248
Then we're going to define some
examples of ordered growth.

00:26:54.248 --> 00:26:56.480
And we're going to start
looking at algorithms.

00:26:56.480 --> 00:26:58.230
Here's a piece of code
you've seen before.

00:26:58.230 --> 00:27:00.150
Again, hopefully, you
recognize or can see

00:27:00.150 --> 00:27:02.340
fairly quickly what it's doing.

00:27:02.340 --> 00:27:04.110
Computing factorials
the iterative way.

00:27:04.110 --> 00:27:05.930
Basically, remember
n factorial is

00:27:05.930 --> 00:27:09.060
n times n minus 1 times n
minus 2 all the way down to 1.

00:27:09.060 --> 00:27:12.360
Hopefully, assuming n is
a non-negative integer.

00:27:12.360 --> 00:27:14.880
Here, we're going to set up
an internal variable called

00:27:14.880 --> 00:27:15.960
answer.

00:27:15.960 --> 00:27:17.910
And then we're just
going to run over a loop.

00:27:17.910 --> 00:27:19.560
As long as n is
bigger than 1, we're

00:27:19.560 --> 00:27:22.640
going to multiply answer by
n, store it back into answer,

00:27:22.640 --> 00:27:23.730
decrease n by 1.

00:27:23.730 --> 00:27:26.130
We'll keep doing that until
we get out of the loop.

00:27:26.130 --> 00:27:28.850
And we're going
to return answer.

00:27:28.850 --> 00:27:30.110
We'll start by counting steps.

00:27:32.587 --> 00:27:35.170
And that's, by the way, just to
remind you that in fact, there

00:27:35.170 --> 00:27:35.750
are two steps here.

00:27:35.750 --> 00:27:36.500
So what do I have?

00:27:36.500 --> 00:27:38.070
I've got one step up there.

00:27:38.070 --> 00:27:40.070
Set answer to one.

00:27:40.070 --> 00:27:43.670
I'm setting up n-- sorry,
I'm not setting up n.

00:27:43.670 --> 00:27:45.320
I'm going to test n.

00:27:45.320 --> 00:27:47.646
And then I'm going
to do two steps here,

00:27:47.646 --> 00:27:50.270
because I got a multiply answer
by n and then set it to answer.

00:27:50.270 --> 00:27:52.103
And now similarly, we've
got two steps there

00:27:52.103 --> 00:27:55.700
because I'm subtracting 1 from
n and then setting it to n.

00:27:55.700 --> 00:28:01.480
So I've got 2 plus 4 plus
the test, which is 5.

00:28:01.480 --> 00:28:02.930
I've got 1 outside here.

00:28:02.930 --> 00:28:04.070
I got 1 outside there.

00:28:04.070 --> 00:28:07.740
And I'm going to go
through this loop n times.

00:28:07.740 --> 00:28:10.130
So I would suggest that if
I count the number of steps,

00:28:10.130 --> 00:28:13.310
it's 1 plus 5n plus 1.

00:28:17.330 --> 00:28:19.450
Sort of what we did before.

00:28:19.450 --> 00:28:23.800
5n plus 2 is the total number
of steps that I use here.

00:28:23.800 --> 00:28:26.270
But now, I'm interested in
what's the worst case behavior?

00:28:26.270 --> 00:28:28.311
Well, in this case, it is
the worst case behavior

00:28:28.311 --> 00:28:30.780
because it doesn't have
decisions anywhere in here.

00:28:30.780 --> 00:28:34.760
But I just want to know what's
the asymptotic complexity?

00:28:34.760 --> 00:28:37.402
And I'm going to
say-- oh, sorry-- that

00:28:37.402 --> 00:28:39.110
is to say I could do
this different ways.

00:28:39.110 --> 00:28:41.240
I could have done this
with two steps like that.

00:28:41.240 --> 00:28:43.730
That would have made it
not just 1 plus 5n plus 1.

00:28:43.730 --> 00:28:46.010
It would have made
it 1 plus 6n plus 1

00:28:46.010 --> 00:28:48.630
because I've got an extra step.

00:28:48.630 --> 00:28:50.650
I put that up because
I want to remind you

00:28:50.650 --> 00:28:54.610
I don't care about
implementation differences.

00:28:54.610 --> 00:28:57.640
And so I want to
know what captures

00:28:57.640 --> 00:28:59.440
both of those behaviors.

00:28:59.440 --> 00:29:04.540
And in Big O notation,
I say that's order n.

00:29:04.540 --> 00:29:06.287
Grows linearly.

00:29:06.287 --> 00:29:07.870
So I'm going to keep
doing this to you

00:29:07.870 --> 00:29:09.700
until you really do wince at me.

00:29:09.700 --> 00:29:13.210
If I were to double
the size of n,

00:29:13.210 --> 00:29:16.060
whether I use this
version or that version,

00:29:16.060 --> 00:29:18.030
the amount of time
the number of steps

00:29:18.030 --> 00:29:20.346
is basically going to double.

00:29:20.346 --> 00:29:21.470
Now you say, wait a minute.

00:29:21.470 --> 00:29:26.320
5n plus 2-- if n
is 10 that's 52.

00:29:26.320 --> 00:29:28.620
And if n is 20, that's 102.

00:29:28.620 --> 00:29:29.850
That's not quite doubling it.

00:29:29.850 --> 00:29:31.075
And you're right.

00:29:31.075 --> 00:29:32.700
But remember, we
really care about this

00:29:32.700 --> 00:29:33.750
in the asymptotic case.

00:29:33.750 --> 00:29:36.030
When n gets really big,
those extra little pieces

00:29:36.030 --> 00:29:37.230
don't matter.

00:29:37.230 --> 00:29:39.000
And so what we're
going to do is we're

00:29:39.000 --> 00:29:42.320
going to ignore the
additive constants

00:29:42.320 --> 00:29:45.370
and we're going to ignore the
multiplicative constants when

00:29:45.370 --> 00:29:48.630
we talk about orders of growth.

00:29:48.630 --> 00:29:51.074
So what does o of n measure?

00:29:51.074 --> 00:29:52.490
Well, we're just
summarizing here.

00:29:52.490 --> 00:29:54.796
We want to describe how much
time is needed to compute

00:29:54.796 --> 00:29:56.420
or how does the amount
of time, rather,

00:29:56.420 --> 00:29:58.420
needed to computer
problem growth

00:29:58.420 --> 00:30:01.130
as the size of the
problem itself grows.

00:30:01.130 --> 00:30:03.590
So we want an
expression that counts

00:30:03.590 --> 00:30:05.434
that asymptotic behavior.

00:30:05.434 --> 00:30:07.850
And we're going to focus as a
consequence on the term that

00:30:07.850 --> 00:30:11.190
grows most rapidly.

00:30:11.190 --> 00:30:13.309
So here are some examples.

00:30:13.309 --> 00:30:14.850
And I know if you're
following along,

00:30:14.850 --> 00:30:16.070
you can already see
the answers here.

00:30:16.070 --> 00:30:17.570
But we're going to
do this to simply

00:30:17.570 --> 00:30:19.010
give you a sense of that.

00:30:19.010 --> 00:30:21.350
If I'm counting
operations and I come up

00:30:21.350 --> 00:30:23.810
with an expression
that has n squared

00:30:23.810 --> 00:30:28.880
plus 2n plus 2 operations,
that expression I say

00:30:28.880 --> 00:30:31.740
is order n squared.

00:30:31.740 --> 00:30:33.720
The 2 and the 2n don't matter.

00:30:33.720 --> 00:30:36.450
And think about what happens
if you make n really big.

00:30:36.450 --> 00:30:39.510
n squared is much more
dominant than the other terms.

00:30:39.510 --> 00:30:42.590
We say that's order n squared.

00:30:42.590 --> 00:30:47.460
Even this expression we
say is order n squared.

00:30:47.460 --> 00:30:49.690
So in this case, for
lower values of n,

00:30:49.690 --> 00:30:51.749
this term is going to
be the big one in terms

00:30:51.749 --> 00:30:52.540
of number of steps.

00:30:52.540 --> 00:30:56.080
I have no idea how I wrote
such an inefficient algorithm

00:30:56.080 --> 00:30:57.970
that it took 100,000
steps to do something.

00:30:57.970 --> 00:31:01.150
But if I had that expression
for smaller values of n,

00:31:01.150 --> 00:31:02.590
this matters a lot.

00:31:02.590 --> 00:31:04.727
This is a really big number.

00:31:04.727 --> 00:31:06.310
But when I'm interested
in the growth,

00:31:06.310 --> 00:31:09.010
then that's the
term that dominates.

00:31:09.010 --> 00:31:11.200
And you see the idea or
begin to see the idea here

00:31:11.200 --> 00:31:13.300
that when I have-- sorry,
let me go back there--

00:31:13.300 --> 00:31:17.820
when I have expressions, if
it's a polynomial expression,

00:31:17.820 --> 00:31:19.240
it's the highest order term.

00:31:19.240 --> 00:31:21.090
It's the term that
captures the complexity.

00:31:21.090 --> 00:31:23.930
Both of these are quadratic.

00:31:23.930 --> 00:31:31.710
This term is order n, because
n grows faster than log of n.

00:31:31.710 --> 00:31:33.420
This funky looking
term, even though that

00:31:33.420 --> 00:31:35.190
looks like the big
number there and it

00:31:35.190 --> 00:31:40.530
is a big number, that expression
we see is order n log n.

00:31:40.530 --> 00:31:42.420
Because again, if
I plot out as how

00:31:42.420 --> 00:31:44.670
this changes as I
make n really large,

00:31:44.670 --> 00:31:48.780
this term eventually takes
over as the dominant term.

00:31:48.780 --> 00:31:51.050
What about that one?

00:31:51.050 --> 00:31:53.340
What's the big term there?

00:31:53.340 --> 00:31:56.530
How many people think
it's n to the 30th?

00:31:56.530 --> 00:31:58.560
Show of hands.

00:31:58.560 --> 00:32:00.590
How many people think
it's 3 to the n?

00:32:00.590 --> 00:32:02.050
Show of hands.

00:32:02.050 --> 00:32:02.550
Thank you.

00:32:02.550 --> 00:32:03.508
You're following along.

00:32:03.508 --> 00:32:04.880
You're also paying attention.

00:32:04.880 --> 00:32:07.088
How many people think I
should stop asking questions?

00:32:07.088 --> 00:32:07.800
No show of hands.

00:32:07.800 --> 00:32:09.200
All right.

00:32:09.200 --> 00:32:10.790
But you're right.

00:32:10.790 --> 00:32:14.845
Exponentials are much
worse than powers.

00:32:14.845 --> 00:32:16.220
Even something
like this-- again,

00:32:16.220 --> 00:32:18.594
it's going to take a big value
of n before it gets there,

00:32:18.594 --> 00:32:20.300
but it does get there.

00:32:20.300 --> 00:32:21.850
And that, by the
way, is important,

00:32:21.850 --> 00:32:23.470
because we're going to
see later on in the term

00:32:23.470 --> 00:32:24.886
that there are
some problems where

00:32:24.886 --> 00:32:29.410
it's believed that all of the
solutions are exponential.

00:32:29.410 --> 00:32:30.940
And that's a pain,
because it says

00:32:30.940 --> 00:32:32.815
it's always going to be
expensive to compute.

00:32:32.815 --> 00:32:36.700
So that's how we're going to
reason about these things.

00:32:36.700 --> 00:32:39.580
And to see it visually,
here are the differences

00:32:39.580 --> 00:32:42.014
between those different classes.

00:32:42.014 --> 00:32:43.930
Something that's constant--
the amount of time

00:32:43.930 --> 00:32:47.210
doesn't change as I change
the size of the input.

00:32:47.210 --> 00:32:49.460
Something that linear
grows as a straight line,

00:32:49.460 --> 00:32:50.990
as you would expect.

00:32:50.990 --> 00:32:51.620
Nice behavior.

00:32:51.620 --> 00:32:55.140
Quadratic starts to
grow more quickly.

00:32:55.140 --> 00:32:56.930
The log is always
better than linear

00:32:56.930 --> 00:33:01.300
because it slows down
as we increase the size.

00:33:01.300 --> 00:33:03.860
n log n or log linear
is a funky term,

00:33:03.860 --> 00:33:06.770
but we're going to see it's
a very common complexity

00:33:06.770 --> 00:33:09.830
for really valuable algorithms
in computer science.

00:33:09.830 --> 00:33:12.840
And it has a nice behavior,
sort of between the linear

00:33:12.840 --> 00:33:14.230
and the quadratic.

00:33:14.230 --> 00:33:16.566
And exponential blows up.

00:33:16.566 --> 00:33:18.420
Just to remind you
of that-- well,

00:33:18.420 --> 00:33:20.045
sorry-- let me show
you how we're going

00:33:20.045 --> 00:33:22.480
to do the reasoning about this.

00:33:22.480 --> 00:33:25.114
So here's how we're
going to reason about it.

00:33:25.114 --> 00:33:26.530
We've already seen
some code where

00:33:26.530 --> 00:33:30.250
I started working through this
process of counting operations.

00:33:30.250 --> 00:33:32.020
Here are the tools
I want you to use.

00:33:32.020 --> 00:33:34.090
Given a piece of
code, you're going

00:33:34.090 --> 00:33:37.660
to reason about each
chunk of code separately.

00:33:37.660 --> 00:33:41.966
If you've got sequential
pieces of code,

00:33:41.966 --> 00:33:43.840
then the rules are called
the law of addition

00:33:43.840 --> 00:33:47.530
for order of growth is
that the order of growth

00:33:47.530 --> 00:33:50.080
of the combination
is the combination

00:33:50.080 --> 00:33:52.380
of the order of the growth.

00:33:52.380 --> 00:33:54.340
Say that quickly 10 times.

00:33:54.340 --> 00:33:56.130
But let's look at
an example of that.

00:33:56.130 --> 00:33:58.200
Here are two loops.

00:33:58.200 --> 00:34:00.094
You've already seen
examples of how

00:34:00.094 --> 00:34:01.260
to reason about those loops.

00:34:01.260 --> 00:34:05.524
For this one, it's
linear in the size of n.

00:34:05.524 --> 00:34:08.190
I'm going to go through the loop
n times doing a constant amount

00:34:08.190 --> 00:34:09.489
of things each time around.

00:34:09.489 --> 00:34:12.389
But what I just
showed, that's order n.

00:34:12.389 --> 00:34:15.270
This one-- again, I'm doing
just a constant number of things

00:34:15.270 --> 00:34:20.380
inside the loop-- but
notice, that it's n squared.

00:34:20.380 --> 00:34:23.360
So that's order n squared.

00:34:23.360 --> 00:34:24.870
n times n.

00:34:24.870 --> 00:34:27.719
The combination is I have to do
this work and then that work.

00:34:27.719 --> 00:34:31.850
So I write that as saying that
is order of n plus order of n

00:34:31.850 --> 00:34:33.260
squared.

00:34:33.260 --> 00:34:35.510
But by this up here,
that is the same

00:34:35.510 --> 00:34:39.179
as saying what's the order of
growth of n plus n squared.

00:34:39.179 --> 00:34:39.679
Oh yeah.

00:34:39.679 --> 00:34:41.060
We just saw that.

00:34:41.060 --> 00:34:43.150
Says it's n squared.

00:34:43.150 --> 00:34:44.794
So addition or the
law of addition

00:34:44.794 --> 00:34:46.210
let's be reasonable
about the fact

00:34:46.210 --> 00:34:50.139
that this will be an
order n squared algorithm.

00:34:50.139 --> 00:34:52.310
Second one I'm going
to use is called

00:34:52.310 --> 00:34:54.510
the law of multiplication.

00:34:54.510 --> 00:35:01.260
And this says when I have nested
statements or nested loops,

00:35:01.260 --> 00:35:03.140
I need to reason about those.

00:35:03.140 --> 00:35:06.410
And in that case, what I want
to argue-- or not argue-- state

00:35:06.410 --> 00:35:09.650
is that the order of growth
here is a multiplication.

00:35:09.650 --> 00:35:11.266
That is, when I
have nested things,

00:35:11.266 --> 00:35:12.890
I figure out what's
the order of growth

00:35:12.890 --> 00:35:16.080
of the inner part, what's the
order growth of the outer part,

00:35:16.080 --> 00:35:18.890
and I'm going to multiply--
bleh, try again-- I'm

00:35:18.890 --> 00:35:22.050
going to multiply together
those orders of growth,

00:35:22.050 --> 00:35:24.150
get the overall order of growth.

00:35:24.150 --> 00:35:26.120
If you think about
it, it makes sense.

00:35:26.120 --> 00:35:27.710
Look at my little example here.

00:35:27.710 --> 00:35:28.960
It's a trivial little example.

00:35:28.960 --> 00:35:33.110
But I'm looping for
i from 0 up to n.

00:35:33.110 --> 00:35:36.470
For every value of i, I'm
looping for j from 0 up to n.

00:35:36.470 --> 00:35:39.250
And then I'm printing
out A. I'm the Fonz.

00:35:39.250 --> 00:35:41.870
I'm saying heyyy a lot.

00:35:41.870 --> 00:35:42.370
Oh, come on.

00:35:42.370 --> 00:35:44.578
At least throw something,
I mean, when it's that bad.

00:35:44.578 --> 00:35:45.179
Right?

00:35:45.179 --> 00:35:46.720
Want to make sure
you're still awake.

00:35:46.720 --> 00:35:46.930
OK.

00:35:46.930 --> 00:35:47.650
You get the idea.

00:35:47.650 --> 00:35:52.360
But what I want to show you here
is notice the order of growth.

00:35:52.360 --> 00:35:54.400
That's order n.

00:35:54.400 --> 00:35:55.240
Right?

00:35:55.240 --> 00:35:57.340
I'm doing that n times.

00:35:57.340 --> 00:35:59.650
But I'm doing that
for each value of i.

00:35:59.650 --> 00:36:03.100
The outer piece here
loops also n times.

00:36:03.100 --> 00:36:05.430
For each value of i, I'm
doing order n things.

00:36:05.430 --> 00:36:10.770
So I'm doing order of n
times order of n steps.

00:36:10.770 --> 00:36:13.930
And by that law, that is
the same order of n times

00:36:13.930 --> 00:36:16.100
n or n squared.

00:36:16.100 --> 00:36:19.849
So this is a
quadratic expression.

00:36:19.849 --> 00:36:21.140
You're going to see that a lot.

00:36:21.140 --> 00:36:25.590
Nested loops typically
have that kind of behavior.

00:36:25.590 --> 00:36:29.117
Not always, but typically
have that kind of behavior.

00:36:29.117 --> 00:36:30.700
So what you're going
to see is there's

00:36:30.700 --> 00:36:32.110
a set of complexity classes.

00:36:32.110 --> 00:36:34.960
And we're about to
start filling these in.

00:36:34.960 --> 00:36:37.810
Order one is constant.

00:36:37.810 --> 00:36:39.790
Says amount of time it
takes doesn't depend

00:36:39.790 --> 00:36:42.170
on the size of the problem.

00:36:42.170 --> 00:36:44.080
These are really
rare that you get.

00:36:44.080 --> 00:36:47.330
They tend to be trivial pieces
of code, but they're valuable.

00:36:47.330 --> 00:36:50.307
Log n reflects
logarithmic runtime.

00:36:50.307 --> 00:36:51.890
You can sort of read
the rest of them.

00:36:51.890 --> 00:36:54.390
These are the kinds of things
that we're going to deal with.

00:36:54.390 --> 00:36:58.494
We are going to see examples
here, here, and here.

00:36:58.494 --> 00:37:00.410
And later on, we're going
to come back and see

00:37:00.410 --> 00:37:03.660
these, which are really
nice examples to have.

00:37:03.660 --> 00:37:05.732
Just to remind you why
these orders of growth

00:37:05.732 --> 00:37:07.440
matter-- sorry, that's
just reminding you

00:37:07.440 --> 00:37:08.273
what they look like.

00:37:08.273 --> 00:37:11.040
We've already done that.

00:37:11.040 --> 00:37:13.780
Here is the difference
between constant log,

00:37:13.780 --> 00:37:17.940
linear, log linear
squared, and exponential.

00:37:17.940 --> 00:37:23.520
When n is equal to 10,
100, 1,000 or a million.

00:37:23.520 --> 00:37:26.670
I know you know this, but I want
to drive home the difference.

00:37:26.670 --> 00:37:29.190
Something that's constant
is wonderful, no matter

00:37:29.190 --> 00:37:30.150
how big the problem is.

00:37:30.150 --> 00:37:31.920
Takes the same amount of time.

00:37:31.920 --> 00:37:34.710
Something that is
log is pretty nice.

00:37:34.710 --> 00:37:36.840
Increase the size of
the problem by 10,

00:37:36.840 --> 00:37:39.870
it increases by a factor of 2.

00:37:39.870 --> 00:37:41.310
From another 10,
it only increases

00:37:41.310 --> 00:37:43.290
by a factor of another 50%.

00:37:43.290 --> 00:37:44.910
It only increases a little bit.

00:37:44.910 --> 00:37:47.150
That's a gorgeous kind
of problem to have.

00:37:47.150 --> 00:37:48.570
Linear-- not so bad.

00:37:48.570 --> 00:37:51.570
I go from 10 to 100
to 1,000 to a million.

00:37:51.570 --> 00:37:54.380
You can see log linear
is not bad either.

00:37:54.380 --> 00:37:55.250
Right?

00:37:55.250 --> 00:37:58.600
A factor of 10 increase here
is only a factor of 20 increase

00:37:58.600 --> 00:37:59.100
there.

00:37:59.100 --> 00:38:03.550
A factor of 10 increase there
is only a factor of 30 increase

00:38:03.550 --> 00:38:04.050
there.

00:38:04.050 --> 00:38:07.450
So log linear doesn't
grow that badly.

00:38:07.450 --> 00:38:09.890
But look at the difference
between n squared and 2

00:38:09.890 --> 00:38:11.296
to the n.

00:38:11.296 --> 00:38:13.460
I actually did think
of printing this out.

00:38:13.460 --> 00:38:15.320
By the way, Python
will compute this.

00:38:15.320 --> 00:38:17.360
But it was taken pages
and pages and pages.

00:38:17.360 --> 00:38:18.530
I didn't want to do it.

00:38:18.530 --> 00:38:20.420
You get the point.

00:38:20.420 --> 00:38:22.730
Exponential-- always much worse.

00:38:22.730 --> 00:38:26.780
Always much worse than
a quadratic or a power

00:38:26.780 --> 00:38:27.620
expression.

00:38:27.620 --> 00:38:31.100
And you really see
the difference here.

00:38:31.100 --> 00:38:32.600
All right.

00:38:32.600 --> 00:38:35.570
The reason I put this up is
as you design algorithms,

00:38:35.570 --> 00:38:41.369
your goal is to be as high up
in this listing as you can.

00:38:41.369 --> 00:38:43.160
The closer you are to
the top of this list,

00:38:43.160 --> 00:38:44.720
the better off you are.

00:38:44.720 --> 00:38:47.900
If you have a solution
that's down here,

00:38:47.900 --> 00:38:49.550
bring a sleeping
bag and some coffee.

00:38:49.550 --> 00:38:50.690
You're going to be
there for a while.

00:38:50.690 --> 00:38:51.290
Right?

00:38:51.290 --> 00:38:54.230
You really want to try
and avoid that if you can.

00:38:54.230 --> 00:38:57.780
So now what we want to do,
both for the rest of today

00:38:57.780 --> 00:39:01.900
in the last 15 minutes
and then next week,

00:39:01.900 --> 00:39:06.200
is start identifying
common algorithms

00:39:06.200 --> 00:39:07.991
and what is their complexity.

00:39:07.991 --> 00:39:09.740
As I said to you way
back at the beginning

00:39:09.740 --> 00:39:11.180
of this lecture, which
I'm sure you remember,

00:39:11.180 --> 00:39:13.760
it's not just to be able
to identify the complexity.

00:39:13.760 --> 00:39:17.330
I want you to see
how choices algorithm

00:39:17.330 --> 00:39:20.120
design are going to
lead to particular kinds

00:39:20.120 --> 00:39:22.970
of consequences in terms of
what this is going to cost you.

00:39:22.970 --> 00:39:25.000
That's your goal here.

00:39:25.000 --> 00:39:26.110
All right.

00:39:26.110 --> 00:39:27.500
We've already seen
some examples.

00:39:27.500 --> 00:39:28.750
I'm going to do one more here.

00:39:28.750 --> 00:39:30.670
But simple iterative
loop algorithms

00:39:30.670 --> 00:39:33.760
are typically linear.

00:39:33.760 --> 00:39:35.830
Here's another
version of searching.

00:39:35.830 --> 00:39:38.050
Imagine I'll have
an unsorted list.

00:39:38.050 --> 00:39:39.340
Arbitrary order.

00:39:39.340 --> 00:39:41.380
Here's another way of
doing the linear search.

00:39:41.380 --> 00:39:43.300
Looks a little bit faster.

00:39:43.300 --> 00:39:46.810
I'm going to set a flag
initially to false.

00:39:46.810 --> 00:39:49.630
And then I'm going
to loop for i from 0

00:39:49.630 --> 00:39:52.330
up to the length of L.
I'm going to use that

00:39:52.330 --> 00:39:55.300
to index into the list, pull
out each element of the list

00:39:55.300 --> 00:39:58.510
in turn, and check to see is
it the thing I'm looking for.

00:39:58.510 --> 00:40:01.780
As soon as I find it, I'm
going to send-- sorry--

00:40:01.780 --> 00:40:04.650
set the flag to true.

00:40:04.650 --> 00:40:05.440
OK?

00:40:05.440 --> 00:40:08.350
So that when I return out of the
loop, I can just return found.

00:40:08.350 --> 00:40:11.200
And if I found it to be
true, if I never found it,

00:40:11.200 --> 00:40:14.610
found will still be
false and I'll return it.

00:40:14.610 --> 00:40:16.320
We could count the
operations here,

00:40:16.320 --> 00:40:19.290
but you've already seen
examples of doing that.

00:40:19.290 --> 00:40:23.270
This is linear,
because I'm looping

00:40:23.270 --> 00:40:26.220
n times if n is the length
of the list over there.

00:40:26.220 --> 00:40:30.080
And the number of things I do
inside the loop is constant.

00:40:30.080 --> 00:40:32.360
Now, you might
say, wait a minute.

00:40:32.360 --> 00:40:34.110
This is really brain
damaged, or if you're

00:40:34.110 --> 00:40:37.391
being more politically correct,
computationally challenged.

00:40:37.391 --> 00:40:37.890
OK?

00:40:37.890 --> 00:40:40.720
In the sense of
once I've found it,

00:40:40.720 --> 00:40:43.110
why bother looking at
the rest of the list?

00:40:43.110 --> 00:40:48.160
So in fact, I could just
return true right here.

00:40:48.160 --> 00:40:52.202
Does that change the order
of growth of this algorithm?

00:40:52.202 --> 00:40:54.170
No.

00:40:54.170 --> 00:40:56.180
Changes the average time.

00:40:56.180 --> 00:40:58.040
I'm going to stop faster.

00:40:58.040 --> 00:41:00.260
But remember the order
of growth captures

00:41:00.260 --> 00:41:01.820
what's the worst case behavior.

00:41:01.820 --> 00:41:04.017
And the worst case
behavior is the elements

00:41:04.017 --> 00:41:05.850
not in the list I got
to look at everything.

00:41:05.850 --> 00:41:09.980
So this will be an example
of a linear algorithm.

00:41:09.980 --> 00:41:12.740
And you can see, I'm
looping length of L times

00:41:12.740 --> 00:41:14.490
over the loop inside of there.

00:41:14.490 --> 00:41:17.060
It's taking the
order one to test it.

00:41:17.060 --> 00:41:19.835
So it's order n.

00:41:19.835 --> 00:41:22.370
And if I were to actually count
it, there's the expression.

00:41:22.370 --> 00:41:25.910
It's 1 plus 4n plus 1, which
is 4n plus 2, which by my rule

00:41:25.910 --> 00:41:28.280
says I don't care about
the additive constant.

00:41:28.280 --> 00:41:30.060
I only care about
the dominant term.

00:41:30.060 --> 00:41:32.650
And I don't care about that
multiplicative constant.

00:41:32.650 --> 00:41:35.250
It's order n.

00:41:35.250 --> 00:41:38.790
An example of a template
you're going to see a lot.

00:41:38.790 --> 00:41:43.306
Now, order n where n is
the length of the list

00:41:43.306 --> 00:41:44.430
and I need to specify that.

00:41:44.430 --> 00:41:46.860
That's the thing I'm after.

00:41:46.860 --> 00:41:48.920
If you think about
it, I cheated.

00:41:48.920 --> 00:41:50.020
Sorry-- I never cheat.

00:41:50.020 --> 00:41:50.530
I'm tenure.

00:41:50.530 --> 00:41:51.340
I never cheat.

00:41:51.340 --> 00:41:52.785
I just mislead you badly.

00:41:55.300 --> 00:41:57.710
Not a chance.

00:41:57.710 --> 00:42:02.250
How do I know that accessing
an element of the list

00:42:02.250 --> 00:42:03.930
takes constant time?

00:42:03.930 --> 00:42:06.530
I made an assumption about that.

00:42:06.530 --> 00:42:09.450
And this is a reasonable
thing to ask about-- both

00:42:09.450 --> 00:42:11.950
what am I assuming about
the constant operations

00:42:11.950 --> 00:42:14.249
and how do I know
that's actually true?

00:42:14.249 --> 00:42:16.290
Well, it gives me a chance
to point out something

00:42:16.290 --> 00:42:17.910
that Python does
very effectively.

00:42:17.910 --> 00:42:19.872
Not all languages do.

00:42:19.872 --> 00:42:20.830
But think about a list.

00:42:20.830 --> 00:42:22.710
Suppose I've got a list
that's all integers.

00:42:22.710 --> 00:42:24.390
I'm going to need
some amount of memory

00:42:24.390 --> 00:42:26.410
to represent each integer.

00:42:26.410 --> 00:42:30.600
So if a byte is 8 bits, I might
reserve 4 bytes or 32 bits

00:42:30.600 --> 00:42:33.660
to cover any reasonable
sized integer.

00:42:33.660 --> 00:42:36.450
When I represent a list, I
could simply have each of them

00:42:36.450 --> 00:42:37.000
in turn.

00:42:37.000 --> 00:42:38.850
So what do I need to know?

00:42:38.850 --> 00:42:40.980
I'm going to allocate
out a particular length--

00:42:40.980 --> 00:42:44.940
say 4 bytes, 32 bits, 32
sequential elements of memory

00:42:44.940 --> 00:42:46.860
to represent each integer.

00:42:46.860 --> 00:42:49.794
And then I just
need to know where's

00:42:49.794 --> 00:42:51.210
the first part of
the list, what's

00:42:51.210 --> 00:42:53.418
the address and memory of
the first part of the list.

00:42:53.418 --> 00:42:55.440
And to get to the
i-th element, I take

00:42:55.440 --> 00:42:59.930
that base plus 4 bytes times i.

00:42:59.930 --> 00:43:02.190
And I can go straight
to this point

00:43:02.190 --> 00:43:04.680
without having to
walk down the list.

00:43:04.680 --> 00:43:05.750
That's nice.

00:43:05.750 --> 00:43:06.570
OK?

00:43:06.570 --> 00:43:09.660
It says, in fact, I can get
to any element of memory--

00:43:09.660 --> 00:43:14.921
I'm sorry-- any element of
the list in constant time.

00:43:14.921 --> 00:43:15.420
OK.

00:43:15.420 --> 00:43:17.740
Now, what if the things I'm
representing aren't integers?

00:43:17.740 --> 00:43:19.448
They're arbitrary
things and they take up

00:43:19.448 --> 00:43:21.820
a big chunk of space.

00:43:21.820 --> 00:43:23.710
Well, if the list
is heterogeneous,

00:43:23.710 --> 00:43:26.520
we use a nice technique
called indirection.

00:43:26.520 --> 00:43:28.930
And that simply says
we, again, have a list.

00:43:28.930 --> 00:43:30.610
We know the address
of this point.

00:43:30.610 --> 00:43:33.730
We know the address there for
the i-th element of this list.

00:43:33.730 --> 00:43:38.005
But inside of here, we don't
store the actual value.

00:43:38.005 --> 00:43:40.907
We store a pointer to
where it is in memory.

00:43:40.907 --> 00:43:42.490
Just what these
things are indicating.

00:43:42.490 --> 00:43:44.740
So they can be arbitrary size.

00:43:44.740 --> 00:43:47.500
But again, I can get to any
element in constant time, which

00:43:47.500 --> 00:43:49.730
is exactly what I want.

00:43:49.730 --> 00:43:52.550
So that's great.

00:43:52.550 --> 00:43:53.350
OK.

00:43:53.350 --> 00:43:55.730
Now, suppose I tell you
that the list is sorted.

00:43:55.730 --> 00:43:57.750
It's in increasing order.

00:43:57.750 --> 00:44:00.190
I can be more clever
about my algorithm.

00:44:00.190 --> 00:44:01.720
Because now, as I
loop through it,

00:44:01.720 --> 00:44:05.680
I can say if it's the thing I'm
looking for, just return true.

00:44:05.680 --> 00:44:09.010
If the element of the list
is bigger than the thing

00:44:09.010 --> 00:44:10.795
I'm looking for, I'm done.

00:44:10.795 --> 00:44:12.670
I don't need to look at
the rest of the list,

00:44:12.670 --> 00:44:15.340
because I know it can't be there
because it's ordered or sorted.

00:44:15.340 --> 00:44:16.770
I can just return false.

00:44:16.770 --> 00:44:19.735
If I get all the way through
the loop, I can return false.

00:44:19.735 --> 00:44:21.610
So I only have to look
until I get to a point

00:44:21.610 --> 00:44:23.151
where the thing in
the list is bigger

00:44:23.151 --> 00:44:24.572
than what I'm looking for.

00:44:24.572 --> 00:44:27.180
It's the order of growth here.

00:44:27.180 --> 00:44:31.670
Again, the average time
behavior will be faster.

00:44:31.670 --> 00:44:33.320
But the order of
growth is I've got

00:44:33.320 --> 00:44:34.970
to do order of
length of the list

00:44:34.970 --> 00:44:38.720
to go through the loop,
order of one to do the test,

00:44:38.720 --> 00:44:40.430
and in the worst
case, again, I still

00:44:40.430 --> 00:44:42.754
have to go through
the entire list.

00:44:42.754 --> 00:44:44.420
So the order of growth
here is the same.

00:44:44.420 --> 00:44:47.780
It is, again, linear in
the length of the list,

00:44:47.780 --> 00:44:49.950
even though the runtime
will be different depending

00:44:49.950 --> 00:44:52.742
whether it's sorted or not.

00:44:52.742 --> 00:44:54.200
I want you to hold
on to that idea,

00:44:54.200 --> 00:44:56.360
because we're going to come
back to the sorted list

00:44:56.360 --> 00:44:57.860
next week to see
that there actually

00:44:57.860 --> 00:45:00.850
are much more efficient ways
to use the fact that a list is

00:45:00.850 --> 00:45:02.960
sorted to do the search.

00:45:02.960 --> 00:45:07.710
But both of these versions
same order growth, order n.

00:45:07.710 --> 00:45:08.820
OK.

00:45:08.820 --> 00:45:11.130
So lurching through a
list-- right, sorry--

00:45:11.130 --> 00:45:13.530
searching through
a list in sequence

00:45:13.530 --> 00:45:16.110
is linear because of that loop.

00:45:16.110 --> 00:45:18.210
There are other things
that have a similar flavor.

00:45:18.210 --> 00:45:19.585
And I'm going to
do these quickly

00:45:19.585 --> 00:45:21.370
to get to the last example.

00:45:21.370 --> 00:45:25.260
Imagine I give you a string of
characters that are all soon

00:45:25.260 --> 00:45:26.760
to be composed of
decimal digits.

00:45:26.760 --> 00:45:28.710
I just want to add them all up.

00:45:28.710 --> 00:45:31.464
This is also linear,
because there's the loop.

00:45:31.464 --> 00:45:33.630
I'm going to loop over the
characters in the string.

00:45:33.630 --> 00:45:35.730
I'm going to cast
them into integers,

00:45:35.730 --> 00:45:37.620
add them in, and
return the value.

00:45:37.620 --> 00:45:43.730
This is linear in the
length of the input s.

00:45:43.730 --> 00:45:44.990
Notice the pattern.

00:45:44.990 --> 00:45:46.527
That loop-- that
in-iterative loop--

00:45:46.527 --> 00:45:48.110
it's got that linear
behavior, because

00:45:48.110 --> 00:45:49.943
inside of the loop
constant number of things

00:45:49.943 --> 00:45:52.380
that I'm executing.

00:45:52.380 --> 00:45:53.760
We already looked at fact iter.

00:45:53.760 --> 00:45:55.569
Same idea.

00:45:55.569 --> 00:45:58.110
There's the loop I'm going to
do that n times inside the loop

00:45:58.110 --> 00:46:00.380
a constant amount of things.

00:46:00.380 --> 00:46:02.780
So looping around it is order n.

00:46:02.780 --> 00:46:04.070
There's the actual expression.

00:46:04.070 --> 00:46:06.170
But again, the pattern
I want you to see here

00:46:06.170 --> 00:46:09.620
is that this is order n.

00:46:09.620 --> 00:46:11.600
OK.

00:46:11.600 --> 00:46:13.250
Last example for today.

00:46:13.250 --> 00:46:17.470
I know you're all secretly
looking at your watches.

00:46:17.470 --> 00:46:20.340
Standard loops,
typically linear.

00:46:20.340 --> 00:46:21.820
What about nested loops?

00:46:21.820 --> 00:46:24.310
What about loops that
have loops inside of them?

00:46:24.310 --> 00:46:26.270
How long do they take?

00:46:26.270 --> 00:46:28.830
I want to show you a
couple of examples.

00:46:28.830 --> 00:46:32.820
And mostly, I want to show
you how to reason about them.

00:46:32.820 --> 00:46:36.080
Suppose I gave you two
lists composed of integers,

00:46:36.080 --> 00:46:38.010
and I want to know
is the first list

00:46:38.010 --> 00:46:41.131
a subset of the second list.

00:46:41.131 --> 00:46:43.630
Codes in the handbook, by the
way, if you want to go run it.

00:46:43.630 --> 00:46:45.370
But basically, the
simple idea would

00:46:45.370 --> 00:46:48.965
be I'm going to loop over every
element in the first list.

00:46:48.965 --> 00:46:50.340
And for each one
of those, I want

00:46:50.340 --> 00:46:52.362
to say is it in the second list?

00:46:52.362 --> 00:46:53.820
So I'll use the
same kind of trick.

00:46:53.820 --> 00:46:56.310
I'll set up a flag
that's initially false.

00:46:56.310 --> 00:46:59.550
And then I'm going to loop over
everything in the second list.

00:46:59.550 --> 00:47:03.350
And if that thing is equal
to the thing I'm looking for,

00:47:03.350 --> 00:47:05.750
I'll set match to true and
break out of the loop--

00:47:05.750 --> 00:47:07.287
the inner loop.

00:47:07.287 --> 00:47:09.120
If I get all the way
through the second list

00:47:09.120 --> 00:47:10.495
and I haven't
found the thing I'm

00:47:10.495 --> 00:47:14.160
looking for, when I break
out or come out of this loop,

00:47:14.160 --> 00:47:18.420
matched in that case, will still
be false and all return false.

00:47:18.420 --> 00:47:20.550
But if up here, I found
something that matched,

00:47:20.550 --> 00:47:21.400
match would be true.

00:47:21.400 --> 00:47:22.560
I break out of it.

00:47:22.560 --> 00:47:23.760
It's not false.

00:47:23.760 --> 00:47:27.664
Therefore, a return true.

00:47:27.664 --> 00:47:28.830
I want you look at the code.

00:47:28.830 --> 00:47:30.246
You should be able
to look at this

00:47:30.246 --> 00:47:32.002
and realize what it's doing.

00:47:32.002 --> 00:47:33.460
For each element
in the first list,

00:47:33.460 --> 00:47:36.100
I walk through the second list
to say is that element there.

00:47:36.100 --> 00:47:37.650
And if it is, I return true.

00:47:37.650 --> 00:47:40.150
If that's true for all of the
elements in the first list,

00:47:40.150 --> 00:47:42.930
I return true overall.

00:47:42.930 --> 00:47:44.214
OK.

00:47:44.214 --> 00:47:44.880
Order of growth.

00:47:47.790 --> 00:47:51.290
Outer loop-- this loop
I'm going to execute

00:47:51.290 --> 00:47:53.190
the length of L1 times.

00:47:53.190 --> 00:47:53.690
Right?

00:47:53.690 --> 00:47:55.950
I've got to walk
down that first list.

00:47:55.950 --> 00:47:57.770
If I call that n, it's
going to take that n

00:47:57.770 --> 00:48:00.760
times over the outer loop.

00:48:00.760 --> 00:48:03.180
But what about n here?

00:48:03.180 --> 00:48:06.300
All of the earlier examples,
we had a constant number

00:48:06.300 --> 00:48:09.150
of operations
inside of the loop.

00:48:09.150 --> 00:48:09.780
Here, we don't.

00:48:09.780 --> 00:48:13.980
We've got another loop that's
looping over in principle

00:48:13.980 --> 00:48:17.220
all the elements
of the second list.

00:48:17.220 --> 00:48:20.310
So in each iteration is going
to execute the inner loop up

00:48:20.310 --> 00:48:24.240
to length of L2 times, where
inside of this inner loop

00:48:24.240 --> 00:48:27.910
there is a constant
number of operations.

00:48:27.910 --> 00:48:30.070
Ah, nice.

00:48:30.070 --> 00:48:32.800
That's the multiplicative
law of orders of growth.

00:48:32.800 --> 00:48:35.309
It says if this is
order length L1.

00:48:35.309 --> 00:48:36.850
And we're going to
do that then order

00:48:36.850 --> 00:48:41.941
length of L2 times, the
order of growth is a product.

00:48:41.941 --> 00:48:44.260
And the most common or
the worst case behavior

00:48:44.260 --> 00:48:47.040
is going to be when the
lists are of the same length

00:48:47.040 --> 00:48:49.950
and none of the elements
of L1 are in L2.

00:48:49.950 --> 00:48:52.680
And in that case, we're
going to get something that's

00:48:52.680 --> 00:48:56.160
order n squared
quadratic, where n

00:48:56.160 --> 00:49:00.900
is the length of the list in
terms of number of operations.

00:49:00.900 --> 00:49:03.280
I don't really
care about subsets.

00:49:03.280 --> 00:49:05.230
I've got one more example.

00:49:05.230 --> 00:49:07.730
We could similarly
do intersection.

00:49:07.730 --> 00:49:10.440
If I wanted to say what is
the intersection of two lists?

00:49:10.440 --> 00:49:13.380
What elements are on
both list 1 and list 2?

00:49:13.380 --> 00:49:14.820
Same basic idea.

00:49:14.820 --> 00:49:17.230
Here, I've got a
pair of nested loops.

00:49:17.230 --> 00:49:19.140
I'm looping over
everything in L1.

00:49:19.140 --> 00:49:22.050
For that, I'm looping
over everything in L2.

00:49:22.050 --> 00:49:24.930
And if they are the same,
I'm going to put that

00:49:24.930 --> 00:49:27.620
into a temporary variable.

00:49:27.620 --> 00:49:29.596
Once I've done that, I
need to clean things up.

00:49:29.596 --> 00:49:31.220
So I'm going to write
another loop that

00:49:31.220 --> 00:49:34.070
sets up an internal variable
and then runs through everything

00:49:34.070 --> 00:49:36.410
in the list I
accumulated, making sure

00:49:36.410 --> 00:49:38.074
that it's not already there.

00:49:38.074 --> 00:49:40.490
And as long as it isn't, I'm
going to put it in the result

00:49:40.490 --> 00:49:42.580
and return it.

00:49:42.580 --> 00:49:43.380
I did it quickly.

00:49:43.380 --> 00:49:44.200
You can look through it.

00:49:44.200 --> 00:49:45.658
You'll see it does
the right thing.

00:49:45.658 --> 00:49:48.140
What I want it to see is
what's the order of growth.

00:49:48.140 --> 00:49:49.830
I need to look at this piece.

00:49:49.830 --> 00:49:51.840
Then I need to
look at that piece.

00:49:51.840 --> 00:49:58.240
This piece-- well, it's order
length L1 to do the outer loop.

00:49:58.240 --> 00:50:01.120
For each version of
e1, I've got to do

00:50:01.120 --> 00:50:05.080
order of length L2 things
inside to accumulate them.

00:50:05.080 --> 00:50:08.400
So that's quadratic.

00:50:08.400 --> 00:50:10.410
What about the second loop?

00:50:10.410 --> 00:50:12.600
Well, this one is a
little more subtle.

00:50:12.600 --> 00:50:17.430
I'm only looping over temp,
which is at most going

00:50:17.430 --> 00:50:20.433
to be length L1 long.

00:50:20.433 --> 00:50:27.730
But I'm checking to see
is that element in a list?

00:50:27.730 --> 00:50:29.400
And it depends on
the implementation.

00:50:29.400 --> 00:50:31.032
But typically, that's
going to take up

00:50:31.032 --> 00:50:32.490
to the length of
the list to do it.

00:50:32.490 --> 00:50:34.590
I got to look to see
is it there or not.

00:50:34.590 --> 00:50:36.930
And so that inner
loop if we assume

00:50:36.930 --> 00:50:38.850
the lists are the
same size is also

00:50:38.850 --> 00:50:42.280
going to take potentially
up to length L1 steps.

00:50:42.280 --> 00:50:45.174
And so this is,
again, quadratic.

00:50:45.174 --> 00:50:46.590
It's actually two
quadratics-- one

00:50:46.590 --> 00:50:49.560
for the first nested loop,
one for the second one,

00:50:49.560 --> 00:50:52.680
because there's an implicit
second loop right there.

00:50:52.680 --> 00:50:56.320
But overall, it's quadratic.

00:50:56.320 --> 00:50:58.630
So what you see
in general-- this

00:50:58.630 --> 00:51:01.430
is a really dumb way
to compute n squared.

00:51:01.430 --> 00:51:03.730
When you have nested
loops, typically, it's

00:51:03.730 --> 00:51:06.100
going to be quadratic behavior.

00:51:06.100 --> 00:51:07.760
And so what we've
done then is we've

00:51:07.760 --> 00:51:10.040
started to build up examples.

00:51:10.040 --> 00:51:12.800
We've now seen simple looping
mechanisms, simple iterative

00:51:12.800 --> 00:51:14.480
mechanisms, nested loops.

00:51:14.480 --> 00:51:17.870
They tend to naturally give
rise to linear and quadratic

00:51:17.870 --> 00:51:18.869
complexity.

00:51:18.869 --> 00:51:20.660
And next time, we're
going to start looking

00:51:20.660 --> 00:51:22.670
at more interesting classes.

00:51:22.670 --> 00:51:25.153
And we'll see you next time.