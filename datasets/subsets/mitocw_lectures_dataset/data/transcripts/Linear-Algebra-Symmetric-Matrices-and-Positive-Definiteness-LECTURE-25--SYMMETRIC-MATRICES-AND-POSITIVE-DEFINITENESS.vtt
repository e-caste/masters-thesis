WEBVTT

00:00:00.000 --> 00:00:11.200
-- one and --

00:00:11.200 --> 00:00:14.470
the lecture on
symmetric matrixes.

00:00:14.470 --> 00:00:17.270
So that's the most
important class

00:00:17.270 --> 00:00:19.960
of matrixes, symmetric matrixes.

00:00:19.960 --> 00:00:22.710
A equals A transpose.

00:00:22.710 --> 00:00:26.930
So the first points, the
main points of the lecture

00:00:26.930 --> 00:00:31.170
I'll tell you right away.

00:00:31.170 --> 00:00:33.740
What's special about
the eigenvalues?

00:00:33.740 --> 00:00:35.900
What's special about
the eigenvectors?

00:00:35.900 --> 00:00:42.000
This is -- the way we
now look at a matrix.

00:00:42.000 --> 00:00:45.180
We want to know about its
eigenvalues and eigenvectors

00:00:45.180 --> 00:00:48.580
and if we have a
special type of matrix,

00:00:48.580 --> 00:00:51.490
that should tell us
something about eigenvalues

00:00:51.490 --> 00:00:53.220
and eigenvectors.

00:00:53.220 --> 00:00:56.800
Like Markov matrixes, they
have an eigenvalue equal

00:00:56.800 --> 00:00:57.440
one.

00:00:57.440 --> 00:01:03.270
Now symmetric matrixes, can I
just tell you right off what

00:01:03.270 --> 00:01:06.250
the main facts -- the
two main facts are?

00:01:06.250 --> 00:01:09.510
The eigenvalues of a
symmetric matrix, real --

00:01:09.510 --> 00:01:12.320
this is a real
symmetric matrix, we --

00:01:12.320 --> 00:01:15.400
talking mostly
about real matrixes.

00:01:15.400 --> 00:01:17.850
The eigenvalues are also real.

00:01:21.440 --> 00:01:25.600
So our examples of
rotation matrixes, where --

00:01:25.600 --> 00:01:29.170
where we got E- eigenvalues
that were complex,

00:01:29.170 --> 00:01:31.750
that won't happen now.

00:01:31.750 --> 00:01:34.610
For symmetric matrixes,
the eigenvalues are real

00:01:34.610 --> 00:01:37.960
and the eigenvectors
are also very special.

00:01:37.960 --> 00:01:41.330
The eigenvectors are
perpendicular, orthogonal,

00:01:41.330 --> 00:01:42.520
so which do you prefer?

00:01:42.520 --> 00:01:44.300
I'll say perpendicular.

00:01:44.300 --> 00:01:46.830
Perp- well, they're
both long words.

00:01:52.560 --> 00:01:55.290
Okay, right.

00:01:55.290 --> 00:02:01.090
So -- I have a --
you should say "why?"

00:02:01.090 --> 00:02:06.650
and I'll at least answer why
for case one, maybe case two,

00:02:06.650 --> 00:02:09.070
the checking the Eigen --
that the eigenvectors are

00:02:09.070 --> 00:02:16.330
perpendicular, I'll leave
to, the -- to the book.

00:02:16.330 --> 00:02:19.640
But let's just realize what --

00:02:19.640 --> 00:02:24.520
well, first I have to say, it --

00:02:24.520 --> 00:02:29.230
it could happen, like for
the identity matrix --

00:02:29.230 --> 00:02:31.170
there's a symmetric matrix.

00:02:31.170 --> 00:02:33.400
Its eigenvalues are
certainly all real,

00:02:33.400 --> 00:02:36.460
they're all one for
the identity matrix.

00:02:36.460 --> 00:02:38.480
What about the eigenvectors?

00:02:38.480 --> 00:02:44.020
Well, for the identity, every
vector is an eigenvector.

00:02:44.020 --> 00:02:46.930
So how can I say
they're perpendicular?

00:02:46.930 --> 00:02:49.400
What I really mean
is the -- they --

00:02:49.400 --> 00:02:57.520
this word are should really
be written can be chosen

00:02:57.520 --> 00:02:59.230
perpendicular.

00:02:59.230 --> 00:03:02.650
That is, if we have --
it's the usual case.

00:03:02.650 --> 00:03:05.210
If the eigenvalues
are all different,

00:03:05.210 --> 00:03:09.280
then each eigenvalue has
one line of eigenvectors

00:03:09.280 --> 00:03:13.250
and those lines are
perpendicular here.

00:03:13.250 --> 00:03:16.530
But if an eigenvalue's
repeated, then there's

00:03:16.530 --> 00:03:20.730
a whole plane of eigenvectors
and all I'm saying

00:03:20.730 --> 00:03:25.330
is that in that plain, we can
choose perpendicular ones.

00:03:25.330 --> 00:03:28.520
So that's why it's a can
be chosen part, is --

00:03:28.520 --> 00:03:31.160
this is in the case of a
repeated eigenvalue where

00:03:31.160 --> 00:03:34.160
there's some real,
substantial freedom.

00:03:34.160 --> 00:03:39.110
But the typical case is
different eigenvalues,

00:03:39.110 --> 00:03:43.670
all real, one dimensional
eigenvector space,

00:03:43.670 --> 00:03:46.030
Eigen spaces, and
all perpendicular.

00:03:46.030 --> 00:03:50.470
So, just -- let's just
see the conclusion.

00:03:50.470 --> 00:03:56.270
If we accept those as
correct, what happens --

00:03:56.270 --> 00:03:59.332
and I also mean that
there's a full set of them.

00:03:59.332 --> 00:04:01.790
so forgive me for doing such
a thing, but, I'll look at the

00:04:01.790 --> 00:04:06.480
I -- so that's part of this
picture here, that there --

00:04:06.480 --> 00:04:08.880
there's a complete
set of eigenvectors,

00:04:08.880 --> 00:04:10.160
perpendicular ones.

00:04:10.160 --> 00:04:14.260
So, having a complete set
of eigenvectors means --

00:04:14.260 --> 00:04:16.600
so normal --

00:04:16.600 --> 00:04:20.649
so the usual -- maybe I put
the -- usually -- usual --

00:04:20.649 --> 00:04:26.060
usual case is that the matrix
A we can write in terms of its

00:04:26.060 --> 00:04:31.610
eigenvalue matrix and its
eigenvector matrix this way,

00:04:31.610 --> 00:04:33.350
right?

00:04:33.350 --> 00:04:36.430
We can do that in
the usual case,

00:04:36.430 --> 00:04:42.460
but now what's special when
the matrix is symmetric?

00:04:42.460 --> 00:04:45.740
So this is the
usual case, and now

00:04:45.740 --> 00:04:48.960
let me go to the symmetric case.

00:04:52.260 --> 00:04:55.990
So in the symmetric
case, A, this --

00:04:55.990 --> 00:04:58.900
this should become
somehow a little special.

00:04:58.900 --> 00:05:03.710
Well, the lambdas on the
diagonal are still on the

00:05:03.710 --> 00:05:04.500
diagonal.

00:05:04.500 --> 00:05:08.370
They're -- they're real,
but that's where they are.

00:05:08.370 --> 00:05:12.160
What about the
eigenvector matrix?

00:05:12.160 --> 00:05:15.620
So what can I do now special
about the eigenvector matrix

00:05:15.620 --> 00:05:18.850
when -- when the A
itself is symmetric,

00:05:18.850 --> 00:05:22.670
that says something good
about the eigenvector matrix,

00:05:22.670 --> 00:05:25.520
so what is this --
what does this lead to?

00:05:25.520 --> 00:05:31.120
This -- these perpendicular
eigenvectors, I can not only --

00:05:31.120 --> 00:05:33.590
I can not only guarantee
they're perpendicular,

00:05:33.590 --> 00:05:36.680
I could also make them
unit vectors, no problem,

00:05:36.680 --> 00:05:38.910
just s- scale their length to

00:05:38.910 --> 00:05:39.640
one.

00:05:39.640 --> 00:05:41.260
So what do I have?

00:05:41.260 --> 00:05:45.990
I have orthonormal eigenvectors.

00:05:49.950 --> 00:05:55.320
And what does that tell me
about the eigenvector matrix?

00:05:55.320 --> 00:05:59.617
What -- what letter should
I now use in place of S --

00:05:59.617 --> 00:06:02.200
I've got -- those two equations
are identical,1 remember S has

00:06:02.200 --> 00:06:06.080
the eigenvectors in its columns,
but now those columns are

00:06:06.080 --> 00:06:11.810
orthonormal, so the
right letter to use is Q.

00:06:11.810 --> 00:06:15.000
So that's where -- so we've got
the letter all set up, book.

00:06:15.000 --> 00:06:19.730
so this should be
Q lambda Q inverse.

00:06:19.730 --> 00:06:25.150
Q standing in our minds always
for this matrix -- in this case

00:06:25.150 --> 00:06:27.550
it's square, it's --

00:06:27.550 --> 00:06:29.915
so these are the Okay.
columns of Q, of course.

00:06:35.190 --> 00:06:39.040
And one more thing.

00:06:39.040 --> 00:06:39.950
What's Q inverse?

00:06:43.070 --> 00:06:45.120
For a matrix that has
these orthonormal columns,

00:06:45.120 --> 00:06:47.370
So I took the dot product
-- ye, somehow, it didn't --

00:06:47.370 --> 00:06:52.010
I we know that the inverse
is the same as the transpose.

00:06:52.010 --> 00:06:56.710
So here is the beautiful --

00:06:56.710 --> 00:06:59.150
there is the -- the great
haven't learned anything.

00:06:59.150 --> 00:07:03.900
description, the factorization
of a symmetric matrix.

00:07:03.900 --> 00:07:06.300
And this is, like, one
of the famous theorems

00:07:06.300 --> 00:07:10.230
of linear algebra, that if
I have a symmetric matrix,

00:07:10.230 --> 00:07:14.160
it can be factored in this form.

00:07:14.160 --> 00:07:17.230
An orthogonal matrix
times diagonal times

00:07:17.230 --> 00:07:20.270
the transpose of that
orthogonal matrix.

00:07:20.270 --> 00:07:24.160
And, of course, everybody
immediately says yes,

00:07:24.160 --> 00:07:28.510
and if this is
possible, then that's

00:07:28.510 --> 00:07:29.780
clearly symmetric, right?

00:07:29.780 --> 00:07:33.890
That -- take -- we've looked
at products of three guys like

00:07:33.890 --> 00:07:38.550
that and taken their transpose
and we got it back again.

00:07:38.550 --> 00:07:42.060
So do you -- do you see
the beauty of this --

00:07:42.060 --> 00:07:45.030
of this factorization, then?

00:07:45.030 --> 00:07:49.800
It -- it completely displays
the eigenvalues and eigenvectors

00:07:49.800 --> 00:07:53.760
the symmetry of the -- of
the whole thing, because --

00:07:53.760 --> 00:07:56.990
that product, Q times
lambda times Q transpose,

00:07:56.990 --> 00:08:02.200
if I transpose it, it -- this
comes in this position and we

00:08:02.200 --> 00:08:04.260
get that matrix back again.

00:08:04.260 --> 00:08:08.450
So that's -- in mathematics,
that's called the spectral

00:08:08.450 --> 00:08:12.260
Spectrum is the set of
eigenvalues of a matrix.

00:08:12.260 --> 00:08:12.900
theorem.

00:08:12.900 --> 00:08:17.410
Spec- it somehow comes from the
idea of the spectrum of light

00:08:17.410 --> 00:08:22.330
as a combination
of pure things --

00:08:22.330 --> 00:08:28.060
where our matrix is broken
down into pure eigenvalues

00:08:28.060 --> 00:08:30.100
and eigenvectors --

00:08:30.100 --> 00:08:35.940
in mechanics it's often called
the principle axis theorem.

00:08:35.940 --> 00:08:37.100
It's very useful.

00:08:37.100 --> 00:08:40.080
It means that if you have --

00:08:40.080 --> 00:08:42.730
we'll see it geometrically.

00:08:42.730 --> 00:08:45.020
It means that if I
have some material --

00:08:45.020 --> 00:08:49.960
if I look at the right axis, it
becomes diagonal, it becomes --

00:08:49.960 --> 00:08:50.460
the -- the

00:08:50.460 --> 00:08:51.530
I- I've done something dumb,
because I've got the --

00:08:51.530 --> 00:08:52.700
I should've taken the dot
product of this guy here with

00:08:52.700 --> 00:08:55.180
-- that's directions
don't couple together.

00:08:55.180 --> 00:08:55.920
Okay.

00:08:55.920 --> 00:08:59.420
So that's -- that -- that's
what to remember from --

00:08:59.420 --> 00:09:00.770
from this lecture.

00:09:00.770 --> 00:09:06.020
Now, I would like to say why
are the eigenvalues real?

00:09:06.020 --> 00:09:07.080
Can I do that?

00:09:07.080 --> 00:09:10.200
So -- so -- because that --
something useful comes out.

00:09:10.200 --> 00:09:16.251
So I'll just come back --
come to that question why real

00:09:16.251 --> 00:09:16.750
eigenvalues?

00:09:22.910 --> 00:09:24.120
Okay.

00:09:24.120 --> 00:09:25.920
So I have to start
from the only thing

00:09:25.920 --> 00:09:28.435
we know, Ax equal lambda x.

00:09:33.710 --> 00:09:34.560
Okay.

00:09:34.560 --> 00:09:37.090
But as far as I
know at this moment,

00:09:37.090 --> 00:09:40.100
lambda could be complex.

00:09:40.100 --> 00:09:44.570
I'm going to prove it's not
-- and x could be complex.

00:09:44.570 --> 00:09:48.370
In fact, for the moment,
even A could be --

00:09:48.370 --> 00:09:51.120
we could even think, well,
what happens if A is complex?

00:09:51.120 --> 00:09:53.750
Well, one thing we can
always do -- this is --

00:09:53.750 --> 00:09:56.000
this is like always --

00:09:56.000 --> 00:09:58.080
always okay --

00:09:58.080 --> 00:10:03.030
I can -- if I have an equation,
I can take the complex

00:10:03.030 --> 00:10:05.130
conjugate of everything.

00:10:05.130 --> 00:10:09.610
That's -- no -- no -- so A
conjugate x conjugate equal

00:10:09.610 --> 00:10:16.036
lambda conjugate x conjugate, it
just means that everywhere over

00:10:16.036 --> 00:10:18.410
here that there was a -- an
equals x bar transpose lambda

00:10:18.410 --> 00:10:21.780
bar x bar. i, then here
I changed it to a-i.

00:10:21.780 --> 00:10:24.500
That's -- that -- you
know that that step --

00:10:24.500 --> 00:10:28.100
that conjugate
business, that a+ib,

00:10:28.100 --> 00:10:31.650
if I conjugate it it's a-ib.

00:10:31.650 --> 00:10:36.820
That's the meaning of conjugate
-- and products behave right,

00:10:36.820 --> 00:10:39.390
I can conjugate every factor.

00:10:39.390 --> 00:10:43.640
So I haven't done anything yet
except to say what would be

00:10:43.640 --> 00:10:53.200
true if, x -- in any case, even
if x and lambda were complex.

00:10:53.200 --> 00:10:56.130
Of course, our -- we're
speaking about real matrixes A,

00:10:56.130 --> 00:10:58.630
so I can take that out.

00:10:58.630 --> 00:11:04.050
Actually, this already tells me
something about real matrixes.

00:11:04.050 --> 00:11:07.060
I haven't used any
assumption of A --

00:11:07.060 --> 00:11:08.950
A transpose yet.

00:11:08.950 --> 00:11:12.440
Symmetry is waiting in
the wings to be used.

00:11:12.440 --> 00:11:18.580
This tells me that if a real
matrix has an eigenvalue lambda

00:11:18.580 --> 00:11:22.040
what I was going to do. and an
eigenvector x, it also has --

00:11:22.040 --> 00:11:25.040
another of its
eigenvalues is lambda bar

00:11:25.040 --> 00:11:27.800
with eigenvector x bar.

00:11:27.800 --> 00:11:32.500
Real matrixes, the eigenvalues
come in lambda, lambda bar --

00:11:32.500 --> 00:11:37.500
the complex eigenvalues come
in lambda and lambda bar pairs.

00:11:37.500 --> 00:11:39.600
But, of course,
I'm aiming to show

00:11:39.600 --> 00:11:44.190
that they're not complex at all,
here, by getting symmetry in.

00:11:44.190 --> 00:11:46.100
So how I going to use symmetry?

00:11:46.100 --> 00:11:51.630
I'm going to transpose
this equation to x bar

00:11:51.630 --> 00:11:59.040
transpose A transpose equals
x bar transpose lambda bar.

00:11:59.040 --> 00:12:03.150
That's just a number, so I don't
mind wear I put that number.

00:12:03.150 --> 00:12:05.750
This is -- this is --

00:12:05.750 --> 00:12:06.480
this is a -- then

00:12:06.480 --> 00:12:08.250
again okay.

00:12:08.250 --> 00:12:10.000
Ax equals lambda x bar
transpose x, right?

00:12:10.000 --> 00:12:14.880
But now I'm ready
to use symmetry.

00:12:14.880 --> 00:12:18.890
I'm ready -- so this
was all just mechanics.

00:12:18.890 --> 00:12:22.040
Now -- now comes the
moment to say, okay,

00:12:22.040 --> 00:12:24.040
if the matrix is this
from the right with x bar,

00:12:24.040 --> 00:12:25.665
I get x bar transpose
Ax bar symmetric,

00:12:25.665 --> 00:12:29.030
then this A transpose
is the same as A.

00:12:29.030 --> 00:12:31.910
You see, at that moment
I used the assumption.

00:12:31.910 --> 00:12:34.380
Now let me finish
the discussion.

00:12:34.380 --> 00:12:37.610
Here -- here's the way I finish.

00:12:37.610 --> 00:12:42.460
I look at this original equation
and I take the inner product.

00:12:42.460 --> 00:12:45.170
I multiply both sides by --

00:12:45.170 --> 00:12:46.680
oh, maybe I'll do
it with this one.

00:12:49.260 --> 00:12:51.540
I take --

00:12:51.540 --> 00:12:55.200
I multiply both sides
by x bar transpose.

00:12:55.200 --> 00:12:58.670
x bar transpose Ax
bar equals lambda

00:12:58.670 --> 00:13:03.620
bar x bar transpose x bar.

00:13:03.620 --> 00:13:04.470
Okay, fine.

00:13:08.330 --> 00:13:12.170
All right, now
what's the other one?

00:13:12.170 --> 00:13:16.880
Oh, for the other one I'll
probably use this guy.

00:13:16.880 --> 00:13:20.580
A- I happy about this?

00:13:20.580 --> 00:13:21.080
No.

00:13:21.080 --> 00:13:22.910
For some reason I'm not.

00:13:22.910 --> 00:13:26.830
I'm -- I want to --

00:13:26.830 --> 00:14:04.850
if I take the inner
product of Okay.

00:14:24.690 --> 00:14:26.660
So that -- that
was -- that's fine.

00:14:26.660 --> 00:14:30.230
That comes directly from that,
multiplying both sides by x bar

00:14:30.230 --> 00:14:33.860
transpose, but now
I'd like to get --

00:14:33.860 --> 00:14:38.320
why do I have x bars over there?

00:14:38.320 --> 00:14:40.860
Oh, yes.

00:14:40.860 --> 00:14:43.161
Forget this.

00:14:43.161 --> 00:14:43.660
Okay.

00:14:43.660 --> 00:14:45.820
On this one -- right.

00:14:45.820 --> 00:14:49.010
On this one, I took it like
that, I multiply on the right

00:14:49.010 --> 00:14:50.400
by x.

00:14:50.400 --> 00:14:54.080
That's the idea.

00:14:54.080 --> 00:14:55.540
Okay.

00:14:55.540 --> 00:15:02.150
Now why I happier with
this situation now?

00:15:02.150 --> 00:15:04.180
A proof is coming here.

00:15:04.180 --> 00:15:10.140
Because I compare this
guy with this one.

00:15:10.140 --> 00:15:12.570
And they have the
same left hand side.

00:15:12.570 --> 00:15:14.410
So they have the
same right hand side.

00:15:14.410 --> 00:15:16.240
So comparing those two, can --

00:15:16.240 --> 00:15:19.910
I'll raise the board to
do this comparison --

00:15:19.910 --> 00:15:25.150
this thing, lambda
x bar transpose x

00:15:25.150 --> 00:15:32.880
is equal to lambda
bar x bar transpose x.

00:15:32.880 --> 00:15:33.380
Okay.

00:15:35.890 --> 00:15:38.190
And the conclusion
I'm going to reach --

00:15:38.190 --> 00:15:43.280
I -- I on the right track here?

00:15:43.280 --> 00:15:44.830
The conclusion
I'm going to reach

00:15:44.830 --> 00:15:47.060
is lambda equal lambda bar.

00:15:52.170 --> 00:15:55.330
I would have to track down the
other possibility that this --

00:15:55.330 --> 00:15:58.400
this thing is
zero, but let me --

00:15:58.400 --> 00:16:01.330
oh -- oh, yes, that's important.

00:16:01.330 --> 00:16:03.370
It's not zero.

00:16:03.370 --> 00:16:08.760
So once I know that this
isn't zero, I just cancel it

00:16:08.760 --> 00:16:11.220
and I learn that lambda
equals lambda bar.

00:16:11.220 --> 00:16:14.360
And so what can you -- do you --

00:16:14.360 --> 00:16:17.823
have you got the
reasoning altogether?

00:16:20.640 --> 00:16:24.130
What does this tell us?

00:16:24.130 --> 00:16:27.750
Lambda's an eigenvalue
of this symmetric matrix.

00:16:27.750 --> 00:16:30.300
We've just proved that
it equaled lambda bar,

00:16:30.300 --> 00:16:34.630
so we have just proved
that lambda is real,

00:16:34.630 --> 00:16:35.710
right?

00:16:35.710 --> 00:16:39.980
If, if a number is equal to
its own complex conjugate,

00:16:39.980 --> 00:16:42.290
then there's no
imaginary part at all.

00:16:42.290 --> 00:16:43.280
The number is real.

00:16:43.280 --> 00:16:45.435
So lambda is real.

00:16:48.260 --> 00:16:49.380
Good.

00:16:49.380 --> 00:16:51.740
Good.

00:16:51.740 --> 00:16:56.310
Now, what -- but it depended
on this little expression,

00:16:56.310 --> 00:16:59.040
on knowing that
that wasn't zero,

00:16:59.040 --> 00:17:03.960
so that I could cancel it out
-- so can we just take a second

00:17:03.960 --> 00:17:05.400
on that one?

00:17:05.400 --> 00:17:08.650
Because it's an
important quantity.

00:17:08.650 --> 00:17:11.160
x bar transpose x.

00:17:11.160 --> 00:17:18.280
Okay, now remember, as far
as we know, x is complex.

00:17:18.280 --> 00:17:20.650
So this is --

00:17:20.650 --> 00:17:25.260
here -- x is complex, x
has these components, x1,

00:17:25.260 --> 00:17:28.310
x2 down to xn.

00:17:28.310 --> 00:17:35.890
And x bar transpose, well, it's
transposed and it's conjugated,

00:17:35.890 --> 00:17:42.570
so that's x1 conjugated x2
conjugated up to xn conjugated.

00:17:42.570 --> 00:17:43.520
I'm -- I'm --

00:17:43.520 --> 00:17:46.390
I'm really reminding
you of crucial facts

00:17:46.390 --> 00:17:48.440
about complex numbers
that are going

00:17:48.440 --> 00:17:51.230
to come into the next
lecture as well as this one.

00:17:51.230 --> 00:17:57.700
So w- what can you tell
me about that product --

00:17:57.700 --> 00:18:01.150
I -- I guess what
I'm trying to say is,

00:18:01.150 --> 00:18:05.830
if I had a complex vector, this
would be the quantity I would

00:18:05.830 --> 00:18:06.330
--

00:18:06.330 --> 00:18:07.730
I would like.

00:18:07.730 --> 00:18:09.310
This is the quantity I like.

00:18:09.310 --> 00:18:13.390
I would take the vector times
its transpose -- now what --

00:18:13.390 --> 00:18:17.070
what happens usually if I take a
vector -- a -- a -- x transpose

00:18:17.070 --> 00:18:18.070
x?

00:18:18.070 --> 00:18:22.330
I mean, that's a quantity we
see all the time, x transpose x.

00:18:22.330 --> 00:18:25.200
That's the length
of x squared, right?

00:18:25.200 --> 00:18:28.350
That's this positive length
squared, it's Pythagoras,

00:18:28.350 --> 00:18:31.640
it's x1 squared plus
x2 squared and so on.

00:18:31.640 --> 00:18:36.100
Now our vector's complex,
and you see the effect?

00:18:36.100 --> 00:18:39.090
I'm conjugating
one of these guys.

00:18:39.090 --> 00:18:41.180
So now when I do
this multiplication,

00:18:41.180 --> 00:18:49.370
I have x1 bar times x1 and
x2 bar times x2 and so on.

00:18:49.370 --> 00:18:53.640
So this is an --
this is sum a+ib.

00:18:53.640 --> 00:18:57.960
And this is sum a-ib.

00:18:57.960 --> 00:19:02.440
I mean, what's the point here?

00:19:02.440 --> 00:19:05.450
What's the point -- when
I multiply a number by its

00:19:05.450 --> 00:19:11.480
conjugate, a complex number by
its conjugate, what do I get?

00:19:11.480 --> 00:19:16.140
I get a n- the -- the
imaginary part is gone.

00:19:16.140 --> 00:19:20.680
When I multiply a+ib by
its conjugate, what's --

00:19:20.680 --> 00:19:23.400
what's the result of that -- of
each of those separate little

00:19:23.400 --> 00:19:25.210
multiplications?

00:19:25.210 --> 00:19:29.970
There's an a squared and -- and
what -- how many -- what's --

00:19:29.970 --> 00:19:34.010
b squared comes in
with a plus or a minus?

00:19:34.010 --> 00:19:35.290
A plus.

00:19:35.290 --> 00:19:39.260
i times minus i is
a plus b squared.

00:19:39.260 --> 00:19:41.240
And what about the
imaginary part?

00:19:43.830 --> 00:19:46.770
Gone, right?

00:19:46.770 --> 00:19:49.050
An iab and a minus iab.

00:19:49.050 --> 00:19:52.870
So this -- this is
the right thing to do.

00:19:52.870 --> 00:20:00.600
If you want a decent answer,
then multiply numbers

00:20:00.600 --> 00:20:02.910
by their conjugates.

00:20:02.910 --> 00:20:08.900
Multiply vectors by the
conjugates of x transpose.

00:20:08.900 --> 00:20:13.750
So this quantity is positive,
this quantity is positive --

00:20:13.750 --> 00:20:17.460
the whole thing is positive
except for the zero vector

00:20:17.460 --> 00:20:23.010
and that allows me to know
that this is a positive number,

00:20:23.010 --> 00:20:27.220
which I safely cancel out
and I reach the conclusion.

00:20:27.220 --> 00:20:33.290
So actually, in this discussion
here, I've done two things.

00:20:33.290 --> 00:20:35.690
If I reached the
conclusion that lambda's

00:20:35.690 --> 00:20:38.510
real, which I wanted to do.

00:20:38.510 --> 00:20:41.600
But at the same time,
we sort of saw what

00:20:41.600 --> 00:20:43.790
to do if things were complex.

00:20:43.790 --> 00:20:48.490
If a vector is complex,
then it's x bar transpose x,

00:20:48.490 --> 00:20:55.350
this is its length squared.

00:20:55.350 --> 00:20:59.510
And as I said, the next
lecture Monday, we'll --

00:20:59.510 --> 00:21:03.220
we'll repeat that this is
the right thing and then do

00:21:03.220 --> 00:21:06.280
the right thing for
matrixes and all other --

00:21:06.280 --> 00:21:10.950
all other, complex
possibilities.

00:21:10.950 --> 00:21:12.070
Okay.

00:21:12.070 --> 00:21:17.070
But the main point, then,
is that the eigenvalues

00:21:17.070 --> 00:21:20.720
of a symmetric matrix, it
just -- do you -- do --

00:21:20.720 --> 00:21:23.620
where did we use
symmetry, by the way?

00:21:23.620 --> 00:21:24.860
We used it here, right?

00:21:24.860 --> 00:21:27.870
Let -- can I just --

00:21:27.870 --> 00:21:31.980
let -- suppose A was a complex.

00:21:31.980 --> 00:21:34.800
Suppose A had been
a complex number.

00:21:34.800 --> 00:21:37.230
Could -- could I have
made all this work?

00:21:37.230 --> 00:21:40.780
If A was a complex
number -- complex matrix,

00:21:40.780 --> 00:21:45.070
then here I should
have written A bar.

00:21:45.070 --> 00:21:47.600
I erased the bar because
I assumed A was real.

00:21:47.600 --> 00:21:50.560
But now let's suppose
for a moment it's not.

00:21:50.560 --> 00:21:55.260
Then when I took this
step, what should I have?

00:21:55.260 --> 00:21:56.710
What did I do on that step?

00:21:56.710 --> 00:21:57.820
I transposed.

00:21:57.820 --> 00:22:00.215
So I should have
A bar transpose.

00:22:03.560 --> 00:22:05.990
In the symmetric
case, that was A,

00:22:05.990 --> 00:22:08.590
and that's what made
everything work, right?

00:22:08.590 --> 00:22:12.720
This -- this led
immediately to that.

00:22:12.720 --> 00:22:17.840
This one led immediately to
this when the matrix was real,

00:22:17.840 --> 00:22:20.300
so that didn't matter,
and it was symmetric,

00:22:20.300 --> 00:22:21.790
so that didn't matter.

00:22:21.790 --> 00:22:23.600
Then I got A.

00:22:23.600 --> 00:22:26.760
But -- so now I
just get to ask you.

00:22:26.760 --> 00:22:30.900
Suppose the matrix
had been complex.

00:22:30.900 --> 00:22:35.620
What's the right equivalent
of sym- symmetry?

00:22:38.320 --> 00:22:41.480
So the good matrix --
so here, let me say --

00:22:41.480 --> 00:22:52.776
good matrixes -- by good I mean
real lambdas and perpendicular

00:22:52.776 --> 00:22:53.276
x-s.

00:22:57.310 --> 00:23:02.860
And tell me now, which
matrixes are good?

00:23:02.860 --> 00:23:04.800
If they're --

00:23:04.800 --> 00:23:08.070
If they're real
matrixes, the good ones

00:23:08.070 --> 00:23:10.790
are symmetric, because then
everything went through.

00:23:10.790 --> 00:23:13.010
The -- so the good --

00:23:13.010 --> 00:23:15.050
I'm saying now what's good.

00:23:15.050 --> 00:23:17.430
This is -- this is -- these
are the good matrixes.

00:23:17.430 --> 00:23:21.030
They have real eigenvalues,
perpendicular eigenvectors --

00:23:21.030 --> 00:23:27.910
good means A equal
A transpose if real.

00:23:27.910 --> 00:23:30.560
Then -- then that was
what -- our proof worked.

00:23:30.560 --> 00:23:35.690
But if A is complex, all -- our
proof will still work provided

00:23:35.690 --> 00:23:38.060
A bar transpose is A.

00:23:38.060 --> 00:23:41.740
Do you see what I'm saying?

00:23:41.740 --> 00:23:47.700
I'm saying if we have complex
matrixes and we want to say are

00:23:47.700 --> 00:23:51.330
they -- are they as good
as symmetric matrixes,

00:23:51.330 --> 00:23:56.750
then we should not only
transpose the thing,

00:23:56.750 --> 00:23:58.760
but conjugate it.

00:23:58.760 --> 00:24:00.800
Those are good matrixes.

00:24:00.800 --> 00:24:03.090
And of course,
the most important

00:24:03.090 --> 00:24:06.980
s- the most important
case is when they're real,

00:24:06.980 --> 00:24:09.410
this part doesn't
matter and I just have

00:24:09.410 --> 00:24:11.330
A equal A transpose symmetric.

00:24:11.330 --> 00:24:12.070
Do you -- I --

00:24:12.070 --> 00:24:15.260
I'll just repeat that.

00:24:15.260 --> 00:24:20.290
The good matrixes, if
complex, are these.

00:24:20.290 --> 00:24:23.590
If real, that doesn't
make any difference

00:24:23.590 --> 00:24:25.900
so I'm just saying symmetric.

00:24:25.900 --> 00:24:30.530
And of course, 99% of
examples and applications

00:24:30.530 --> 00:24:34.740
to the matrixes are real
and we don't have that

00:24:34.740 --> 00:24:38.230
and then symmetric
is the key property.

00:24:38.230 --> 00:24:40.950
Okay.

00:24:40.950 --> 00:24:48.170
So that -- that's, these main
facts and now let me just --

00:24:48.170 --> 00:24:53.690
let me just -- so that's
this x bar transpose x, okay.

00:24:53.690 --> 00:24:59.210
So I'll just, write it
once more in this form.

00:24:59.210 --> 00:25:05.370
So perpendicular orthonormal
eigenvectors, real eigenvalues,

00:25:05.370 --> 00:25:08.410
transposes of
orthonormal eigenvectors.

00:25:08.410 --> 00:25:13.690
That's the symmetric
case, A equal A transpose.

00:25:13.690 --> 00:25:15.320
Okay.

00:25:15.320 --> 00:25:18.030
Good.

00:25:18.030 --> 00:25:23.340
Actually, I'll even
take one more step here.

00:25:23.340 --> 00:25:25.860
Suppose -- I --

00:25:25.860 --> 00:25:29.350
I can break this down
to show you really

00:25:29.350 --> 00:25:34.000
what that says about
a symmetric matrix.

00:25:34.000 --> 00:25:35.180
I can break that down.

00:25:35.180 --> 00:25:40.110
Let me here -- here
go these eigenvectors.

00:25:40.110 --> 00:25:46.540
I -- here go these eigenvalues,
lambda one, lambda two and so

00:25:46.540 --> 00:25:47.130
on.

00:25:47.130 --> 00:25:50.355
Here go these
eigenvectors transposed.

00:25:54.870 --> 00:25:59.640
And what happens if I actually
do out that multiplication?

00:25:59.640 --> 00:26:03.630
Do you see what will happen?

00:26:03.630 --> 00:26:07.700
There's lambda one
times q1 transpose.

00:26:07.700 --> 00:26:11.850
So the first row here is
just lambda one q1 transpose.

00:26:11.850 --> 00:26:15.650
If I multiply
column times row --

00:26:15.650 --> 00:26:17.850
you remember I could do that?

00:26:17.850 --> 00:26:24.180
When I multiply matrixes, I can
multiply columns times rows?

00:26:24.180 --> 00:26:27.130
So when I do that, I
get lambda one and then

00:26:27.130 --> 00:26:31.900
the column and then
the row and then

00:26:31.900 --> 00:26:34.885
lambda two and then
the column and the row.

00:26:41.770 --> 00:26:46.980
Every symmetric matrix
breaks up into these pieces.

00:26:46.980 --> 00:26:54.950
So these pieces have real
lambdas and they have these

00:26:54.950 --> 00:26:56.833
Eigen -- these
orthonormal eigenvectors.

00:27:00.490 --> 00:27:04.560
And, maybe you even could
tell me what kind of a matrix

00:27:04.560 --> 00:27:07.370
have I got there?

00:27:07.370 --> 00:27:12.750
Suppose I take a unit
vector times its transpose?

00:27:12.750 --> 00:27:17.620
So column times row,
I'm getting a matrix.

00:27:17.620 --> 00:27:22.040
That's a matrix
with a special name.

00:27:22.040 --> 00:27:24.290
What's it's -- what
kind of a matrix is it?

00:27:24.290 --> 00:27:27.860
We've seen those matrixes,
now, in chapter four.

00:27:27.860 --> 00:27:32.440
It's -- is A A transpose
with a unit vector,

00:27:32.440 --> 00:27:36.210
so I don't have to
divide by A transpose A.

00:27:36.210 --> 00:27:40.580
That matrix is a
projection matrix.

00:27:40.580 --> 00:27:41.920
That's a projection matrix.

00:27:41.920 --> 00:27:46.550
It's symmetric and if I square
it there'll be another --

00:27:46.550 --> 00:27:50.040
there'll be a q1 transpose
q1, which is one.

00:27:50.040 --> 00:27:53.820
So I'll get that
matrix back again.

00:27:53.820 --> 00:27:57.070
Every -- so every
symmetric matrix --

00:27:57.070 --> 00:28:06.740
every symmetric matrix
is a combination of --

00:28:06.740 --> 00:28:14.230
of mutually perpendicular --
so perpendicular projection

00:28:14.230 --> 00:28:15.500
matrixes.

00:28:15.500 --> 00:28:17.442
Projection matrixes.

00:28:20.610 --> 00:28:21.530
Okay.

00:28:21.530 --> 00:28:23.940
That's another way
that people like

00:28:23.940 --> 00:28:27.770
to think of the
spectral theorem,

00:28:27.770 --> 00:28:31.790
that every symmetric matrix
can be broken up that way.

00:28:31.790 --> 00:28:35.220
That -- I guess
at this moment --

00:28:35.220 --> 00:28:36.800
first I haven't done an example.

00:28:36.800 --> 00:28:41.700
I could create a symmetric
matrix, check that it's --

00:28:41.700 --> 00:28:44.430
find its eigenvalues,
they would come out real,

00:28:44.430 --> 00:28:47.710
find its eigenvectors, they
would come out perpendicular

00:28:47.710 --> 00:28:51.230
and you would see it in numbers,
but maybe I'll leave it here

00:28:51.230 --> 00:28:54.650
for the moment in letters.

00:28:54.650 --> 00:28:58.520
Oh, I -- maybe I will do it
with numbers, for this reason.

00:28:58.520 --> 00:29:02.720
Because there's one
more remarkable fact.

00:29:02.720 --> 00:29:05.730
Can I just put this
further great fact

00:29:05.730 --> 00:29:07.890
about symmetric
matrixes on the board?

00:29:11.720 --> 00:29:13.560
When I have
symmetric matrixes, I

00:29:13.560 --> 00:29:18.140
know their eigenvalues are
So then I can get interested

00:29:18.140 --> 00:29:22.040
in the question are they
positive real. or negative?

00:29:22.040 --> 00:29:23.980
And you remember why
that's important.

00:29:23.980 --> 00:29:27.740
For differential equations,
that decides between instability

00:29:27.740 --> 00:29:29.920
and stability.

00:29:29.920 --> 00:29:32.620
So I'm -- after I
know they're real,

00:29:32.620 --> 00:29:34.970
then the next question
is are they positive,

00:29:34.970 --> 00:29:37.480
are they negative?

00:29:37.480 --> 00:29:44.140
And I hate to have to compute
those eigenvalues to answer

00:29:44.140 --> 00:29:46.120
that question, right?

00:29:46.120 --> 00:29:49.120
Because computing the
eigenvalues of a symmetric

00:29:49.120 --> 00:29:51.230
matrix of order let's say 50 --

00:29:51.230 --> 00:29:53.930
compute its 50 eigenvalues --

00:29:53.930 --> 00:29:58.630
is a job.

00:29:58.630 --> 00:30:04.050
I mean, by pencil and paper
it's a lifetime's job.

00:30:04.050 --> 00:30:11.480
I mean, which -- and in fact,
a few years ago -- well, say,

00:30:11.480 --> 00:30:17.660
20 years ago, or 30, nobody
really knew how to do it.

00:30:17.660 --> 00:30:22.140
I mean, so, like, science
was stuck on this problem.

00:30:22.140 --> 00:30:24.810
If you have a matrix
of order 50 or 100,

00:30:24.810 --> 00:30:27.320
how do you find its eigenvalues?

00:30:27.320 --> 00:30:29.530
Numerically, now,
I'm just saying,

00:30:29.530 --> 00:30:34.330
because pencil and paper is --
we're going to run out of time

00:30:34.330 --> 00:30:36.380
or paper or something
before we get it.

00:30:38.970 --> 00:30:41.840
Well -- and you
might think, okay,

00:30:41.840 --> 00:30:47.850
get Matlab to compute the
determinant of lambda minus A,

00:30:47.850 --> 00:30:52.330
A minus lambda I, this
polynomial of 50th degree,

00:30:52.330 --> 00:30:53.465
and then find the roots.

00:30:56.230 --> 00:30:59.340
Matlab will do it,
but it will complain,

00:30:59.340 --> 00:31:04.920
because it's a very bad way
to find the eigenvalues.

00:31:04.920 --> 00:31:08.480
I'm sorry to be saying
this, because it's the way I

00:31:08.480 --> 00:31:10.220
taught you to do it, right?

00:31:10.220 --> 00:31:12.000
I taught you to
find the eigenvalues

00:31:12.000 --> 00:31:14.830
by doing that
determinant and taking

00:31:14.830 --> 00:31:16.790
the roots of that polynomial.

00:31:16.790 --> 00:31:20.030
But now I'm saying, okay,
I really meant that for two

00:31:20.030 --> 00:31:21.820
by twos and three
by threes but I

00:31:21.820 --> 00:31:24.680
didn't mean you to
do it on a 50 by 50

00:31:24.680 --> 00:31:27.980
and you're not too
unhappy, probably,

00:31:27.980 --> 00:31:29.620
because you didn't
want to do it.

00:31:29.620 --> 00:31:36.650
But -- good, because it would
be a very unstable way --

00:31:36.650 --> 00:31:40.590
the 50 answers that would come
out would be highly unreliable.

00:31:40.590 --> 00:31:45.590
So, new ways are -- are
much better to find those 50

00:31:45.590 --> 00:31:46.310
eigenvalues.

00:31:46.310 --> 00:31:50.270
That's a -- that's a part
of numerical linear algebra.

00:31:50.270 --> 00:31:54.850
But here's the
remarkable fact --

00:31:54.850 --> 00:32:00.170
that Matlab would quite happily
find the 50 pivots, right?

00:32:00.170 --> 00:32:03.720
Now the pivots are not the
same as the eigenvalues.

00:32:03.720 --> 00:32:06.440
But here's the great thing.

00:32:06.440 --> 00:32:11.070
If I had a real matrix, I
could find those 50 pivots

00:32:11.070 --> 00:32:14.340
and I could see maybe
28 of them are positive

00:32:14.340 --> 00:32:15.680
and 22 are negative

00:32:15.680 --> 00:32:16.660
pivots.

00:32:16.660 --> 00:32:20.150
And I can compute those
safely and quickly.

00:32:20.150 --> 00:32:23.900
And the great fact is that 28
of the eigenvalues would be

00:32:23.900 --> 00:32:27.410
positive and 22
would be negative --

00:32:27.410 --> 00:32:31.740
that the sines of the pivots
-- so this is, like --

00:32:31.740 --> 00:32:34.780
I hope you think this --
this is kind of a nice thing,

00:32:34.780 --> 00:32:39.970
that the sines of the pivots --

00:32:39.970 --> 00:32:42.650
for symmetric, I'm always
talking about symmetric

00:32:42.650 --> 00:32:43.700
matrixes --

00:32:43.700 --> 00:32:45.890
so I'm really, like,
trying to convince you

00:32:45.890 --> 00:32:50.100
that symmetric matrixes
are better than the rest.

00:32:50.100 --> 00:32:58.700
So the sines of the pivots
are same as the sines

00:32:58.700 --> 00:33:02.070
of the eigenvalues.

00:33:02.070 --> 00:33:04.780
The same number.

00:33:04.780 --> 00:33:10.490
The number of pivots
greater than zero,

00:33:10.490 --> 00:33:16.120
the number of positive
pivots is equal to the number

00:33:16.120 --> 00:33:21.330
of positive eigenvalues.

00:33:21.330 --> 00:33:25.750
So that, actually, is a very
useful -- that gives you a g-

00:33:25.750 --> 00:33:31.340
a good start on a decent
way to compute eigenvalues,

00:33:31.340 --> 00:33:33.470
because you can
narrow them down,

00:33:33.470 --> 00:33:35.410
you can find out how
many are positive,

00:33:35.410 --> 00:33:37.490
how many are negative.

00:33:37.490 --> 00:33:42.680
Then you could shift the matrix
by seven times the identity.

00:33:42.680 --> 00:33:46.420
That would shift all the
eigenvalues by seven.

00:33:46.420 --> 00:33:48.500
Then you could take the
pivots of that matrix

00:33:48.500 --> 00:33:52.700
and you would know how many
eigenvalues of the original

00:33:52.700 --> 00:33:53.760
were above seven and

00:33:53.760 --> 00:33:54.870
below seven.

00:33:54.870 --> 00:33:59.070
So this -- this neat
little theorem, that,

00:33:59.070 --> 00:34:05.490
symmetric matrixes have this
connection between the --

00:34:05.490 --> 00:34:09.150
nobody's mixing up and thinking
the pivots are the eigenvalues

00:34:09.150 --> 00:34:10.820
--

00:34:10.820 --> 00:34:13.050
I mean, the only
thing I can think of

00:34:13.050 --> 00:34:15.880
is the product of
the pivots equals

00:34:15.880 --> 00:34:19.210
the product of the
eigenvalues, why is that?

00:34:19.210 --> 00:34:21.139
So if I asked you for
the reason on that,

00:34:21.139 --> 00:34:24.679
why is the product of the
pivots for a symmetric matrix

00:34:24.679 --> 00:34:27.639
the same as the product
of the eigenvalues?

00:34:27.639 --> 00:34:34.500
Because they both
equal the determinant.

00:34:34.500 --> 00:34:35.000
Right.

00:34:35.000 --> 00:34:37.060
The product of the pivots
gives the determinant

00:34:37.060 --> 00:34:40.710
if no row exchanges, the
product of the eigenvalues

00:34:40.710 --> 00:34:42.340
always gives the determinant.

00:34:42.340 --> 00:34:47.100
So -- so the products -- but
that doesn't tell you anything

00:34:47.100 --> 00:34:51.020
about the 50 individual
ones, which this does.

00:34:51.020 --> 00:34:51.810
Okay.

00:34:51.810 --> 00:34:57.566
So that's -- those are essential
facts about symmetric matrixes.

00:34:57.566 --> 00:34:58.066
Okay.

00:35:00.930 --> 00:35:06.410
Now I -- I said in the -- in
the lecture description that I

00:35:06.410 --> 00:35:13.680
would take the last minutes
to start on positive definite

00:35:13.680 --> 00:35:15.970
matrixes, because
we're right there,

00:35:15.970 --> 00:35:22.506
we're ready to say what's
a positive definite matrix?

00:35:31.740 --> 00:35:34.470
It's symmetric, first of all.

00:35:34.470 --> 00:35:37.090
On -- always I will
mean symmetric.

00:35:40.090 --> 00:35:43.690
So this is the -- this is
the next section of the book.

00:35:43.690 --> 00:35:45.420
It's about this --

00:35:45.420 --> 00:35:50.030
if symmetric matrixes are
good, which was, like,

00:35:50.030 --> 00:35:54.020
the point of my lecture
so far, then positive,

00:35:54.020 --> 00:35:57.650
definite matrixes are --

00:35:57.650 --> 00:36:03.180
a subclass that are
excellent, okay.

00:36:03.180 --> 00:36:05.430
Just the greatest.

00:36:05.430 --> 00:36:07.380
so what are they?

00:36:07.380 --> 00:36:10.930
They're matrixes --
they're symmetric matrixes,

00:36:10.930 --> 00:36:13.240
so all their
eigenvalues are real.

00:36:13.240 --> 00:36:15.420
You can guess what they are.

00:36:15.420 --> 00:36:20.070
These are symmetric
matrixes with all --

00:36:20.070 --> 00:36:21.190
the eigenvalues are --

00:36:25.790 --> 00:36:27.270
okay, tell me what to write.

00:36:31.240 --> 00:36:34.040
What -- well, it --
it's hinted, of course,

00:36:34.040 --> 00:36:36.200
by the name for these things.

00:36:36.200 --> 00:36:39.750
All the eigenvalues
are positive.

00:36:39.750 --> 00:36:40.250
Okay.

00:36:45.080 --> 00:36:46.900
Tell me about the pivots.

00:36:46.900 --> 00:36:50.200
We can check the eigenvalues
or we can check the pivots.

00:36:50.200 --> 00:36:53.270
All the pivots are what?

00:36:58.430 --> 00:36:59.730
And then I'll --

00:36:59.730 --> 00:37:01.230
then I'll finally
give an example.

00:37:01.230 --> 00:37:04.460
I feel awful that I have got
to this point in the lecture

00:37:04.460 --> 00:37:05.920
and I haven't given you a single

00:37:05.920 --> 00:37:06.790
example.

00:37:06.790 --> 00:37:08.690
So let me give you one.

00:37:08.690 --> 00:37:13.880
Five three two two.

00:37:13.880 --> 00:37:17.460
That's symmetric, fine.

00:37:17.460 --> 00:37:21.620
It's eigenvalues
are real, for sure.

00:37:21.620 --> 00:37:27.850
But more than that, I know the
sines of those eigenvalues.

00:37:27.850 --> 00:37:32.440
And also I know the
sines of those pivots,

00:37:32.440 --> 00:37:34.430
so what's the deal
with the pivots?

00:37:34.430 --> 00:37:39.800
The Ei- if the eigenvalues are
all positive and if this little

00:37:39.800 --> 00:37:44.240
fact is true that the pivots
and eigenvalues have the same

00:37:44.240 --> 00:37:48.680
sines, then this must be true
-- all the pivots are positive.

00:37:51.450 --> 00:37:54.330
And that's the good way to test.

00:37:54.330 --> 00:37:56.090
This is the good
test, because I can --

00:37:56.090 --> 00:37:59.090
what are the pivots
for that matrix?

00:37:59.090 --> 00:38:02.610
The pivots for that
matrix are five.

00:38:02.610 --> 00:38:08.840
So pivots are five and
what's the second pivot?

00:38:08.840 --> 00:38:13.570
Have we, like, noticed the
formula for the second pivot

00:38:13.570 --> 00:38:14.560
in a matrix?

00:38:18.332 --> 00:38:19.790
It doesn't necessarily
-- you know,

00:38:19.790 --> 00:38:22.200
it may come out a
fraction for sure,

00:38:22.200 --> 00:38:24.200
but what is that fraction?

00:38:24.200 --> 00:38:25.070
Can you tell me?

00:38:25.070 --> 00:38:30.070
Well, here, the product of
the pivots is the determinant.

00:38:30.070 --> 00:38:31.790
What's the determinant
of this matrix?

00:38:34.840 --> 00:38:36.460
Eleven?

00:38:36.460 --> 00:38:41.180
So the second pivot must
be eleven over five,

00:38:41.180 --> 00:38:44.600
so that the product is eleven.

00:38:44.600 --> 00:38:47.430
They're both positive.

00:38:47.430 --> 00:38:50.150
Then I know that the
eigenvalues of that matrix

00:38:50.150 --> 00:38:51.820
are both positive.

00:38:51.820 --> 00:38:53.140
What are the eigenvalues?

00:38:53.140 --> 00:38:55.550
Well, I've got to take
the roots of -- you know,

00:38:55.550 --> 00:38:57.770
do I put in a minus lambda?

00:38:57.770 --> 00:39:03.800
You mentally do this -- lambda
squared minus how many lambdas?

00:39:03.800 --> 00:39:04.310
Eight?

00:39:04.310 --> 00:39:04.810
Right.

00:39:04.810 --> 00:39:07.510
Five and three, the
trace comes in there,

00:39:07.510 --> 00:39:11.410
plus what number comes here?

00:39:11.410 --> 00:39:14.185
The determinant, the
eleven, so I set that to

00:39:14.185 --> 00:39:14.685
zero.

00:39:17.190 --> 00:39:20.190
So the eigenvalues are --

00:39:20.190 --> 00:39:24.040
let's see, half of that is four,
look at that positive number,

00:39:24.040 --> 00:39:28.600
plus or minus the square
root of sixteen minus eleven,

00:39:28.600 --> 00:39:29.360
I think five.

00:39:32.480 --> 00:39:35.450
The eigenvalues -- well, two
by two they're not so terrible,

00:39:35.450 --> 00:39:37.750
but they're not so perfect.

00:39:37.750 --> 00:39:40.235
Pivots are really simple.

00:39:44.580 --> 00:39:48.370
And this is a -- this is the
family of matrixes that you

00:39:48.370 --> 00:39:51.180
really want in
differential equations,

00:39:51.180 --> 00:39:55.720
because you know the
sines of the eigenvalues,

00:39:55.720 --> 00:39:58.520
so you know the
stability or not.

00:39:58.520 --> 00:39:59.340
Okay.

00:39:59.340 --> 00:40:03.700
There's one other related
fact I can pop in here in --

00:40:03.700 --> 00:40:07.745
in the time available for
positive definite matrixes.

00:40:10.380 --> 00:40:14.737
The related fact is to ask
you about determinants.

00:40:14.737 --> 00:40:15.820
So what's the determinant?

00:40:24.990 --> 00:40:27.470
What can you tell
me if I -- remember,

00:40:27.470 --> 00:40:32.090
positive definite means all
eigenvalues are positive,

00:40:32.090 --> 00:40:34.710
all pivots are positive, so
what can you tell me about

00:40:34.710 --> 00:40:36.890
the determinant?

00:40:36.890 --> 00:40:40.240
It's positive, too.

00:40:40.240 --> 00:40:45.070
But somehow that --
that's not quite enough.

00:40:45.070 --> 00:40:50.730
Here -- here's a matrix
minus one minus three,

00:40:50.730 --> 00:40:54.330
what's the determinant
of that guy?

00:40:54.330 --> 00:40:55.880
It's positive, right?

00:40:55.880 --> 00:40:58.010
Is this a positive,
definite matrix?

00:40:58.010 --> 00:41:00.000
Are the pivots --
what are the pivots?

00:41:00.000 --> 00:41:02.230
Well, negative.

00:41:02.230 --> 00:41:03.400
What are the eigenvalues?

00:41:03.400 --> 00:41:05.470
Well, they're also the same.

00:41:05.470 --> 00:41:12.240
So somehow I don't just want
the determinant of the whole

00:41:12.240 --> 00:41:12.850
matrix.

00:41:12.850 --> 00:41:14.770
Here is eleven, that's great.

00:41:14.770 --> 00:41:16.540
Here the determinant
of the whole matrix

00:41:16.540 --> 00:41:20.220
is three, that's positive.

00:41:20.220 --> 00:41:26.450
I also -- I've got to check,
like, little sub-determinants,

00:41:26.450 --> 00:41:29.330
say maybe coming
down from the left.

00:41:29.330 --> 00:41:32.950
So the one by one and the two
by two have to be positive.

00:41:32.950 --> 00:41:36.840
So there -- that's
where I get the all.

00:41:36.840 --> 00:41:41.140
All -- can I call them
sub-determinants --

00:41:41.140 --> 00:41:43.400
are -- see, I have to --

00:41:43.400 --> 00:41:45.670
I need to make the thing plural.

00:41:45.670 --> 00:41:51.230
I need to test n things, not
just the big determinant.

00:41:51.230 --> 00:41:55.130
All sub-determinants
are positive.

00:41:55.130 --> 00:41:58.110
Then I'm okay.

00:41:58.110 --> 00:42:00.610
Then I'm okay.

00:42:00.610 --> 00:42:03.310
This passes the test.

00:42:03.310 --> 00:42:06.830
Five is positive and
eleven is positive.

00:42:06.830 --> 00:42:12.220
This fails the test because that
minus one there is negative.

00:42:12.220 --> 00:42:16.050
And then the big determinant
is positive three.

00:42:16.050 --> 00:42:18.800
So t- this --

00:42:18.800 --> 00:42:23.320
these -- this fact -- you see
that actually the course, like,

00:42:23.320 --> 00:42:24.110
coming together.

00:42:27.210 --> 00:42:29.000
And that's really my point now.

00:42:29.000 --> 00:42:33.850
In the next -- in this lecture
and particularly next Wednesday

00:42:33.850 --> 00:42:38.150
and Friday, the
course comes together.

00:42:38.150 --> 00:42:41.810
These pivots that we
met in the first week,

00:42:41.810 --> 00:42:46.180
these determinants that we met
in the middle of the course,

00:42:46.180 --> 00:42:50.810
these eigenvalues that
we met most recently --

00:42:50.810 --> 00:42:56.010
all matrixes are square here,
so coming together for square

00:42:56.010 --> 00:43:00.150
matrixes means these three
pieces come together and they

00:43:00.150 --> 00:43:05.210
come together in that
beautiful fact, that if --

00:43:05.210 --> 00:43:07.430
that all the -- that
if I have one of these,

00:43:07.430 --> 00:43:09.150
I have the others.

00:43:09.150 --> 00:43:10.140
That if I --

00:43:10.140 --> 00:43:12.180
but for symmetric matrixes.

00:43:12.180 --> 00:43:17.570
So that -- this will be the
positive definite section

00:43:17.570 --> 00:43:22.540
and then the real climax of the
course is to make everything

00:43:22.540 --> 00:43:27.150
come together for
n by n matrixes,

00:43:27.150 --> 00:43:30.330
not necessarily symmetric --

00:43:30.330 --> 00:43:33.070
bring everything
together there and that

00:43:33.070 --> 00:43:34.850
will be the final thing.

00:43:34.850 --> 00:43:35.490
Okay.

00:43:35.490 --> 00:43:38.970
So have a great
weekend and don't

00:43:38.970 --> 00:43:40.710
forget symmetric matrixes.

00:43:40.710 --> 00:43:42.260
Thanks.