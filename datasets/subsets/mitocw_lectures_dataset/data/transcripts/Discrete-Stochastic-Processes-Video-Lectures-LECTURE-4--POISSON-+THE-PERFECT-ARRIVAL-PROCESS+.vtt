WEBVTT

00:00:00.530 --> 00:00:02.960
The following content is
provided under a Creative

00:00:02.960 --> 00:00:04.370
Commons license.

00:00:04.370 --> 00:00:07.410
Your support will help MIT
OpenCourseWare continue to

00:00:07.410 --> 00:00:11.060
offer high quality educational
resources for free.

00:00:11.060 --> 00:00:13.960
To make a donation or view
additional materials from

00:00:13.960 --> 00:00:17.890
hundreds of MIT courses, visit
MIT OpenCourseWare at

00:00:17.890 --> 00:00:19.140
ocw.mit.edu.

00:00:22.424 --> 00:00:25.930
PROFESSOR: As the question said,
where are we as far as

00:00:25.930 --> 00:00:26.800
the text goes.

00:00:26.800 --> 00:00:28.960
We're just going to
start Chapter 2

00:00:28.960 --> 00:00:31.290
today, Poisson processes.

00:00:31.290 --> 00:00:35.270
I want to spend about five
minutes reviewing a little bit

00:00:35.270 --> 00:00:39.130
about convergence, the things
we said last time,

00:00:39.130 --> 00:00:41.880
and then move on.

00:00:41.880 --> 00:00:46.240
There's a big break in the
course, at this point between

00:00:46.240 --> 00:00:51.570
Chapter 1 and Chapter 2 in the
sense that Chapter 1 is very

00:00:51.570 --> 00:00:54.940
abstract, a little
theoretical.

00:00:54.940 --> 00:01:00.750
It's dealing with probability
theory in general and the most

00:01:00.750 --> 00:01:04.239
general theorems in probability
stated in very

00:01:04.239 --> 00:01:06.270
simple and elementary form.

00:01:06.270 --> 00:01:11.490
But still, we're essentially
not dealing with

00:01:11.490 --> 00:01:12.660
applications at all.

00:01:12.660 --> 00:01:16.850
We're dealing with very, very
abstract things in Chapter 1.

00:01:16.850 --> 00:01:19.140
Chapter 2 is just the reverse.

00:01:19.140 --> 00:01:22.480
A Poisson process is the
most concrete thing

00:01:22.480 --> 00:01:24.420
you can think of.

00:01:24.420 --> 00:01:28.150
People use that as a model
for almost everything.

00:01:28.150 --> 00:01:29.710
Whether it's a reasonable
model or

00:01:29.710 --> 00:01:31.400
not is another question.

00:01:31.400 --> 00:01:34.300
But people use it as
a model constantly.

00:01:34.300 --> 00:01:37.570
And everything about
it is simple.

00:01:37.570 --> 00:01:41.570
For a Poisson process, you can
almost characterize it as

00:01:41.570 --> 00:01:44.310
saying everything you could
think of about a Poisson

00:01:44.310 --> 00:01:49.270
process is either true or
it's obviously false.

00:01:49.270 --> 00:01:50.830
And when you get to
that point, you

00:01:50.830 --> 00:01:52.670
understand Poisson processes.

00:01:52.670 --> 00:01:55.630
And you can go on to other
things and never have to

00:01:55.630 --> 00:01:57.530
really think about them
very hard again.

00:01:57.530 --> 00:02:00.820
Because at that point, you
really understand them.

00:02:00.820 --> 00:02:01.130
OK.

00:02:01.130 --> 00:02:06.130
So let's go on and review
things a little bit.

00:02:06.130 --> 00:02:11.060
What's convergence and how does
it affect sequences of

00:02:11.060 --> 00:02:12.550
IID random variables?

00:02:12.550 --> 00:02:14.550
Convergence is more
general than

00:02:14.550 --> 00:02:16.840
just IID random variables.

00:02:16.840 --> 00:02:21.700
But it applies to any sequence
of random variables.

00:02:21.700 --> 00:02:26.270
And the definition is that a
sequence of random variables

00:02:26.270 --> 00:02:30.970
converges in distribution to
another random variable z, if

00:02:30.970 --> 00:02:36.120
the limit, as n goes to
infinity, of the distribution

00:02:36.120 --> 00:02:39.200
function of zn converges to the

00:02:39.200 --> 00:02:41.700
distribution function of z.

00:02:41.700 --> 00:02:46.190
For this definition to make
sense, it doesn't matter what

00:02:46.190 --> 00:02:52.880
z is. z can be outside of
the sample space even.

00:02:52.880 --> 00:02:54.720
The only thing we're interested
in is this

00:02:54.720 --> 00:02:57.110
particular distribution
function.

00:02:57.110 --> 00:03:00.760
So what we're really saying is
a sequence of distributions

00:03:00.760 --> 00:03:05.670
converges to another
distribution function if in

00:03:05.670 --> 00:03:12.980
fact this limit occurs at every
point where f of z is

00:03:12.980 --> 00:03:13.450
continuous.

00:03:13.450 --> 00:03:17.590
In other words, if f of z is
discontinuous someplace, we

00:03:17.590 --> 00:03:20.440
had an example of that where
we're looking at the law of

00:03:20.440 --> 00:03:24.490
large numbers and the
distribution function.

00:03:24.490 --> 00:03:27.330
Looked at in the right way
was a step function.

00:03:27.330 --> 00:03:30.010
It wasn't continuous
at the step.

00:03:30.010 --> 00:03:34.000
And therefore, you can't expect
anything to be said

00:03:34.000 --> 00:03:36.360
about that.

00:03:36.360 --> 00:03:40.710
So the typical example of
convergence in distribution is

00:03:40.710 --> 00:03:45.460
the central limit theorem which
says, if x1, x2 are IID,

00:03:45.460 --> 00:03:47.900
they have a variance
sigma squared.

00:03:47.900 --> 00:03:51.370
And if s sub n the sum of these
random variables is a

00:03:51.370 --> 00:03:56.410
sum of x1 to xn, then zn
is the normalized sum.

00:03:56.410 --> 00:03:58.660
In other words, you
take this sum.

00:03:58.660 --> 00:04:01.530
You subtract off
the mean of it.

00:04:01.530 --> 00:04:06.430
And I think in the reproduction
of these slides

00:04:06.430 --> 00:04:16.232
that you have, I think that
n, right there was--

00:04:16.232 --> 00:04:17.492
here we go.

00:04:17.492 --> 00:04:19.680
That's the other y.

00:04:19.680 --> 00:04:24.280
I think that n was left off.

00:04:24.280 --> 00:04:27.880
If that n wasn't left off,
another n was left off.

00:04:27.880 --> 00:04:29.470
It's obviously needed there.

00:04:29.470 --> 00:04:32.640
This is a normalized random
variable because the variance

00:04:32.640 --> 00:04:42.120
of sn and of sn minus nx bar is
just sigma squared times n.

00:04:42.120 --> 00:04:44.340
Because they're any of these
random variables.

00:04:44.340 --> 00:04:49.020
So we're dividing by the
standard deviation.

00:04:49.020 --> 00:04:53.700
So this is a random variable
for each n which has a

00:04:53.700 --> 00:04:57.250
standard deviation
1 and mean 0.

00:04:57.250 --> 00:04:58.680
So it's normalized.

00:04:58.680 --> 00:05:01.610
And it converges in distribution
to a Gaussian

00:05:01.610 --> 00:05:06.470
random variable of mean 0 and
standard deviation 1.

00:05:06.470 --> 00:05:08.870
This notation here is
sort of standard.

00:05:08.870 --> 00:05:11.180
And we'll use it at
various times.

00:05:11.180 --> 00:05:15.780
It means a Gaussian distribution
with mean 0 and

00:05:15.780 --> 00:05:17.690
variance 1.

00:05:17.690 --> 00:05:22.270
So for an example of that, if
x1, x2, so forth, are IID with

00:05:22.270 --> 00:05:30.410
mean expected value of x and the
sum here, then sn over n

00:05:30.410 --> 00:05:33.670
converges in distribution to
the deterministic random

00:05:33.670 --> 00:05:36.230
variable x bar.

00:05:36.230 --> 00:05:37.730
That's a nice example of this.

00:05:37.730 --> 00:05:42.850
So we have two examples of
convergence and distribution.

00:05:42.850 --> 00:05:49.500
And that's what that says.

00:05:49.500 --> 00:05:53.290
So next, a sequence of random
variables converges in

00:05:53.290 --> 00:05:55.700
probability.

00:05:55.700 --> 00:05:59.400
When we start talking about
convergence in probability,

00:05:59.400 --> 00:06:04.360
there's another idea which we
are going to bring in, mostly

00:06:04.360 --> 00:06:07.510
in Chapter 4 when we get
to it, which is called

00:06:07.510 --> 00:06:09.840
convergence with
probability 1.

00:06:09.840 --> 00:06:11.950
Don't confuse those two
things because they're

00:06:11.950 --> 00:06:14.880
very different ideas.

00:06:14.880 --> 00:06:20.050
Because people confuse them,
many people call convergence

00:06:20.050 --> 00:06:24.490
with probability 1 almost sure
convergence or almost

00:06:24.490 --> 00:06:27.030
everywhere convergence.

00:06:27.030 --> 00:06:30.200
I don't like that notation.

00:06:30.200 --> 00:06:34.090
So I'll stay with the notation
with probability 1.

00:06:34.090 --> 00:06:36.950
But it means something very
different than converging in

00:06:36.950 --> 00:06:38.230
probability.

00:06:38.230 --> 00:06:42.120
So the definition is that a
set of random variables

00:06:42.120 --> 00:06:48.280
converges in probability to some
other random variable if

00:06:48.280 --> 00:06:50.000
this limit holds true.

00:06:50.000 --> 00:06:54.840
And you remember that diagram
we showed you last time.

00:06:54.840 --> 00:06:57.890
Let me just quickly redraw it.

00:07:01.124 --> 00:07:04.420
Have this set of distribution
functions.

00:07:04.420 --> 00:07:09.370
Here's the mean here,
x bar, limits,

00:07:09.370 --> 00:07:12.840
plus and minus epsilon.

00:07:12.840 --> 00:07:18.000
And this sequence of random
variables has to come in down

00:07:18.000 --> 00:07:22.280
here and go out up there.

00:07:22.280 --> 00:07:27.930
This distance here and the
distance there gets very small

00:07:27.930 --> 00:07:30.570
and goes to 0 as n
gets very large.

00:07:30.570 --> 00:07:32.580
And that's the meaning
of what this says.

00:07:32.580 --> 00:07:35.860
So if you don't remember that
diagram, go look at it in the

00:07:35.860 --> 00:07:39.370
lecture notes last time or in
the text where it's explained

00:07:39.370 --> 00:07:42.830
in a lot more detail.

00:07:42.830 --> 00:07:47.590
So the typical example of that
is the weak law of large

00:07:47.590 --> 00:07:52.370
numbers of x1, blah, blah,
blah, are IID with mean,

00:07:52.370 --> 00:07:53.420
expected value of x.

00:07:53.420 --> 00:07:58.170
Remember now that we say that
a random variable has a mean

00:07:58.170 --> 00:08:04.370
if the expected value of the
absolute value of x is finite.

00:08:04.370 --> 00:08:09.510
It's not enough to have things
which have a distribution

00:08:09.510 --> 00:08:15.060
function which is badly behaved
for very big x, and

00:08:15.060 --> 00:08:17.770
badly behaved for very small
x, and the two of them

00:08:17.770 --> 00:08:18.510
cancelled out.

00:08:18.510 --> 00:08:19.800
That doesn't work.

00:08:19.800 --> 00:08:21.620
That doesn't mean
you have a mean.

00:08:21.620 --> 00:08:24.140
You need the expected value
as the absolute

00:08:24.140 --> 00:08:25.420
value of x to be finite.

00:08:28.240 --> 00:08:33.530
Now, the weak law of large
numbers says that the random

00:08:33.530 --> 00:08:38.929
variables sn over n, in other
words, the sample average, in

00:08:38.929 --> 00:08:42.870
fact converges to the
deterministic

00:08:42.870 --> 00:08:44.840
random variable x bar.

00:08:44.840 --> 00:08:48.220
And that convergence is
convergence in probability.

00:08:48.220 --> 00:08:50.530
Which means it's this kind
of convergence here.

00:08:50.530 --> 00:08:53.300
Which means that it's going
to a distribution

00:08:53.300 --> 00:08:55.260
which is a step function.

00:08:55.260 --> 00:08:59.010
There's a very big difference
between a distribution which

00:08:59.010 --> 00:09:01.870
is a step function and a
distribution which is

00:09:01.870 --> 00:09:04.510
something like a Gaussian
random variable.

00:09:04.510 --> 00:09:08.450
And what the big difference is
is that the random variables

00:09:08.450 --> 00:09:11.570
that are converging to each
other, if a bunch of random

00:09:11.570 --> 00:09:14.670
variables are all converging to
a constant, then they all

00:09:14.670 --> 00:09:16.730
have to be very close
to each other.

00:09:16.730 --> 00:09:21.930
And that's the property you're
really interested in in

00:09:21.930 --> 00:09:25.000
convergence in probability.

00:09:25.000 --> 00:09:29.430
So convergence in mean square,
finally, last definition which

00:09:29.430 --> 00:09:31.725
is easy to deal with.

00:09:31.725 --> 00:09:34.990
If a sequence of random
variables converges in the

00:09:34.990 --> 00:09:39.740
mean square to another random
variable, if this limit of the

00:09:39.740 --> 00:09:43.150
expected value, of the
difference between the two

00:09:43.150 --> 00:09:47.690
random variables squared, goes
to 0, this n gets big.

00:09:47.690 --> 00:09:51.500
That's what we had with the weak
law of large numbers if

00:09:51.500 --> 00:09:55.035
you assume that the random
variables each had a variance.

00:09:58.090 --> 00:10:00.350
So on to something new.

00:10:00.350 --> 00:10:02.890
On to Poisson processes.

00:10:02.890 --> 00:10:05.830
We first have to explain what
an arrival process is.

00:10:05.830 --> 00:10:08.760
And then we can get into
Poisson processes.

00:10:08.760 --> 00:10:13.540
Because arrival processes are
a very broad class of

00:10:13.540 --> 00:10:17.930
stochastic processes, in fact
discrete stochastic processes.

00:10:17.930 --> 00:10:21.750
But they have this property of
being characterized by things

00:10:21.750 --> 00:10:27.090
happening at various random
instance of time as opposed to

00:10:27.090 --> 00:10:30.710
a noise waveform or something
of that sort.

00:10:30.710 --> 00:10:34.390
So an arrival process is a
sequence of increasing random

00:10:34.390 --> 00:10:38.540
variables, 0 less than
s1, less than s2.

00:10:38.540 --> 00:10:42.850
What's it mean for a random
variable s1 to be less than a

00:10:42.850 --> 00:10:45.760
random variable s2?

00:10:45.760 --> 00:10:47.870
It means exactly the
same thing as it

00:10:47.870 --> 00:10:50.870
means for real numbers.

00:10:50.870 --> 00:10:56.850
s1 is less than s2 if the random
variable s2 minus s1 is

00:10:56.850 --> 00:11:00.710
a positive random variable,
namely if it only takes on

00:11:00.710 --> 00:11:06.520
non-negative values for all
omega in the sample space or

00:11:06.520 --> 00:11:13.060
for all omega except for some
peculiar set of probability 0.

00:11:13.060 --> 00:11:19.050
The differences in these arrival
epochs, why do I call

00:11:19.050 --> 00:11:20.580
them arrival epochs?

00:11:20.580 --> 00:11:23.200
Why do other people call
them arrival epochs?

00:11:23.200 --> 00:11:27.780
Because time is something which
gets used so often here

00:11:27.780 --> 00:11:29.000
that it gets confusing.

00:11:29.000 --> 00:11:32.680
So it's nice to call one thing
epochs instead of time.

00:11:32.680 --> 00:11:35.930
And then you know what you're
talking about a little better.

00:11:35.930 --> 00:11:42.070
The difference is s sub i minus
s sub i minus 1 for all

00:11:42.070 --> 00:11:49.040
i greater than or equal to 2
here, with x1 equal to s1.

00:11:49.040 --> 00:11:52.640
These are called interarrival
times and the si are called

00:11:52.640 --> 00:11:53.910
arrival epochs.

00:11:53.910 --> 00:11:58.170
The picture here really
shows it all.

00:11:58.170 --> 00:12:02.380
Here we have a sequence of
arrival instance, which is

00:12:02.380 --> 00:12:04.260
where these arrivals occur.

00:12:04.260 --> 00:12:09.970
By definition, x1 is the time at
which is the first arrival

00:12:09.970 --> 00:12:13.450
occurs, x2 is the difference
between the time when the

00:12:13.450 --> 00:12:16.030
second arrival occurs and
the first arrival

00:12:16.030 --> 00:12:18.080
occurs, and so forth.

00:12:18.080 --> 00:12:21.560
n of t is the number of arrivals
that have occurred up

00:12:21.560 --> 00:12:23.390
until time t.

00:12:23.390 --> 00:12:27.280
Which is, if we draw a staircase
function for each of

00:12:27.280 --> 00:12:30.920
these arrivals, n of t
is just the value of

00:12:30.920 --> 00:12:32.310
that staircase function.

00:12:32.310 --> 00:12:34.490
In other words, the counting
process, the

00:12:34.490 --> 00:12:36.360
arrival counting process--

00:12:36.360 --> 00:12:39.400
here's another typo in the
notes that you've got.

00:12:39.400 --> 00:12:41.050
It calls it Poisson
counting process.

00:12:41.050 --> 00:12:43.625
It should be arrival
counting process.

00:12:48.540 --> 00:12:52.290
What this staircase function
is is in fact

00:12:52.290 --> 00:12:54.120
the counting process.

00:12:54.120 --> 00:12:56.550
It says how many arrivals
there have been

00:12:56.550 --> 00:12:57.950
up until time t.

00:12:57.950 --> 00:13:00.320
And every once in a while,
that jumps up by 1.

00:13:00.320 --> 00:13:03.920
So it keeps increasing by
1 at various times.

00:13:03.920 --> 00:13:06.600
So that's the arrival
counting process.

00:13:06.600 --> 00:13:11.970
The important thing to get out
of this is if you understand

00:13:11.970 --> 00:13:15.760
everything about these random
variables, then you understand

00:13:15.760 --> 00:13:18.170
everything about these
random variables.

00:13:18.170 --> 00:13:20.580
And then you understand
everything about

00:13:20.580 --> 00:13:22.800
these random variables.

00:13:22.800 --> 00:13:25.950
There's a countable number of
these random variables.

00:13:25.950 --> 00:13:29.050
There's a countable number of
these random variables.

00:13:29.050 --> 00:13:31.230
There's an unaccountably
infinite number of these

00:13:31.230 --> 00:13:32.060
random variables.

00:13:32.060 --> 00:13:35.070
In other words, for every
t, n of t is a

00:13:35.070 --> 00:13:37.240
different random variable.

00:13:37.240 --> 00:13:42.020
I mean, it tends to be the
same for relatively large

00:13:42.020 --> 00:13:44.020
intervals of t sometimes.

00:13:44.020 --> 00:13:49.110
But this is a different random
variable for each value of t.

00:13:49.110 --> 00:13:53.600
So let's proceed with that.

00:13:53.600 --> 00:13:56.970
A sample path or sample function
of the process is a

00:13:56.970 --> 00:13:58.870
sequence of sample values.

00:13:58.870 --> 00:14:01.180
That's the same as we
have everywhere.

00:14:01.180 --> 00:14:05.450
You look at a sample point
of the process.

00:14:05.450 --> 00:14:13.160
Sample point of the whole
probability space maps into

00:14:13.160 --> 00:14:15.520
this sequence of random
variables, s1,

00:14:15.520 --> 00:14:18.990
s2, s3, and so forth.

00:14:18.990 --> 00:14:22.200
If you know what the sample
value is of each one of these

00:14:22.200 --> 00:14:27.570
random variables, then in fact,
you can draw this step

00:14:27.570 --> 00:14:28.890
function here.

00:14:28.890 --> 00:14:32.460
If you know the value, the
sample value, of each one of

00:14:32.460 --> 00:14:37.640
these, in the same way,
you can again

00:14:37.640 --> 00:14:39.890
draw this step function.

00:14:39.890 --> 00:14:42.330
And if you know what the
subfunction is, the step

00:14:42.330 --> 00:14:46.390
function in fact is the sample
value of n of t.

00:14:46.390 --> 00:14:51.380
Now, there's one thing a
little peculiar here.

00:14:51.380 --> 00:14:53.620
Each sample path corresponds
to a

00:14:53.620 --> 00:14:56.030
particular staircase function.

00:14:56.030 --> 00:14:58.880
And the process can be viewed
as the ensemble with joint

00:14:58.880 --> 00:15:03.110
probability distributions of
such staircase functions.

00:15:03.110 --> 00:15:07.280
Now, what does all that
gobbledygook mean?

00:15:07.280 --> 00:15:09.230
Very, very often in probability

00:15:09.230 --> 00:15:11.590
theory, we draw pictures.

00:15:11.590 --> 00:15:15.790
And these pictures are pictures
of what happens to

00:15:15.790 --> 00:15:17.490
random variables.

00:15:17.490 --> 00:15:19.840
And there's a cheat
in all of that.

00:15:19.840 --> 00:15:26.160
And the cheat here is that in
fact, this step function here

00:15:26.160 --> 00:15:28.910
is just a generic
step function.

00:15:28.910 --> 00:15:34.860
These points at which changes
occur are generic values at

00:15:34.860 --> 00:15:36.880
which changes occur.

00:15:36.880 --> 00:15:41.400
And we're representing those
values as random variables.

00:15:41.400 --> 00:15:44.380
When you represent these as
random variables, this whole

00:15:44.380 --> 00:15:48.010
function here, namely n
of t itself, becomes--

00:15:56.510 --> 00:15:59.680
if you have a particular set
of values for each one of

00:15:59.680 --> 00:16:03.140
these, then you have a
particular staircase function.

00:16:03.140 --> 00:16:06.350
With that particular staircase
function, you have a

00:16:06.350 --> 00:16:09.860
particular sample
path for n of t.

00:16:09.860 --> 00:16:13.350
In other words, a sample path
for any set of these random

00:16:13.350 --> 00:16:16.620
variables-- the arrival epochs,
or the interarrival

00:16:16.620 --> 00:16:19.740
intervals, or n of
t for each t--

00:16:19.740 --> 00:16:22.400
all of these are equivalent
to each other.

00:16:22.400 --> 00:16:29.130
For this reason, when we talk
about arrival processes, it's

00:16:29.130 --> 00:16:31.240
a little different than
what we usually do.

00:16:31.240 --> 00:16:36.470
Because usually, we say a random
process is a sequence

00:16:36.470 --> 00:16:39.740
or an uncountable number
of random variables.

00:16:39.740 --> 00:16:43.470
Here, just because we can
describe it in three different

00:16:43.470 --> 00:16:51.330
ways, this same stochastic
process gets described either

00:16:51.330 --> 00:16:56.700
as a sequence of interarrival
intervals, or as a sequence of

00:16:56.700 --> 00:17:01.660
arrival epochs, or as a
countable number these n of t

00:17:01.660 --> 00:17:03.010
random variables.

00:17:03.010 --> 00:17:06.839
And from now on, we make no
distinction between any of

00:17:06.839 --> 00:17:07.589
these things.

00:17:07.589 --> 00:17:11.560
We will, every once in while,
have to remind ourselves what

00:17:11.560 --> 00:17:15.300
these pictures mean because
they look very simple.

00:17:15.300 --> 00:17:17.180
They look like the pictures
of functions that

00:17:17.180 --> 00:17:18.550
you're used to drawing.

00:17:18.550 --> 00:17:20.980
But they don't really
mean the same thing.

00:17:20.980 --> 00:17:25.579
Because this picture is drawing
a generic sample path.

00:17:25.579 --> 00:17:29.380
For that generic sample path,
you have a set of sample

00:17:29.380 --> 00:17:37.580
values for the Xs, a sample path
for the arrival epochs, a

00:17:37.580 --> 00:17:41.750
sample set of values
for n of t.

00:17:41.750 --> 00:17:44.050
And when we draw the picture
calling these random

00:17:44.050 --> 00:17:46.120
variables, we really
mean the set of

00:17:46.120 --> 00:17:47.850
all such step functions.

00:17:47.850 --> 00:17:51.340
And we just automatically use
all those properties and those

00:17:51.340 --> 00:17:53.390
relationships.

00:17:53.390 --> 00:17:57.510
So it's not quite as simple as
what it appears to be, but

00:17:57.510 --> 00:17:58.760
it's almost as simple.

00:18:01.090 --> 00:18:06.280
You can also see that any sample
path can be specified

00:18:06.280 --> 00:18:10.370
by the sample values n of t for
all t, by si for all i, or

00:18:10.370 --> 00:18:13.050
by xi for all i.

00:18:13.050 --> 00:18:17.190
So that essentially, an arrival
process is specified

00:18:17.190 --> 00:18:18.310
by any one of these things.

00:18:18.310 --> 00:18:20.830
That's exactly what
I just said.

00:18:20.830 --> 00:18:25.720
The major relation we need to
relate the counting process to

00:18:25.720 --> 00:18:31.040
the arrival process, well,
there's one relationship here,

00:18:31.040 --> 00:18:34.260
which is perhaps the simplest
relationship.

00:18:34.260 --> 00:18:38.790
But this relationship is a nice
relationship to say what

00:18:38.790 --> 00:18:42.980
n of t is if you know
what s sub n is.

00:18:42.980 --> 00:18:48.430
It's not quite so nice if you
know what n of t is to figure

00:18:48.430 --> 00:18:50.820
what s sub n is.

00:18:50.820 --> 00:18:53.960
I mean, the information is
tucked into the statement, but

00:18:53.960 --> 00:18:56.800
it's tucked in a more convenient
way into this

00:18:56.800 --> 00:18:59.280
statement down here.

00:18:59.280 --> 00:19:04.520
This statement, I can see it
now after many years of

00:19:04.520 --> 00:19:06.910
dealing with it.

00:19:06.910 --> 00:19:10.400
I'm sure that you can
see it if you stare

00:19:10.400 --> 00:19:11.650
at it for five minutes.

00:19:14.700 --> 00:19:16.900
You will keep forgetting
the intuitive picture

00:19:16.900 --> 00:19:18.310
that goes with it.

00:19:18.310 --> 00:19:21.340
So I suggest that this is one
of the rare things in this

00:19:21.340 --> 00:19:24.780
course that you just
ought to remember.

00:19:24.780 --> 00:19:27.530
And then once you remember
it, you can always figure

00:19:27.530 --> 00:19:28.740
out why it's true.

00:19:28.740 --> 00:19:31.670
Here's the reason
why it's true.

00:19:31.670 --> 00:19:35.960
If s sub n is equal to tau for
some tau less than or equal to

00:19:35.960 --> 00:19:40.466
t, then n of tau has
to be equal to n.

00:19:40.466 --> 00:19:43.880
If s sub n is equal to tau,
here's the picture here,

00:19:43.880 --> 00:19:45.300
except there's not a tau here.

00:19:45.300 --> 00:19:52.180
If s sub 2 is equal to
tau, then n of 2--

00:19:52.180 --> 00:19:56.460
these are right continuous,
so n of 2 is equal to 2.

00:19:56.460 --> 00:20:01.100
And therefore, n of tau is less
than or equal to n of t.

00:20:01.100 --> 00:20:04.360
So that's the whole
reason down there.

00:20:04.360 --> 00:20:06.350
You can turn this
argument around.

00:20:06.350 --> 00:20:12.160
You can start out with n of t is
greater than or equal to n.

00:20:12.160 --> 00:20:15.220
It means n of t is equal
to some particular n.

00:20:15.220 --> 00:20:18.030
And turn the argument
upside down.

00:20:18.030 --> 00:20:19.490
And you get the same argument.

00:20:19.490 --> 00:20:25.800
So this tells you
what this is.

00:20:25.800 --> 00:20:28.160
This tells you what this is.

00:20:28.160 --> 00:20:32.290
If you do this for every n and
every t, then you do this for

00:20:32.290 --> 00:20:36.100
every n and every t.

00:20:36.100 --> 00:20:40.590
It's a very bizarre statement
because usually when you have

00:20:40.590 --> 00:20:44.820
relationships between functions,
you don't have the

00:20:44.820 --> 00:20:47.380
Ns and the Ts switching
around.

00:20:47.380 --> 00:20:50.330
And in this case, the
n is the subscript.

00:20:50.330 --> 00:20:52.620
That's the thing which says
which random variable you're

00:20:52.620 --> 00:20:53.970
talking about.

00:20:53.970 --> 00:20:57.670
And over here, t is the thing
which says what random

00:20:57.670 --> 00:20:59.690
variable you're talking about.

00:20:59.690 --> 00:21:01.860
So it's peculiar
in that sense.

00:21:01.860 --> 00:21:03.440
It's a statement which
requires a

00:21:03.440 --> 00:21:04.860
little bit of thought.

00:21:04.860 --> 00:21:07.910
I apologize for dwelling
on it because once you

00:21:07.910 --> 00:21:09.580
see it, it's obvious.

00:21:09.580 --> 00:21:12.940
But many of these obvious
things are not obvious.

00:21:19.030 --> 00:21:21.830
What we're going to do as we
move on is we're going to talk

00:21:21.830 --> 00:21:26.040
about these arrival processes in
any of these three ways we

00:21:26.040 --> 00:21:28.010
choose to talk about them.

00:21:28.010 --> 00:21:30.760
And we're going to go back
and forth between them.

00:21:30.760 --> 00:21:34.670
And with Poisson processes,
that's particularly easy.

00:21:34.670 --> 00:21:37.770
We can't do a whole lot more
with arrival processes.

00:21:37.770 --> 00:21:39.910
They're just too complicated.

00:21:39.910 --> 00:21:43.650
I mean, arrival processes
involve almost any kind of

00:21:43.650 --> 00:21:47.790
thing where things happen at
various points in time.

00:21:47.790 --> 00:21:51.960
So we simplify it to something
called a renewal process.

00:21:51.960 --> 00:21:55.750
Renewal processes are the
topic of Chapter 4.

00:21:55.750 --> 00:21:59.010
When you get to Chapter 4, you
will perhaps say that renewal

00:21:59.010 --> 00:22:03.320
processes are too complicated
to talk about also.

00:22:03.320 --> 00:22:06.730
I hope after we finish Chapter
4, you won't believe that it's

00:22:06.730 --> 00:22:08.990
too complicated to talk about.

00:22:08.990 --> 00:22:12.280
But these are fairly complicated
processes.

00:22:12.280 --> 00:22:16.370
But even here, it's an arrival
process where the interarrival

00:22:16.370 --> 00:22:21.060
intervals are independent and
identically distributed.

00:22:21.060 --> 00:22:25.730
Finally, a Poisson process is
a renewal process for which

00:22:25.730 --> 00:22:30.240
each x sub i has an exponential
distribution.

00:22:30.240 --> 00:22:35.070
Each interarrival has to have
the same distribution because

00:22:35.070 --> 00:22:39.110
since it's a renewal process,
these are all IID.

00:22:39.110 --> 00:22:44.040
And we let this distribution
function X be the generic

00:22:44.040 --> 00:22:45.330
random variable.

00:22:45.330 --> 00:22:47.350
And this is talking about
the distribution

00:22:47.350 --> 00:22:49.610
function of all of them.

00:22:49.610 --> 00:22:53.760
I don't know whether that 1
minus is in the slides I

00:22:53.760 --> 00:22:54.470
passed out.

00:22:54.470 --> 00:22:57.910
There's one kind of
error like that.

00:22:57.910 --> 00:22:59.960
And I'm not sure where it is.

00:22:59.960 --> 00:23:03.090
So anyway, lambda is some fixed
parameter called the

00:23:03.090 --> 00:23:05.180
rate of the Poisson process.

00:23:05.180 --> 00:23:09.770
So for each lambda greater than
0, you have a Poisson

00:23:09.770 --> 00:23:14.190
process where each of these
interarrival intervals are

00:23:14.190 --> 00:23:18.300
exponential random variables
of rate lambda.

00:23:18.300 --> 00:23:20.460
So that defines a
Poisson process.

00:23:20.460 --> 00:23:22.940
So we can all go home now
because we now know everything

00:23:22.940 --> 00:23:26.890
about Poisson processes
in principle.

00:23:26.890 --> 00:23:31.490
Everything we're going to say
from now on comes from this

00:23:31.490 --> 00:23:35.620
one simple statement here that
these interarrival intervals

00:23:35.620 --> 00:23:37.070
are exponential.

00:23:37.070 --> 00:23:39.910
There's something very, very
special about this exponential

00:23:39.910 --> 00:23:41.740
distribution.

00:23:41.740 --> 00:23:44.975
And that's what makes Poisson
processes so very special.

00:23:53.570 --> 00:23:58.070
And that special thing is this
memoryless property.

00:23:58.070 --> 00:24:01.760
A random variable is memoryless
if it's positive.

00:24:01.760 --> 00:24:06.530
And for all real t greater than
0 and x greater than 0,

00:24:06.530 --> 00:24:10.580
the probability that x is
greater than t plus x is equal

00:24:10.580 --> 00:24:13.580
to the probability that x is
greater than t times the

00:24:13.580 --> 00:24:16.260
probability that x is
greater than x.

00:24:16.260 --> 00:24:21.470
If you plug that in, then the
statement is the same whether

00:24:21.470 --> 00:24:24.620
you're dealing with densities,
or PMFs, or

00:24:24.620 --> 00:24:27.640
distribution function.

00:24:27.640 --> 00:24:30.980
You get the same product
relationship in each case.

00:24:30.980 --> 00:24:34.470
Since the interarrival interval
is exponential, the

00:24:34.470 --> 00:24:38.540
probability that a random
variable x is greater than

00:24:38.540 --> 00:24:43.570
some particular value x is equal
to e to the minus lambda

00:24:43.570 --> 00:24:46.330
x for x greater than zero.

00:24:46.330 --> 00:24:50.870
This you'll recognize, not as a
distribution function but as

00:24:50.870 --> 00:24:53.190
the complementary distribution
function.

00:24:53.190 --> 00:24:56.560
It's the probability that
X is greater than x.

00:24:56.560 --> 00:25:00.120
So it's the complementary
distribution function

00:25:00.120 --> 00:25:02.430
evaluated at the value of x.

00:25:02.430 --> 00:25:05.530
This is an exponential
which is going down.

00:25:05.530 --> 00:25:11.650
So these random variables
have a probability

00:25:11.650 --> 00:25:16.694
density which is this.

00:25:16.694 --> 00:25:25.110
This is f sub x of X. And they
have a distribution function

00:25:25.110 --> 00:25:28.150
which is this.

00:25:28.150 --> 00:25:30.420
And they have a complementary
distribution

00:25:30.420 --> 00:25:33.300
function which is this.

00:25:33.300 --> 00:25:35.940
Now, this is f of c.

00:25:35.940 --> 00:25:38.290
This is f.

00:25:38.290 --> 00:25:40.080
So there's nothing
much to them.

00:25:42.780 --> 00:25:47.770
And now there's a theorem
which says that a random

00:25:47.770 --> 00:25:52.200
variable is memoryless if and
only if it is exponential.

00:25:52.200 --> 00:25:55.520
We just showed here that an
exponential random variable is

00:25:55.520 --> 00:25:57.180
memoryless.

00:25:57.180 --> 00:26:02.700
To show it the other way
is almost obvious.

00:26:02.700 --> 00:26:04.810
You take this definition here.

00:26:04.810 --> 00:26:08.600
You take the logarithm of
each of these sides.

00:26:08.600 --> 00:26:12.850
When you get the logarithm of
this, it says the logarithm of

00:26:12.850 --> 00:26:18.070
the probability x is greater
than p plus x is the logarithm

00:26:18.070 --> 00:26:21.920
of this plus the logarithm
of this.

00:26:21.920 --> 00:26:25.600
What we have to show to get an
exponential is that this

00:26:25.600 --> 00:26:31.430
logarithm is linear
in its argument t.

00:26:31.430 --> 00:26:36.990
Now, if you have this is equal
to the sum of this and this

00:26:36.990 --> 00:26:41.220
for all t and x, it's sort
of says it's linear.

00:26:41.220 --> 00:26:47.235
There's an exercise, I think
it's Exercise 2.4, which shows

00:26:47.235 --> 00:26:49.660
that you have to be a
little bit careful.

00:26:49.660 --> 00:26:53.310
Or at least as it points
out, very, very picky

00:26:53.310 --> 00:26:57.050
mathematicians have to be a
little bit careful with that.

00:26:57.050 --> 00:27:01.980
And you can worry about that
or not as you choose.

00:27:01.980 --> 00:27:03.620
So that's the theorem.

00:27:03.620 --> 00:27:07.140
That's why Poisson processes
are special.

00:27:07.140 --> 00:27:08.900
And that's why we can
do all the things

00:27:08.900 --> 00:27:11.480
we can do with them.

00:27:11.480 --> 00:27:15.400
The reason why we call it
memoryless is more apparent if

00:27:15.400 --> 00:27:17.670
we use conditional
probabilities.

00:27:17.670 --> 00:27:20.210
With conditional probabilities,
the probability

00:27:20.210 --> 00:27:25.390
that the random variable X is
greater than t plus x, given

00:27:25.390 --> 00:27:29.340
that it's greater than t, is
equal to the probability that

00:27:29.340 --> 00:27:31.410
X is greater than x.

00:27:31.410 --> 00:27:35.530
If people in a checkout line
have exponential service times

00:27:35.530 --> 00:27:40.420
and you've waited 15 minutes for
the person in front, what

00:27:40.420 --> 00:27:44.130
is his or her remaining service
time, assuming the

00:27:44.130 --> 00:27:46.760
service time is exponential?

00:27:46.760 --> 00:27:47.310
What's the answer?

00:27:47.310 --> 00:27:48.890
You've waited 15 minutes.

00:27:48.890 --> 00:27:53.460
Your original service time is
exponential with rate lambda.

00:27:53.460 --> 00:27:55.990
What's the remaining
service time?

00:27:55.990 --> 00:27:58.560
Well, the answer is
it's exponential.

00:27:58.560 --> 00:28:00.700
That's this memoryless
property.

00:28:00.700 --> 00:28:05.880
It's called memoryless because
the random variable doesn't

00:28:05.880 --> 00:28:09.330
remember how long it
hasn't happened.

00:28:11.880 --> 00:28:15.190
So you can think of an
exponential random variable as

00:28:15.190 --> 00:28:17.520
something which takes
place in time.

00:28:17.520 --> 00:28:21.000
And in each instant of time, it
might or might not happen.

00:28:21.000 --> 00:28:24.110
And if it hasn't happened yet,
there's still the same

00:28:24.110 --> 00:28:27.210
probability in every remaining
increment that it's going to

00:28:27.210 --> 00:28:28.310
happen then.

00:28:28.310 --> 00:28:32.510
So you haven't gained anything
and you haven't lost anything

00:28:32.510 --> 00:28:34.280
by having to wait this long.

00:28:37.320 --> 00:28:41.120
Here's an interesting question
which you can tie yourself in

00:28:41.120 --> 00:28:42.800
knots for a little bit.

00:28:42.800 --> 00:28:44.835
Has your time waiting
been wasted?

00:28:48.360 --> 00:28:52.860
Namely the time you still have
to wait is exponential with

00:28:52.860 --> 00:28:56.700
the same rate as
it was before.

00:28:56.700 --> 00:29:00.070
So the expected amount of time
you have to wait is still the

00:29:00.070 --> 00:29:07.790
same as when you got into line
15 minutes ago with this one

00:29:07.790 --> 00:29:11.320
very slow person in
front of you.

00:29:11.320 --> 00:29:14.400
So have you wasting your time?

00:29:14.400 --> 00:29:15.700
Well, you haven't
gained anything.

00:29:19.010 --> 00:29:21.510
But you haven't really wasted
your time either.

00:29:21.510 --> 00:29:27.210
Because if you have to get
served in that line, then at

00:29:27.210 --> 00:29:31.500
some point, you're going to
have to go in that line.

00:29:31.500 --> 00:29:34.370
And you might look for a time
when the line is very short.

00:29:34.370 --> 00:29:36.910
You might be lucky and find
a time when the line is

00:29:36.910 --> 00:29:37.860
completely empty.

00:29:37.860 --> 00:29:40.360
And then you start getting
served right away.

00:29:40.360 --> 00:29:47.760
But if you ignore those issues,
then in fact, in a

00:29:47.760 --> 00:29:51.590
sense, you have wasted
your time.

00:29:51.590 --> 00:29:54.390
Another more interesting
question then is why do you

00:29:54.390 --> 00:29:58.410
move to another line if somebody
takes a long time?

00:29:58.410 --> 00:30:00.160
All of you have had
this experience.

00:30:00.160 --> 00:30:03.170
You're in a supermarket.

00:30:03.170 --> 00:30:08.130
Or you're at an airplane counter
or any of the places

00:30:08.130 --> 00:30:10.790
where you have to wait
for service.

00:30:10.790 --> 00:30:14.200
There's somebody, one person in
front of you, who has been

00:30:14.200 --> 00:30:16.150
there forever.

00:30:16.150 --> 00:30:18.990
And it seems as if they're going
to stay there forever.

00:30:18.990 --> 00:30:21.200
You notice another line
that only has one

00:30:21.200 --> 00:30:22.950
person being served.

00:30:22.950 --> 00:30:27.595
And most of us, especially very
impatient people like me,

00:30:27.595 --> 00:30:31.610
I'm going to walk over and
get into that other line.

00:30:31.610 --> 00:30:36.180
And the question is, is that
rational or isn't it rational?

00:30:36.180 --> 00:30:37.740
If the service times are

00:30:37.740 --> 00:30:40.750
exponential, it is not rational.

00:30:40.750 --> 00:30:44.030
It doesn't make any difference
whether I stay where I am or

00:30:44.030 --> 00:30:46.390
go to the other line.

00:30:46.390 --> 00:30:51.110
If the service times are fixed
duration, namely suppose every

00:30:51.110 --> 00:30:55.190
service time takes 10 minutes
and I've waited for a long

00:30:55.190 --> 00:30:59.410
time, is it rational for me
to move to the other line?

00:30:59.410 --> 00:31:03.840
Absolutely not because I'm
almost at the end of that 10

00:31:03.840 --> 00:31:05.190
minutes now.

00:31:05.190 --> 00:31:08.160
And I'm about to be served.

00:31:08.160 --> 00:31:09.470
So why do we move?

00:31:09.470 --> 00:31:14.020
Is it just psychology, that
we're very impatient?

00:31:14.020 --> 00:31:14.890
I don't think so.

00:31:14.890 --> 00:31:19.090
I think it's because we have all
seen that an awful lot of

00:31:19.090 --> 00:31:23.730
lines, particularly airline
reservation lines, and if your

00:31:23.730 --> 00:31:25.950
plane doesn't fly or something,
and you're trying

00:31:25.950 --> 00:31:31.960
to get rescheduled, or any of
these things, the service time

00:31:31.960 --> 00:31:36.860
is worse than Poisson in the
sense that if you've waited

00:31:36.860 --> 00:31:41.390
for 10 minutes, your expected
remaining waiting time is

00:31:41.390 --> 00:31:44.400
greater than it was before
you started waiting.

00:31:44.400 --> 00:31:48.190
The longer you wait, the longer
your expected remaining

00:31:48.190 --> 00:31:49.685
waiting time is.

00:31:49.685 --> 00:31:52.693
And that's called a heavy-tailed
distribution.

00:31:56.000 --> 00:31:59.500
What most of us have noticed, I
think, in our lives is that

00:31:59.500 --> 00:32:03.390
an awful lot of waiting lines
that human beings wait in are

00:32:03.390 --> 00:32:05.140
in fact heavy-tailed.

00:32:05.140 --> 00:32:09.920
So that in fact is part of the
reason why we move if somebody

00:32:09.920 --> 00:32:11.780
takes a long time.

00:32:11.780 --> 00:32:15.520
It's interesting to see
how the brain works.

00:32:15.520 --> 00:32:19.080
Because I'm sure that none
of you have ever really

00:32:19.080 --> 00:32:22.240
rationally analyzed this
question of why you move.

00:32:22.240 --> 00:32:23.390
Have you?

00:32:23.390 --> 00:32:25.390
I mean, I have because
I teach probability

00:32:25.390 --> 00:32:27.760
courses all the time.

00:32:27.760 --> 00:32:30.420
But I don't think anyone who
doesn't teach probability

00:32:30.420 --> 00:32:35.290
courses would be crazy enough
to waste their time on a

00:32:35.290 --> 00:32:37.240
question like this.

00:32:37.240 --> 00:32:40.730
But your brain automatically
figures that out.

00:32:40.730 --> 00:32:43.500
I mean, your brain is smart
enough to know that if you've

00:32:43.500 --> 00:32:46.300
waited for a long time, you're
probably going to have to wait

00:32:46.300 --> 00:32:48.510
for an even longer time.

00:32:48.510 --> 00:32:51.570
And it makes sense to move to
another line where your

00:32:51.570 --> 00:32:55.090
waiting time is probably
going to be shorter.

00:32:55.090 --> 00:32:59.460
So you're pretty smart if you
don't think about it too much.

00:33:01.980 --> 00:33:05.980
Here's an interesting theorem
now that makes use of this

00:33:05.980 --> 00:33:08.890
memoryless property.

00:33:08.890 --> 00:33:12.720
This is Theorem 2.2.1
in the text.

00:33:12.720 --> 00:33:14.990
It's not stated terribly
well there.

00:33:14.990 --> 00:33:17.660
And I'll tell you why
in a little bit.

00:33:17.660 --> 00:33:19.450
It's not stated too badly.

00:33:19.450 --> 00:33:20.680
I mean, it's stated correctly.

00:33:20.680 --> 00:33:23.620
But it's just a little hard to
understand what it says.

00:33:23.620 --> 00:33:27.460
If you have a Poisson process
of rate lambda and you're

00:33:27.460 --> 00:33:32.020
looking at any given time
t, here's t down here.

00:33:32.020 --> 00:33:35.380
You're looking at the
process of time t.

00:33:35.380 --> 00:33:37.690
The interval z--

00:33:37.690 --> 00:33:39.580
here's the interval z here--

00:33:39.580 --> 00:33:50.760
from t until the next arrival
has distribution e to the

00:33:50.760 --> 00:33:52.080
minus lambda z.

00:33:52.080 --> 00:33:57.930
And it has this distribution
for all real numbers

00:33:57.930 --> 00:33:59.480
greater than 0.

00:33:59.480 --> 00:34:04.880
The random variable Z is
independent of n of t.

00:34:04.880 --> 00:34:09.260
In other words, this random
variable here is independent

00:34:09.260 --> 00:34:14.080
of how many arrivals there
have been at time t.

00:34:14.080 --> 00:34:20.540
And given this, it's independent
of s sub n, which

00:34:20.540 --> 00:34:24.330
is the time at which the
last arrival occurred.

00:34:24.330 --> 00:34:27.429
Namely, here's n
of t equals 2.

00:34:27.429 --> 00:34:30.870
Here's s of 2 at time tau.

00:34:30.870 --> 00:34:37.630
So given both n of t and s sub
2 in this case, or s sub n of

00:34:37.630 --> 00:34:40.549
t as we might call it, and
that's what gets confusing.

00:34:40.549 --> 00:34:42.980
And I'll talk about
that later.

00:34:42.980 --> 00:34:48.380
Given those two things, the
number n of arrivals in 0t--

00:34:48.380 --> 00:34:51.900
well, I got off.

00:34:51.900 --> 00:34:54.960
The random variable Z is
independent of n of t.

00:34:54.960 --> 00:34:59.110
And given n of t, Z is
independent of all of these

00:34:59.110 --> 00:35:02.075
arrival epochs up
until time t.

00:35:02.075 --> 00:35:07.800
And it's also independent of
n of t for all values of

00:35:07.800 --> 00:35:10.660
tau up until t.

00:35:10.660 --> 00:35:12.260
That's what the theorem
states.

00:35:12.260 --> 00:35:16.120
What the theorem states is that
this memoryless property

00:35:16.120 --> 00:35:19.890
that we've just stated for
random variables is really a

00:35:19.890 --> 00:35:23.470
property of the Poisson
process.

00:35:23.470 --> 00:35:26.340
When we say that if a random
variable, it's a little hard

00:35:26.340 --> 00:35:30.080
to see why would anyone was
calling it memoryless.

00:35:30.080 --> 00:35:33.390
When you state it for a Poisson
process, it's very

00:35:33.390 --> 00:35:37.190
obvious why we want to
call it memoryless.

00:35:37.190 --> 00:35:41.850
It says that this time here from
t, from any arbitrary t,

00:35:41.850 --> 00:35:45.790
until the next arrival occurs,
that this is independent of

00:35:45.790 --> 00:35:52.380
all this junk that happens
before or up to time t.

00:35:52.380 --> 00:35:54.570
That's what the theorem says.

00:35:54.570 --> 00:35:57.600
Here's a sort of a
half proof of it.

00:35:57.600 --> 00:35:59.780
There's a careful proof
in the notes.

00:35:59.780 --> 00:36:01.800
The statement in the notes
is not that careful,

00:36:01.800 --> 00:36:02.790
but the proof is.

00:36:02.790 --> 00:36:09.010
And the proof is drawn
out perhaps too much.

00:36:09.010 --> 00:36:12.730
You can find your comfort level
between this and the

00:36:12.730 --> 00:36:14.960
much longer version
in the notes.

00:36:14.960 --> 00:36:17.910
You might understand
it well from this.

00:36:17.910 --> 00:36:21.430
Given n of t is equal to 2 in
this case, and in general,

00:36:21.430 --> 00:36:26.410
given that n of t is equal to
any constant n, and given that

00:36:26.410 --> 00:36:31.140
s sub 2 where this 2 is equal to
that 2, given that s sub 2

00:36:31.140 --> 00:36:38.010
is equal to tau, then x3, this
value here, the interarrival

00:36:38.010 --> 00:36:43.120
arrival time from this previous
arrival before t to

00:36:43.120 --> 00:36:48.470
the next arrival after t, namely
x3, is the thing which

00:36:48.470 --> 00:36:54.160
bridges across this time that
we selected, t. t is not a

00:36:54.160 --> 00:36:55.770
random thing.

00:36:55.770 --> 00:36:58.700
t is just something you're
interested in.

00:36:58.700 --> 00:37:02.550
I want to catch a plane at 7
o'clock tomorrow evening.

00:37:02.550 --> 00:37:05.520
t then is 7 o'clock
tomorrow evening.

00:37:05.520 --> 00:37:09.080
What's the time from the last
plane that went out to New

00:37:09.080 --> 00:37:12.460
York until the next plane that's
going out to New York?

00:37:12.460 --> 00:37:16.510
If the planes are so screwed
up that the schedule means

00:37:16.510 --> 00:37:18.990
nothing, then they're just
flying out whenever

00:37:18.990 --> 00:37:21.920
they can fly out.

00:37:21.920 --> 00:37:25.410
That's the meaning
of this x3 here.

00:37:25.410 --> 00:37:31.020
That says that x3, in fact,
has to be bigger

00:37:31.020 --> 00:37:32.680
than t minus tau.

00:37:32.680 --> 00:37:38.380
If we're given that n of t is
equal to 2 and that the time

00:37:38.380 --> 00:37:42.210
of the previous arrival is at
tau, we're given that there

00:37:42.210 --> 00:37:45.390
haven't been any arrivals
between the last arrival

00:37:45.390 --> 00:37:47.100
before t and t.

00:37:47.100 --> 00:37:48.470
That's what we're given.

00:37:48.470 --> 00:37:52.060
This was the last arrival before
t by the assumption

00:37:52.060 --> 00:37:52.940
we've made.

00:37:52.940 --> 00:37:56.450
So we're assuming there's
nothing in this interval.

00:37:56.450 --> 00:38:01.720
And then we're asking what is
the remaining time until x3 is

00:38:01.720 --> 00:38:02.840
all finished.

00:38:02.840 --> 00:38:06.850
And that's the random variable
that we call Z. So Z is x3

00:38:06.850 --> 00:38:09.270
minus t minus tau.

00:38:09.270 --> 00:38:14.260
The complementary distribution
function of Z conditional on

00:38:14.260 --> 00:38:20.640
both n and on s, this n here
and this s here is then

00:38:20.640 --> 00:38:24.580
exponential with e to
the minus lambda Z.

00:38:24.580 --> 00:38:29.740
Now, if I know that this is
exponential, what can I say

00:38:29.740 --> 00:38:31.800
about the random variable
Z itself?

00:38:34.510 --> 00:38:39.100
Well, there's an easy way to
find the distribution of Z

00:38:39.100 --> 00:38:42.315
when you know Z conditional
onto other things.

00:38:48.200 --> 00:38:51.910
You take what the distribution
is conditional on, each value

00:38:51.910 --> 00:38:53.600
of n and s.

00:38:53.600 --> 00:38:58.510
You then multiply that by the
probability that n and s have

00:38:58.510 --> 00:39:00.120
those particular values.

00:39:00.120 --> 00:39:03.120
And then you integrate.

00:39:03.120 --> 00:39:06.830
Now, we can look at this and
say we don't have to go

00:39:06.830 --> 00:39:08.260
through all of that.

00:39:08.260 --> 00:39:11.720
And in fact, we won't know what
the distribution of n is.

00:39:11.720 --> 00:39:14.670
And we certainly won't know what
the distribution of this

00:39:14.670 --> 00:39:18.760
previous arrival is for
quite a long time.

00:39:18.760 --> 00:39:21.310
Why don't we need
to know that?

00:39:21.310 --> 00:39:26.590
Well, because we know that
whatever n of t is and

00:39:26.590 --> 00:39:31.185
whatever s sub n of t is doesn't
make any difference.

00:39:31.185 --> 00:39:35.170
The distribution of Z is
still the same thing.

00:39:35.170 --> 00:39:39.620
So we know this has to be the
unconditional distribution

00:39:39.620 --> 00:39:43.880
function of Z also even without
knowing anything about

00:39:43.880 --> 00:39:47.020
n or knowing about s.

00:39:47.020 --> 00:39:51.930
And that means that the
complementary distribution

00:39:51.930 --> 00:39:57.790
function of Z is equal to e to
the minus lambda Z also.

00:39:57.790 --> 00:40:03.210
So that's sort of a proof if you
want to be really picky.

00:40:03.210 --> 00:40:05.870
And I would suggest you
try to be picky.

00:40:05.870 --> 00:40:09.640
When you read the notes, try to
understand why one has to

00:40:09.640 --> 00:40:12.260
say a little more than
one says here.

00:40:12.260 --> 00:40:13.620
Because that's the
way you really

00:40:13.620 --> 00:40:15.720
understand these things.

00:40:15.720 --> 00:40:18.630
But this really gives you
the idea of the proof.

00:40:18.630 --> 00:40:20.630
And it's pretty close
to a complete proof.

00:40:24.830 --> 00:40:26.430
This is saying what
we just said.

00:40:26.430 --> 00:40:30.380
The conditional distribution
of Z doesn't vary with the

00:40:30.380 --> 00:40:33.050
conditioning values.

00:40:33.050 --> 00:40:35.240
n of t equals n.

00:40:35.240 --> 00:40:37.150
And s sub n equals tau.

00:40:37.150 --> 00:40:42.040
So Z is statistically
independent of n of t and s

00:40:42.040 --> 00:40:43.550
sub n of t.

00:40:43.550 --> 00:40:47.890
You should look at the text
again, as I said, for more

00:40:47.890 --> 00:40:49.880
careful proof of that.

00:40:49.880 --> 00:40:53.940
What is this random variable
s sub n of t?

00:40:53.940 --> 00:40:57.640
It's clear from the picture
what it is.

00:40:57.640 --> 00:41:04.400
s sub n of t is the last
arrival before

00:41:04.400 --> 00:41:07.580
we're at time t.

00:41:07.580 --> 00:41:10.970
That's what it is in
the picture here.

00:41:10.970 --> 00:41:13.540
How do you define a random
variable like that?

00:41:17.080 --> 00:41:21.700
There's a temptation to
do it the following

00:41:21.700 --> 00:41:24.170
way which is incorrect.

00:41:24.170 --> 00:41:28.710
There's a temptation to say,
well, conditional on n of t,

00:41:28.710 --> 00:41:31.500
suppose n of t is equal to n.

00:41:31.500 --> 00:41:36.730
Let me then find the
distribution of s sub n.

00:41:36.730 --> 00:41:39.510
And that's not the right
way to do it.

00:41:39.510 --> 00:41:44.500
Because s sub n of t and n
of t are certainly not

00:41:44.500 --> 00:41:45.510
independent.

00:41:45.510 --> 00:41:48.590
n of t tells you what random
variable you want to look at.

00:41:52.110 --> 00:41:55.750
How do you define a random
variable in terms of a mapping

00:41:55.750 --> 00:42:02.520
from the sample space omega onto
the set of real numbers?

00:42:02.520 --> 00:42:07.040
So what you do here is you look
at a sample point omega.

00:42:07.040 --> 00:42:12.200
It maps into this random
variable n of t, the sample

00:42:12.200 --> 00:42:16.520
value of that at omega,
that's sum value n.

00:42:16.520 --> 00:42:22.210
And then you map that same
sample point into--

00:42:22.210 --> 00:42:24.520
now, you know which random
variable it is

00:42:24.520 --> 00:42:25.660
you're looking at.

00:42:25.660 --> 00:42:30.160
You take that same omega and
map it into sub time tau.

00:42:30.160 --> 00:42:34.740
So that's what we mean
by s sub n of t.

00:42:34.740 --> 00:42:38.800
If your mind glazes over at
that, don't worry about it.

00:42:38.800 --> 00:42:40.650
Think about it a
little bit now.

00:42:40.650 --> 00:42:42.900
Come back and think
about it later.

00:42:42.900 --> 00:42:46.220
Every time I don't think about
this for two weeks, my mind

00:42:46.220 --> 00:42:47.910
glazes over when I look at it.

00:42:47.910 --> 00:42:51.310
And I have to think very hard
about what this very peculiar

00:42:51.310 --> 00:42:53.610
looking random variable is.

00:42:53.610 --> 00:42:58.660
When I have a random variable
where I have a sequence of

00:42:58.660 --> 00:43:02.460
random variables, and I have a
random variable which is a

00:43:02.460 --> 00:43:06.270
random selection among those
random variables, it's a very

00:43:06.270 --> 00:43:08.180
complicated animal.

00:43:08.180 --> 00:43:09.977
And that's what this is.

00:43:09.977 --> 00:43:12.990
But we've just said
what it is.

00:43:12.990 --> 00:43:18.760
So you can think about
it as you go.

00:43:18.760 --> 00:43:20.600
The theorem essentially
extends the idea of

00:43:20.600 --> 00:43:23.340
memorylessness to the entire
Poisson process.

00:43:23.340 --> 00:43:26.390
In other words, this says that
a Poisson process is

00:43:26.390 --> 00:43:27.910
memoryless.

00:43:27.910 --> 00:43:30.440
You look at a particular
time t.

00:43:30.440 --> 00:43:33.980
And the time until the next
arrival is independent of

00:43:33.980 --> 00:43:35.780
everything that's going
before that.

00:43:40.490 --> 00:43:43.530
Starting at any time tau,
yeah, well, subsequent

00:43:43.530 --> 00:43:47.360
interrarrival times are
independent of Z

00:43:47.360 --> 00:43:50.150
and also of the past.

00:43:50.150 --> 00:43:53.420
I'm waving my hands
a little bit here.

00:43:53.420 --> 00:43:55.370
But in fact, what I'm
saying is right.

00:43:55.370 --> 00:43:58.930
We have these interarrival
intervals that we know are

00:43:58.930 --> 00:44:00.130
independent.

00:44:00.130 --> 00:44:04.090
The interarrival intervals which
have occurred completely

00:44:04.090 --> 00:44:10.400
before time t are independent of
this random variable Z. The

00:44:10.400 --> 00:44:14.760
next interarrival interval after
Z is independent of all

00:44:14.760 --> 00:44:17.370
the interarrival intervals
before that.

00:44:17.370 --> 00:44:25.550
And those interarrival intervals
before that are

00:44:25.550 --> 00:44:30.580
determined by the counting
process up until time t.

00:44:30.580 --> 00:44:33.230
So the counting process
corresponds to this

00:44:33.230 --> 00:44:37.020
corresponding interarrival
process.

00:44:37.020 --> 00:44:41.830
It's n of t prime minus n of t
for t prime greater than t.

00:44:41.830 --> 00:44:44.520
In other words, we now want to
look at a counting process

00:44:44.520 --> 00:44:49.460
which starts at time t and
follows whatever it has to

00:44:49.460 --> 00:44:52.960
follow from this original
counting process.

00:44:52.960 --> 00:44:56.800
And what we're saying is this
first arrival and this process

00:44:56.800 --> 00:45:00.740
starting at time t is
independent of everything that

00:45:00.740 --> 00:45:02.100
went before.

00:45:02.100 --> 00:45:05.960
And every subsequent
interarrival time after that

00:45:05.960 --> 00:45:10.070
is independent of everything
before time t.

00:45:10.070 --> 00:45:15.420
So this says that the process n
of t prime minus n of t as a

00:45:15.420 --> 00:45:17.270
process nt prime.

00:45:17.270 --> 00:45:21.560
This is a counting process nt
prime defined for t prime

00:45:21.560 --> 00:45:22.590
greater than t.

00:45:22.590 --> 00:45:27.870
So for fixed t, we now have
something which we can view

00:45:27.870 --> 00:45:31.670
over variable t prime as
a counting process.

00:45:31.670 --> 00:45:36.050
It's a Poisson process shifted
to start at time t, ie, for

00:45:36.050 --> 00:45:40.520
each t prime, n of t prime minus
the n of t has the same

00:45:40.520 --> 00:45:45.230
distribution as n of
t prime minus t.

00:45:45.230 --> 00:45:47.040
Same for joint distributions.

00:45:47.040 --> 00:45:49.740
In other words, this
random variable Z

00:45:49.740 --> 00:45:50.710
is exponential again.

00:45:50.710 --> 00:45:53.960
And all the future interarrival
times are

00:45:53.960 --> 00:45:55.000
exponential.

00:45:55.000 --> 00:45:58.940
So it's defined in exactly the
same way as the original

00:45:58.940 --> 00:46:01.100
random process is.

00:46:01.100 --> 00:46:03.155
So it's statistically
the same process.

00:46:05.770 --> 00:46:08.390
Which says two things
about it.

00:46:08.390 --> 00:46:09.810
Everything is the same.

00:46:09.810 --> 00:46:12.890
And everything is independent.

00:46:12.890 --> 00:46:15.560
We will call that stationary.

00:46:15.560 --> 00:46:17.140
Everything is the same.

00:46:17.140 --> 00:46:20.400
And independent, everything
is independent.

00:46:20.400 --> 00:46:23.310
And then we'll try to sort out
how things can be the same but

00:46:23.310 --> 00:46:25.750
also be independent.

00:46:25.750 --> 00:46:27.850
Oh, we already know that.

00:46:27.850 --> 00:46:33.010
We have two IID random
variables, x1 and x2.

00:46:33.010 --> 00:46:34.250
They're IID.

00:46:34.250 --> 00:46:36.890
They're independent and
identically distributed.

00:46:36.890 --> 00:46:39.330
Identity distributed means
that in one sense,

00:46:39.330 --> 00:46:40.870
they are the same.

00:46:40.870 --> 00:46:43.630
But they're also independent
of each other.

00:46:43.630 --> 00:46:48.080
So the random variables are
defined in the same way.

00:46:48.080 --> 00:46:50.180
And in that sense, they're
stationary.

00:46:50.180 --> 00:46:53.160
But they're independent of each
other by the definition

00:46:53.160 --> 00:46:55.390
of independence.

00:46:55.390 --> 00:47:00.570
So our new process is
independent of the old process

00:47:00.570 --> 00:47:08.950
in the interval 0 up to t.

00:47:08.950 --> 00:47:11.750
When we're talking about Poisson
processes and also

00:47:11.750 --> 00:47:17.350
arrival processes, we always
talk about intervals which are

00:47:17.350 --> 00:47:21.460
open on the left and closed
on the right.

00:47:21.460 --> 00:47:23.920
That's completely arbitrary.

00:47:23.920 --> 00:47:27.250
But if you don't make one
convention or the other, you

00:47:27.250 --> 00:47:31.940
could make them closed on the
left and open on the right,

00:47:31.940 --> 00:47:34.830
and that would be
consistent also.

00:47:34.830 --> 00:47:35.675
But nobody does.

00:47:35.675 --> 00:47:38.210
And it would be much
more confusing.

00:47:38.210 --> 00:47:42.870
So it's much easier to make
things closed on the right.

00:47:47.280 --> 00:47:50.820
So we're up to stationary and
independent increments.

00:47:50.820 --> 00:47:52.990
Well, we're not up to there.

00:47:52.990 --> 00:47:54.960
We're almost finished
with that.

00:47:54.960 --> 00:47:59.340
We've virtually already said
that increments are stationary

00:47:59.340 --> 00:48:00.000
and independent.

00:48:00.000 --> 00:48:04.560
And an increment is just a piece
of a Poisson process.

00:48:04.560 --> 00:48:06.220
That's an increment,
a piece of it.

00:48:09.620 --> 00:48:16.070
So a counting process has the
stationary increment property

00:48:16.070 --> 00:48:21.493
if n of t prime minus n of t has
the same distribution as n

00:48:21.493 --> 00:48:26.130
of t prime minus t for all
t prime greater than t

00:48:26.130 --> 00:48:28.060
greater than 0.

00:48:28.060 --> 00:48:30.950
In other words, you look at
this counting process.

00:48:30.950 --> 00:48:32.740
Goes up.

00:48:32.740 --> 00:48:35.970
Then you start at some
particular value of t.

00:48:35.970 --> 00:48:38.300
Let me draw a picture of that.

00:48:38.300 --> 00:48:39.550
Make it a little clearer.

00:49:00.330 --> 00:49:05.960
And the new Poisson process
starts at this value and goes

00:49:05.960 --> 00:49:08.360
up from there.

00:49:08.360 --> 00:49:13.970
So this thing here is
what we call n of t

00:49:13.970 --> 00:49:18.000
prime minus n of t.

00:49:18.000 --> 00:49:19.520
Because here's n of t.

00:49:22.190 --> 00:49:26.560
Here's t prime out here for
any value out here.

00:49:26.560 --> 00:49:29.880
And we're looking at the number
of arrivals up until

00:49:29.880 --> 00:49:31.410
time t prime.

00:49:31.410 --> 00:49:34.442
And what we're talking about,
when we're talking about n of

00:49:34.442 --> 00:49:40.110
t prime minus n of t, we're
talking about what happens in

00:49:40.110 --> 00:49:43.210
this region here.

00:49:43.210 --> 00:49:48.010
And we're saying that this is
a Poisson process again.

00:49:48.010 --> 00:49:50.530
And now in a minute, we're going
to say that this Poisson

00:49:50.530 --> 00:49:57.720
process is independent of what
happened up until time t.

00:49:57.720 --> 00:49:59.460
But Poisson processes have this

00:49:59.460 --> 00:50:02.540
stationary increment property.

00:50:02.540 --> 00:50:08.930
And a counting process has the
independent increment property

00:50:08.930 --> 00:50:14.970
if for every sequence of times,
t1, t2, up to t sub n.

00:50:14.970 --> 00:50:22.830
The random variables n of t1 and
tilde of t1, t2, we didn't

00:50:22.830 --> 00:50:23.830
talk about that.

00:50:23.830 --> 00:50:27.750
But I think it's defined
on one of those slides.

00:50:27.750 --> 00:50:41.480
n of t and t prime is defined as
n of t prime minus n of t.

00:50:44.830 --> 00:50:49.740
So n of t and t prime is really
the number of arrivals

00:50:49.740 --> 00:50:55.360
that have occurred from
t up until t prime--

00:50:55.360 --> 00:50:59.370
open on t, closed on t prime.

00:50:59.370 --> 00:51:07.410
So a counting process has the
independent increment property

00:51:07.410 --> 00:51:11.310
if for every finite set of
times, these random variables

00:51:11.310 --> 00:51:13.920
here are independent.

00:51:13.920 --> 00:51:16.860
The number of arrivals in the
first increment, number of

00:51:16.860 --> 00:51:19.860
arrivals in the second
increment, number of arrivals

00:51:19.860 --> 00:51:23.320
in the third increment, no
matter how you choose t1, t2,

00:51:23.320 --> 00:51:27.490
up to t sub n, what happens here
is independent of what

00:51:27.490 --> 00:51:29.600
happens here, is independent
of what happens

00:51:29.600 --> 00:51:31.790
here, and so forth.

00:51:31.790 --> 00:51:33.270
It's not only that
what happens in

00:51:33.270 --> 00:51:35.540
Las Vegas stays there.

00:51:35.540 --> 00:51:38.200
It's that what happens in
Boston stays there, what

00:51:38.200 --> 00:51:41.050
happens in Philadelphia stays
there, and so forth.

00:51:41.050 --> 00:51:43.740
What happens anywhere
stays anywhere.

00:51:43.740 --> 00:51:45.850
It never gets out of there.

00:51:45.850 --> 00:51:48.170
That's what we mean by
independence in this case.

00:51:48.170 --> 00:51:51.710
So it's a strong statement.

00:51:51.710 --> 00:51:55.000
But we've essentially said that
Poisson processes have

00:51:55.000 --> 00:51:57.410
that property.

00:51:57.410 --> 00:52:00.540
So this property implies is the
number of arrivals in each

00:52:00.540 --> 00:52:04.640
of the set of non-overlapping
intervals are independent

00:52:04.640 --> 00:52:06.270
random variables.

00:52:06.270 --> 00:52:18.010
For a Poisson process, we've
seen that the number of

00:52:18.010 --> 00:52:23.450
arrivals in t sub i minus 1 to t
sub i is independent of this

00:52:23.450 --> 00:52:26.960
whole set of random
variables here.

00:52:26.960 --> 00:52:30.520
Now, remember that when we're
talking about multiple random

00:52:30.520 --> 00:52:34.010
variables, we say that multiple
random variables are

00:52:34.010 --> 00:52:35.400
independent.

00:52:35.400 --> 00:52:37.740
It's not enough to be pairwise
independent.

00:52:37.740 --> 00:52:40.080
They all have to
be independent.

00:52:40.080 --> 00:52:44.260
But this thing we've just said
says that this is independent

00:52:44.260 --> 00:52:46.560
of all of these things.

00:52:46.560 --> 00:52:50.260
If this is independent of all of
these things, and then the

00:52:50.260 --> 00:52:55.610
next interval n of ti, ti plus
1, is independent of

00:52:55.610 --> 00:52:59.290
everything in the past, and so
forth all the way up, then all

00:52:59.290 --> 00:53:02.870
of those random variables are
statistically independent of

00:53:02.870 --> 00:53:03.710
each other.

00:53:03.710 --> 00:53:08.130
So in fact, we're saying more
than pairwise statistical

00:53:08.130 --> 00:53:10.790
independence.

00:53:10.790 --> 00:53:14.970
If you're panicking about
these minor differences

00:53:14.970 --> 00:53:19.860
between pairwise independence
and real independence, don't

00:53:19.860 --> 00:53:21.240
worry about it too much.

00:53:21.240 --> 00:53:24.430
Because the situations
where that happens

00:53:24.430 --> 00:53:26.240
are relatively rare.

00:53:26.240 --> 00:53:28.030
They don't happen
all the time.

00:53:28.030 --> 00:53:29.720
But they do happen
occasionally.

00:53:29.720 --> 00:53:33.157
So you should be aware of it.

00:53:33.157 --> 00:53:35.100
You shouldn't get in
a panic about it.

00:53:35.100 --> 00:53:38.200
Because normally, you don't
have to worry about it.

00:53:38.200 --> 00:53:42.240
In other words, when you're
taking a quiz, don't worry

00:53:42.240 --> 00:53:45.520
about any of the fine points.

00:53:45.520 --> 00:53:49.060
Figure out roughly how
to do the problems.

00:53:49.060 --> 00:53:51.620
Do them more or less.

00:53:51.620 --> 00:53:56.010
And then come back and deal with
the fine points later.

00:53:56.010 --> 00:53:59.430
Don't spend the whole quiz time
wrapped up on one little

00:53:59.430 --> 00:54:03.370
fine point and not get
to anything else.

00:54:03.370 --> 00:54:06.340
One of the important things to
learn in understanding a

00:54:06.340 --> 00:54:10.380
subject like this is to figure
out what are the fine points,

00:54:10.380 --> 00:54:12.370
what are the important points.

00:54:12.370 --> 00:54:14.740
How do you tell whether
something is important in a

00:54:14.740 --> 00:54:16.000
particular context.

00:54:16.000 --> 00:54:19.870
And that just takes intuition.

00:54:19.870 --> 00:54:23.730
That takes some intuition from
working with these processes.

00:54:23.730 --> 00:54:26.970
And you pick that
up as you go.

00:54:26.970 --> 00:54:30.840
But anyway, we wind up now
with the statement that

00:54:30.840 --> 00:54:34.440
Poisson processes have
stationary and independent

00:54:34.440 --> 00:54:35.070
increments.

00:54:35.070 --> 00:54:37.730
Which means that what happens
in each interval is

00:54:37.730 --> 00:54:42.800
independent of what happens
in each other interval.

00:54:42.800 --> 00:54:47.390
So we're done with that until
we get to alternate

00:54:47.390 --> 00:54:50.045
definitions of a Poisson
process.

00:54:50.045 --> 00:54:54.590
And we now want to deal with
the Erlang and the Poisson

00:54:54.590 --> 00:55:00.400
distributions, which are just
very plug and chug kinds of

00:55:00.400 --> 00:55:03.700
things to a certain extent.

00:55:03.700 --> 00:55:09.695
For a Poisson process of rate
lambda, the density function

00:55:09.695 --> 00:55:17.130
of arrival epoch s2, s2 is
the sum of x1 plus x2.

00:55:17.130 --> 00:55:20.730
x1 is an exponential random
variable of rate lambda.

00:55:20.730 --> 00:55:26.420
x2 is an independent random
variable of rate lambda.

00:55:26.420 --> 00:55:31.470
How do you find the probability
density function

00:55:31.470 --> 00:55:34.470
as a sum of two independent
random variables, which both

00:55:34.470 --> 00:55:35.810
have a density?

00:55:35.810 --> 00:55:37.930
You convolve them.

00:55:37.930 --> 00:55:43.610
That's something that you've
known ever since you studied

00:55:43.610 --> 00:55:47.170
any kind of linear systems, or
from any probability, or

00:55:47.170 --> 00:55:47.970
anything else.

00:55:47.970 --> 00:55:51.050
Convolution is the way to
solve this problem.

00:55:51.050 --> 00:55:54.670
When you convolve these
two random variables,

00:55:54.670 --> 00:55:56.400
here I've done it.

00:55:56.400 --> 00:56:00.322
You get lambda squared t times
e to the minus lambda t.

00:56:03.040 --> 00:56:07.400
This kind of form here with an
e to the minus lambda t, and

00:56:07.400 --> 00:56:11.430
with a t, or t squared, or so
forth, is a particularly easy

00:56:11.430 --> 00:56:13.500
form to integrate.

00:56:13.500 --> 00:56:16.790
So we just do this
again and again.

00:56:16.790 --> 00:56:19.400
And when we do it again and
again, we find out that the

00:56:19.400 --> 00:56:25.010
density function as a sum of n
of these random variables, you

00:56:25.010 --> 00:56:27.840
keep picking up an extra lambda
every time you convolve

00:56:27.840 --> 00:56:31.860
in another exponential
random variable.

00:56:31.860 --> 00:56:36.450
You pick up an extra factor of
t whenever you do this again.

00:56:36.450 --> 00:56:39.770
This stays the same
as it does here.

00:56:39.770 --> 00:56:45.480
And strangely enough, this n
minus 1 factorial appears down

00:56:45.480 --> 00:56:51.770
here when you start integrating
something with

00:56:51.770 --> 00:56:54.315
some power of t in it.

00:56:54.315 --> 00:56:57.030
So when you integrate this,
this is what you get.

00:56:57.030 --> 00:57:01.030
And it's called the
Erlang density.

00:57:01.030 --> 00:57:02.230
Any questions about this?

00:57:02.230 --> 00:57:04.620
Or any questions
about anything?

00:57:08.650 --> 00:57:09.420
I'm getting hoarse.

00:57:09.420 --> 00:57:10.310
I need questions.

00:57:10.310 --> 00:57:14.860
[LAUGHS]

00:57:14.860 --> 00:57:19.380
There's nothing much to
worry about there.

00:57:19.380 --> 00:57:21.900
But now, we want to stop and
smell the roses while doing

00:57:21.900 --> 00:57:24.710
all this computation.

00:57:24.710 --> 00:57:27.640
Let's do this a slightly
different way.

00:57:27.640 --> 00:57:40.100
The joint density of x1 up to
x sub n is lambda x1 times e

00:57:40.100 --> 00:57:44.850
to the minus lambda x1, times
lambda x2, times e to the

00:57:44.850 --> 00:57:48.270
minus lambda x2, and so forth.

00:57:48.270 --> 00:57:51.180
So excuse me.

00:57:51.180 --> 00:57:54.880
The probability density of an
exponential random variable is

00:57:54.880 --> 00:57:57.950
lambda times e to the
minus lambda x.

00:57:57.950 --> 00:58:06.900
So the joint density is lambda
e to the minus lambda x1.

00:58:06.900 --> 00:58:08.270
I told you I was
getting hoarse.

00:58:08.270 --> 00:58:09.580
And my mind is getting hoarse.

00:58:09.580 --> 00:58:14.280
So you better start asking
some questions or I will

00:58:14.280 --> 00:58:16.480
evolve into meaningless
chatter.

00:58:20.490 --> 00:58:26.770
And this is just lambda to the
n times e to the minus lambda

00:58:26.770 --> 00:58:34.840
times the summation of x sub
i from i equals 1 to n.

00:58:34.840 --> 00:58:37.160
Now, that's sort of interesting
because this joint

00:58:37.160 --> 00:58:43.710
density is just this
simple-minded thing.

00:58:43.710 --> 00:58:47.010
You can write it as lambda to
the n times e to the minus

00:58:47.010 --> 00:58:50.730
lambda s sub n, where
s sub n is the

00:58:50.730 --> 00:58:53.610
time of the n-th arrival.

00:58:53.610 --> 00:58:57.910
This says that the joint
distribution of all of these

00:58:57.910 --> 00:59:01.610
interarrival times only
depends on when the

00:59:01.610 --> 00:59:04.310
last one comes in.

00:59:04.310 --> 00:59:09.750
And you can transform that to a
joint density on each of the

00:59:09.750 --> 00:59:14.900
arrival epochs as lambda to
the n times e to the minus

00:59:14.900 --> 00:59:17.290
lambda s sub n.

00:59:17.290 --> 00:59:18.540
Is this obvious to everyone?

00:59:21.620 --> 00:59:22.990
You're lying.

00:59:22.990 --> 00:59:26.700
If you're not shaking your
head, you're lying.

00:59:26.700 --> 00:59:29.350
Because it's not
obvious at all.

00:59:29.350 --> 00:59:35.110
What we're doing here, it's sort
of obvious if you look at

00:59:35.110 --> 00:59:37.040
the picture.

00:59:37.040 --> 00:59:40.420
It's not obvious when you
do the mathematics.

00:59:40.420 --> 00:59:44.280
What the picture says
is-- let me see if I

00:59:44.280 --> 00:59:45.530
find the picture again.

00:59:52.930 --> 00:59:53.720
OK.

00:59:53.720 --> 00:59:55.800
Here's the picture up here.

00:59:55.800 --> 00:59:58.953
We're looking at these
interarrival intervals.

01:00:01.890 --> 01:00:04.450
I think it'll be clearer if
I draw it a different way.

01:00:07.820 --> 01:00:09.070
There we go.

01:00:14.360 --> 01:00:17.410
Let's just draw this
in a line.

01:00:17.410 --> 01:00:19.590
Here's 0.

01:00:19.590 --> 01:00:21.970
Here's s1.

01:00:21.970 --> 01:00:24.360
Here's s2.

01:00:24.360 --> 01:00:26.826
Here's s3.

01:00:26.826 --> 01:00:28.076
And here's s4.

01:00:30.790 --> 01:00:33.970
And here's x1.

01:00:33.970 --> 01:00:35.220
Here's x2.

01:00:41.130 --> 01:00:42.380
Here's x3.

01:00:44.726 --> 01:00:45.976
And here's x4.

01:00:49.930 --> 01:00:53.370
Now, what we're talking about,
we can go from the density of

01:00:53.370 --> 01:01:01.150
each of these intervals to the
density of each of these sums

01:01:01.150 --> 01:01:02.980
in a fairly straightforward
way.

01:01:02.980 --> 01:01:12.530
If you write this all out as a
density, what you find is that

01:01:12.530 --> 01:01:15.780
in making a transformation
from the density of these

01:01:15.780 --> 01:01:21.230
interarrival intervals to the
density of these, what you're

01:01:21.230 --> 01:01:25.170
essentially doing is taking this
density and multiplying

01:01:25.170 --> 01:01:27.900
it by a matrix.

01:01:27.900 --> 01:01:33.770
And the matrix is a diagonal
matrix, is an

01:01:33.770 --> 01:01:35.840
upper triangular matrix.

01:01:35.840 --> 01:01:38.490
Because this depends
only on this.

01:01:38.490 --> 01:01:40.680
This depends only on
this and this.

01:01:40.680 --> 01:01:42.930
This depends only on
this and this.

01:01:42.930 --> 01:01:45.620
This depends only on
each of these.

01:01:45.620 --> 01:01:50.670
So it's a triangular matrix with
terms on the diagonal.

01:01:50.670 --> 01:01:53.300
And when you look at a matrix
like that, the terms on the

01:01:53.300 --> 01:01:57.720
diagonal are 1s because what's
getting added each time is 1

01:01:57.720 --> 01:01:58.900
times a new variable.

01:01:58.900 --> 01:02:03.430
So we have a matrix with 1s on
the main diagonal and other

01:02:03.430 --> 01:02:06.250
stuff above that.

01:02:06.250 --> 01:02:09.000
And what that means is that
when you make this

01:02:09.000 --> 01:02:11.770
transformation in densities,
the determinant of

01:02:11.770 --> 01:02:13.550
that matrix is 1.

01:02:13.550 --> 01:02:17.400
And the value that you then
get when you go from the

01:02:17.400 --> 01:02:24.420
density of these to the density
of these, it's a

01:02:24.420 --> 01:02:26.160
uniform density again.

01:02:26.160 --> 01:02:29.760
So in fact, it has to
look like what we

01:02:29.760 --> 01:02:32.530
said it looks like.

01:02:32.530 --> 01:02:34.300
So I was kidding you there.

01:02:34.300 --> 01:02:39.310
It's not so obvious how to
do that although it looks

01:02:39.310 --> 01:02:40.560
reasonable.

01:02:50.195 --> 01:02:51.180
AUDIENCE: [INAUDIBLE].

01:02:51.180 --> 01:02:51.870
PROFESSOR: Yeah.

01:02:51.870 --> 01:02:53.380
AUDIENCE: I'm sorry.

01:02:53.380 --> 01:02:58.140
Is it also valid to make an
argument based on symmetry?

01:02:58.140 --> 01:03:01.700
PROFESSOR: It will be later.

01:03:01.700 --> 01:03:04.670
The symmetry is not
clear here yet.

01:03:04.670 --> 01:03:06.570
I mean, the symmetry isn't
clear because you're

01:03:06.570 --> 01:03:08.996
starting at time 0.

01:03:08.996 --> 01:03:14.050
And because you're starting
at time 0, you don't have

01:03:14.050 --> 01:03:16.530
symmetry here yet.

01:03:16.530 --> 01:03:20.750
If we started at time 0 and we
ended at some time t, we could

01:03:20.750 --> 01:03:23.120
try to claim there is some
kind of symmetry between

01:03:23.120 --> 01:03:25.010
everything that happened
in the middle.

01:03:25.010 --> 01:03:27.880
And we'll try to
do that later.

01:03:27.880 --> 01:03:33.370
But at the moment, we would get
into even more trouble if

01:03:33.370 --> 01:03:34.620
we try to do it by symmetry.

01:03:41.220 --> 01:03:44.440
But anyway, what this is saying
is that this joint

01:03:44.440 --> 01:03:48.310
density is really--

01:03:48.310 --> 01:03:51.740
if you know where this point is,
the joint density of all

01:03:51.740 --> 01:03:55.570
of these things remains the same
no matter how you move

01:03:55.570 --> 01:03:57.450
these things around.

01:03:57.450 --> 01:04:01.210
If I move s1 around a little
bit, it means that x1 gets a

01:04:01.210 --> 01:04:04.500
little smaller, x2 gets
a little bit bigger.

01:04:04.500 --> 01:04:07.200
And if you look at the joint
density there, the joint

01:04:07.200 --> 01:04:11.120
density stays absolutely the
same because you have e to the

01:04:11.120 --> 01:04:15.240
minus lambda x1 times e to
the minus lambda x2.

01:04:15.240 --> 01:04:17.980
And the sum of the two for a
fixed value here is the same

01:04:17.980 --> 01:04:19.370
as it was before.

01:04:19.370 --> 01:04:22.120
So you can move all of these
things around in any

01:04:22.120 --> 01:04:23.610
way you want to.

01:04:23.610 --> 01:04:28.760
And the joint density depends
only on the last one.

01:04:28.760 --> 01:04:30.600
And that's a very strange
property and it's a very

01:04:30.600 --> 01:04:32.910
interesting property.

01:04:32.910 --> 01:04:36.200
And it sort of is the same as
this independent increment

01:04:36.200 --> 01:04:37.960
property that we've been
talking about.

01:04:37.960 --> 01:04:41.790
But we'll see why that
is in just a minute.

01:04:41.790 --> 01:04:46.280
But anyway, once we have that
property, we can then

01:04:46.280 --> 01:04:55.480
integrate this over the volume
of s1, s2, s3, and s4, over

01:04:55.480 --> 01:05:01.310
that volume which has the
property that it stops at that

01:05:01.310 --> 01:05:03.480
one particular point there.

01:05:03.480 --> 01:05:07.490
And we do that integration
subject to the fact that s3

01:05:07.490 --> 01:05:11.490
has to be less than or equal to
s4, s2 has to be less than

01:05:11.490 --> 01:05:14.790
or equal to s3, and
so forth down.

01:05:14.790 --> 01:05:18.360
When you do that integration,
you get exactly the same thing

01:05:18.360 --> 01:05:20.540
as you got before when you
did the integration.

01:05:20.540 --> 01:05:23.420
The integration that you did
before was essentially doing

01:05:23.420 --> 01:05:26.180
this, if you look at what
you did before.

01:05:29.700 --> 01:05:35.700
You were taking lambda times e
to the minus lambda x times

01:05:35.700 --> 01:05:38.650
lambda times t minus x.

01:05:38.650 --> 01:05:41.390
And the x doesn't make
any difference here.

01:05:41.390 --> 01:05:43.340
The x cancels out.

01:05:43.340 --> 01:05:45.410
That's exactly what's
going on.

01:05:45.410 --> 01:05:50.185
And if you do it in terms of s1
and s2, the s1 cancels out.

01:05:50.185 --> 01:05:52.330
The s1 is the same as x here.

01:05:52.330 --> 01:05:54.810
So there is that cancellation
here.

01:05:54.810 --> 01:05:59.090
And therefore, this Erlang
density is just a marginal

01:05:59.090 --> 01:06:02.100
distribution of a very
interesting joint

01:06:02.100 --> 01:06:04.670
distribution, which depends
only on the last term.

01:06:09.040 --> 01:06:14.760
So next, we have a theorem
which says for a Poisson

01:06:14.760 --> 01:06:21.250
process, the PMF for n of t, the
Probability Mass Function,

01:06:21.250 --> 01:06:23.360
is the Poisson PMF.

01:06:23.360 --> 01:06:26.710
It sounds like I'm not really
saying anything because what

01:06:26.710 --> 01:06:28.640
else would it be?

01:06:28.640 --> 01:06:35.320
Because you've always heard that
the Poisson PMF is this

01:06:35.320 --> 01:06:37.750
particular function here.

01:06:37.750 --> 01:06:40.960
Well, in fact, there's
some reason for that.

01:06:40.960 --> 01:06:44.180
And in fact, if we want to say
that a Poisson process is

01:06:44.180 --> 01:06:49.000
defined in terms of these
exponential interarrival

01:06:49.000 --> 01:06:50.960
times, then we have to
show that this is

01:06:50.960 --> 01:06:54.320
consistent with that.

01:06:54.320 --> 01:06:58.370
The way I'll prove that
here, this is a

01:06:58.370 --> 01:07:01.330
little more than a PF.

01:07:01.330 --> 01:07:05.445
Maybe we should say it's a
P-R-O-F. Leave out the double

01:07:05.445 --> 01:07:08.980
O because it's not
quite complete.

01:07:08.980 --> 01:07:13.350
But what we want to do is to
calculate the probability that

01:07:13.350 --> 01:07:18.770
the n plus first arrival occurs
sometime between t and

01:07:18.770 --> 01:07:21.770
t plus delta.

01:07:21.770 --> 01:07:24.160
And we'll do it in two
different ways.

01:07:24.160 --> 01:07:28.220
And one way involves the
probability mass function for

01:07:28.220 --> 01:07:29.440
the Poisson.

01:07:29.440 --> 01:07:33.730
The other way involves
the Erlang density.

01:07:33.730 --> 01:07:36.630
And since we already know the
Erlang density, we can use

01:07:36.630 --> 01:07:39.850
that to get the PMF
for n of t.

01:07:42.480 --> 01:07:47.930
So using the Erlang density,
the probability that the n

01:07:47.930 --> 01:07:51.730
plus first arrival falls in
this little tiny interval

01:07:51.730 --> 01:07:53.950
here, we're thinking of
delta as being small.

01:07:53.950 --> 01:07:56.740
And we're going to let
delta approach 0.

01:07:56.740 --> 01:08:02.520
It's going to be the density
of the n plus first arrival

01:08:02.520 --> 01:08:05.690
times delta plus o of delta.

01:08:05.690 --> 01:08:10.490
o of delta is something that
goes to 0 as delta increases

01:08:10.490 --> 01:08:12.110
faster than delta does.

01:08:12.110 --> 01:08:17.170
It's something which has the
property that o of delta

01:08:17.170 --> 01:08:20.670
divided by delta goes to
0 as delta gets large.

01:08:20.670 --> 01:08:24.310
So this is just saying that this
is approximately equal to

01:08:24.310 --> 01:08:30.540
the density of the n plus
first arrival times this

01:08:30.540 --> 01:08:31.100
[INAUDIBLE]

01:08:31.100 --> 01:08:31.479
here.

01:08:31.479 --> 01:08:34.609
The density stays essentially
constant over

01:08:34.609 --> 01:08:36.029
a very small delta.

01:08:36.029 --> 01:08:38.609
It's a continuous density.

01:08:38.609 --> 01:08:42.479
Next, we use the independent
increment property, which says

01:08:42.479 --> 01:08:46.729
that the probability that t is
less than sn plus 1, is less

01:08:46.729 --> 01:08:52.930
than or equal to t plus delta,
is the PMF that n of t is

01:08:52.930 --> 01:08:59.649
equal to n at the beginning is
the interval, and then that in

01:08:59.649 --> 01:09:05.279
the middle of the interval,
there's exactly one arrival.

01:09:05.279 --> 01:09:09.970
And the probabilities of exactly
one arrival, is just

01:09:09.970 --> 01:09:12.830
lambda delta plus o of delta.

01:09:15.500 --> 01:09:16.580
Namely, that's because of the

01:09:16.580 --> 01:09:19.830
independent increment property.

01:09:19.830 --> 01:09:22.990
What's this o of delta
doing out here?

01:09:22.990 --> 01:09:26.609
Why isn't this exactly
equal to this?

01:09:26.609 --> 01:09:28.179
And why do I need
something else?

01:09:31.460 --> 01:09:35.279
What am I leaving out
of this equation?

01:09:35.279 --> 01:09:37.710
The probability that
our arrival comes--

01:09:45.720 --> 01:09:47.789
here's t.

01:09:47.789 --> 01:09:51.755
Here's t plus delta.

01:09:51.755 --> 01:09:54.880
I'm talking about something
happening here.

01:09:54.880 --> 01:09:56.990
At this point, n of t is here.

01:09:59.710 --> 01:10:04.290
And I'm finding the probability
that n of t plus

01:10:04.290 --> 01:10:10.850
delta is equal to n of
t plus 1 essentially.

01:10:10.850 --> 01:10:12.500
I'm looking for the probability
of there being one

01:10:12.500 --> 01:10:15.070
arrival in this interval here.

01:10:18.980 --> 01:10:20.515
So what's the matter
with that equation?

01:10:28.870 --> 01:10:33.520
This is the probability that
the n plus first arrival

01:10:33.520 --> 01:10:37.220
occurs somewhere in this
interval here.

01:10:37.220 --> 01:10:37.940
Yeah.

01:10:37.940 --> 01:10:41.144
AUDIENCE: Is that last term
then the probability that

01:10:41.144 --> 01:10:44.360
there's not anymore other
parameter standards as well?

01:10:44.360 --> 01:10:46.380
PROFESSOR: It doesn't
include-- yes.

01:10:46.380 --> 01:10:52.560
This last term which I had to
add is in fact the negligible

01:10:52.560 --> 01:11:01.440
term that at time n of t, there
is less than n arrivals.

01:11:01.440 --> 01:11:04.530
And then I get 2 arrivals in
this little interval delta.

01:11:07.150 --> 01:11:09.300
So that's why I need
that extra term.

01:11:09.300 --> 01:11:13.390
But anyway, when I relate these
two terms, I get the

01:11:13.390 --> 01:11:18.050
probability mass function of n
of t is equal to the Erlang

01:11:18.050 --> 01:11:22.820
density at t, where
the n plus first

01:11:22.820 --> 01:11:25.140
arrival divided by lambda.

01:11:25.140 --> 01:11:28.130
And that's what that
term is there.

01:11:34.440 --> 01:11:37.940
So that gives us the
Poisson PMF.

01:11:37.940 --> 01:11:42.500
Interesting observation about
this, it's a function

01:11:42.500 --> 01:11:43.840
only of lambda t.

01:11:43.840 --> 01:11:47.340
It's not a function of lambda
or t separately.

01:11:47.340 --> 01:11:50.210
It's a function only of the
two of them together.

01:11:50.210 --> 01:11:52.860
It has to be that.

01:11:52.860 --> 01:11:56.180
Because you can use scaling
arguments on this.

01:11:56.180 --> 01:12:00.530
If you have a Poisson process
of rate lambda and I measure

01:12:00.530 --> 01:12:03.520
things in millimeters instead
of centimeters,

01:12:03.520 --> 01:12:05.400
what's going to happen?

01:12:05.400 --> 01:12:09.250
My rate is going to change
by a factor of 10.

01:12:09.250 --> 01:12:13.060
My values of t are going to
change by a factor of 10.

01:12:15.615 --> 01:12:17.630
This is a probability
mass function.

01:12:17.630 --> 01:12:20.240
That has to stay the same.

01:12:20.240 --> 01:12:24.860
So this has to be a function
only of the product lambda t

01:12:24.860 --> 01:12:26.910
because of scaling
argument here.

01:12:30.020 --> 01:12:35.290
Now, the other thing here, and
this is interesting because if

01:12:35.290 --> 01:12:40.830
you look at n of t, the number
of arrivals up until time t is

01:12:40.830 --> 01:12:45.500
the sum of the number of
arrivals up until some shorter

01:12:45.500 --> 01:12:52.800
time t1 plus the number of
arrivals between t1 and t.

01:12:52.800 --> 01:12:54.650
We know that the number
of arrivals up

01:12:54.650 --> 01:12:57.100
until time t1 is Poisson.

01:12:57.100 --> 01:13:01.280
The number of arrivals between
t1 and t is Poisson.

01:13:01.280 --> 01:13:04.830
Those two values are independent
of each other.

01:13:04.830 --> 01:13:07.580
I can choose t1 in the middle
to be anything I

01:13:07.580 --> 01:13:09.190
want to make it.

01:13:09.190 --> 01:13:13.080
And this says that the sum of
two Poisson random variables

01:13:13.080 --> 01:13:16.180
has to be Poisson.

01:13:16.180 --> 01:13:17.330
Now, I'm very lazy.

01:13:17.330 --> 01:13:23.800
And I've gone through life
without ever convolving this

01:13:23.800 --> 01:13:28.530
PMF to find out that in fact
the sum of 2 Poisson random

01:13:28.530 --> 01:13:32.920
variables is in fact
Poisson itself.

01:13:32.920 --> 01:13:35.700
Because I actually believe the
argument I just went through.

01:13:40.510 --> 01:13:44.600
If you're skeptical, you will
probably want to actually do

01:13:44.600 --> 01:13:48.420
the digital convolution to
show that the sum of two

01:13:48.420 --> 01:13:54.540
independent Poisson random
variables is in fact Poisson.

01:13:54.540 --> 01:13:56.860
And it extends to any [? tay ?]
disjoint interval.

01:13:56.860 --> 01:14:01.050
So the same argument says that
any sum of Poisson random

01:14:01.050 --> 01:14:04.210
variables is Poisson.

01:14:04.210 --> 01:14:07.650
I do want to get through any
alternate definitions of a

01:14:07.650 --> 01:14:12.240
Poisson process because
that makes a natural

01:14:12.240 --> 01:14:14.630
stopping point here.

01:14:14.630 --> 01:14:18.810
Question-- is it true that any
arrival process for which n of

01:14:18.810 --> 01:14:22.680
t has a Poisson probability
mass function for a given

01:14:22.680 --> 01:14:26.070
lambda and for all
t is a Poisson

01:14:26.070 --> 01:14:27.330
process of rate lambda?

01:14:30.150 --> 01:14:33.695
In other words, that's a
pretty strong property.

01:14:33.695 --> 01:14:36.620
It says I found the probability
mass functions for

01:14:36.620 --> 01:14:38.915
n of t at every value of t.

01:14:38.915 --> 01:14:42.690
Does that describe a process?

01:14:42.690 --> 01:14:44.525
Well, you see the
answer there.

01:14:44.525 --> 01:14:50.950
As usual, marginal PMFs,
distribution functions don't

01:14:50.950 --> 01:14:55.480
specify a process because they
don't specify the joint

01:14:55.480 --> 01:14:57.180
probabilities.

01:14:57.180 --> 01:15:00.080
But here, we've just pointed
out that these joint

01:15:00.080 --> 01:15:01.690
probabilities are
all independent.

01:15:01.690 --> 01:15:05.590
You can take a set of
probability mass functions for

01:15:05.590 --> 01:15:08.840
this interval, this interval,
this interval, this interval,

01:15:08.840 --> 01:15:10.020
and so forth.

01:15:10.020 --> 01:15:18.090
And for any set of t1, t2, and
so forth up, we know that the

01:15:18.090 --> 01:15:22.820
number of arrivals in zero to
t1, the number of arrivals in

01:15:22.820 --> 01:15:26.850
t1 to t2, and so forth all the
way up are all independent

01:15:26.850 --> 01:15:28.110
random variables.

01:15:28.110 --> 01:15:32.830
And therefore, when we know the
Poisson probability mass

01:15:32.830 --> 01:15:40.770
function, we really also know,
and we've also shown, that

01:15:40.770 --> 01:15:43.990
these random variables are
independent of each other.

01:15:43.990 --> 01:15:49.010
We have the joint PMF for any
sum of these random variables.

01:15:49.010 --> 01:15:59.690
So in fact, in this particular
case, it's enough to know what

01:15:59.690 --> 01:16:03.790
the probability mass function
is at each time t plus the

01:16:03.790 --> 01:16:05.820
fact that we have this

01:16:05.820 --> 01:16:08.020
independent increment property.

01:16:08.020 --> 01:16:10.610
And we need the stationary
increment property, too, to

01:16:10.610 --> 01:16:14.030
know that these values are
the same at each t.

01:16:14.030 --> 01:16:17.300
So the theorem is that if an
arrival process has the

01:16:17.300 --> 01:16:21.420
stationary and independent
increment properties, and if n

01:16:21.420 --> 01:16:26.840
of t has the Poisson PMF for
given lambda and all t greater

01:16:26.840 --> 01:16:32.090
than 0, then the process itself
has to be Poisson.

01:16:32.090 --> 01:16:36.890
VHW stands for Violently
Hand Waving.

01:16:39.690 --> 01:16:43.150
So that's even a little
worse than a PF.

01:16:43.150 --> 01:16:45.220
Says the stationary and
independent increment

01:16:45.220 --> 01:16:48.580
properties show that the joint
distribution of arrivals over

01:16:48.580 --> 01:16:51.580
any given set of disjoint
intervals is that

01:16:51.580 --> 01:16:53.410
of a Poisson process.

01:16:53.410 --> 01:16:55.390
And clearly that's enough.

01:16:55.390 --> 01:16:56.660
And it almost is.

01:16:56.660 --> 01:16:59.470
And you should read the proof in
the notes which does just a

01:16:59.470 --> 01:17:03.240
little more than that to make
this an actual proof.

01:17:03.240 --> 01:17:03.550
OK.

01:17:03.550 --> 01:17:05.330
I think I'll stop there.

01:17:05.330 --> 01:17:08.020
And we will talk a little
bit about the Bernoulli

01:17:08.020 --> 01:17:10.060
process next time.

01:17:10.060 --> 01:17:11.310
Thank you.