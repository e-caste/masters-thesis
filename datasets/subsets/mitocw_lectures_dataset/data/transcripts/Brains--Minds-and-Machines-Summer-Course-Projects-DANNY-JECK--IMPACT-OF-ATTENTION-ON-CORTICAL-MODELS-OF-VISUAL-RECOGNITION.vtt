WEBVTT

00:00:04.810 --> 00:00:10.110
[MUSIC PLAYS]

00:00:10.110 --> 00:00:11.490
DANNY JECK: Hi, I'm Danny Jeck.

00:00:11.490 --> 00:00:14.040
I'm a fifth year grad
student getting my PhD

00:00:14.040 --> 00:00:16.500
at Johns Hopkins in
biomedical engineering.

00:00:16.500 --> 00:00:19.110
My project, what I'm
trying to look into

00:00:19.110 --> 00:00:28.360
is how attention is related
to models of visual cortex.

00:00:28.360 --> 00:00:31.440
The first lecture
was by Jim DiCarlo.

00:00:31.440 --> 00:00:35.400
He gave a talk about how the
models of object recognition

00:00:35.400 --> 00:00:37.920
seem to match pretty
well with the behavior

00:00:37.920 --> 00:00:42.090
of inferotemporal
cortex in macaques.

00:00:42.090 --> 00:00:44.880
We know that also in
macaques earlier areas

00:00:44.880 --> 00:00:48.330
of visual processing are
modulated by attention.

00:00:48.330 --> 00:00:49.680
So the question is, well, OK.

00:00:49.680 --> 00:00:52.282
We have this model, let's
say we add some modulation

00:00:52.282 --> 00:00:53.740
due to attention,
what does that do

00:00:53.740 --> 00:00:57.900
downstream as that information
propagates through the network?

00:00:57.900 --> 00:01:01.440
I'm building a model
in Python right now.

00:01:01.440 --> 00:01:02.550
And it's running.

00:01:02.550 --> 00:01:04.920
The main goal of
the model is to see

00:01:04.920 --> 00:01:07.620
how some modulation
in earlier cortex

00:01:07.620 --> 00:01:11.130
would propagate through a
model like what we believe is

00:01:11.130 --> 00:01:12.530
happening in the brain already.

00:01:12.530 --> 00:01:17.400
A boring finding would be
that a 10% modulation results

00:01:17.400 --> 00:01:20.760
in a 10% modulation downstream.

00:01:20.760 --> 00:01:23.070
I'm expecting that that's
not the case, because there's

00:01:23.070 --> 00:01:25.830
a whole bunch of nonlinearities
and normalization

00:01:25.830 --> 00:01:28.920
that happens that should
propagate through this network.

00:01:28.920 --> 00:01:31.200
The question is what is
the magnitude of that,

00:01:31.200 --> 00:01:36.870
how does that affect things
if the 10% modulation is not

00:01:36.870 --> 00:01:41.100
actually the right number
because of some measurements

00:01:41.100 --> 00:01:43.320
or the way I'm interpreting
the measurements that

00:01:43.320 --> 00:01:46.140
have been made already, what
would different numbers allow

00:01:46.140 --> 00:01:47.190
for.

00:01:47.190 --> 00:01:50.850
Or perhaps the modulations
we found downstream are all

00:01:50.850 --> 00:01:53.160
due to other feedback
from other areas

00:01:53.160 --> 00:01:56.400
rather than this going
back to the beginning

00:01:56.400 --> 00:01:58.410
and propagating all
the way through.

00:01:58.410 --> 00:02:01.000
So the idea came about
from Ethan Meyers.

00:02:01.000 --> 00:02:03.990
He was originally
interested in trying

00:02:03.990 --> 00:02:08.021
to do this kind of two
passes through a network, one

00:02:08.021 --> 00:02:09.479
in which you sort
of try and figure

00:02:09.479 --> 00:02:11.770
out the location of an object,
and another in which you

00:02:11.770 --> 00:02:12.960
try to recognize it.

00:02:12.960 --> 00:02:15.210
I kind of took that in
a different direction

00:02:15.210 --> 00:02:18.660
because I was more interested
in the neurophysiology side

00:02:18.660 --> 00:02:19.200
of things.

00:02:19.200 --> 00:02:21.840
In my current lab, I wouldn't
have had time to do something

00:02:21.840 --> 00:02:24.030
like this because
I wasn't planning

00:02:24.030 --> 00:02:25.980
on investing a lot
of time understanding

00:02:25.980 --> 00:02:27.030
what deep networks were.

00:02:27.030 --> 00:02:31.380
So really, having the time to
sort of work on a free project

00:02:31.380 --> 00:02:32.260
has been really nice.

00:02:32.260 --> 00:02:34.550
[MUSIC PLAYS]