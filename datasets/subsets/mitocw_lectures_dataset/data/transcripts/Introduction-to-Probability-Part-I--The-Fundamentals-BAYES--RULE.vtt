WEBVTT

00:00:02.310 --> 00:00:06.930
We now come to the third and
final kind of calculation out

00:00:06.930 --> 00:00:08.780
of the calculations that
we carried out

00:00:08.780 --> 00:00:10.640
in our earlier example.

00:00:10.640 --> 00:00:13.110
The setting is exactly the same
as in our discussion of

00:00:13.110 --> 00:00:15.000
the total probability theorem.

00:00:15.000 --> 00:00:19.040
We have a sample space which is
partitioned into a number

00:00:19.040 --> 00:00:22.090
of disjoint subsets
or events which

00:00:22.090 --> 00:00:24.330
we think of as scenarios.

00:00:24.330 --> 00:00:27.440
We're given the probability
of each scenario.

00:00:27.440 --> 00:00:30.790
And we think of these
probabilities as being some

00:00:30.790 --> 00:00:32.800
kind of initial beliefs.

00:00:32.800 --> 00:00:41.140
They capture how likely we
believe each scenario to be.

00:00:41.140 --> 00:00:47.050
Now, under each scenario, we
also have the probability that

00:00:47.050 --> 00:00:51.520
an event of interest,
event B, will occur.

00:00:51.520 --> 00:00:54.920
Then the probabilistic
experiment is carried out.

00:00:54.920 --> 00:01:00.100
And perhaps we observe that
event B did indeed occur.

00:01:00.100 --> 00:01:05.239
Once that happens, maybe this
should cause us to revise our

00:01:05.239 --> 00:01:09.050
beliefs about the likelihood
of the different scenarios.

00:01:09.050 --> 00:01:13.039
Having observed that B occurred,
perhaps certain

00:01:13.039 --> 00:01:16.820
scenarios are more likely
than others.

00:01:16.820 --> 00:01:18.510
How do we revise our beliefs?

00:01:18.510 --> 00:01:21.220
By calculating conditional
probabilities.

00:01:21.220 --> 00:01:24.340
And how do we calculate
conditional probabilities?

00:01:24.340 --> 00:01:28.130
We start from the definition of
conditional probabilities.

00:01:28.130 --> 00:01:31.520
The probability of one event
given another is the

00:01:31.520 --> 00:01:35.700
probability that both events
occur divided by the

00:01:35.700 --> 00:01:39.180
probability of the conditioning
event.

00:01:39.180 --> 00:01:41.450
How do we continue?

00:01:41.450 --> 00:01:45.200
We simply realize that the
numerator is what we can

00:01:45.200 --> 00:01:48.229
calculate using the
multiplication rule.

00:01:48.229 --> 00:01:52.590
And the denominator is exactly
what we calculate using the

00:01:52.590 --> 00:01:54.720
total probability theorem.

00:01:54.720 --> 00:01:58.860
So we have everything we need
to calculate those revised

00:01:58.860 --> 00:02:01.740
beliefs, or conditional
probabilities.

00:02:01.740 --> 00:02:04.340
And this all there is
in the Bayes rule.

00:02:04.340 --> 00:02:06.730
It is actually a very
simple calculation.

00:02:09.720 --> 00:02:11.710
It's a very simple
calculation.

00:02:11.710 --> 00:02:14.900
However, it is a quite
important one.

00:02:14.900 --> 00:02:17.650
Its history goes way back.

00:02:17.650 --> 00:02:20.960
In the middle of the 18th
century, a Presbyterian

00:02:20.960 --> 00:02:24.310
minister, Thomas Bayes,
worked it out.

00:02:24.310 --> 00:02:27.350
It was published a few years
after his death.

00:02:27.350 --> 00:02:30.650
And it was quickly reorganized
for its significance.

00:02:30.650 --> 00:02:34.370
It's a systematic way for
incorporating new evidence.

00:02:34.370 --> 00:02:38.220
It's a systematic way for
learning from experience.

00:02:38.220 --> 00:02:42.090
And it forms the foundation of a
major branch of mathematics,

00:02:42.090 --> 00:02:46.090
so-called Bayesian inference,
which we will study in some

00:02:46.090 --> 00:02:49.880
detail later in this course.

00:02:49.880 --> 00:02:53.860
The general idea is that we
start with a probabilistic

00:02:53.860 --> 00:02:57.160
model, which involves a number
of possible scenarios.

00:02:57.160 --> 00:03:00.820
And we have some initial beliefs
on the likelihood of

00:03:00.820 --> 00:03:04.430
each possible scenario.

00:03:04.430 --> 00:03:08.590
There's also some particular
event that may occur under

00:03:08.590 --> 00:03:09.600
each scenario.

00:03:09.600 --> 00:03:13.450
And we know how likely it is to
occur under each scenario.

00:03:13.450 --> 00:03:15.800
This is our model of
the situation.

00:03:15.800 --> 00:03:19.120
Under each particular situation,
the model tells us

00:03:19.120 --> 00:03:22.272
how likely event
B is to occur.

00:03:22.272 --> 00:03:27.350
If we actually observe that B
occurred, then we use that

00:03:27.350 --> 00:03:31.180
information to draw conclusions
about the possible

00:03:31.180 --> 00:03:36.820
causes of B, or conclusions
about the more likely or less

00:03:36.820 --> 00:03:41.579
likely scenarios that may have
caused this events to occur.

00:03:41.579 --> 00:03:44.050
That's what inference is.

00:03:44.050 --> 00:03:49.450
Having observed b, we make
inferences as to how likely a

00:03:49.450 --> 00:03:52.670
particular scenario,
Ai, is going to be.

00:03:52.670 --> 00:03:55.490
And that likelihood is captured
by this conditional

00:03:55.490 --> 00:04:00.260
probabilities of Ai, given the
event B. So that's what the

00:04:00.260 --> 00:04:02.000
Bayes rule is doing.

00:04:02.000 --> 00:04:04.430
Starting from conditional
probabilities going in one

00:04:04.430 --> 00:04:07.020
direction, it allows us to
calculate conditional

00:04:07.020 --> 00:04:11.320
probabilities going in the
opposite direction.

00:04:11.320 --> 00:04:15.340
It allows us to revise the
probabilities of the different

00:04:15.340 --> 00:04:18.540
scenarios, taking into account
the new information.

00:04:18.540 --> 00:04:22.520
And that's exactly what
inference is all about, as

00:04:22.520 --> 00:04:25.430
we're going to see later
in this class.