WEBVTT

00:00:16.533 --> 00:00:17.450
YUFEI ZHAO: All right.

00:00:17.450 --> 00:00:20.870
Last time we started talking
about pseudorandom graphs,

00:00:20.870 --> 00:00:25.740
and we considered this theorem
of Chung, Graham, and Wilson,

00:00:25.740 --> 00:00:30.470
which, for dense graphs, gave
several equivalent notions

00:00:30.470 --> 00:00:34.800
of quasi-randomness that,
at least the phase values,

00:00:34.800 --> 00:00:37.420
do not appear to be
all that equivalent.

00:00:37.420 --> 00:00:41.870
But they are actually-- you
can deduce one from the other.

00:00:41.870 --> 00:00:44.510
There was one condition
at the very end which

00:00:44.510 --> 00:00:46.850
had to do with eigenvalues.

00:00:46.850 --> 00:00:51.125
And, basically, it said that if
your second largest eigenvalue

00:00:51.125 --> 00:00:56.690
in absolute value is small,
then the graph is pseudorandom.

00:00:56.690 --> 00:00:59.660
So that's something that I
want to explore further today

00:00:59.660 --> 00:01:02.240
to better understand
the relationship

00:01:02.240 --> 00:01:06.410
between eigenvalues of a
graph and the pseudorandomness

00:01:06.410 --> 00:01:07.455
properties.

00:01:10.310 --> 00:01:12.860
For much of-- pretty
much all of today,

00:01:12.860 --> 00:01:15.470
we're going to look at a
special class of graphs

00:01:15.470 --> 00:01:17.420
known as n, d, lambda graphs.

00:01:22.080 --> 00:01:25.290
This just means we
have n vertices,

00:01:25.290 --> 00:01:27.610
and we're only going
to consider, mostly

00:01:27.610 --> 00:01:30.300
out of convenience,
d regular graphs.

00:01:33.680 --> 00:01:36.110
So this will make our
life somewhat simpler.

00:01:36.110 --> 00:01:38.210
And the lambda stands for that--

00:01:38.210 --> 00:01:41.940
if you look at the
adjacency matrix,

00:01:41.940 --> 00:01:45.860
and if you write
down the eigenvalues

00:01:45.860 --> 00:01:56.380
of the adjacency
matrix, then, well,

00:01:56.380 --> 00:01:57.630
what are these eigenvalues?

00:01:57.630 --> 00:02:03.030
The top one, because it's
d regular, is equal to d.

00:02:03.030 --> 00:02:07.710
And lambda corresponds
to the statement

00:02:07.710 --> 00:02:13.170
that all the other
eigenvalues are, at most,

00:02:13.170 --> 00:02:16.760
lambda in absolute value.

00:02:16.760 --> 00:02:18.530
So the top one is equal to d.

00:02:18.530 --> 00:02:20.750
All the other ones
in absolute value--

00:02:20.750 --> 00:02:25.520
so it could be basically
the maximum of these two--

00:02:25.520 --> 00:02:28.890
is bounded above by lambda.

00:02:28.890 --> 00:02:32.970
And at the end of last time,
we showed this expander

00:02:32.970 --> 00:02:48.205
mixing lemma, which, in this
language, says that if G is n,

00:02:48.205 --> 00:02:58.540
d, lambda, then one has the
following discrepancy type.

00:02:58.540 --> 00:03:01.770
So the randomness
property, namely

00:03:01.770 --> 00:03:04.290
that if you look
at two vertex sets

00:03:04.290 --> 00:03:07.170
and look at how many actual
edges are between them compared

00:03:07.170 --> 00:03:09.330
to what you expect if
this were a random graph

00:03:09.330 --> 00:03:14.050
of a similar density, then these
two numbers are very similar,

00:03:14.050 --> 00:03:19.290
and the amount of error is
controlled by your lambda.

00:03:19.290 --> 00:03:23.940
In particular, a smaller lambda
gives you a more pseudorandom

00:03:23.940 --> 00:03:25.800
graph.

00:03:25.800 --> 00:03:27.360
So the second part
of today's class,

00:03:27.360 --> 00:03:29.040
I want to explore
the question of how

00:03:29.040 --> 00:03:31.590
small this lambda can be.

00:03:31.590 --> 00:03:36.170
So what's the optimal
amount of pseudorandomness?

00:03:36.170 --> 00:03:39.150
But, first, I want to
show you some examples.

00:03:39.150 --> 00:03:42.730
So, so far, we've been talking
about pseudorandom graphs,

00:03:42.730 --> 00:03:45.580
and the only
example, really, I've

00:03:45.580 --> 00:03:49.340
talked about is that a
random graph is pseudorandom.

00:03:49.340 --> 00:03:50.100
Which is true.

00:03:50.100 --> 00:03:53.460
A random graph is pseudorandom
with high probability,

00:03:53.460 --> 00:03:55.360
but some of the spirit
of pseudorandomness

00:03:55.360 --> 00:03:58.390
is to come up with
non-random examples, come up

00:03:58.390 --> 00:04:01.390
with deterministic
constructions that give you

00:04:01.390 --> 00:04:03.920
pseudorandom properties.

00:04:03.920 --> 00:04:06.310
So I want to begin
today with an example.

00:04:14.720 --> 00:04:17.690
A lot of examples, especially
for pseudorandomness,

00:04:17.690 --> 00:04:20.779
come from this class of
graphs called Cayley graphs,

00:04:20.779 --> 00:04:24.710
which are built from a group.

00:04:24.710 --> 00:04:27.440
So we're going to reserve the
letter G for graphs, so I'm

00:04:27.440 --> 00:04:30.300
going to use gamma for a group.

00:04:30.300 --> 00:04:37.970
And I have a subset S of
gamma, and S is symmetric,

00:04:37.970 --> 00:04:40.980
in that if you invert
the elements of S,

00:04:40.980 --> 00:04:42.770
they remain in S.

00:04:42.770 --> 00:04:53.900
Then we define the Cayley
graph given by this group

00:04:53.900 --> 00:05:01.400
and the set S to be
the following graph,

00:05:01.400 --> 00:05:07.190
where V, the set of vertices, is
just the set of group elements.

00:05:07.190 --> 00:05:16.240
And the edges are obtained
by taking a group element

00:05:16.240 --> 00:05:20.340
and multiplying it by S
to go to its neighbor.

00:05:26.480 --> 00:05:28.030
So this is a Cayley graph.

00:05:28.030 --> 00:05:30.670
And Cayley graphs are--

00:05:30.670 --> 00:05:33.162
start with any group, start
with any subset of the group,

00:05:33.162 --> 00:05:34.120
you get a Cayley graph.

00:05:34.120 --> 00:05:37.260
And this is a very important
construction of graphs.

00:05:37.260 --> 00:05:39.630
They have lots of
nice properties

00:05:39.630 --> 00:05:44.230
And, in particular, an
example of a Cayley graph

00:05:44.230 --> 00:05:45.700
is a Paley graph.

00:05:49.830 --> 00:05:51.750
They're not related.

00:05:51.750 --> 00:05:54.650
So a Paley graph
is a special case

00:05:54.650 --> 00:06:00.260
of a Cayley graph
obtained by considering

00:06:00.260 --> 00:06:08.570
the group, the cyclic group mod
p, where p is prime 1 mod 4.

00:06:12.110 --> 00:06:17.190
And I'm looking at S being the
set of quadratic residues, mod

00:06:17.190 --> 00:06:17.690
p.

00:06:24.810 --> 00:06:28.580
It's actually nonzero
quadratic residues.

00:06:28.580 --> 00:06:33.970
So elements of mod p
that could be a square.

00:06:33.970 --> 00:06:38.130
So we will show in a second
that this Paley graph has

00:06:38.130 --> 00:06:40.830
nice pseudorandom
properties by showing

00:06:40.830 --> 00:06:45.030
that it is an n, d, lambda
graph with lambda fairly small

00:06:45.030 --> 00:06:47.280
compared to the degree.

00:06:47.280 --> 00:06:49.800
Just a historical note--

00:06:49.800 --> 00:06:55.240
so Raymond Paley-- so the
Paley graph named after him--

00:06:55.240 --> 00:06:59.380
he actually-- he was from the
earlier part of 20th century.

00:06:59.380 --> 00:07:02.961
So from 1907 to 1932.

00:07:02.961 --> 00:07:05.210
So he died very young
at the age of 26,

00:07:05.210 --> 00:07:07.220
and he actually
died in an avalanche

00:07:07.220 --> 00:07:09.510
when he was skiing
by himself in Banff.

00:07:09.510 --> 00:07:13.150
So Banff is a national
park in Alberta in Canada.

00:07:13.150 --> 00:07:17.400
And when I was in Banff
earlier this year for a math

00:07:17.400 --> 00:07:20.580
conference-- so there's also a
math conference center there--

00:07:20.580 --> 00:07:22.740
so I had a chance
to go visit the--

00:07:25.770 --> 00:07:27.000
Raymond Paley's tomb.

00:07:27.000 --> 00:07:33.596
So there's a graveyard there
where you can find his tomb.

00:07:33.596 --> 00:07:37.920
And it's very sad that, in his
short mathematical timespan,

00:07:37.920 --> 00:07:41.730
actually he managed to do a
lot of amazing mathematical--

00:07:41.730 --> 00:07:44.100
find a lot of amazing
mathematical discoveries.

00:07:44.100 --> 00:07:46.890
And there are many important
concepts named after him.

00:07:46.890 --> 00:07:50.760
So things like Paley-Wiener
theorem, Paley-Zygmund,

00:07:50.760 --> 00:07:54.270
Littlewood-Paley, all this
important ideas and analysis

00:07:54.270 --> 00:07:55.380
named after Paley.

00:07:55.380 --> 00:08:00.660
And Paley graph is also
one of his contributions.

00:08:00.660 --> 00:08:06.390
So what we'll claim is that
this Paley graph has the desired

00:08:06.390 --> 00:08:10.890
pseudorandom properties, in that
if you look at its eigenvalues,

00:08:10.890 --> 00:08:21.120
then the top
eigenvalues, except--

00:08:21.120 --> 00:08:22.810
so except for the
top eigenvalue,

00:08:22.810 --> 00:08:28.350
all the other eigenvalue
are quite small.

00:08:28.350 --> 00:08:30.870
So keep in mind
that the size of S

00:08:30.870 --> 00:08:33.020
is basically half of the group.

00:08:33.020 --> 00:08:34.770
So p minus 1 over 2.

00:08:34.770 --> 00:08:38.970
So for especially
larger values of p,

00:08:38.970 --> 00:08:41.760
p's eigenvalues are quite
small compared to the degree.

00:08:46.480 --> 00:08:48.780
So the main way to
show that Cayley graphs

00:08:48.780 --> 00:08:54.090
like that have small
eigenvalues is to just compute

00:08:54.090 --> 00:08:55.833
what the eigenvalues are.

00:08:55.833 --> 00:08:58.500
And this is actually not so hard
to do for Cayley graphs, so let

00:08:58.500 --> 00:09:02.200
me do this explicitly.

00:09:02.200 --> 00:09:04.850
So I will tell you
very explicitly a set

00:09:04.850 --> 00:09:08.220
of eigenvectors.

00:09:08.220 --> 00:09:10.690
And they are-- the
first eigenvector

00:09:10.690 --> 00:09:13.540
is just the all 1's vector.

00:09:13.540 --> 00:09:15.850
The second eigenvector
is the vector

00:09:15.850 --> 00:09:19.500
coming from 1,
omega, omega squared,

00:09:19.500 --> 00:09:22.300
so omega to the p
minus 1, where omega

00:09:22.300 --> 00:09:24.905
is a parameter of
p-th root of unity.

00:09:29.760 --> 00:09:35.305
The next one is 1, omega
square, omega fourth,

00:09:35.305 --> 00:09:38.090
all the way to omega p--

00:09:38.090 --> 00:09:40.890
omega to the 2 times p minus 1.

00:09:40.890 --> 00:09:42.850
And so on.

00:09:42.850 --> 00:09:45.870
So I want to have--

00:09:45.870 --> 00:09:46.440
yes, so OK.

00:09:46.440 --> 00:09:50.510
So I make this list,
and I have p of them.

00:09:59.510 --> 00:10:01.230
So these are my eigenvectors.

00:10:01.230 --> 00:10:04.660
And let me check that they
are actually eigenvectors.

00:10:04.660 --> 00:10:07.150
And then we can also
compute their eigenvalues.

00:10:07.150 --> 00:10:12.580
So the top eigenvector
corresponds to d.

00:10:12.580 --> 00:10:14.360
So the all 1's in
a d regular graph

00:10:14.360 --> 00:10:17.180
is always an eigenvector
with eigenvalue d.

00:10:17.180 --> 00:10:19.617
And the other ones, we'll
just do this computation.

00:10:22.860 --> 00:10:25.570
So instead of getting
confused with indices,

00:10:25.570 --> 00:10:28.000
let me just compute,
as an example,

00:10:28.000 --> 00:10:37.712
the j-th coordinate of the
adjacency matrix times V2.

00:10:37.712 --> 00:10:42.360
So the j-th coordinate,
so what it comes to,

00:10:42.360 --> 00:10:45.630
is the following sum.

00:10:45.630 --> 00:10:52.110
If I run over S, then
omega raised to j plus s.

00:10:52.110 --> 00:10:56.090
So S is symmetric, so I don't
have to worry so much about

00:10:56.090 --> 00:10:57.080
plus or minus.

00:10:57.080 --> 00:10:58.878
So I say j plus s.

00:10:58.878 --> 00:11:00.920
So if you think about what
this Cayley graph, how

00:11:00.920 --> 00:11:05.000
it is defined, if you hit
this vector with that matrix,

00:11:05.000 --> 00:11:09.370
the j-th coordinate
is that sum there.

00:11:09.370 --> 00:11:15.690
But I can rewrite the sum by
taking out this common factor

00:11:15.690 --> 00:11:16.700
omega to j.

00:11:22.640 --> 00:11:28.890
And you see that this is
the j-th coordinate of V2.

00:11:33.790 --> 00:11:34.970
And this is true for all j.

00:11:34.970 --> 00:11:38.030
So this number here is lambda 2.

00:11:44.640 --> 00:11:55.410
And, more generally, lambda
k is the following sum,

00:11:55.410 --> 00:11:59.340
for k from 0--

00:12:04.010 --> 00:12:08.768
so from k being 1 through p.

00:12:08.768 --> 00:12:11.580
So when you plug in k
equals to 1, you just get d.

00:12:11.580 --> 00:12:17.450
And the others are sums
of these exponential sums.

00:12:17.450 --> 00:12:19.610
Now, this is a pretty
straightforward computation.

00:12:19.610 --> 00:12:21.193
And, in fact, we're
not using anything

00:12:21.193 --> 00:12:22.440
about quadratic residues.

00:12:22.440 --> 00:12:29.076
This is a generic fact about
Cayley graphs of z mod p.

00:12:29.076 --> 00:12:35.010
So this is true for all Cayley
graphs S, not necessarily

00:12:35.010 --> 00:12:38.000
for quadratic residues.

00:12:38.000 --> 00:12:40.350
And the basic reason
is that, here, you

00:12:40.350 --> 00:12:45.990
have this set of eigenvectors,
and they do not depend on S.

00:12:45.990 --> 00:12:48.240
So you might know this
concept from other places,

00:12:48.240 --> 00:12:50.880
such as circular
matrices and whatnot,

00:12:50.880 --> 00:12:55.430
but this is true in
this simple computation.

00:12:55.430 --> 00:13:02.110
So now we have the values
of lambda explicitly.

00:13:02.110 --> 00:13:04.870
I can now compute their sizes.

00:13:04.870 --> 00:13:07.780
I want to know how
big this lambda is.

00:13:07.780 --> 00:13:09.830
Well, the first one,
when k equals to 1,

00:13:09.830 --> 00:13:15.720
it's exactly d, the degree,
which is p minus 1 over 2.

00:13:15.720 --> 00:13:18.420
But what about the other ones?

00:13:18.420 --> 00:13:22.350
So, for the other ones, we can
do a computation as follows.

00:13:22.350 --> 00:13:26.570
So note that I can
rewrite lambda k

00:13:26.570 --> 00:13:30.320
by noting that if
I take twice it

00:13:30.320 --> 00:13:34.550
and plus 1, then I
obtain the following sum.

00:13:41.500 --> 00:13:45.550
Because here I am using the S
as a set of quadratic residues.

00:13:45.550 --> 00:13:48.750
So if I consider this sum
here, every quadratic residue

00:13:48.750 --> 00:13:52.050
gets counted twice, except for
0, which gets counted once.

00:13:55.110 --> 00:13:59.140
And now I would like to
evaluate the size of this sum,

00:13:59.140 --> 00:14:00.850
this exponential sum.

00:14:00.850 --> 00:14:03.610
And this is something
that's known as a Gauss sum.

00:14:07.940 --> 00:14:09.470
So, basically, a
Gauss sum is what

00:14:09.470 --> 00:14:11.525
happens when you
have something that's

00:14:11.525 --> 00:14:15.950
like a quadratic, an exponential
sum with a quadratic dependence

00:14:15.950 --> 00:14:17.930
in the exponent.

00:14:17.930 --> 00:14:22.340
And the trick here is to
consider the square of the sum.

00:14:26.540 --> 00:14:29.080
So the magnitude squared.

00:14:29.080 --> 00:14:32.480
Now if I expand the square--

00:14:32.480 --> 00:14:36.930
so squaring is a common
feature of many of the things

00:14:36.930 --> 00:14:38.370
we do in this course.

00:14:38.370 --> 00:14:40.900
It really simplifies your life.

00:14:40.900 --> 00:14:43.140
You do the square,
you expand the sum.

00:14:43.140 --> 00:14:54.258
You can re-parameterize one
of the summands like that.

00:14:54.258 --> 00:14:57.850
So do two steps at once.

00:14:57.850 --> 00:15:01.390
I'm re-parameterizing
and I'm expanding.

00:15:01.390 --> 00:15:10.630
But now you see, if I expand
the exponent, we find--

00:15:16.520 --> 00:15:18.350
so that's just algebra.

00:15:18.350 --> 00:15:25.960
And now you notice that this
sum here, the sum over a

00:15:25.960 --> 00:15:29.680
is equal to--

00:15:29.680 --> 00:15:36.850
when b is nonzero, I
claim that this sum is 0.

00:15:36.850 --> 00:15:41.030
And when b is nonzero, then I'm
summing over some permutations

00:15:41.030 --> 00:15:43.910
of the roots of unity.

00:15:43.910 --> 00:15:49.150
So here I'm assuming
that k is bigger than--

00:15:49.150 --> 00:15:52.080
let's say here k is not 0.

00:15:52.080 --> 00:15:54.445
So I'm re-parameterizing
k a little bit.

00:15:54.445 --> 00:15:56.020
So k is not 0.

00:15:56.020 --> 00:15:59.740
Then when b is not 0,
the sum over a is 0.

00:15:59.740 --> 00:16:01.720
And otherwise it equals to p.

00:16:01.720 --> 00:16:05.880
So the sum over
here equals to p.

00:16:09.090 --> 00:16:13.671
And, therefore, lambda
k, lambda sub k--

00:16:17.520 --> 00:16:20.270
how about if I--

00:16:20.270 --> 00:16:22.980
so what should I change that to?

00:16:22.980 --> 00:16:30.480
So if I-- k is 0, then I want
this to be lambda sub k plus 1.

00:16:30.480 --> 00:16:38.350
Then lambda sub k plus
1 is equal to plus/minus

00:16:38.350 --> 00:16:45.470
p plus 1 over 2, for all
lambda not equal to 0.

00:16:51.800 --> 00:16:54.230
So, really, except for
the top eigenvalue,

00:16:54.230 --> 00:16:56.570
which is just the degree,
all the other ones

00:16:56.570 --> 00:17:01.140
are one of these two values,
and they're all quite small.

00:17:01.140 --> 00:17:03.230
So this is an explicit
computation showing you

00:17:03.230 --> 00:17:06.829
that this Paley graph is
indeed a pseudorandom graph.

00:17:06.829 --> 00:17:08.599
It's an example of a
quasi-random graph.

00:17:08.599 --> 00:17:10.198
Yes.

00:17:10.198 --> 00:17:12.875
AUDIENCE: Do we know
what the sign is?

00:17:12.875 --> 00:17:15.250
YUFEI ZHAO: The question is,
do we know what the sign is?

00:17:15.250 --> 00:17:17.990
So we actually--
so here I am not

00:17:17.990 --> 00:17:21.398
telling you what the sign
is, but you can look up.

00:17:21.398 --> 00:17:23.190
Actually, people have
computed exactly what

00:17:23.190 --> 00:17:24.750
the sign should be.

00:17:24.750 --> 00:17:28.000
And this is something that you
can find in a number theory

00:17:28.000 --> 00:17:29.530
textbook, like Aaron and Rosen.

00:17:33.218 --> 00:17:35.810
Any more questions?

00:17:35.810 --> 00:17:38.270
There is a concept
here I just want

00:17:38.270 --> 00:17:43.110
to bring out, that you might
recognize sums like this.

00:17:43.110 --> 00:17:44.150
So this kind of sum.

00:17:46.830 --> 00:17:50.930
That's a Fourier coefficient.

00:17:50.930 --> 00:17:53.320
So if you have some
Fourier transform, I mean,

00:17:53.320 --> 00:17:56.890
this is exactly what Fourier
transforms look like.

00:17:56.890 --> 00:18:02.150
And it is indeed the
case that, in general,

00:18:02.150 --> 00:18:13.560
if you have an Abelian
group, then the eigenvalues

00:18:13.560 --> 00:18:22.550
and the spectral information of
the corresponding Cayley graph

00:18:22.550 --> 00:18:25.537
corresponds to
Fourier coefficients.

00:18:29.020 --> 00:18:32.080
And this is the connection
that we'll see also

00:18:32.080 --> 00:18:35.710
later on in the course when we
consider additive combinatorics

00:18:35.710 --> 00:18:38.470
and giving a Fourier analytic
proof of Roth's theorem.

00:18:38.470 --> 00:18:42.820
And there Fourier analysis
will play a central role.

00:18:42.820 --> 00:18:45.800
But this is actually-- this
analogy, as I've written it,

00:18:45.800 --> 00:18:47.360
is only for Abelian groups.

00:18:47.360 --> 00:18:50.480
If you try to do the same
for non-Abelian groups,

00:18:50.480 --> 00:18:55.530
you will get something
somewhat different.

00:18:55.530 --> 00:18:58.790
So for non-Abelian
groups, you do not

00:18:58.790 --> 00:19:01.790
have this nice notion
of Fourier analysis,

00:19:01.790 --> 00:19:05.450
at least in the versions
that generalizes what's

00:19:05.450 --> 00:19:07.450
above in a straightforward way.

00:19:07.450 --> 00:19:10.680
But, instead, you have something
else, which many of you

00:19:10.680 --> 00:19:12.530
have seen before but
under a different name.

00:19:12.530 --> 00:19:19.500
And that's representation
theory, which, in some sense,

00:19:19.500 --> 00:19:21.540
is Fourier analysis,
except, instead

00:19:21.540 --> 00:19:24.900
of one-dimensional objects
and complex numbers,

00:19:24.900 --> 00:19:26.660
we're looking at
higher-dimensional

00:19:26.660 --> 00:19:28.250
representations.

00:19:28.250 --> 00:19:31.120
So I just want to point
out this connection,

00:19:31.120 --> 00:19:35.940
and we'll see more
of it later on.

00:19:35.940 --> 00:19:36.980
Any questions?

00:19:41.780 --> 00:19:44.390
So let's talk more
about Cayley graphs.

00:19:44.390 --> 00:19:46.970
So, last time, we
mentioned these notions

00:19:46.970 --> 00:19:49.820
of quasi-randomness.

00:19:49.820 --> 00:19:51.980
And I said at the
end of the class

00:19:51.980 --> 00:19:55.280
that many of these equivalences
between quasi-random graphs,

00:19:55.280 --> 00:19:59.090
they fail for sparse graphs.

00:19:59.090 --> 00:20:02.220
If your density, if your
x density is a constant,

00:20:02.220 --> 00:20:06.100
then the equivalences
no longer hold.

00:20:06.100 --> 00:20:09.560
But what about
for Cayley graphs?

00:20:09.560 --> 00:20:13.790
And, in particular, I
would like to consider

00:20:13.790 --> 00:20:16.880
two specific notions that
we discussed last time

00:20:16.880 --> 00:20:20.750
and try to understand how they
relate to each other for Cayley

00:20:20.750 --> 00:20:21.250
graphs.

00:20:30.052 --> 00:20:33.848
So for dense Cayley
graphs, it's a special case

00:20:33.848 --> 00:20:34.890
of what we did yesterday.

00:20:34.890 --> 00:20:37.430
So I'm really interested
in sparser Cayley graphs,

00:20:37.430 --> 00:20:39.306
even down the degree.

00:20:39.306 --> 00:20:40.687
So even down the degree.

00:20:40.687 --> 00:20:42.270
So that's much sparser
than the regime

00:20:42.270 --> 00:20:44.010
we were looking at last time.

00:20:44.010 --> 00:20:47.820
And the main result
I want to tell you

00:20:47.820 --> 00:20:52.440
is that the DISC condition
is, in a very strong sense,

00:20:52.440 --> 00:20:56.610
actually equivalent to
the eigenvalue condition

00:20:56.610 --> 00:21:10.460
for all Cayley graphs, including
non-Abelian Cayley graphs.

00:21:13.687 --> 00:21:15.520
So before telling you
what the statement is,

00:21:15.520 --> 00:21:18.370
I first want to give
an example showing you

00:21:18.370 --> 00:21:21.310
that this equivalence
is definitely not true

00:21:21.310 --> 00:21:24.850
if you remove the
assumption of Cayley graphs.

00:21:24.850 --> 00:21:27.830
For example, if you--

00:21:27.830 --> 00:21:32.334
so example that this is
false for non-Cayley.

00:21:37.080 --> 00:21:40.950
Because if you take,
let's say, a large--

00:21:40.950 --> 00:21:42.710
so let's say d regular graph.

00:21:42.710 --> 00:21:46.260
So let's say a large
random d regular graph.

00:21:50.164 --> 00:21:53.390
d here can be a constant
or growing with n,

00:21:53.390 --> 00:21:55.670
but this is a pretty
robust example.

00:21:55.670 --> 00:22:00.842
And then I add to it an extra
destroying copy of k sub d

00:22:00.842 --> 00:22:04.940
plus 1 that's much smaller in
terms of number of vertices.

00:22:07.500 --> 00:22:09.360
The big, large
random graph, well,

00:22:09.360 --> 00:22:11.370
by virtue of being
a random graph,

00:22:11.370 --> 00:22:12.930
has the discrepancy property.

00:22:18.610 --> 00:22:22.320
And because we're only
adding in a very small number

00:22:22.320 --> 00:22:25.710
of vertices, it does not destroy
the discrepancy property.

00:22:25.710 --> 00:22:27.570
The discrepancy
property, if you're just

00:22:27.570 --> 00:22:29.430
adding a small
number of vertices,

00:22:29.430 --> 00:22:30.840
it doesn't change much.

00:22:30.840 --> 00:22:33.784
So this whole thing
has discrepancy.

00:22:36.390 --> 00:22:38.190
However, what about
the eigenvalues?

00:22:41.030 --> 00:22:43.040
Claim that the top
two eigenvalues

00:22:43.040 --> 00:22:45.530
are in fact both equal to d.

00:22:45.530 --> 00:22:49.310
And that's because you have
two eigenvectors, one which

00:22:49.310 --> 00:22:52.580
is the all 1's vector
on this graph, another

00:22:52.580 --> 00:22:55.120
which is the all 1's
vector on that graph.

00:22:55.120 --> 00:22:58.100
These two DISC components each
give you a top eigenvector

00:22:58.100 --> 00:23:00.920
of d, so you get d twice.

00:23:00.920 --> 00:23:03.920
And, in particular, the second
eigenvalue is not small.

00:23:07.200 --> 00:23:11.720
So the implication from
DISC to eigenvalue really

00:23:11.720 --> 00:23:16.380
fails for non-Cayley
graph for general graphs.

00:23:19.110 --> 00:23:21.890
The implication, the other
direction is actually OK.

00:23:26.040 --> 00:23:30.240
In fact, the eigenvalue implies
DISC is actually the content

00:23:30.240 --> 00:23:33.020
of the expander mixing lemma.

00:23:33.020 --> 00:23:40.920
So this follows by
expander mixing lemma.

00:23:40.920 --> 00:23:43.470
And that's because, if you
look at the expander mixing

00:23:43.470 --> 00:23:47.060
lemma for a Cayley graph--

00:23:47.060 --> 00:23:50.700
or for a-- not for
Cayley graph-- for--

00:23:50.700 --> 00:23:52.890
if you have the
eigenvalue condition,

00:23:52.890 --> 00:23:58.320
then, automatically, you would
find that these two guys here

00:23:58.320 --> 00:23:59.380
are at most n.

00:24:02.520 --> 00:24:10.570
So if lambda is quite small
compared to the degree,

00:24:10.570 --> 00:24:15.952
then you still have the desired
type of quasi-randomness.

00:24:15.952 --> 00:24:18.160
So I'll make the statements
more precise in a second.

00:24:21.420 --> 00:24:25.060
So the question is, how
can we certify, how can we

00:24:25.060 --> 00:24:29.920
show that, in fact, DISC, which
is seemingly weaker property,

00:24:29.920 --> 00:24:33.733
implies a stronger property of
eigenvalue for Cayley graphs.

00:24:33.733 --> 00:24:35.650
And what is a special
about Cayley graphs that

00:24:35.650 --> 00:24:38.350
would allow to do this, that
the statement is generally

00:24:38.350 --> 00:24:40.758
false for non-Cayley graphs?

00:24:47.330 --> 00:24:49.430
So let me define--

00:24:49.430 --> 00:24:53.290
so let me first
tell you the result.

00:24:53.290 --> 00:24:56.015
So this is the result
due to David Conlon

00:24:56.015 --> 00:24:57.920
and myself two years ago.

00:25:00.570 --> 00:25:05.200
So many of you may not have been
to many seminar talks, where

00:25:05.200 --> 00:25:08.160
there's this convention
in mathematics talks

00:25:08.160 --> 00:25:12.090
where you don't write out your
full name, only by the initial.

00:25:12.090 --> 00:25:13.740
Although some kind
of false modesty.

00:25:13.740 --> 00:25:17.280
But, of course, we all love
talking about our own results,

00:25:17.280 --> 00:25:19.140
but somehow we
don't like to write

00:25:19.140 --> 00:25:21.420
our own name for some reason.

00:25:21.420 --> 00:25:23.850
So here's the theorem.

00:25:23.850 --> 00:25:29.700
So I start with a
finite group, gamma.

00:25:29.700 --> 00:25:34.168
And let me consider a subset
S of gamma that is symmetric.

00:25:37.040 --> 00:25:39.320
And consider G the Cayley graph.

00:25:44.220 --> 00:25:49.060
Let me right n as the number of
vertices, and d the size of S.

00:25:49.060 --> 00:25:52.370
So this is a d regular graph.

00:25:52.370 --> 00:25:54.662
Let me define the
following properties.

00:26:01.550 --> 00:26:05.680
The first property, I'll
call DISC with epsilon.

00:26:05.680 --> 00:26:07.930
So I give you an
explicit parameter.

00:26:12.290 --> 00:26:14.960
The number of edges
between x and y

00:26:14.960 --> 00:26:19.070
differs from the number of
edges that you would expect.

00:26:19.070 --> 00:26:21.980
So as in the expander
mixing lemma.

00:26:21.980 --> 00:26:25.490
So the DISC property is that
this quantity is small relative

00:26:25.490 --> 00:26:26.980
to the total number of edges.

00:26:30.670 --> 00:26:35.710
The second property, which we'll
call the eigenvalue property,

00:26:35.710 --> 00:26:50.110
EIG, is that G is an n, d,
lambda graph, with lambda,

00:26:50.110 --> 00:26:52.796
at most, epsilon d.

00:26:52.796 --> 00:26:56.338
So lambda is quite small
as a function of d.

00:26:59.326 --> 00:27:03.250
The conclusion of the
theorem is that, up

00:27:03.250 --> 00:27:07.560
to a small change of
parameters, these two properties

00:27:07.560 --> 00:27:08.790
are equivalent.

00:27:08.790 --> 00:27:13.800
In particular,
eigenvalue implies a--

00:27:13.800 --> 00:27:19.350
epsilon implies DISC of epsilon.

00:27:19.350 --> 00:27:23.430
And DISC of
epsilon-- and this is

00:27:23.430 --> 00:27:26.160
the-- the second one is the
more interesting direction--

00:27:26.160 --> 00:27:29.550
it implies EIG.

00:27:29.550 --> 00:27:31.180
Well, you lose a
little bit, but,

00:27:31.180 --> 00:27:33.200
at most, a constant factor.

00:27:33.200 --> 00:27:34.530
EIG of 8 epsilon.

00:27:40.680 --> 00:27:43.435
Any questions about
the statement so far?

00:27:43.435 --> 00:27:46.070
And so, as I mentioned,
this is completely false

00:27:46.070 --> 00:27:52.490
if you consider
non-Cayley graphs.

00:27:52.490 --> 00:27:56.000
And we also, using
expander mixing lemma,

00:27:56.000 --> 00:28:00.130
using that implication up
there, this direction follows.

00:28:11.707 --> 00:28:13.290
One of the main
reasons I want to show

00:28:13.290 --> 00:28:15.060
you a proof of this
theorem is that it

00:28:15.060 --> 00:28:17.870
uses this tool which I
think is worth knowing.

00:28:17.870 --> 00:28:21.030
And this is an important
inequality known

00:28:21.030 --> 00:28:22.898
as Grothendieck's inequality.

00:28:35.460 --> 00:28:37.530
So many of you probably
know Grothendieck

00:28:37.530 --> 00:28:41.100
as this famous French
mathematician who

00:28:41.100 --> 00:28:44.040
reinvented modern
algebraic geometry

00:28:44.040 --> 00:28:45.678
and spent the rest
of his life writing

00:28:45.678 --> 00:28:47.220
tomes and tomes of
text that have yet

00:28:47.220 --> 00:28:48.387
to be translated to English.

00:28:51.110 --> 00:28:53.600
But he also did some
important foundational work

00:28:53.600 --> 00:28:56.480
in functional analysis
before he became

00:28:56.480 --> 00:28:59.690
an algebraic geometry nerd.

00:28:59.690 --> 00:29:03.540
And this is one of the important
results in that area that he--

00:29:06.710 --> 00:29:08.580
so Grothendieck's
inequality tells us

00:29:08.580 --> 00:29:13.140
that there exists some
absolute constant k

00:29:13.140 --> 00:29:22.855
such that for every matrix A--

00:29:22.855 --> 00:29:24.600
so a real-valued matrix--

00:29:31.100 --> 00:29:33.020
we have that the--

00:29:40.040 --> 00:29:41.330
so we have that, if you--

00:29:41.330 --> 00:29:42.810
so here's the idea.

00:29:42.810 --> 00:29:46.070
Let's consider the supremum--

00:29:46.070 --> 00:29:50.410
so let's consider the
following quantity.

00:29:50.410 --> 00:29:51.530
This is a bilinear form.

00:29:55.310 --> 00:29:57.970
So this is a bilinear form.

00:29:57.970 --> 00:29:59.950
This is basically a--

00:30:03.740 --> 00:30:07.770
so bilinear form, if you
hit it by a vector x and y

00:30:07.770 --> 00:30:09.450
from the two sides.

00:30:09.450 --> 00:30:13.080
And I'm interested in
what is the maximum value

00:30:13.080 --> 00:30:16.110
of this bilinear form
if you are allowed

00:30:16.110 --> 00:30:23.480
to take x and y to be plus/minus
1-valued real numbers?

00:30:30.720 --> 00:30:32.920
So this is an
important quantity,

00:30:32.920 --> 00:30:34.870
and it gives you a matrix.

00:30:34.870 --> 00:30:38.230
And it's basically asking you,
you get a sign of plus or minus

00:30:38.230 --> 00:30:40.240
to each row and
column, and I want

00:30:40.240 --> 00:30:42.323
to maximize this number here.

00:30:42.323 --> 00:30:43.990
This is an important
quantity that we'll

00:30:43.990 --> 00:30:47.680
see actually much more in the
next chapter on graph limits.

00:30:47.680 --> 00:30:49.420
But, for now, just take my word.

00:30:49.420 --> 00:30:51.580
This is a very
important quantity.

00:30:51.580 --> 00:30:53.080
And this is actually
a quantity that

00:30:53.080 --> 00:30:54.820
is very difficult to evaluate.

00:30:54.820 --> 00:30:56.800
If I give you a
very large matrix

00:30:56.800 --> 00:30:59.710
and ask you to compute
this number here,

00:30:59.710 --> 00:31:01.810
there is no good
algorithm for it.

00:31:01.810 --> 00:31:05.710
And it's believed that there
is no good algorithm for it.

00:31:05.710 --> 00:31:09.370
On the other hand,
there is a relaxation

00:31:09.370 --> 00:31:13.130
of this problem, which
is the following.

00:31:13.130 --> 00:31:19.430
It's still a sum, but now,
instead of considering

00:31:19.430 --> 00:31:25.190
the bilinear form there, let's
consider the xi's and yi's.

00:31:25.190 --> 00:31:32.600
Not-- take them not form real
numbers, but take vectors.

00:31:32.600 --> 00:31:39.680
So let's consider
the sum where I'm

00:31:39.680 --> 00:31:47.330
taking a similar-looking sum,
except that xi's and yi's come

00:31:47.330 --> 00:31:51.770
from a unit ball in
some vector space

00:31:51.770 --> 00:31:57.920
with an inner product,
where B is the unit

00:31:57.920 --> 00:32:07.395
ball in some Rm, where here
the dimension is actually not

00:32:07.395 --> 00:32:07.895
so relevant.

00:32:07.895 --> 00:32:10.470
The dimension is arbitrary.

00:32:10.470 --> 00:32:15.620
If you like, you can make
m n or 2n because you only

00:32:15.620 --> 00:32:19.050
have that many vectors.

00:32:19.050 --> 00:32:22.160
So this quantity here,
just by very definition,

00:32:22.160 --> 00:32:25.010
is a relaxation of the
right-hand of this quantity

00:32:25.010 --> 00:32:25.510
here.

00:32:25.510 --> 00:32:27.620
So it's at least this large.

00:32:27.620 --> 00:32:30.650
So, in particular, if you
have whatever plus/minus,

00:32:30.650 --> 00:32:34.160
you can always look at the same
quantity with m equal to 1,

00:32:34.160 --> 00:32:35.660
and you obtain
this quantity here.

00:32:35.660 --> 00:32:38.310
But this quantity may
be substantially larger.

00:32:38.310 --> 00:32:43.880
So the x and y's have more
room to put themselves in

00:32:43.880 --> 00:32:45.220
to maximize the sum.

00:32:48.330 --> 00:32:52.650
And Grothendieck's inequality
tells us that the left-hand

00:32:52.650 --> 00:32:57.210
side actually cannot be too
much larger than the right-hand

00:32:57.210 --> 00:32:58.110
side.

00:32:58.110 --> 00:33:02.650
It exceeds it by, at
most, a constant factor.

00:33:02.650 --> 00:33:04.870
So, in other words,
the left-hand side,

00:33:04.870 --> 00:33:15.060
which is known as a
semi-definite relaxation,

00:33:15.060 --> 00:33:17.910
you are not losing by more
than a constant factor

00:33:17.910 --> 00:33:20.730
compared to the
original problem.

00:33:20.730 --> 00:33:22.500
And this is important
in computer science

00:33:22.500 --> 00:33:24.210
because the left-hand
side turns out

00:33:24.210 --> 00:33:27.960
to be a Semidefinite
Program, an SDP, which

00:33:27.960 --> 00:33:32.740
does have efficient
algorithms to compute.

00:33:32.740 --> 00:33:35.610
So you can give a constant
factor approximation

00:33:35.610 --> 00:33:38.430
to this difficult compute
but important quantity

00:33:38.430 --> 00:33:41.030
by using semidefinite
relaxation.

00:33:41.030 --> 00:33:43.020
And Grothendieck's
inequality promises us

00:33:43.020 --> 00:33:44.540
that it is a good relaxation.

00:33:50.560 --> 00:33:53.050
You might ask, what
is the value of k?

00:33:53.050 --> 00:33:55.790
So I said there exists
some constant k.

00:33:55.790 --> 00:33:58.720
So this is actually a mystery.

00:33:58.720 --> 00:34:04.000
So the current proofs have
been improved over time.

00:34:04.000 --> 00:34:06.340
And Grothendieck himself
proved this theorem,

00:34:06.340 --> 00:34:08.889
but it constantly has
been improved over time.

00:34:08.889 --> 00:34:11.469
And, currently, the
best-known result

00:34:11.469 --> 00:34:20.604
is something along the lines
of k roughly 1.78 works.

00:34:20.604 --> 00:34:25.719
But the optimal value, which
is known as Grothendieck's

00:34:25.719 --> 00:34:33.310
constant, is unknown.

00:34:40.600 --> 00:34:42.317
So this is
Grothendieck's constant.

00:34:42.317 --> 00:34:43.900
Actually, this, what
I've written down

00:34:43.900 --> 00:34:47.949
is what's called the real
Grothendieck's constant.

00:34:47.949 --> 00:34:49.780
Because you can
also write a version

00:34:49.780 --> 00:34:52.810
for complex numbers
and complex vectors,

00:34:52.810 --> 00:34:55.130
and that's the complex
Grothendieck's constant.

00:34:55.130 --> 00:34:55.630
Yes.

00:34:55.630 --> 00:34:57.518
AUDIENCE: Is there
a lower bound that's

00:34:57.518 --> 00:34:59.305
known [INAUDIBLE]
greater than 1?

00:34:59.305 --> 00:35:00.790
YUFEI ZHAO: Is there a
lower bound that is known?

00:35:00.790 --> 00:35:01.290
Yes.

00:35:01.290 --> 00:35:03.430
It's known that it's
strictly bigger than 1.

00:35:03.430 --> 00:35:04.922
AUDIENCE: Do we
know [INAUDIBLE]??

00:35:04.922 --> 00:35:06.880
YUFEI ZHAO: So there are
some specific numbers,

00:35:06.880 --> 00:35:07.840
but I forget what they are.

00:35:07.840 --> 00:35:08.632
You can look it up.

00:35:12.350 --> 00:35:13.500
Any more questions?

00:35:16.080 --> 00:35:18.790
So we'll leave
Grothendieck's inequality.

00:35:18.790 --> 00:35:20.650
We'll use it as a black box.

00:35:20.650 --> 00:35:22.440
So if you wish to
learn the proof,

00:35:22.440 --> 00:35:23.810
I encourage you to do so.

00:35:23.810 --> 00:35:26.700
There are some quite
nice proofs out there.

00:35:26.700 --> 00:35:29.520
And we'll use it to
prove this theorem here

00:35:29.520 --> 00:35:31.360
about quasi-random
Cayley graphs.

00:35:44.890 --> 00:35:49.040
So let's suppose DISC holds.

00:35:55.880 --> 00:35:58.160
So what would we like to--
what do we like to show?

00:35:58.160 --> 00:36:04.910
We want to show that this
eigenvalue condition holds.

00:36:04.910 --> 00:36:07.570
And we'll use the--

00:36:07.570 --> 00:36:11.240
some min-max characterization
of eigenvalues.

00:36:11.240 --> 00:36:13.400
But, first, some preliminaries.

00:36:13.400 --> 00:36:19.700
Suppose you have vectors x
and y which have plus/minus 1

00:36:19.700 --> 00:36:21.065
coordinate values.

00:36:24.270 --> 00:36:34.810
Then, by letting-- so let's
consider the following vectors,

00:36:34.810 --> 00:36:37.820
where I split up
x and y according

00:36:37.820 --> 00:36:40.503
to where they're positive
and where they're negative.

00:36:46.070 --> 00:36:50.740
So, here, these are such
that x plus is equal to--

00:36:50.740 --> 00:36:55.396
so if I evaluate it on a
coordinate g, then it's 1.

00:36:55.396 --> 00:37:04.861
So if x sub g is plus
1, and 0 otherwise.

00:37:04.861 --> 00:37:12.720
xg sub minus is 1 if
x sub g is minus 1.

00:37:12.720 --> 00:37:14.150
0 otherwise.

00:37:14.150 --> 00:37:19.360
So x splits into x plus
minus x minus, and y splits

00:37:19.360 --> 00:37:22.450
into y plus minus y minus.

00:37:25.710 --> 00:37:39.310
Let's consider a matrix A
where the g comma h entry of A

00:37:39.310 --> 00:37:41.970
is the following quantity.

00:37:41.970 --> 00:37:49.090
I have the set S, and I look at
whether g inverse h lies in S.

00:37:49.090 --> 00:37:50.960
And I can consider
an indicator of that.

00:37:50.960 --> 00:37:52.880
So it's 1 or 0.

00:37:52.880 --> 00:38:01.010
And then subtract d over n so
that this value has mean 0.

00:38:01.010 --> 00:38:03.950
So this is a matrix.

00:38:03.950 --> 00:38:10.720
And now if I consider
the bilinear form,

00:38:10.720 --> 00:38:14.680
hit A from left and
right with x and y,

00:38:14.680 --> 00:38:22.330
then the bilinear form
splits according to the plus

00:38:22.330 --> 00:38:24.835
and minuses of the x's.

00:38:39.230 --> 00:38:41.330
And I claim that each
one of these terms

00:38:41.330 --> 00:38:42.950
is controlled because of DISC.

00:38:45.674 --> 00:38:54.010
So, for example,
the first term is,

00:38:54.010 --> 00:38:55.820
if you expand out
what this guy is--

00:38:55.820 --> 00:38:57.070
so here's an indicator vector.

00:38:57.070 --> 00:38:58.480
That's an indicator vector.

00:38:58.480 --> 00:39:00.400
And if you look
at the definition,

00:39:00.400 --> 00:39:05.830
then this is precisely the
number of edges between x plus

00:39:05.830 --> 00:39:10.960
and y plus minus d
over n times the size

00:39:10.960 --> 00:39:14.410
of x plus times
the size of y plus,

00:39:14.410 --> 00:39:21.140
where x plus is the
set of group elements

00:39:21.140 --> 00:39:26.236
such that x sub g
is 1, and so on.

00:39:34.668 --> 00:39:35.660
All right.

00:39:43.110 --> 00:39:47.830
So the punchline up there
is that this quantity--

00:39:47.830 --> 00:40:01.520
so this quantity is, at most,
by discrepancy, epsilon dn.

00:40:05.020 --> 00:40:08.410
So this sum here, by
triangle inequality,

00:40:08.410 --> 00:40:12.920
is, at most, 4 epsilon dn.

00:40:53.580 --> 00:40:56.548
All right.

00:40:56.548 --> 00:40:59.920
So, so far, we've reinterpreted
the discrepancy property.

00:40:59.920 --> 00:41:02.460
And what we really want
to show is that this graph

00:41:02.460 --> 00:41:05.255
satisfies eigenvalue condition.

00:41:05.255 --> 00:41:07.005
So what does that
actually mean to satisfy

00:41:07.005 --> 00:41:08.510
the eigenvalue condition?

00:41:08.510 --> 00:41:11.440
So by the min-max
characterization

00:41:11.440 --> 00:41:21.670
of eigenvalues, it follows
that the maximum of these two

00:41:21.670 --> 00:41:23.440
eigenvalues, which is
the quantity that we

00:41:23.440 --> 00:41:33.010
would like to control, is
equal to the following.

00:41:33.010 --> 00:41:41.400
It is equal to the supremum
of this bilinear form

00:41:41.400 --> 00:41:46.770
when x and y are
unit-length vectors.

00:41:53.240 --> 00:41:56.900
And this is simply
because A is the matrix--

00:41:56.900 --> 00:41:59.390
it's not the adjacency matrix.

00:41:59.390 --> 00:42:01.570
A is not the adjacency matrix.

00:42:01.570 --> 00:42:04.790
A is the matrix obtained
by essentially taking

00:42:04.790 --> 00:42:07.250
the adjacency matrix
and subtracting

00:42:07.250 --> 00:42:09.500
that constant there.

00:42:09.500 --> 00:42:12.900
And subtracting that constant
gets rid of the top eigenvalue.

00:42:12.900 --> 00:42:15.552
And what you remained
is whatever that's left.

00:42:15.552 --> 00:42:17.510
And you want to show that
whatever you remained

00:42:17.510 --> 00:42:20.550
has small spectral radius.

00:42:20.550 --> 00:42:22.910
So we would like to show
that this quantity here

00:42:22.910 --> 00:42:23.600
is quite small.

00:42:27.880 --> 00:42:29.370
Well, let's do it.

00:42:29.370 --> 00:42:33.360
So give me a pair
of vectors, x and y.

00:42:37.340 --> 00:42:44.290
And let's set the
following quantities,

00:42:44.290 --> 00:42:49.090
where I take a twist
on this x vector

00:42:49.090 --> 00:42:57.480
by rotating the
coordinates, setting

00:42:57.480 --> 00:43:04.720
x super s sub g, the
coordinate g, to be x sub sg.

00:43:04.720 --> 00:43:08.820
So x is a vector indexed
by the group elements,

00:43:08.820 --> 00:43:14.520
and then rotating this indexing
of the group elements by s.

00:43:14.520 --> 00:43:18.180
So that's what I mean
by superscript s.

00:43:18.180 --> 00:43:24.904
And, likewise, y superscript
s is defined similarly.

00:43:32.140 --> 00:43:36.760
So I claim that these
twists, these rotations,

00:43:36.760 --> 00:43:40.820
do not change the
norm of these vectors.

00:43:40.820 --> 00:43:43.080
And that should be pretty
clear, because I'm simply

00:43:43.080 --> 00:43:47.540
relabeling the coordinates
in a uniform way.

00:43:47.540 --> 00:43:50.130
And, likewise, same for y.

00:44:00.660 --> 00:44:09.970
So I would like to show this
quantity up here is small.

00:44:09.970 --> 00:44:13.120
So let's consider
two unit vectors.

00:44:25.450 --> 00:44:28.410
And consider this bilinear form.

00:44:28.410 --> 00:44:42.490
If I expand out this bilinear
form, it looks like that.

00:44:42.490 --> 00:44:45.080
I'm just writing it out.

00:44:45.080 --> 00:44:52.390
But now let me just throw in
an extra variable of summation.

00:44:52.390 --> 00:45:04.270
What we'll do is essentially
look at the same sum,

00:45:04.270 --> 00:45:18.620
but now I add in an extra
s, and put this s over here.

00:45:18.620 --> 00:45:21.450
So convince yourself that
this is the same sum.

00:45:24.220 --> 00:45:27.890
So it's simply
re-parameterizing the sum.

00:45:27.890 --> 00:45:28.890
So this is the same sum.

00:45:33.590 --> 00:45:36.080
But now, if you look
at the definition of A,

00:45:36.080 --> 00:45:37.986
there's this cancellation.

00:45:41.720 --> 00:45:44.800
So the two s's cancel out.

00:45:44.800 --> 00:45:46.360
So let's rewrite the sum.

00:45:49.630 --> 00:45:54.541
1 over n, then g, h,
s, all group elements.

00:45:54.541 --> 00:46:04.040
Then-- now, if I bring
this summation of s,

00:46:04.040 --> 00:46:08.880
now I bring it inside, and then
you see that what's inside is

00:46:08.880 --> 00:46:16.695
simply the inner product between
the two vectors, x sub g--

00:46:16.695 --> 00:46:17.695
between the two vectors.

00:46:24.490 --> 00:46:26.730
So this is-- so what's
inside is simply

00:46:26.730 --> 00:46:36.600
the product, inner
product, between these two.

00:46:36.600 --> 00:46:42.100
So I may need to redefine.

00:46:45.840 --> 00:46:46.340
Yes.

00:46:46.340 --> 00:46:47.510
So when you're looking
at-- when you're

00:46:47.510 --> 00:46:48.930
talking about
non-Abelian groups,

00:46:48.930 --> 00:46:50.630
it's always a
question of which side

00:46:50.630 --> 00:46:53.160
should you multiply things by.

00:46:53.160 --> 00:46:57.220
And you guys are OK?

00:46:57.220 --> 00:46:59.770
Or I need to change
this s to over here.

00:46:59.770 --> 00:47:03.650
But anyway, it should work.

00:47:03.650 --> 00:47:04.250
Yes, question.

00:47:04.250 --> 00:47:05.995
AUDIENCE: yh [INAUDIBLE].

00:47:05.995 --> 00:47:06.620
YUFEI ZHAO: yh.

00:47:06.620 --> 00:47:07.606
Thank you.

00:47:17.480 --> 00:47:19.830
Yes, I think--

00:47:19.830 --> 00:47:20.820
OK.

00:47:20.820 --> 00:47:21.320
Question.

00:47:21.320 --> 00:47:26.912
AUDIENCE: [INAUDIBLE]

00:47:26.912 --> 00:47:29.900
YUFEI ZHAO: Great.

00:47:29.900 --> 00:47:32.883
So maybe I need to switch
the definition here,

00:47:32.883 --> 00:47:35.050
but, in any case, some
version of this should be OK.

00:47:35.050 --> 00:47:35.550
Yes.

00:47:35.550 --> 00:47:37.220
So figure it out
later in the notes.

00:47:39.800 --> 00:47:45.010
But now-- OK.

00:47:45.010 --> 00:47:48.190
So you have this--

00:47:48.190 --> 00:47:49.770
we have this here.

00:47:49.770 --> 00:47:55.720
And if you look at
this quantity here,

00:47:55.720 --> 00:47:57.850
it is the kind of
quantity that comes up

00:47:57.850 --> 00:48:00.521
in Grothendieck's inequality.

00:48:03.565 --> 00:48:05.190
So this is basically
the left-hand side

00:48:05.190 --> 00:48:08.550
of Grothendieck's inequality.

00:48:08.550 --> 00:48:12.230
What about the right-hand side
of Grothendieck's inequality?

00:48:12.230 --> 00:48:14.790
Well, we already
controlled that.

00:48:14.790 --> 00:48:16.340
We already controlled
that because we

00:48:16.340 --> 00:48:20.340
said, whenever you have up
there little x and little y--

00:48:26.000 --> 00:48:31.900
so the conclusion of
this board was that--

00:48:31.900 --> 00:48:34.210
let me erase over here.

00:48:34.210 --> 00:48:41.160
So the conclusion of this board
was that this bilinear form

00:48:41.160 --> 00:48:45.690
is bounded by, at
most, 4 epsilon d,

00:48:45.690 --> 00:48:52.450
for all x and y being
plus/minus 1 coordinate valued.

00:48:52.450 --> 00:48:57.700
So combining them
by Grothendieck,

00:48:57.700 --> 00:49:03.830
we have an upper bound, which is
the Grothendieck constant times

00:49:03.830 --> 00:49:05.510
4 epsilon--

00:49:09.030 --> 00:49:11.870
so 4 epsilon dn.

00:49:15.570 --> 00:49:16.820
There's a-- sorry.

00:49:16.820 --> 00:49:19.250
There's an n missing here.

00:49:23.480 --> 00:49:26.720
And, therefore, because
the Grothendieck constant

00:49:26.720 --> 00:49:30.846
is less than 2, we have
a bound of 8 epsilon d.

00:49:34.580 --> 00:49:38.370
And this shows that this
variational problem, which

00:49:38.370 --> 00:49:42.450
characterizes the largest
eigenvalue in absolute value,

00:49:42.450 --> 00:49:47.090
is, at most, 8
epsilon d, thereby

00:49:47.090 --> 00:49:50.450
implying the
eigenvalue property.

00:49:58.430 --> 00:50:00.680
So the main takeaway from
this proof, two things.

00:50:00.680 --> 00:50:03.810
One is Grothendieck's inequality
is a nice thing to know.

00:50:03.810 --> 00:50:06.170
So it's a semidefinite
relaxation

00:50:06.170 --> 00:50:09.050
that changes the problem,
which is initially somewhat

00:50:09.050 --> 00:50:11.750
intractable, to a
semidefinite problem which

00:50:11.750 --> 00:50:13.730
is both, from a computer
science point of view,

00:50:13.730 --> 00:50:15.590
algorithmically
tractable, but also has

00:50:15.590 --> 00:50:17.680
nice mathematical properties.

00:50:17.680 --> 00:50:19.580
And for this application
here, there's

00:50:19.580 --> 00:50:21.380
this nice trick in
this proof where

00:50:21.380 --> 00:50:26.660
I'm symmetrizing the coordinates
using the group symmetries.

00:50:26.660 --> 00:50:32.170
And that allows me to obtain
this characterization showing

00:50:32.170 --> 00:50:35.680
that eigenvalue condition and
this discrepancy condition

00:50:35.680 --> 00:50:38.900
are equivalent
for Cayley graphs.

00:50:42.228 --> 00:50:43.270
Let's take a quick break.

00:50:46.465 --> 00:50:47.340
Any questions so far?

00:50:51.320 --> 00:50:54.220
So we've been talking
about n, d, lambda graphs.

00:50:54.220 --> 00:50:55.250
So d regular graphs.

00:50:55.250 --> 00:50:57.170
And the next question
I would like to address

00:50:57.170 --> 00:51:02.536
is, In an n, d, lambda graph,
how small can lambda be?

00:51:02.536 --> 00:51:06.840
So smaller lambda corresponds
to a more pseudorandom graph.

00:51:06.840 --> 00:51:09.830
So how small can this be?

00:51:09.830 --> 00:51:12.650
And the right kind of setting
that I want you to think about

00:51:12.650 --> 00:51:15.770
is think of d as a constant.

00:51:15.770 --> 00:51:19.370
So think of d as a constant,
and n getting large.

00:51:24.540 --> 00:51:29.830
So how small can lambda be.

00:51:32.860 --> 00:51:36.340
And it turns out there is a
limit to how small it can be.

00:51:36.340 --> 00:51:46.520
And it is known as the
Alon-Boppana bound,

00:51:46.520 --> 00:51:50.480
which tells you that
if you have a fixed d--

00:51:50.480 --> 00:52:00.440
and so G is an n-vertex
graph with adjacency matrix

00:52:00.440 --> 00:52:08.620
eigenvalues lambda
1 through lambda n,

00:52:08.620 --> 00:52:11.940
sorted in non-increasing order.

00:52:11.940 --> 00:52:17.550
Then the second largest
eigenvalue has to be at least,

00:52:17.550 --> 00:52:23.190
basically, 2 root d minus
1 minus a small error term,

00:52:23.190 --> 00:52:24.640
little on--

00:52:24.640 --> 00:52:30.590
little o1, where the
little o1 goes to 0 as n

00:52:30.590 --> 00:52:31.996
goes to infinity.

00:52:38.190 --> 00:52:46.700
So the Alon-Boppana bound tells
you that the lambda cannot be

00:52:46.700 --> 00:52:49.120
below this quantity here.

00:52:49.120 --> 00:52:51.050
And I want to explain
what is the significance

00:52:51.050 --> 00:52:54.080
of this quantity, and you
will see it in the proof.

00:52:54.080 --> 00:52:56.080
And this quantity is
the best possible.

00:52:56.080 --> 00:52:59.930
And it also says what do we know
about the existence of graphs

00:52:59.930 --> 00:53:04.980
which have lambda 2
close to this number.

00:53:04.980 --> 00:53:07.580
So this is the optimal
number you can put here.

00:53:07.580 --> 00:53:08.080
Question.

00:53:08.080 --> 00:53:09.747
AUDIENCE: Does it say
anything about how

00:53:09.747 --> 00:53:11.813
negative lambda n can be?

00:53:11.813 --> 00:53:13.230
YUFEI ZHAO:
Question-- does it say

00:53:13.230 --> 00:53:15.630
how negative lambda n can be?

00:53:15.630 --> 00:53:17.313
So I'll address
that in a second,

00:53:17.313 --> 00:53:19.730
but, essentially, if you have
a bipartite graph and lambda

00:53:19.730 --> 00:53:22.478
n equals to minus lambda 1.

00:53:22.478 --> 00:53:30.706
AUDIENCE: [INAUDIBLE]

00:53:30.706 --> 00:53:32.900
YUFEI ZHAO: More questions?

00:53:32.900 --> 00:53:36.780
So I want to show you a
proof and, time permitting,

00:53:36.780 --> 00:53:39.320
a couple of proofs of
Alon-Boppana bound.

00:53:39.320 --> 00:53:43.970
And they're all quite
simple to execute, but the--

00:53:43.970 --> 00:53:46.100
I think it's a good
way to understand how

00:53:46.100 --> 00:53:47.560
these special techniques work.

00:53:56.490 --> 00:53:59.990
So, first, as with all of the
proofs that we did concerning--

00:53:59.990 --> 00:54:01.130
or most of them--

00:54:01.130 --> 00:54:03.560
concerning eigenvalues,
we're looking

00:54:03.560 --> 00:54:10.500
at the Courant-Fischer
characterization

00:54:10.500 --> 00:54:14.340
of eigenvalues.

00:54:14.340 --> 00:54:25.980
It suffices to show, to
exhibit some vector z--

00:54:29.850 --> 00:54:33.000
so a nonzero vector--

00:54:33.000 --> 00:54:39.010
such that z is orthogonal
to the all 1's vector

00:54:39.010 --> 00:54:49.350
and this quotient is at
least the claimed bound.

00:54:53.170 --> 00:54:54.920
So by the Courant-Fischer
characterization

00:54:54.920 --> 00:54:57.650
of the second
eigenvalue, if you vary

00:54:57.650 --> 00:55:00.020
over all such d that are
orthogonal to the unit

00:55:00.020 --> 00:55:04.250
vector, then the maximum
value this quantity attains

00:55:04.250 --> 00:55:06.450
is equal to lambda 2.

00:55:06.450 --> 00:55:09.080
So to show the
lambda 2 is large,

00:55:09.080 --> 00:55:14.270
it suffices to exhibit such a z.

00:55:14.270 --> 00:55:18.550
So let me construct
such a z for you.

00:55:18.550 --> 00:55:22.380
So let r be a positive integer.

00:55:22.380 --> 00:55:29.490
And let's pick an
arbitrary vertex v. So v

00:55:29.490 --> 00:55:32.500
is a vertex in the graph.

00:55:32.500 --> 00:55:48.074
And let V sub i denote vertices
at distance exactly i from V.

00:55:48.074 --> 00:55:55.670
From-- yes, from V. So, in
particular, V0 is equal to V--

00:55:55.670 --> 00:55:57.610
and I can just
draw you a picture.

00:55:57.610 --> 00:56:05.110
So you have V0, and then
the neighbors of V0,

00:56:05.110 --> 00:56:13.907
and each of them
have more neighbors.

00:56:16.829 --> 00:56:18.780
Like that.

00:56:18.780 --> 00:56:26.270
So I'm calling V0
this stuff, big V0.

00:56:26.270 --> 00:56:31.470
And then big V1, V
sub 2, and so on.

00:56:34.900 --> 00:56:39.130
So I'm going to define a
vector, which I'll eventually

00:56:39.130 --> 00:56:41.560
make into z, by
telling you what is

00:56:41.560 --> 00:56:48.440
the value of this vector
on each of these vertices.

00:56:48.440 --> 00:56:53.920
I will do this by
setting very explicitly--

00:56:53.920 --> 00:57:03.660
so set x to be a
vector with value

00:57:03.660 --> 00:57:14.820
x sub u to be wi, where wi
is d minus 1 raised to power

00:57:14.820 --> 00:57:23.740
minus i over 2 whenever u
lies in set big V sub i.

00:57:23.740 --> 00:57:27.580
So u is distance exactly i from
V. I set it to this number.

00:57:27.580 --> 00:57:33.630
So notice that they decrease
as you get further away from V.

00:57:33.630 --> 00:57:41.436
And I do this for all
distances less than r.

00:57:44.760 --> 00:57:47.670
So this is my x vector.

00:57:47.670 --> 00:57:53.860
And I set all the other
vect-- all the other corners

00:57:53.860 --> 00:58:02.050
to be 0 if the distance
between u and V is at least r.

00:58:04.990 --> 00:58:06.690
So that gives you this vector.

00:58:06.690 --> 00:58:10.590
And I would like to compute
that quotient over there

00:58:10.590 --> 00:58:12.450
for this vector.

00:58:12.450 --> 00:58:22.760
And I claim that this
quotient here is at least

00:58:22.760 --> 00:58:24.060
the following quantity.

00:58:33.970 --> 00:58:36.577
But this is a computation,
so let's just do it.

00:58:36.577 --> 00:58:37.410
So why is this true?

00:58:40.570 --> 00:58:45.210
Well, if you compute
the norm of x--

00:58:45.210 --> 00:58:47.460
so I'm just taking
the sum of the squares

00:58:47.460 --> 00:58:49.530
of these coordinates.

00:58:49.530 --> 00:58:57.130
Well, that comes from
adding up these values.

00:58:57.130 --> 00:59:01.370
So for each element in
the i-th neighborhood.

00:59:01.370 --> 00:59:05.010
So I have wi squared.

00:59:05.010 --> 00:59:14.620
And if I look at that quantity
up there, so what is this?

00:59:14.620 --> 00:59:17.120
A is the adjacency matrix.

00:59:17.120 --> 00:59:21.740
So over here, A is
the adjacency matrix.

00:59:25.460 --> 00:59:35.170
So this quantity, I can write
it as a sum over all vertices u.

00:59:35.170 --> 00:59:39.010
And I look at x sub
u, and now I sum again

00:59:39.010 --> 00:59:50.600
over all neighbors of u,
and consider x sub u prime.

00:59:50.600 --> 00:59:52.190
It's that sum there.

00:59:52.190 --> 00:59:57.570
But this sum, I have some
control over, because it is--

00:59:57.570 --> 01:00:00.000
so what's happening here?

01:00:00.000 --> 01:00:04.250
I claim it has at least
the following quantity.

01:00:04.250 --> 01:00:05.660
Consider where u is.

01:00:05.660 --> 01:00:07.660
So u could be--

01:00:07.660 --> 01:00:11.000
I mean, it's only
nonzero if u lies

01:00:11.000 --> 01:00:13.031
in the r minus 1th neighborhood.

01:00:16.800 --> 01:00:21.480
So in that neighborhood, I
have V sub i possible choices

01:00:21.480 --> 01:00:24.960
for the vertex u.

01:00:24.960 --> 01:00:31.990
For that choice, this
x sub u is w sub i.

01:00:31.990 --> 01:00:35.230
But what about
all its neighbors?

01:00:35.230 --> 01:00:38.280
So it could have
neighbors, well,

01:00:38.280 --> 01:00:40.400
in the same set going left.

01:00:40.400 --> 01:00:44.500
But there's-- so there's
one neighbor going left,

01:00:44.500 --> 01:00:47.380
and all the other
neighbors are--

01:00:47.380 --> 01:00:50.080
maybe it's in the same set,
maybe it's in the next set.

01:00:50.080 --> 01:00:53.110
But, in any case, I have
the following inequality.

01:00:53.110 --> 01:00:57.730
There's one neighbor
in the same--

01:00:57.730 --> 01:01:01.660
in the left, if you look
at that picture just now.

01:01:01.660 --> 01:01:04.900
And then all the
remaining neighbors

01:01:04.900 --> 01:01:12.640
have x sub u primes at
least w sub i plus 1,

01:01:12.640 --> 01:01:14.860
because these weights
are decreasing.

01:01:14.860 --> 01:01:17.840
So I can-- the worst
case, so to speak,

01:01:17.840 --> 01:01:23.390
is if you-- all the neighbors
point to the next set.

01:01:23.390 --> 01:01:24.760
So I had that inequality there.

01:01:28.600 --> 01:01:30.990
There's an issue.

01:01:30.990 --> 01:01:42.900
Because if you go to
the very last set,

01:01:42.900 --> 01:01:45.000
if you go to the very
last set and think

01:01:45.000 --> 01:01:49.410
about what happens, when i
in in that very last set,

01:01:49.410 --> 01:01:55.260
I'm overcounting neighbors
that no longer has weights.

01:01:55.260 --> 01:01:58.260
So I need to take them out.

01:01:58.260 --> 01:02:02.815
So I should subtract
d minus 1 times--

01:02:05.680 --> 01:02:09.590
and so this is the
maximum possible weight

01:02:09.590 --> 01:02:12.010
sum I could have--

01:02:12.010 --> 01:02:15.360
maximum possible overcount.

01:02:15.360 --> 01:02:18.430
So each product here has t
minus 1 neighbors at most.

01:02:18.430 --> 01:02:19.330
All right.

01:02:19.330 --> 01:02:21.740
So this is-- should be
pretty straightforward if you

01:02:21.740 --> 01:02:23.000
do the counting correctly.

01:02:23.000 --> 01:02:26.960
But now let's plug in
what these weights are.

01:02:26.960 --> 01:02:30.620
And you'll find that this
sum here, this quantity,

01:02:30.620 --> 01:02:32.780
is equal to-- so
the key point here

01:02:32.780 --> 01:02:36.890
is that this thing simplifies
very nicely if you consider

01:02:36.890 --> 01:02:39.500
what this is.

01:02:39.500 --> 01:02:42.680
So what ends up
happening is that you

01:02:42.680 --> 01:02:45.250
get this extra factor
of 2 root d minus 1.

01:02:45.250 --> 01:03:01.160
And then the sum minus
1/2 of V sub [INAUDIBLE]..

01:03:01.160 --> 01:03:05.320
It's pretty
straightforward computation

01:03:05.320 --> 01:03:09.490
using the specific
weights that we have.

01:03:09.490 --> 01:03:17.490
And one more thing is
that notice that this--

01:03:17.490 --> 01:03:22.250
so notice that the sizes of
each neighborhood cannot expand

01:03:22.250 --> 01:03:30.050
by more than a factor of
d minus 1, because, well,

01:03:30.050 --> 01:03:34.280
you only have d minus 1 outward
edges going forward at each

01:03:34.280 --> 01:03:36.390
step.

01:03:36.390 --> 01:03:43.260
And, as a result, I
can bound this guy.

01:03:43.260 --> 01:03:48.510
And so what you find is
that this whole thing here

01:03:48.510 --> 01:03:53.826
is that least 2
times root d minus 1.

01:03:53.826 --> 01:03:56.520
The main term is the sum.

01:04:03.500 --> 01:04:08.750
And this here is less than
each individual summand.

01:04:08.750 --> 01:04:12.440
So I can do 1 minus 1 over 2r.

01:04:19.150 --> 01:04:21.930
Putting these two together,
you find the claim.

01:04:28.835 --> 01:04:30.400
All right.

01:04:30.400 --> 01:04:33.730
So I've exhibited
this vector x, which

01:04:33.730 --> 01:04:36.160
has that quotient property.

01:04:36.160 --> 01:04:39.840
But that's not quite enough,
because we need a vector--

01:04:39.840 --> 01:04:42.100
so it's called z up here--

01:04:42.100 --> 01:04:46.530
that is orthogonal to
the all 1's vector.

01:04:46.530 --> 01:04:51.940
And that you can do, because
if the number of vertices

01:04:51.940 --> 01:04:56.320
is quite a bit larger than--

01:04:59.230 --> 01:05:11.080
compared to the degree, then I
claim that there exists u and v

01:05:11.080 --> 01:05:12.640
vectors--

01:05:12.640 --> 01:05:18.520
vertices that are at
distance at least 2r.

01:05:22.480 --> 01:05:25.340
So if I let--

01:05:30.150 --> 01:05:32.220
this is the size of this tree.

01:05:32.220 --> 01:05:35.520
So if you have--

01:05:35.520 --> 01:05:37.950
everything is
within distance r--

01:05:37.950 --> 01:05:44.738
distance 2r from a vertex, then
they all lie on this tree edge.

01:05:44.738 --> 01:05:46.780
If you count the number
of vertices in that tree,

01:05:46.780 --> 01:05:49.840
it's what I have-- the
sum I've written here.

01:05:49.840 --> 01:05:55.820
So if I consider
these two vectors--

01:05:55.820 --> 01:06:05.250
so be-- so x be the vector
obtained above, which is,

01:06:05.250 --> 01:06:06.440
in some sense--

01:06:06.440 --> 01:06:13.780
and I'm being somewhat
informal here-- centered at v.

01:06:13.780 --> 01:06:19.280
And if I let y be the vector but
I center it now at the vector--

01:06:19.280 --> 01:06:27.020
at u, then I claim that,
essentially, x and y

01:06:27.020 --> 01:06:32.830
are supported on disjoint
vertex sets that have

01:06:32.830 --> 01:06:35.950
no edges even between them.

01:06:35.950 --> 01:06:40.020
So, in particular,
this inner product--

01:06:40.020 --> 01:06:43.870
this bilinear form-- not inner
product but this bilinear

01:06:43.870 --> 01:06:45.100
form--

01:06:45.100 --> 01:06:59.120
is equal to 0, since no edge
between the supports of x

01:06:59.120 --> 01:07:00.056
and y.

01:07:04.550 --> 01:07:09.700
So now I have two vectors
that do not interact,

01:07:09.700 --> 01:07:12.760
but both have this
nice property above.

01:07:12.760 --> 01:07:14.998
And now I can take a
linear combination.

01:07:18.590 --> 01:07:22.030
Let me choose a constant c--

01:07:22.030 --> 01:07:24.860
so it's a real constant--

01:07:24.860 --> 01:07:31.315
such that this z equal
to x minus cy has--

01:07:37.035 --> 01:07:39.110
and I can choose this constant.

01:07:39.110 --> 01:07:41.080
So x and y are both
non-negative entries.

01:07:41.080 --> 01:07:43.840
They're both nonzero, and I
can choose this constant c

01:07:43.840 --> 01:07:45.900
so that it is--

01:07:45.900 --> 01:07:48.840
this z is orthogonal
to the all 1's vector.

01:07:48.840 --> 01:07:53.570
And I now I have this
extra property I want.

01:07:53.570 --> 01:07:55.830
But what about the
inner products?

01:07:55.830 --> 01:08:01.140
Well, these two vectors, x and
y, they do not interact at all.

01:08:01.140 --> 01:08:05.270
So their inner products
split just fine,

01:08:05.270 --> 01:08:08.450
and the bilinear form
splits just fine.

01:08:19.020 --> 01:08:30.810
So you have this inequality
here, as desired.

01:08:30.810 --> 01:08:37.479
And r, notice that
I can take r going

01:08:37.479 --> 01:08:43.740
to infinity as n going to
infinity, because d is fixed.

01:08:43.740 --> 01:08:47.399
So if n goes to infinity, then
r can go to infinity, roughly

01:08:47.399 --> 01:08:48.661
a logarithmic n.

01:08:52.990 --> 01:08:55.980
And that proves the
Alon-Boppana bound.

01:08:55.980 --> 01:08:58.540
And just to recap,
to prove this bound,

01:08:58.540 --> 01:09:01.990
we needed to exhibit by the
Courant-Fischer some vector

01:09:01.990 --> 01:09:03.355
with a nice--

01:09:03.355 --> 01:09:06.760
this quotient such that
this quotient is large.

01:09:06.760 --> 01:09:11.350
And we exhibit this quotient
by constructing the vector

01:09:11.350 --> 01:09:15.399
explicitly around the vertex and
finding two such vertices that

01:09:15.399 --> 01:09:17.689
are far away from constructing
these two vectors,

01:09:17.689 --> 01:09:19.660
taking the appropriate
linear combination

01:09:19.660 --> 01:09:22.510
so that the final vector is
orthogonal to the unit vector,

01:09:22.510 --> 01:09:26.270
to the all 1's vector,
and then showing

01:09:26.270 --> 01:09:31.220
that the corresponding bilinear
form has-- is large enough.

01:09:34.520 --> 01:09:35.396
Any questions?

01:09:38.140 --> 01:09:42.370
I want to show you a different
proof which gives you

01:09:42.370 --> 01:09:47.229
a slightly worse result, but
the proof is conceptually nice.

01:09:47.229 --> 01:09:51.569
So let me give you
a second proof which

01:09:51.569 --> 01:09:54.864
is slightly weakening.

01:10:00.298 --> 01:10:03.290
And just that we'll show--

01:10:03.290 --> 01:10:06.076
so we'll show that--

01:10:06.076 --> 01:10:09.930
so the earlier proof showed
that lambda 2 is quite large.

01:10:09.930 --> 01:10:17.100
But, next, we'll show that the
max of lambda 2 and the lambda

01:10:17.100 --> 01:10:19.800
n is large.

01:10:24.500 --> 01:10:28.290
So not that the second
largest eigenvalue is large,

01:10:28.290 --> 01:10:30.550
but the second largest
eigenvalue in absolute value

01:10:30.550 --> 01:10:31.050
is large.

01:10:31.050 --> 01:10:34.290
So it's slightly weaker, but,
for all intents and purposes,

01:10:34.290 --> 01:10:36.710
it's the same spirit.

01:10:36.710 --> 01:10:39.850
So I'll show this one here.

01:10:39.850 --> 01:10:42.040
And this is a nice
illustration of what's

01:10:42.040 --> 01:10:48.940
called a trace method,
sometimes also a moment method.

01:10:56.570 --> 01:10:59.090
Here's the idea.

01:10:59.090 --> 01:11:03.610
As we saw in the proof relating
the quasi-randomness of C4

01:11:03.610 --> 01:11:06.380
and eigenvalues,
well, C4's are--

01:11:06.380 --> 01:11:09.320
eigenvalues are related
to counting closed walks

01:11:09.320 --> 01:11:10.430
in a graph.

01:11:10.430 --> 01:11:14.540
And so we'll use that counting
closed walks in a graph.

01:11:14.540 --> 01:11:21.110
And, specifically, the
2k-th moment of the spectrum

01:11:21.110 --> 01:11:28.610
is equal to the trace of
the 2k-th power, which

01:11:28.610 --> 01:11:34.790
counts the number of closed
walks of length exactly 2k.

01:11:49.680 --> 01:11:53.700
So to lower bound
the left-hand side,

01:11:53.700 --> 01:11:56.880
we want to lower-bound
the right-hand side.

01:11:56.880 --> 01:12:02.830
So let's consider closed walks
starting at a fixed vertex.

01:12:02.830 --> 01:12:10.950
So the number of
closed walks of length

01:12:10.950 --> 01:12:25.390
exactly 2k starting at
a fixed vertex v. Here

01:12:25.390 --> 01:12:26.800
we're in a d regular graph.

01:12:26.800 --> 01:12:32.730
So here we are in
a d regular graph.

01:12:32.730 --> 01:12:36.530
I claim, whatever this number
is-- it maybe different

01:12:36.530 --> 01:12:42.860
for each d-- it is at least
the same quantity if I

01:12:42.860 --> 01:12:46.070
do this walk in an
infinite d regular tree.

01:12:53.280 --> 01:12:55.710
So infinite d
regular tree is what?

01:12:58.530 --> 01:13:00.390
This is an infinite
d regulator tree.

01:13:05.659 --> 01:13:09.300
We just start with the
vertex, and go out d regular.

01:13:09.300 --> 01:13:12.880
So why is this true?

01:13:12.880 --> 01:13:14.665
So think about how you walk.

01:13:14.665 --> 01:13:15.820
So let me just explain.

01:13:15.820 --> 01:13:17.868
This is, I think,
pretty easy once you

01:13:17.868 --> 01:13:18.910
see things the right way.

01:13:18.910 --> 01:13:22.360
So start with a vertex v.
Think about how you walk.

01:13:22.360 --> 01:13:26.140
And whatever way you
can walk, well, you

01:13:26.140 --> 01:13:31.740
can walk the same way on
the infinite d regular tree.

01:13:31.740 --> 01:13:34.020
Well, I mean, sorry.

01:13:34.020 --> 01:13:37.560
Whatever walk you can do
an infinite d regular tree,

01:13:37.560 --> 01:13:43.110
if you label the first vertex,
the first edge, second edge,

01:13:43.110 --> 01:13:44.730
if you do a
corresponding labeling

01:13:44.730 --> 01:13:46.350
on your original
graph, you can do

01:13:46.350 --> 01:13:49.440
that walk on your
original graph.

01:13:49.440 --> 01:13:52.020
Although the original graph
may have some additional walks,

01:13:52.020 --> 01:13:54.110
namely things that
involve cycles,

01:13:54.110 --> 01:13:56.575
that are not available
on your tree.

01:13:56.575 --> 01:13:58.200
But, certainly, every
walk, you can do.

01:13:58.200 --> 01:14:01.400
Every closed walk
you can do on a tree,

01:14:01.400 --> 01:14:06.150
you can do the same
walk on your graph.

01:14:06.150 --> 01:14:07.530
So you can make
this more formal.

01:14:07.530 --> 01:14:09.447
So you can write down a
bijection or injection

01:14:09.447 --> 01:14:12.192
to make this more formal, but
it should be fairly convincing

01:14:12.192 --> 01:14:13.400
that this inequality is true.

01:14:16.940 --> 01:14:18.580
But this is just a number.

01:14:18.580 --> 01:14:21.740
So this is a number of 2k walks
in a d regular tree starting

01:14:21.740 --> 01:14:22.410
on the vertex.

01:14:22.410 --> 01:14:23.993
And this number has
been well studied,

01:14:23.993 --> 01:14:27.600
and we don't need to
know the precise number.

01:14:27.600 --> 01:14:29.430
We just need to know
some good lower bound.

01:14:29.430 --> 01:14:31.980
And here is one lower bound,
which is that there's at least

01:14:31.980 --> 01:14:34.080
a Catalan number, the
k-th Catalan number,

01:14:34.080 --> 01:14:43.230
times d minus 1 to the k, where
C sub k is the k-th Catalan

01:14:43.230 --> 01:14:45.867
number, which is equal to--

01:14:50.176 --> 01:14:53.860
so k 2k choose k
divided by k plus 1.

01:14:53.860 --> 01:14:55.750
So let me remind
you what this is.

01:14:55.750 --> 01:14:58.980
A wonder, has many
combinatorial interpretations,

01:14:58.980 --> 01:15:02.140
and it's a fun exercise to
do bijections between them.

01:15:02.140 --> 01:15:06.870
But, in particular,
C3 is the equal to 5,

01:15:06.870 --> 01:15:13.070
which counts the number of
ups and down walks of length

01:15:13.070 --> 01:15:17.423
6 that never dip below
the horizontal line

01:15:17.423 --> 01:15:18.090
where you start.

01:15:23.780 --> 01:15:27.310
So, then, this
corresponds to going away

01:15:27.310 --> 01:15:32.200
from the root versus
coming back to the root.

01:15:32.200 --> 01:15:34.380
Soon you have at
least that many ways.

01:15:34.380 --> 01:15:37.210
And when you are moving
away from the root,

01:15:37.210 --> 01:15:42.196
you have d minus 1 choices
on which branch to go to.

01:15:42.196 --> 01:15:43.172
OK, good.

01:15:49.030 --> 01:15:56.460
Given that, the right-hand
side is at least, then,

01:15:56.460 --> 01:16:02.700
n, the number of vertices,
times the quantity above related

01:16:02.700 --> 01:16:03.780
to Catalan numbers.

01:16:11.310 --> 01:16:14.630
On the other hand, the
left-hand side is at most--

01:16:14.630 --> 01:16:16.730
here we're using that
2k is an even number--

01:16:16.730 --> 01:16:23.220
is at most d to the 2k plus all
the other eigenvalues that are

01:16:23.220 --> 01:16:25.740
most lambda in absolute value.

01:16:25.740 --> 01:16:28.920
So let me call this
quantity lambda.

01:16:32.570 --> 01:16:39.310
Rearranging this inequality,
we find that lambda to the 2k

01:16:39.310 --> 01:16:46.430
is at least this number here.

01:16:56.390 --> 01:17:00.516
Just here, I'm changing
n minus 1 to n.

01:17:00.516 --> 01:17:01.930
So we have that.

01:17:01.930 --> 01:17:04.180
And now what can we do?

01:17:04.180 --> 01:17:10.900
We let n go to infinity and k
go to infinity slowly enough.

01:17:10.900 --> 01:17:16.690
So if k goes to
infinity and n goes to--

01:17:16.690 --> 01:17:21.160
so k goes to infinity with
n, but not too quickly.

01:17:21.160 --> 01:17:25.260
But k is little of log n.

01:17:25.260 --> 01:17:28.450
And we find that
this quantity here

01:17:28.450 --> 01:17:30.550
is essentially 2 to the k--

01:17:33.784 --> 01:17:34.730
2 to the 2k.

01:17:37.250 --> 01:17:40.410
And this guy here is little o1.

01:17:40.410 --> 01:17:45.850
So lambda is at least 2 root
d minus 1 minus little o1.

01:17:51.050 --> 01:17:53.570
That proves, essentially,
the Alon-Boppana bound,

01:17:53.570 --> 01:17:56.990
although a small
weakening because we are--

01:17:56.990 --> 01:17:59.780
this big eigenvalue,
you might find

01:17:59.780 --> 01:18:03.230
might actually be very negative
instead of very positive.

01:18:03.230 --> 01:18:03.830
But that's OK.

01:18:03.830 --> 01:18:09.200
For applications, this
is not such a big deal.

01:18:09.200 --> 01:18:10.790
These are two different proofs.

01:18:10.790 --> 01:18:13.400
And now, we think about, are
they really the same proof?

01:18:13.400 --> 01:18:14.930
Are they different proofs?

01:18:14.930 --> 01:18:16.800
Are they related to each other?

01:18:16.800 --> 01:18:18.450
So it's worth thinking about.

01:18:18.450 --> 01:18:20.750
They look very
different, but how

01:18:20.750 --> 01:18:22.500
are they related to each other?

01:18:22.500 --> 01:18:24.510
And one final remark.

01:18:24.510 --> 01:18:26.730
You already saw two
different proofs as to--

01:18:26.730 --> 01:18:28.340
I mean, that shows
you this number,

01:18:28.340 --> 01:18:30.630
and you see where this
number comes from.

01:18:30.630 --> 01:18:32.750
And let me just offer
one final remark on where

01:18:32.750 --> 01:18:35.330
that number really comes from.

01:18:35.330 --> 01:18:39.580
And it really comes from
this infinite d regular tree.

01:18:42.630 --> 01:18:45.300
So it turns out that
2 root d minus 1

01:18:45.300 --> 01:18:55.320
exactly is the spectral radius
of the infinite d regular tree.

01:18:58.120 --> 01:19:00.400
And that is the
reason, in some sense,

01:19:00.400 --> 01:19:03.790
that this is the correct number
occurring Alon-Boppana bound.

01:19:07.380 --> 01:19:11.310
This is-- if you've seen
things like algebraic topology

01:19:11.310 --> 01:19:16.040
or topology, this is a universal
cover for d regular graphs.

01:19:16.040 --> 01:19:20.330
So I won't talk more about it,
but just some general remarks,

01:19:20.330 --> 01:19:22.310
and you already saw
two different proofs.

01:19:22.310 --> 01:19:24.540
So beginning of next time,
I want to wrap this up

01:19:24.540 --> 01:19:25.740
and to show you--

01:19:25.740 --> 01:19:28.590
to explain some--
what we know about

01:19:28.590 --> 01:19:32.540
are there graphs for
which this bound is tight?

01:19:32.540 --> 01:19:36.240
And the answer is yes, and there
are lots of major open problems

01:19:36.240 --> 01:19:39.403
as well related to
what happens there.

01:19:39.403 --> 01:19:40.820
And then, after
that, I would like

01:19:40.820 --> 01:19:42.830
to start talking
about graph limits.

01:19:42.830 --> 01:19:45.760
So that's the next
chapter of this course.

01:19:45.760 --> 01:19:47.610
OK, good.