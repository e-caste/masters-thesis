WEBVTT

00:00:01.069 --> 00:00:05.050
In the next section we're going to start our
discussion on how to actually engineer the

00:00:05.050 --> 00:00:09.660
bit encodings we'll use in our circuitry,
but first we'll need a way to evaluate the

00:00:09.660 --> 00:00:12.120
efficacy of an encoding.

00:00:12.120 --> 00:00:16.980
The entropy of a random variable is average
amount of information received when learning

00:00:16.980 --> 00:00:19.500
the value of the random variable.

00:00:19.500 --> 00:00:23.439
The mathematician's name for "average" is
"expected value"; that's what the capital

00:00:23.439 --> 00:00:25.050
E means.

00:00:25.050 --> 00:00:30.560
We compute the average in the obvious way:
we take the weighted sum, where the amount

00:00:30.560 --> 00:00:37.329
of information received when learning of particular
choice i -- that's the log-base-2 of 1/p_i

00:00:37.329 --> 00:00:41.160
-- is weighted by the probability of that
choice actually happening.

00:00:41.160 --> 00:00:43.489
Here's an example.

00:00:43.489 --> 00:00:49.149
We have a random variable that can take on
one of four values: A, B, C or D.

00:00:49.149 --> 00:00:53.739
The probabilities of each choice are shown
in the table, along with the associated information

00:00:53.739 --> 00:00:56.280
content.

00:00:56.280 --> 00:01:00.949
Now we'll compute the entropy using the probabilities
and information content.

00:01:00.949 --> 00:01:06.770
So we have the probability of A (1/3) times
its associated information content (1.58 bits),

00:01:06.770 --> 00:01:13.360
plus the probability of B times its associated
information content, and so on.

00:01:13.360 --> 00:01:17.079
The result is 1.626 bits.

00:01:17.079 --> 00:01:21.330
This is telling us that a clever encoding
scheme should be able to do better than simply

00:01:21.330 --> 00:01:27.100
encoding each symbol using 2 bits to represent
which of the four possible values is next.

00:01:27.100 --> 00:01:29.020
Food for thought!

00:01:29.020 --> 00:01:33.399
We'll discuss this further in the third section
of this chapter.

00:01:33.399 --> 00:01:37.820
So, what is the entropy telling us?

00:01:37.820 --> 00:01:42.490
Suppose we have a sequence of data describing
a sequence of values of the random variable

00:01:42.490 --> 00:01:43.490
X.

00:01:43.490 --> 00:01:49.860
If, on the average, we use less than H(X)
bits to transmit each piece of information

00:01:49.860 --> 00:01:54.340
in the sequence, we will not be sending enough
information to resolve the uncertainty about

00:01:54.340 --> 00:01:55.840
the values.

00:01:55.840 --> 00:02:01.649
In other words, the entropy is a lower bound
on the number of bits we need to transmit.

00:02:01.649 --> 00:02:06.579
Getting less than this number of bits wouldn't
be good if the goal was to unambiguously describe

00:02:06.579 --> 00:02:10.970
the sequence of values -- we'd have failed
at our job!

00:02:10.970 --> 00:02:16.140
On the other hand, if we send, on the average,
more than H(X) bits to describe the sequence

00:02:16.140 --> 00:02:20.970
of values, we will not be making the most
effective use of our resources, since the

00:02:20.970 --> 00:02:25.050
same information might have been able to be
represented with fewer bits.

00:02:25.050 --> 00:02:29.560
This is okay, but perhaps with some insights
we could do better.

00:02:29.560 --> 00:02:35.970
Finally, if we send, on the average, exactly
H(X) bits, then we'd have the perfect encoding.

00:02:35.970 --> 00:02:40.660
Alas, perfection is, as always, a tough goal,
so most of the time we'll have to settle for

00:02:40.660 --> 00:02:42.040
getting close.

00:02:42.040 --> 00:02:48.300
In the final set of exercises for this section,
try computing the entropy for various scenarios.s 0*2^11 plus 1*2^10
plus 1*2^9, and so on.

00:03:02.790 --> 00:03:08.870
Keeping only the non-zero terms and expanding
the powers-of-two gives us the sum 1024 +

00:03:08.870 --> 00:03:22.150
512 + 256 + 128 + 64 + 16 which, expressed
in base-10, sums to the number 2000.

00:03:22.150 --> 00:03:27.140
With t3.100
So the information received
is log-base-2 of 2/1,

00:02:43.100 --> 00:02:45.780
or a single bit.

00:02:45.780 --> 00:02:47.330
This makes sense:
it would take us

00:02:47.330 --> 00:02:49.880
one bit to encode which
of the two possibilities

00:02:49.880 --> 00:02:54.920
actually happened, say, "1"
for heads and "0" for tails.

00:02:54.920 --> 00:02:57.160
Reviewing the example
from the previous slide:

00:02:57.160 --> 00:02:59.970
learning that a card drawn
from a fresh deck is a heart

00:02:59.970 --> 00:03:05.590
gives us log-base-2 of 52/13,
or 2 bits of information.

00:03:05.590 --> 00:03:07.290
Again this makes
sense: it would take us

00:03:07.290 --> 00:03:10.200
two bits to encode which
of four possible card suits

00:03:10.200 --> 00:03:12.410
had turned up.

00:03:12.410 --> 00:03:14.190
Finally consider
what information

00:03:14.190 --> 00:03:18.220
we get from rolling two
dice, one red and one green.

00:03:18.220 --> 00:03:23.200
Each die has six faces, so there
are 36 possible combinations.

00:03:23.200 --> 00:03:25.580
Once we learn the exact
outcome of the roll,

00:03:25.580 --> 00:03:30.700
we've received log-base-2
of 36/1 or 5.17 bits

00:03:30.700 --> 00:03:32.320
of information.

00:03:32.320 --> 00:03:32.960
Hmm.

00:03:32.960 --> 00:03:35.470
What do those
fractional bits mean?

00:03:35.470 --> 00:03:38.740
Our circuitry will only
deal with whole bits!

00:03:38.740 --> 00:03:42.360
So to encode a single outcome,
we'd need to use six bits.

00:03:42.360 --> 00:03:44.670
But suppose we wanted to
record the outcome of 10

00:03:44.670 --> 00:03:46.550
successive rolls.

00:03:46.550 --> 00:03:50.760
At 6 bits per roll, we would
need a total of 60 bits.

00:03:50.760 --> 00:03:52.690
What this formula
is telling us is

00:03:52.690 --> 00:03:55.720
that we would need
not 60 bits, but only

00:03:55.720 --> 00:03:59.620
52 bits to unambiguously
encode the results.

00:03:59.620 --> 00:04:01.560
Whether we can come
with an encoding that

00:04:01.560 --> 00:04:04.310
achieves this lower bound
is an interesting question,

00:04:04.310 --> 00:04:07.850
which we will take up
later in the chapter.

00:04:07.850 --> 00:04:10.850
To wrap up, let's return
to our initial example.

00:04:10.850 --> 00:04:13.430
Here's table showing the
different choices for the data

00:04:13.430 --> 00:04:15.630
received, along
with the probability

00:04:15.630 --> 00:04:19.662
of that event and the
computed information content.

00:04:19.662 --> 00:04:21.120
We've already talked
about learning

00:04:21.120 --> 00:04:22.560
that the card was a heart.

00:04:22.560 --> 00:04:26.340
The probability of this event
is 13/52 with an information

00:04:26.340 --> 00:04:28.820
content of 2 bits.

00:04:28.820 --> 00:04:31.250
Learning that a card is
not the Ace of spades

00:04:31.250 --> 00:04:33.870
is quite likely, since
there's only one chance in 52

00:04:33.870 --> 00:04:36.600
that it is the Ace of spades.

00:04:36.600 --> 00:04:40.930
So we only get a small amount of
information from this event --

00:04:40.930 --> 00:04:42.790
.028 bits.

00:04:42.790 --> 00:04:44.990
There twelve face
cards in a card deck,

00:04:44.990 --> 00:04:47.360
so the probability of
this event is 12/52

00:04:47.360 --> 00:04:52.290
and we would receive 2.115 bits.

00:04:52.290 --> 00:04:55.040
A bit more information than
learning about the card's suit

00:04:55.040 --> 00:04:59.200
since there's slightly
less residual uncertainty.

00:04:59.200 --> 00:05:01.880
Finally, we get the most
information when all

00:05:01.880 --> 00:05:04.910
uncertainty is eliminated
-- a bit more than 5.7 bits.

00:05:04.910 --> 00:05:11.120
The results line up nicely with
our and Mr. Blue's intuition:

00:05:11.120 --> 00:05:14.310
the more uncertainty is
resolved, the more information

00:05:14.310 --> 00:05:16.090
we have received.

00:05:16.090 --> 00:05:19.040
Now try your hand at computing
the information for a few

00:05:19.040 --> 00:05:22.070
more examples in the
following exercises.