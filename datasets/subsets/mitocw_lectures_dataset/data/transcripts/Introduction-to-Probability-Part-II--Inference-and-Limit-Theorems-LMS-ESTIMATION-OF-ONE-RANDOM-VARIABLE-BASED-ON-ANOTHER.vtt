WEBVTT

00:00:00.840 --> 00:00:05.070
After our warm-up, we can
now come to the real problem.

00:00:05.070 --> 00:00:07.320
We have, again, a
random variable Theta

00:00:07.320 --> 00:00:09.620
with a known prior distribution.

00:00:09.620 --> 00:00:12.280
And we're interested
in a point estimate.

00:00:12.280 --> 00:00:13.700
What will be
different this time,

00:00:13.700 --> 00:00:16.820
however, is that we now
have an observation.

00:00:16.820 --> 00:00:20.290
And we also have a model
of that observation

00:00:20.290 --> 00:00:22.120
as a conditional
distribution given

00:00:22.120 --> 00:00:24.220
the value of the true parameter.

00:00:24.220 --> 00:00:27.885
We observe a value of
that random variable.

00:00:27.885 --> 00:00:29.410
That value is little x.

00:00:29.410 --> 00:00:31.650
And on the basis
of that value, we

00:00:31.650 --> 00:00:33.910
would like to now
come up with a point

00:00:33.910 --> 00:00:38.140
estimate of the unknown
random variable Theta.

00:00:38.140 --> 00:00:39.380
How do we proceed?

00:00:39.380 --> 00:00:41.640
We can, of course,
use the Bayes rule.

00:00:41.640 --> 00:00:44.510
And the Bayes rule
is going to give us

00:00:44.510 --> 00:00:49.250
a distribution for the
unknown random variable given

00:00:49.250 --> 00:00:52.440
the observation that
we have obtained.

00:00:52.440 --> 00:00:55.850
And that distribution could
be discrete or continuous.

00:00:55.850 --> 00:01:01.010
Let me just plot something
as if it's continuous.

00:01:01.010 --> 00:01:03.150
And now that we have the
posterior distribution

00:01:03.150 --> 00:01:06.610
of Theta, we would like to
come up with a point estimate.

00:01:06.610 --> 00:01:08.160
How do we do it?

00:01:08.160 --> 00:01:11.039
Remember our earlier conclusion.

00:01:11.039 --> 00:01:13.320
If we do not have
any observations,

00:01:13.320 --> 00:01:16.190
if we live in a universe where
we have a distribution of Theta

00:01:16.190 --> 00:01:18.730
and we want a point
estimate, the optimal,

00:01:18.730 --> 00:01:21.840
the one that minimizes
the mean squared error,

00:01:21.840 --> 00:01:24.860
is the expected value
of our random variable.

00:01:24.860 --> 00:01:26.960
But now we live in a
different universe,

00:01:26.960 --> 00:01:29.695
in a universe where we have
a conditional distribution

00:01:29.695 --> 00:01:31.539
of Theta.

00:01:31.539 --> 00:01:37.220
We want to minimize the
conditional mean squared error,

00:01:37.220 --> 00:01:39.450
because this is the
mean squared error that

00:01:39.450 --> 00:01:42.850
applies to this conditional
universe in which we have

00:01:42.850 --> 00:01:45.690
obtained a particular
observation.

00:01:45.690 --> 00:01:49.570
What is going to be the
result of this minimization?

00:01:49.570 --> 00:01:52.610
Well, this is a problem that's
identical to the problem

00:01:52.610 --> 00:01:56.430
of minimizing this quantity,
except that now this problem is

00:01:56.430 --> 00:01:58.840
posed in a conditional universe.

00:01:58.840 --> 00:02:00.690
So we just follow
the same steps.

00:02:00.690 --> 00:02:03.220
And obtain the same
solution, the solution

00:02:03.220 --> 00:02:05.030
is going to be
the expected value

00:02:05.030 --> 00:02:07.770
of the unknown random
variable, except that now we

00:02:07.770 --> 00:02:09.419
live in a conditional universe.

00:02:09.419 --> 00:02:13.210
And therefore, we should take
the relevant expected value

00:02:13.210 --> 00:02:15.890
which is the conditional
expectation given

00:02:15.890 --> 00:02:20.980
the information that we
have available in our hands.

00:02:20.980 --> 00:02:26.690
So to summarize, what we obtain
is that the optimal estimate

00:02:26.690 --> 00:02:29.600
is the conditional expectation.

00:02:29.600 --> 00:02:32.620
And this is a relation
between numbers.

00:02:32.620 --> 00:02:35.950
But if we want to think about
it more abstractly, we have

00:02:35.950 --> 00:02:40.470
designed an estimator which
is based on a random variable,

00:02:40.470 --> 00:02:44.310
capital X, and calculate
the expected value

00:02:44.310 --> 00:02:47.980
of our random variable that
we're trying to estimate,

00:02:47.980 --> 00:02:52.910
namely Theta, on the basis of X.

00:02:52.910 --> 00:02:56.560
Let us now continue
with some observations.

00:02:56.560 --> 00:02:58.960
Remember that the
expected value of Theta

00:02:58.960 --> 00:03:01.430
minimizes this quantity.

00:03:01.430 --> 00:03:03.910
And we can write
this more explicitly

00:03:03.910 --> 00:03:06.900
in terms of the
following inequality--

00:03:06.900 --> 00:03:12.000
that if we use the expected
value as an estimate,

00:03:12.000 --> 00:03:15.420
the resulting mean
squared error is less than

00:03:15.420 --> 00:03:18.980
or equal to the
mean squared error

00:03:18.980 --> 00:03:21.820
that we would have
obtained if we

00:03:21.820 --> 00:03:25.220
had used any other estimate, c.

00:03:25.220 --> 00:03:30.430
So this is a relation
that's true for all c.

00:03:30.430 --> 00:03:34.170
Now, let us take this
inequality and translate it

00:03:34.170 --> 00:03:36.600
into our more
interesting context where

00:03:36.600 --> 00:03:39.870
we have an
observation available.

00:03:39.870 --> 00:03:42.660
Once more, the
conditional expectation

00:03:42.660 --> 00:03:44.829
minimizes the mean
squared error.

00:03:44.829 --> 00:03:47.570
Let us write out
explicitly what this

00:03:47.570 --> 00:03:51.829
means in a form analogous to
what we wrote down earlier.

00:03:51.829 --> 00:03:54.880
What it means is that
the expected value

00:03:54.880 --> 00:04:00.580
of Theta minus the
estimate, namely

00:04:00.580 --> 00:04:06.336
the conditional
expectation, squared.

00:04:06.336 --> 00:04:10.610
In this conditional
universe in which we live,

00:04:10.610 --> 00:04:17.450
this is less than or equal
to the mean squared error

00:04:17.450 --> 00:04:29.130
that we would have obtained if
we had used any other estimate

00:04:29.130 --> 00:04:31.980
in the place of the
conditional expectation.

00:04:31.980 --> 00:04:36.760
So for any value g of x
that we might have used,

00:04:36.760 --> 00:04:40.260
the error would have
been at least as large.

00:04:40.260 --> 00:04:43.230
Why am I using this
notation g here?

00:04:43.230 --> 00:04:45.850
Let us go back to
the bigger picture.

00:04:45.850 --> 00:04:51.250
What we have is that we are
obtaining a numerical value x.

00:04:51.250 --> 00:04:54.020
We do some processing
to it which

00:04:54.020 --> 00:04:57.120
corresponds to some function g.

00:04:57.120 --> 00:05:01.400
And we come up with
an estimate which

00:05:01.400 --> 00:05:06.550
is a function of the little
x that we have observed.

00:05:06.550 --> 00:05:09.740
So no matter what
estimate we use,

00:05:09.740 --> 00:05:12.540
the mean squared error is
going to be at least as large

00:05:12.540 --> 00:05:14.450
as the mean squared
error that we obtain

00:05:14.450 --> 00:05:17.840
if we use the
conditional expectation.

00:05:17.840 --> 00:05:21.770
Now, let us take this
inequality here and write it

00:05:21.770 --> 00:05:24.290
in a more abstract form.

00:05:24.290 --> 00:05:28.370
Suppose that we have settled
on some particular estimator

00:05:28.370 --> 00:05:31.940
and we want to compare this
estimator with the expected

00:05:31.940 --> 00:05:34.040
value estimator.

00:05:34.040 --> 00:05:36.220
Then we're going to get
the following inequality.

00:05:40.110 --> 00:05:44.580
If we use the
conditional expectation

00:05:44.580 --> 00:05:49.390
as an estimator in a
conditional universe

00:05:49.390 --> 00:05:52.750
where we know the value
of the random variable X,

00:05:52.750 --> 00:05:54.740
the corresponding
mean squared error

00:05:54.740 --> 00:05:59.710
is going to be less than or
equal to the mean squared error

00:05:59.710 --> 00:06:06.630
obtained by the
alternative estimator g.

00:06:06.630 --> 00:06:08.690
What does this inequality say?

00:06:08.690 --> 00:06:12.040
This inequality is simply
an abstract version

00:06:12.040 --> 00:06:13.890
of the previous inequality.

00:06:13.890 --> 00:06:20.520
The previous inequality
is true for all little x.

00:06:20.520 --> 00:06:23.720
Here we have an inequality
between random variables.

00:06:23.720 --> 00:06:28.160
This random variable
here is a random variable

00:06:28.160 --> 00:06:32.500
that takes this specific
numerical value, whenever

00:06:32.500 --> 00:06:35.305
capital X takes
the value little x.

00:06:35.305 --> 00:06:37.780
When X takes the
value little x, we're

00:06:37.780 --> 00:06:39.250
conditioning on this event.

00:06:39.250 --> 00:06:44.370
And when X is equal to
little x, this quantity

00:06:44.370 --> 00:06:47.440
takes on this particular
numerical value.

00:06:47.440 --> 00:06:50.060
And similarly, on
the other side,

00:06:50.060 --> 00:06:52.810
this is a random
variable that takes

00:06:52.810 --> 00:06:55.159
this particular
numerical value whenever

00:06:55.159 --> 00:06:58.680
capital X is equal to little x.

00:06:58.680 --> 00:07:01.860
Now that we have an inequality
between random variables,

00:07:01.860 --> 00:07:05.510
actually between
conditional expectations,

00:07:05.510 --> 00:07:08.770
we can take expectations
of both sides.

00:07:08.770 --> 00:07:11.160
And we use the law of
iterated expectations.

00:07:11.160 --> 00:07:14.000
The expectation of a
conditional expectation

00:07:14.000 --> 00:07:18.680
is an unconditional expectation.

00:07:18.680 --> 00:07:25.090
So we obtain this
as the expected

00:07:25.090 --> 00:07:27.950
value of this quantity.

00:07:27.950 --> 00:07:32.130
And it's less than or
equal to the expected value

00:07:32.130 --> 00:07:34.120
of this quantity
using, again, the law

00:07:34.120 --> 00:07:36.659
of iterated expectations.

00:07:36.659 --> 00:07:39.510
We obtain this relation here.

00:07:43.980 --> 00:07:46.970
And what this
inequality is saying

00:07:46.970 --> 00:07:50.310
is that the overall
mean squared error,

00:07:50.310 --> 00:07:53.800
if we use the conditional
expectation, now

00:07:53.800 --> 00:07:59.540
as an estimator, is less than or
equal to the mean squared error

00:07:59.540 --> 00:08:04.110
that we would obtain if we
had used any other estimator.

00:08:06.900 --> 00:08:11.150
So this inequality refers
to the following picture.

00:08:11.150 --> 00:08:15.000
We obtain an observation
which is a random variable.

00:08:15.000 --> 00:08:18.050
We process that random
variable to come up

00:08:18.050 --> 00:08:23.890
with an estimator which is a
function of the random variable

00:08:23.890 --> 00:08:28.350
that we observe and so is
itself a random variable.

00:08:28.350 --> 00:08:31.570
So when we use this random
variable to estimate Theta,

00:08:31.570 --> 00:08:33.799
we obtain a certain
mean squared error.

00:08:33.799 --> 00:08:37.230
This is going to be at least as
large as the mean squared error

00:08:37.230 --> 00:08:42.179
that we obtain if we use
the conditional expectation

00:08:42.179 --> 00:08:44.049
as our estimator.

00:08:44.049 --> 00:08:48.650
So to summarize, the
conditional expectation

00:08:48.650 --> 00:08:52.190
of Theta, viewed as
a random variable,

00:08:52.190 --> 00:08:57.690
as an estimator, what we call
the LMS estimator of Theta,

00:08:57.690 --> 00:08:59.850
has the property
that it minimizes

00:08:59.850 --> 00:09:04.830
the mean squared error over
all possible alternative

00:09:04.830 --> 00:09:06.350
estimators.

00:09:06.350 --> 00:09:11.420
So if you want to design this
box using some other function

00:09:11.420 --> 00:09:15.000
g, you're going to obtain
a mean squared error that's

00:09:15.000 --> 00:09:18.580
going to be no better than
what you obtain if you were

00:09:18.580 --> 00:09:22.050
to use the conditional
expectation.