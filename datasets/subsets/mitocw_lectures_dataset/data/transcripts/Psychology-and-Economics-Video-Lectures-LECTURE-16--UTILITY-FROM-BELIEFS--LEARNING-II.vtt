WEBVTT

00:00:00.000 --> 00:00:02.465
[SQUEAKING]

00:00:02.465 --> 00:00:04.437
[RUSTLING]

00:00:04.437 --> 00:00:07.395
[CLICKING]

00:00:09.583 --> 00:00:10.750
FRANK SCHILLBACH: All right.

00:00:10.750 --> 00:00:11.792
I'm going to get started.

00:00:11.792 --> 00:00:15.250
Welcome to lecture 16 of 14.13.

00:00:15.250 --> 00:00:17.680
We talked a lot about
utility from beliefs

00:00:17.680 --> 00:00:20.950
and how, in particular,
anticipatory utility,

00:00:20.950 --> 00:00:23.650
utility about thinking
about the future and what

00:00:23.650 --> 00:00:27.940
might happen in the future,
may affect people's utility.

00:00:27.940 --> 00:00:30.040
And then how that
in turn might affect

00:00:30.040 --> 00:00:33.370
how people, A, choose
when to consume

00:00:33.370 --> 00:00:36.370
or when to engage in
certain activities.

00:00:36.370 --> 00:00:39.490
Because anticipatory utility
provides some motivation

00:00:39.490 --> 00:00:41.980
to push things forward,
at least a little bit,

00:00:41.980 --> 00:00:44.930
so people can look forward
to positive events.

00:00:44.930 --> 00:00:47.890
Second, we talked about
information acquisition

00:00:47.890 --> 00:00:52.030
about the idea that people,
if they had derived utility

00:00:52.030 --> 00:00:56.500
from beliefs, that might affect
how much information they

00:00:56.500 --> 00:00:58.240
might want to acquire.

00:00:58.240 --> 00:00:59.950
In particular, we
talked about the idea

00:00:59.950 --> 00:01:03.100
that when you think about
potentially negative

00:01:03.100 --> 00:01:06.855
information that
you might receive--

00:01:06.855 --> 00:01:08.980
in particular, we talked
about Huntington's disease

00:01:08.980 --> 00:01:12.790
where people had a negative
health information--

00:01:12.790 --> 00:01:16.510
that they might not want that
information with the motivation

00:01:16.510 --> 00:01:18.970
that they would like
to make themselves

00:01:18.970 --> 00:01:20.990
feel better about themselves.

00:01:20.990 --> 00:01:23.350
They might think that
the healthier than they

00:01:23.350 --> 00:01:25.480
actually are, they
might look forward

00:01:25.480 --> 00:01:28.150
to a healthy life at
least for two more years.

00:01:28.150 --> 00:01:32.140
And that might depress
their willingness

00:01:32.140 --> 00:01:35.350
to gather or
receive information.

00:01:35.350 --> 00:01:40.000
Now we talked a little
bit about then a model

00:01:40.000 --> 00:01:41.710
and how to think about this.

00:01:41.710 --> 00:01:45.760
First, we talked about a
model where people did not

00:01:45.760 --> 00:01:48.550
have the choice whether
they could manipulate

00:01:48.550 --> 00:01:49.900
the information themselves.

00:01:49.900 --> 00:01:52.660
So we just talked
about when somebody

00:01:52.660 --> 00:01:55.690
wants to seek or
reject information,

00:01:55.690 --> 00:01:57.793
but we have the constraint.

00:01:57.793 --> 00:01:58.960
We had imposed a constraint.

00:01:58.960 --> 00:02:00.400
Let me go back for a second.

00:02:00.400 --> 00:02:02.590
We had a constraint imposed
where we said, well,

00:02:02.590 --> 00:02:05.332
the person needs to be--

00:02:05.332 --> 00:02:08.770
have-- hold correct beliefs
conditional on the information

00:02:08.770 --> 00:02:11.085
that they have been
exposed to so far.

00:02:11.085 --> 00:02:13.210
So essentially, that was
just an idea of, like, OK,

00:02:13.210 --> 00:02:17.410
here's some information that
the person could gather.

00:02:17.410 --> 00:02:19.003
But the constraint
was this number one

00:02:19.003 --> 00:02:20.920
that I'm showing you
here is that beliefs need

00:02:20.920 --> 00:02:24.370
to be correct, as in like
the person is a [INAUDIBLE]

00:02:24.370 --> 00:02:28.210
conditional on the information
that they have received so far.

00:02:28.210 --> 00:02:30.895
And then we said, well,
if you think about this,

00:02:30.895 --> 00:02:35.860
if this is just about wanting
to gather information and be

00:02:35.860 --> 00:02:39.130
considered so far the case where
the person could not actually

00:02:39.130 --> 00:02:41.200
affect future
outcomes, so that's

00:02:41.200 --> 00:02:43.760
the case of Huntington's
disease where there's no cure.

00:02:43.760 --> 00:02:45.320
There's nothing you
can do about it.

00:02:45.320 --> 00:02:46.600
The question was
just like, would you

00:02:46.600 --> 00:02:48.475
like to receive some
information that's going

00:02:48.475 --> 00:02:50.800
to happen in the future or not?

00:02:50.800 --> 00:02:54.460
Then we-- and I'm not going to
go to this in detail very much.

00:02:54.460 --> 00:02:58.125
We then looked at
the conditions that

00:02:58.125 --> 00:03:01.240
are involved for when does the
person want to know about this.

00:03:01.240 --> 00:03:02.800
And the condition
was essentially

00:03:02.800 --> 00:03:06.190
about this function of
f of p, where f of p

00:03:06.190 --> 00:03:09.940
was essentially the utility
derived from beliefs.

00:03:09.940 --> 00:03:13.110
And the condition was either
if f is concave or f is convex.

00:03:13.110 --> 00:03:16.180
If f was concave, the person
is information-averse.

00:03:16.180 --> 00:03:19.570
If f was convex, the person
is information-loving

00:03:19.570 --> 00:03:22.540
and would like to find
out about those things.

00:03:22.540 --> 00:03:24.430
Now then we said,
well, consider now also

00:03:24.430 --> 00:03:26.440
the possibility
that people might

00:03:26.440 --> 00:03:28.720
be able to manipulate
their beliefs.

00:03:28.720 --> 00:03:32.050
So you might say,
what are the reasons?

00:03:32.050 --> 00:03:34.713
Why might you ever want
to hold correct beliefs?

00:03:34.713 --> 00:03:36.130
And in the above
framework, if you

00:03:36.130 --> 00:03:37.750
could choose what
your p is, there's

00:03:37.750 --> 00:03:41.800
really no reason whatsoever
to not choose p equals 1.

00:03:41.800 --> 00:03:44.200
Remember, p here
was the probability

00:03:44.200 --> 00:03:48.070
of your health being good, of
being HD, Huntington's Disease

00:03:48.070 --> 00:03:50.270
negative, of not
having the disease.

00:03:50.270 --> 00:03:52.150
So if there's no
negative consequences

00:03:52.150 --> 00:03:54.340
of what's going to
happen in the future,

00:03:54.340 --> 00:03:55.990
surely you have
all the incentives

00:03:55.990 --> 00:03:58.510
in the world to deceive
yourself and make yourself think

00:03:58.510 --> 00:04:00.770
that you are healthy, right?

00:04:00.770 --> 00:04:02.927
And so now then the
person was like, well--

00:04:02.927 --> 00:04:04.510
and this is what
happens [INAUDIBLE]..

00:04:04.510 --> 00:04:05.710
The expression that's here.

00:04:05.710 --> 00:04:09.490
There's essentially f of 1,
and that's larger than f of p

00:04:09.490 --> 00:04:11.050
for any p that are there.

00:04:11.050 --> 00:04:12.270
And then if the future--

00:04:12.270 --> 00:04:14.170
this is the term
here-- what is actually

00:04:14.170 --> 00:04:16.029
going to happen in
the future, if that's

00:04:16.029 --> 00:04:22.630
independent of your beliefs
and the sense of that's

00:04:22.630 --> 00:04:26.740
going to happen anyway,
you might as well

00:04:26.740 --> 00:04:30.970
make yourself believe
that p is large.

00:04:30.970 --> 00:04:31.480
P is 1.

00:04:31.480 --> 00:04:32.710
You're healthy.

00:04:32.710 --> 00:04:36.350
Now then what we left things
at was the question of,

00:04:36.350 --> 00:04:39.310
why might you not want to
choose f of one anyway?

00:04:39.310 --> 00:04:44.630
Why might you not want
to say p equals 1?

00:04:44.630 --> 00:04:45.880
And that's a question to you.

00:04:56.090 --> 00:04:58.910
AUDIENCE: If your belief
of what's going to happen

00:04:58.910 --> 00:05:03.200
might affect the future outcome
in a negative way, you--

00:05:07.710 --> 00:05:09.150
I guess if there
were a treatment

00:05:09.150 --> 00:05:11.830
available for
Huntington's disease,

00:05:11.830 --> 00:05:14.070
you would rather know
whether you have it.

00:05:14.070 --> 00:05:15.070
FRANK SCHILLBACH: Right.

00:05:15.070 --> 00:05:18.180
So Huntington's is not the
greatest of all examples

00:05:18.180 --> 00:05:23.130
that I chose to start with,
because there's no cure.

00:05:23.130 --> 00:05:24.060
But you're right.

00:05:24.060 --> 00:05:25.950
For example, in
particular, for diseases

00:05:25.950 --> 00:05:28.320
such as HIV or the
like, surely it

00:05:28.320 --> 00:05:29.580
would be very helpful to know.

00:05:29.580 --> 00:05:32.880
Because then you can engage
in proper treatment that might

00:05:32.880 --> 00:05:34.597
actually help you get better.

00:05:34.597 --> 00:05:36.180
Presumably, you only
get the treatment

00:05:36.180 --> 00:05:38.880
if you actually know
what the disease is like.

00:05:38.880 --> 00:05:40.643
In the specific
Huntington's disease,

00:05:40.643 --> 00:05:42.060
we also talked
about a few things.

00:05:42.060 --> 00:05:45.000
We talked about other actions
that you might be able to take.

00:05:45.000 --> 00:05:52.680
For example, if you are
able to save for retirement,

00:05:52.680 --> 00:05:55.300
if you are able to go to
travels before you get sick.

00:05:55.300 --> 00:05:57.720
Or there was questions
about whether people

00:05:57.720 --> 00:06:01.770
want to have children, questions
about like what partners

00:06:01.770 --> 00:06:03.760
they want to be with
and so on and so forth.

00:06:03.760 --> 00:06:06.330
So if there's a bunch of
other economic choices,

00:06:06.330 --> 00:06:10.080
or other choices in life that
depend on whether the person is

00:06:10.080 --> 00:06:12.930
positive or negative, it
seems like that person

00:06:12.930 --> 00:06:14.070
should want to know.

00:06:14.070 --> 00:06:16.500
And then deluding yourself
might get in the way

00:06:16.500 --> 00:06:18.960
of making optimal decisions.

00:06:18.960 --> 00:06:21.360
But broadly speaking,
just to summarize,

00:06:21.360 --> 00:06:24.000
if there is no action
item, as in like if there's

00:06:24.000 --> 00:06:27.370
some information about
the future that where

00:06:27.370 --> 00:06:31.142
really, even if you
knew the information,

00:06:31.142 --> 00:06:32.850
there's nothing you
could do about what's

00:06:32.850 --> 00:06:35.220
going to happen in
the future, it's

00:06:35.220 --> 00:06:38.160
not obvious why you not want to
just delude yourself and think

00:06:38.160 --> 00:06:39.690
like things are rosier.

00:06:39.690 --> 00:06:43.020
The world is looking
better than it actually is.

00:06:43.020 --> 00:06:45.060
Because it might just
make you happier, at least

00:06:45.060 --> 00:06:47.640
in the moment, even if
things in the future

00:06:47.640 --> 00:06:51.080
might turn out to be bad.

00:06:51.080 --> 00:06:54.030
And so this is what we
already just discussed.

00:06:54.030 --> 00:06:57.850
So incorrect beliefs can
lead to mistaken decisions.

00:06:57.850 --> 00:07:01.570
Well, that's correct, but sort
of overly positive beliefs

00:07:01.570 --> 00:07:03.640
are an economically
important indication

00:07:03.640 --> 00:07:05.200
of utility from beliefs.

00:07:05.200 --> 00:07:08.440
That is to say on the
one hand, you might--

00:07:08.440 --> 00:07:09.940
and this is what
we just discussed--

00:07:09.940 --> 00:07:12.065
you want to believe that
you're healthy if it makes

00:07:12.065 --> 00:07:15.790
you feel better about yourself
or the currently or the future.

00:07:15.790 --> 00:07:19.870
So you might want to convince
yourself that that's the case.

00:07:19.870 --> 00:07:21.580
But this is what we just said.

00:07:21.580 --> 00:07:23.767
Overoptimism distorts
decision making

00:07:23.767 --> 00:07:25.600
in some ways-- for
example, health behavior,

00:07:25.600 --> 00:07:27.290
whether you want
to seek treatment,

00:07:27.290 --> 00:07:28.870
but also whether
you want to adjust

00:07:28.870 --> 00:07:31.040
to potentially bad events
and some other ways

00:07:31.040 --> 00:07:34.540
in economic choices.

00:07:34.540 --> 00:07:37.000
So you might not want
to-- so you might not

00:07:37.000 --> 00:07:44.410
want to be overoptimistic
because of those distortions.

00:07:44.410 --> 00:07:46.330
And then the
optimal expectations

00:07:46.330 --> 00:07:48.530
will then trade off
these two things.

00:07:48.530 --> 00:07:51.190
On the one hand, you're healthy
and maybe happier right now,

00:07:51.190 --> 00:07:52.480
thinking you're healthy.

00:07:52.480 --> 00:07:55.300
On the other hand-- or thinking
that the future will be bright

00:07:55.300 --> 00:07:56.090
anyway.

00:07:56.090 --> 00:07:58.090
On the other hand,
it might make--

00:07:58.090 --> 00:08:00.620
change your choices
in some bad way.

00:08:00.620 --> 00:08:02.990
Now, there was another
reason that people mentioned,

00:08:02.990 --> 00:08:06.470
which was potential for
disappointment, right?

00:08:06.470 --> 00:08:08.110
If you think the
future is always

00:08:08.110 --> 00:08:12.310
going to be great, even if
you can't effect it in any way

00:08:12.310 --> 00:08:14.570
well, at some point
reality is going to set in,

00:08:14.570 --> 00:08:16.573
and then you might get
really disappointed.

00:08:16.573 --> 00:08:18.490
We haven't really talked
about this very much.

00:08:18.490 --> 00:08:19.948
That's a way in
which you can think

00:08:19.948 --> 00:08:23.390
about perhaps people potentially
choosing their reference point.

00:08:23.390 --> 00:08:26.420
So if you think people have
referenced dependent utility,

00:08:26.420 --> 00:08:29.110
and if you're really
overoptimistic

00:08:29.110 --> 00:08:32.110
very much in a way that allows
you to potentially choose

00:08:32.110 --> 00:08:35.960
your expectations and
your reference point.

00:08:35.960 --> 00:08:40.299
And then you might not want
to be overly optimistic,

00:08:40.299 --> 00:08:42.220
because your reference
point might be then

00:08:42.220 --> 00:08:44.440
too high and any
outcomes that you're

00:08:44.440 --> 00:08:49.000
gonna eventually receive
you might sort of view worse

00:08:49.000 --> 00:08:51.530
because you expect and
believe things to happen.

00:08:51.530 --> 00:08:55.780
So to the extent that
being true, or being overly

00:08:55.780 --> 00:08:58.780
optimistic leads
to disappointment,

00:08:58.780 --> 00:09:02.770
you might not want to engage in
overoptimism, even if there's

00:09:02.770 --> 00:09:05.470
nothing you can do
about the outcomes,

00:09:05.470 --> 00:09:07.690
because of that
potential disappointment.

00:09:07.690 --> 00:09:10.432
But in general for
decision making

00:09:10.432 --> 00:09:11.890
with anticipatory
utility, at least

00:09:11.890 --> 00:09:15.400
some overoptimism leads to
higher utility than realism.

00:09:15.400 --> 00:09:19.000
Because essentially it makes
you feel better in the moment.

00:09:19.000 --> 00:09:20.860
But there's this
trade off potentially

00:09:20.860 --> 00:09:25.270
with how good you feel in the
moment versus how disappointed

00:09:25.270 --> 00:09:30.880
you might be in the future in
case you're too overoptimistic.

00:09:30.880 --> 00:09:32.230
Any questions about this?

00:09:36.590 --> 00:09:37.850
Oh, sorry.

00:09:41.160 --> 00:09:46.620
OK, so then let me show you some
other overoptimistic beliefs

00:09:46.620 --> 00:09:51.630
in addition to the evidence
[INAUDIBLE] showed you.

00:09:51.630 --> 00:09:54.030
So there's a classic
study by Weinstein

00:09:54.030 --> 00:09:55.770
[INAUDIBLE] that
asks students to make

00:09:55.770 --> 00:09:57.930
judgments of their
students' chances

00:09:57.930 --> 00:09:59.220
for a number of outcomes.

00:09:59.220 --> 00:10:02.610
It's similar to the question
that I had asked you

00:10:02.610 --> 00:10:04.208
at the beginning of class.

00:10:04.208 --> 00:10:06.000
There are two measures
that Weinstein uses.

00:10:06.000 --> 00:10:08.160
One is what's called
comparative judgment.

00:10:08.160 --> 00:10:11.040
This is-- excuse me.

00:10:13.970 --> 00:10:15.620
This is how much
more or less likely

00:10:15.620 --> 00:10:18.020
the average student
thinks the event will

00:10:18.020 --> 00:10:20.732
happen to them relative
to the average student.

00:10:20.732 --> 00:10:22.190
And the second one
is what's called

00:10:22.190 --> 00:10:25.333
the optimistic/pessimistic
ratio, the number of students

00:10:25.333 --> 00:10:26.750
who think their
chances are better

00:10:26.750 --> 00:10:29.690
than the average classmates
divided by the number who

00:10:29.690 --> 00:10:31.170
think their chances are worse.

00:10:31.170 --> 00:10:33.735
So these are both sort of
measures of overoptimism.

00:10:33.735 --> 00:10:35.360
They measure slightly
different things,

00:10:35.360 --> 00:10:38.610
but they're broadly-- they're
fairly highly correlated.

00:10:38.610 --> 00:10:41.270
So what does Weinstein find?

00:10:41.270 --> 00:10:44.112
Clear evidence of
overoptimism when

00:10:44.112 --> 00:10:46.070
you look at different
things about stuff that's

00:10:46.070 --> 00:10:49.010
going to happen in your
life, owning a house, salary

00:10:49.010 --> 00:10:51.570
of larger than $10,000 a year--

00:10:51.570 --> 00:10:52.910
I think this is 1980.

00:10:52.910 --> 00:10:54.510
That's been a while ago--

00:10:54.510 --> 00:10:57.470
traveling to Europe,
living past age 80 and on.

00:10:57.470 --> 00:11:00.590
The comparative judgment
is always fairly high.

00:11:00.590 --> 00:11:03.630
So that number would be 0
if people were, on average,

00:11:03.630 --> 00:11:04.130
realistic.

00:11:04.130 --> 00:11:06.510
Remember, comparative judgment
is how much more or less likely

00:11:06.510 --> 00:11:08.300
the average student
thinks the event will

00:11:08.300 --> 00:11:11.810
happen to them relative to the
average actual average student.

00:11:11.810 --> 00:11:16.070
And the
optimistic/pessimistic ratio,

00:11:16.070 --> 00:11:21.240
that should be one if people
were not overoptimistic.

00:11:21.240 --> 00:11:23.295
It's also clearly a large N1.

00:11:23.295 --> 00:11:24.920
Remember, that's the
number of students

00:11:24.920 --> 00:11:27.650
who think the chances are better
than the average classmates

00:11:27.650 --> 00:11:30.920
divided by the number who
think their chances are worse.

00:11:30.920 --> 00:11:32.510
But it works for
positive events,

00:11:32.510 --> 00:11:34.410
but also works for
negative events.

00:11:34.410 --> 00:11:37.490
So when you ask people about
drinking problems, suicide,

00:11:37.490 --> 00:11:39.950
divorce, heart attacks,
all sorts of bad things,

00:11:39.950 --> 00:11:42.080
people think they're
now, of course,

00:11:42.080 --> 00:11:48.470
less likely to happen to
them, these kinds of events.

00:11:48.470 --> 00:11:50.660
And then they end up
optimistic/pessimistic ratio,

00:11:50.660 --> 00:11:51.380
that's positive.

00:11:51.380 --> 00:11:55.040
That's just because it's like
the optimist flips, so people

00:11:55.040 --> 00:11:57.650
are way more optimistic, or--

00:11:57.650 --> 00:12:00.110
about-- they think
they're not going

00:12:00.110 --> 00:12:03.380
to have drinking problems, get
divorced, get heart attacks,

00:12:03.380 --> 00:12:03.900
and so on.

00:12:03.900 --> 00:12:07.730
So people tend to be very
optimistic about their future

00:12:07.730 --> 00:12:11.240
lives compared
with when you asked

00:12:11.240 --> 00:12:14.230
them to compare it to others.

00:12:14.230 --> 00:12:16.477
Now there's other
examples of that.

00:12:16.477 --> 00:12:18.060
Couples believe
there's a small chance

00:12:18.060 --> 00:12:20.960
that their marriage will end.

00:12:20.960 --> 00:12:22.710
Small business owners
think their business

00:12:22.710 --> 00:12:26.370
is far more likely to succeed
than their typical similar

00:12:26.370 --> 00:12:27.210
business.

00:12:27.210 --> 00:12:29.485
Smokers understand the
health risks of smoking

00:12:29.485 --> 00:12:31.860
but don't believe this risk
applies specifically to them.

00:12:31.860 --> 00:12:34.350
There's a long list of kind
of these kinds of behaviors.

00:12:34.350 --> 00:12:37.950
People tend to have
very-- hold rosy beliefs

00:12:37.950 --> 00:12:42.630
about their future, even
in the presence of a sort

00:12:42.630 --> 00:12:43.900
of objective information.

00:12:43.900 --> 00:12:45.600
So even if you give
people information

00:12:45.600 --> 00:12:47.400
about the health
risks of smoking,

00:12:47.400 --> 00:12:48.850
they understand
the health risks.

00:12:48.850 --> 00:12:50.308
But then they're
like, well, that's

00:12:50.308 --> 00:12:51.390
not going to apply to me.

00:12:51.390 --> 00:12:54.960
Of course, there's no good
reason to actually dismiss.

00:12:54.960 --> 00:12:57.090
It should apply to anybody.

00:12:57.090 --> 00:13:01.560
So people just want some
things to be true presumably

00:13:01.560 --> 00:13:05.710
because it makes them
happier in some ways.

00:13:05.710 --> 00:13:08.250
So then beyond future
prospects, people also

00:13:08.250 --> 00:13:11.310
tend to have overly positive
views about their abilities

00:13:11.310 --> 00:13:12.460
and traits.

00:13:12.460 --> 00:13:15.540
So, for example, 99% of
drivers think they're

00:13:15.540 --> 00:13:17.280
better than the average driver.

00:13:17.280 --> 00:13:20.167
94% of professors at the
University of Nebraska

00:13:20.167 --> 00:13:22.500
think they're better teachers
than the average Professor

00:13:22.500 --> 00:13:23.640
at the University.

00:13:23.640 --> 00:13:26.880
I don't know what this
number looks like at MIT,

00:13:26.880 --> 00:13:31.520
but probably it's larger
than 50%, as well.

00:13:31.520 --> 00:13:34.500
So [INAUDIBLE],, you can
find this kind of evidence

00:13:34.500 --> 00:13:35.910
from a range of domains.

00:13:35.910 --> 00:13:38.430
In some cases, there's some
other potential explanation

00:13:38.430 --> 00:13:40.180
for this, but broadly
speaking there's

00:13:40.180 --> 00:13:41.790
a pretty robust
evidence that people

00:13:41.790 --> 00:13:45.630
are overconfident,
overly positive about,

00:13:45.630 --> 00:13:48.360
A, what's going to happen to
them in the future, and B,

00:13:48.360 --> 00:13:53.280
about thinking about their
own skills and abilities.

00:13:53.280 --> 00:13:56.610
Now if you look at MIT
students, however, in

00:13:56.610 --> 00:14:01.200
contrast when you ask about
students different questions,

00:14:01.200 --> 00:14:03.390
and this is from a recent
survey from a few years

00:14:03.390 --> 00:14:07.350
ago that MIT asks students.

00:14:07.350 --> 00:14:08.910
One question here
is academically,

00:14:08.910 --> 00:14:11.300
I would consider myself
above average at MIT.

00:14:13.830 --> 00:14:17.280
People say about 31% of students
say they're above average.

00:14:17.280 --> 00:14:20.160
In particular,
female students tend

00:14:20.160 --> 00:14:23.205
to think they're below average.

00:14:23.205 --> 00:14:25.330
People might have biased
beliefs about the average.

00:14:25.330 --> 00:14:27.870
So what you think is an average
MIT student might actually

00:14:27.870 --> 00:14:31.830
be like the fifth or whatever,
some of the highest percentile,

00:14:31.830 --> 00:14:34.320
because essentially you're
exposed to all the success

00:14:34.320 --> 00:14:37.830
and math Olympics
and whatever, wins

00:14:37.830 --> 00:14:40.470
that people receive,
which is just not what

00:14:40.470 --> 00:14:42.210
the average student is like.

00:14:42.210 --> 00:14:44.520
There's, of course, also
very severe selection

00:14:44.520 --> 00:14:47.730
in terms of there's so many
smart and brilliant people

00:14:47.730 --> 00:14:52.080
at MIT that, in a way, people's
perception is quite biased.

00:14:52.080 --> 00:14:55.380
The questions here were asked
about the average at MIT,

00:14:55.380 --> 00:14:59.490
but even there I think
people might just

00:14:59.490 --> 00:15:01.860
be biased in terms of
thinking what's the average.

00:15:01.860 --> 00:15:04.230
There might also be some
worries about disappointment

00:15:04.230 --> 00:15:06.438
in the sense-- and this is
what I was saying before--

00:15:06.438 --> 00:15:09.600
when you think about
being overconfident,

00:15:09.600 --> 00:15:10.810
maybe in some ways--

00:15:10.810 --> 00:15:14.190
so one issue with, or one
reason why you might not

00:15:14.190 --> 00:15:16.305
end up being overconfident
for a long time

00:15:16.305 --> 00:15:18.150
is if you receive feedback.

00:15:18.150 --> 00:15:20.970
And at MIT, people
receive lots of exams

00:15:20.970 --> 00:15:24.660
and so on where you can learn
or interact with others,

00:15:24.660 --> 00:15:28.050
but you can notice how
people are doing overall.

00:15:28.050 --> 00:15:30.450
And if you're worried
about being disappointed,

00:15:30.450 --> 00:15:33.060
or you have been disappointed
several times already,

00:15:33.060 --> 00:15:38.010
then thinking you are the
smartest person in class

00:15:38.010 --> 00:15:40.470
might not be a good idea because
you might get disappointed

00:15:40.470 --> 00:15:41.490
again.

00:15:41.490 --> 00:15:44.610
And then we'll talk
a little bit more

00:15:44.610 --> 00:15:47.790
about gender in a
different lecture later.

00:15:47.790 --> 00:15:50.632
That's a very common theme--

00:15:50.632 --> 00:15:55.748
one second-- is that female MIT
students, or in general women,

00:15:55.748 --> 00:15:57.540
but in particular also
female MIT students,

00:15:57.540 --> 00:16:02.790
are particularly
under-confident.

00:16:02.790 --> 00:16:05.640
We don't exactly
know why that is,

00:16:05.640 --> 00:16:08.760
but it's a very robust
finding, not just at MIT.

00:16:08.760 --> 00:16:11.550
In some ways, then,
there's still a question--

00:16:11.550 --> 00:16:14.150
so it's a very nice
observation that in a way,

00:16:14.150 --> 00:16:18.100
beliefs can also be a
form of self-motivation.

00:16:18.100 --> 00:16:22.050
So if you are really
interested in academic success

00:16:22.050 --> 00:16:26.620
and being really good at
exams and so on and so forth,

00:16:26.620 --> 00:16:28.950
if you are
under-confident, that can

00:16:28.950 --> 00:16:30.450
be a motivator in
saying that you're

00:16:30.450 --> 00:16:31.680
really worried about failing.

00:16:31.680 --> 00:16:33.810
You're worried about
doing badly and so on.

00:16:33.810 --> 00:16:35.910
Then you work extremely
hard, and then you

00:16:35.910 --> 00:16:36.870
do better in exams.

00:16:36.870 --> 00:16:39.610
You surprise
yourself positively.

00:16:39.610 --> 00:16:43.650
That can be an
important motivator.

00:16:43.650 --> 00:16:45.630
Of course, that could
also go the other way.

00:16:45.630 --> 00:16:46.963
It could be really discouraging.

00:16:46.963 --> 00:16:49.643
If you think you're really
terrible at everything,

00:16:49.643 --> 00:16:50.310
then you might--

00:16:50.310 --> 00:16:53.760
at the end of the day, you
might not study at all anymore,

00:16:53.760 --> 00:16:54.850
because what's the point?

00:16:54.850 --> 00:16:57.880
So there's a bit of a
trade-off here as well.

00:16:57.880 --> 00:17:04.170
And it could be that if you
think that people are-- when

00:17:04.170 --> 00:17:06.310
they're under-confident,
that leads to motivation

00:17:06.310 --> 00:17:09.660
and it makes them
better at school.

00:17:09.660 --> 00:17:11.940
Plus, they're going to
be less disappointed

00:17:11.940 --> 00:17:13.800
and get positive surprises.

00:17:13.800 --> 00:17:15.079
That could be an explanation.

00:17:15.079 --> 00:17:16.829
I think there's a bit
of a question again.

00:17:16.829 --> 00:17:20.880
Why is that the case at MIT and
not in the overall population?

00:17:20.880 --> 00:17:23.250
But it might have
to do with frequency

00:17:23.250 --> 00:17:26.532
of feedback, and school,
and so on and so forth.

00:17:26.532 --> 00:17:28.740
And maybe also just with
social norms or other things

00:17:28.740 --> 00:17:32.730
that maybe are somewhat
trickier to explain.

00:17:32.730 --> 00:17:34.960
I was teaching Harvard
undergrads as a grad student.

00:17:34.960 --> 00:17:38.700
I'm not so sure that Harvard
students are under-confident,

00:17:38.700 --> 00:17:40.720
to be honest.

00:17:40.720 --> 00:17:43.350
I haven't seen the
survey evidence on that,

00:17:43.350 --> 00:17:46.980
but my sense is
that MIT students

00:17:46.980 --> 00:17:50.400
are more under-confident
compared to Harvard students.

00:17:50.400 --> 00:17:51.450
Let's leave it at that.

00:17:54.245 --> 00:17:55.620
But I think it's
true if you look

00:17:55.620 --> 00:17:58.500
at Caltech and other
type of schools

00:17:58.500 --> 00:18:00.420
that are very similar to MIT.

00:18:00.420 --> 00:18:03.390
I think-- my guess is
you'll find similar results

00:18:03.390 --> 00:18:04.740
in those kinds of surveys.

00:18:07.970 --> 00:18:08.630
OK.

00:18:08.630 --> 00:18:10.940
So the next thing
we're talking about

00:18:10.940 --> 00:18:13.610
is very quickly is what's
called ego utility.

00:18:13.610 --> 00:18:16.490
So partly I want to talk about
anticipatory utility, which

00:18:16.490 --> 00:18:19.070
was the idea that people
want to look forward

00:18:19.070 --> 00:18:21.560
to good things in life.

00:18:21.560 --> 00:18:26.900
And therefore, they're--
have inflated beliefs about

00:18:26.900 --> 00:18:28.590
what's going to
happen in the future.

00:18:28.590 --> 00:18:34.870
A very related aspect is what's
called ego utility, which is--

00:18:34.870 --> 00:18:37.610
so just to recap,
inflated beliefs

00:18:37.610 --> 00:18:39.770
might be explained by
anticipatory utility

00:18:39.770 --> 00:18:44.180
essentially to say higher
ability means better

00:18:44.180 --> 00:18:46.790
future prospects, and people
want to convince themselves

00:18:46.790 --> 00:18:48.080
that they have high ability.

00:18:48.080 --> 00:18:50.210
And therefore, if you
think you're really smart,

00:18:50.210 --> 00:18:52.970
that means good things are
going to happen in the future.

00:18:52.970 --> 00:18:56.600
But it might also just
be, like, right now,

00:18:56.600 --> 00:18:58.460
you're just feeling
better about yourself,

00:18:58.460 --> 00:19:03.770
and therefore you have inflated
beliefs about yourself.

00:19:03.770 --> 00:19:06.283
So that's often tricky to
distinguish empirically

00:19:06.283 --> 00:19:07.700
in the sense of
like, I might just

00:19:07.700 --> 00:19:12.200
feel like I'm good looking,
smart, and so on and so forth,

00:19:12.200 --> 00:19:14.570
because it makes me
feel good right now.

00:19:14.570 --> 00:19:16.520
Or I might want to
hold those beliefs,

00:19:16.520 --> 00:19:20.540
because that means
good things is going

00:19:20.540 --> 00:19:22.550
to happen in the
future, and I will have

00:19:22.550 --> 00:19:25.290
a bright future going forward.

00:19:25.290 --> 00:19:29.300
So these things are slightly
tricky to disentangle,

00:19:29.300 --> 00:19:32.630
and both of them probably
play an important role

00:19:32.630 --> 00:19:34.940
at the end of the day.

00:19:34.940 --> 00:19:39.610
So now one thing I'm going to
talk about a little bit more

00:19:39.610 --> 00:19:42.650
also later in the semester
is about mental health.

00:19:42.650 --> 00:19:46.030
And so there is this
literature that argues

00:19:46.030 --> 00:19:48.610
that positive illusions are--

00:19:48.610 --> 00:19:51.700
promote, in fact,
psychological well-being.

00:19:51.700 --> 00:19:53.710
So that's the
argument that is here,

00:19:53.710 --> 00:19:58.120
that overconfident is, in
fact, vital and important

00:19:58.120 --> 00:20:00.130
for maintaining mental health.

00:20:00.130 --> 00:20:05.470
So overconfident and that
makes people happier.

00:20:05.470 --> 00:20:07.510
People are better able
to care for others

00:20:07.510 --> 00:20:10.270
if they think they,
themselves, are doing better.

00:20:10.270 --> 00:20:13.780
They're better at doing
creative and productive work.

00:20:13.780 --> 00:20:16.540
That's also better over
confident helps people

00:20:16.540 --> 00:20:19.090
manage negative feedback.

00:20:19.090 --> 00:20:22.510
So overall, all of those
things are [INAUDIBLE]

00:20:22.510 --> 00:20:25.270
so this literature argues
that overconfidence actually

00:20:25.270 --> 00:20:27.730
helps people with
their everyday lives.

00:20:27.730 --> 00:20:29.410
In particular, helps
people maintain

00:20:29.410 --> 00:20:32.360
a positive-- a
good mental health.

00:20:32.360 --> 00:20:37.450
And there's this term called
depressive realism, which

00:20:37.450 --> 00:20:40.990
is to say that realistic
expectation might actually be

00:20:40.990 --> 00:20:42.280
detrimental to mental health.

00:20:42.280 --> 00:20:44.620
So that the world is just
difficult in various ways

00:20:44.620 --> 00:20:49.880
and bad things are
happening in many lives,

00:20:49.880 --> 00:20:54.503
and if people are not overly
optimistic about what's

00:20:54.503 --> 00:20:56.170
going to happen in
the future and so on,

00:20:56.170 --> 00:20:59.260
that might be actually bad
for their mental health.

00:20:59.260 --> 00:21:02.410
We'll return to this
issue on happiness

00:21:02.410 --> 00:21:04.110
and mental health
in a later lecture,

00:21:04.110 --> 00:21:05.720
but I wanted to flag
as these things.

00:21:05.720 --> 00:21:08.810
They're very much
linked to each other.

00:21:08.810 --> 00:21:17.350
And another reason perhaps why
overoptimism might be, in fact,

00:21:17.350 --> 00:21:22.390
good or not detrimental is
because not only makes people

00:21:22.390 --> 00:21:24.790
happier, but it might
also help people

00:21:24.790 --> 00:21:26.400
protect their mental health.

00:21:29.560 --> 00:21:30.060
OK.

00:21:30.060 --> 00:21:33.713
So then just very briefly,
and this is more like--

00:21:33.713 --> 00:21:35.630
there's some research
in some of those things.

00:21:35.630 --> 00:21:37.640
Some of this is more anecdotal.

00:21:37.640 --> 00:21:39.080
What are some
factors that affects

00:21:39.080 --> 00:21:41.550
the extent of positive biases?

00:21:41.550 --> 00:21:44.180
So people tend to have greater
biases, in fact, with respect

00:21:44.180 --> 00:21:46.908
to prospects and traits that are
personally important to them.

00:21:46.908 --> 00:21:48.450
On the one hand,
you might say, well,

00:21:48.450 --> 00:21:51.020
people should also be better
informed about those things.

00:21:51.020 --> 00:21:54.200
But to the extent that
it affects their utility,

00:21:54.200 --> 00:21:55.550
they might be more biased.

00:21:55.550 --> 00:21:57.090
If it's stuff that's
really important to you,

00:21:57.090 --> 00:21:59.423
you might be more biased
because that's precisely what's

00:21:59.423 --> 00:22:01.700
going to make you happier.

00:22:01.700 --> 00:22:03.980
Available or imminent
objective information

00:22:03.980 --> 00:22:06.980
tends to decrease
biases as in like--

00:22:06.980 --> 00:22:09.680
and this is what I was talking
about with MIT students.

00:22:09.680 --> 00:22:11.930
When you think about
academic performance,

00:22:11.930 --> 00:22:17.190
you get lots and lots of
feedback every single semester.

00:22:17.190 --> 00:22:20.990
So in a way, it's more difficult
to be overly optimistic,

00:22:20.990 --> 00:22:24.440
or to maintain an overly
optimistic picture

00:22:24.440 --> 00:22:29.300
of your academic self if you've
got all these objective signals

00:22:29.300 --> 00:22:30.587
over and over again.

00:22:30.587 --> 00:22:32.670
So one prediction here
would be that, for example,

00:22:32.670 --> 00:22:34.970
if you looked at first,
second, third, and fourth year

00:22:34.970 --> 00:22:40.550
students, that perhaps some
of the under-confidence

00:22:40.550 --> 00:22:42.950
only comes later
in the semester.

00:22:42.950 --> 00:22:47.370
Of course, there's other reasons
why that might not be the case.

00:22:47.370 --> 00:22:49.710
Then if feedback about the
prospect or any prospect

00:22:49.710 --> 00:22:51.710
is more ambiguous and
subject to interpretation,

00:22:51.710 --> 00:22:53.362
biases tend to be greater.

00:22:53.362 --> 00:22:55.070
So in particular,
people are particularly

00:22:55.070 --> 00:22:56.660
biased in situations
where people

00:22:56.660 --> 00:22:59.240
get ambiguous
feedback where you can

00:22:59.240 --> 00:23:00.680
interpret things either way.

00:23:00.680 --> 00:23:04.880
So either you were really smart
and did something really great,

00:23:04.880 --> 00:23:06.200
or you're just lucky.

00:23:06.200 --> 00:23:09.050
And people tend to, in
many things in life,

00:23:09.050 --> 00:23:11.240
people tend to interpret
sort of lucky things

00:23:11.240 --> 00:23:14.390
that happened in their
lives, tend to think

00:23:14.390 --> 00:23:16.840
that those are due to skills.

00:23:16.840 --> 00:23:20.330
And there's this very nice work
in a book by Robert Frank--

00:23:20.330 --> 00:23:23.570
he's at Cornell University-- who
sort of argues that people tend

00:23:23.570 --> 00:23:27.170
to interpret a lot of things
that happen in their lives that

00:23:27.170 --> 00:23:28.760
really are just mostly luck.

00:23:28.760 --> 00:23:32.600
They tend to think that it's
due to their amazing abilities

00:23:32.600 --> 00:23:34.017
and so on.

00:23:34.017 --> 00:23:35.600
And that's the reason
for that, often,

00:23:35.600 --> 00:23:38.030
is that a lot of this feedback,
or a lot of this information

00:23:38.030 --> 00:23:38.572
is ambiguous.

00:23:38.572 --> 00:23:40.190
You can interpret it either way.

00:23:40.190 --> 00:23:42.950
And people like to think that
good things are because they

00:23:42.950 --> 00:23:45.710
did really great, and bad
things are because they just

00:23:45.710 --> 00:23:47.422
happen to be unlucky.

00:23:47.422 --> 00:23:49.130
If people-- and that's
quite interesting.

00:23:49.130 --> 00:23:51.500
If people feel like they have
control over the outcomes,

00:23:51.500 --> 00:23:55.460
biases tend to be greater.

00:23:55.460 --> 00:23:58.520
Expertise sometimes
increases biases,

00:23:58.520 --> 00:24:01.040
but not for experts who
get very good feedback.

00:24:01.040 --> 00:24:04.640
So meteorologists or the
like who get lots of feedback

00:24:04.640 --> 00:24:08.325
all the time, they actually
tend to not be overconfident,

00:24:08.325 --> 00:24:10.700
because, again, if you get so
much feedback all the time,

00:24:10.700 --> 00:24:14.500
it's hard to sort of
maintain your overoptimism.

00:24:14.500 --> 00:24:15.130
OK.

00:24:15.130 --> 00:24:17.080
So now one question
you might have is,

00:24:17.080 --> 00:24:20.410
well, I showed you a
bunch of data information

00:24:20.410 --> 00:24:23.860
on biased beliefs but not
a lot on actual action.

00:24:23.860 --> 00:24:26.497
Of course, the testing
behavior, and the Huntington's

00:24:26.497 --> 00:24:28.330
disease, and so on
[INAUDIBLE] some actions.

00:24:28.330 --> 00:24:30.790
But a lot of the stuff on the
Weinstein and other studies

00:24:30.790 --> 00:24:32.740
were just about
self-appointed beliefs.

00:24:32.740 --> 00:24:34.660
And one question you
might have is, well,

00:24:34.660 --> 00:24:35.800
this is about self reports.

00:24:35.800 --> 00:24:38.860
What about your preference
on choices that people make?

00:24:38.860 --> 00:24:42.147
So there's some evidence
that's from lab experiments,

00:24:42.147 --> 00:24:44.230
and there's some other
evidence that I'll show you

00:24:44.230 --> 00:24:46.090
that is about actual choices.

00:24:46.090 --> 00:24:48.880
So one very nice experiment
is the paper by Eil and Rao

00:24:48.880 --> 00:24:50.690
from 2010.

00:24:50.690 --> 00:24:53.650
The way this works is
people are given feedback

00:24:53.650 --> 00:24:55.540
about people doing
IQ tests, and they're

00:24:55.540 --> 00:24:58.240
rated according to their
physical attractiveness

00:24:58.240 --> 00:25:01.570
by others in the study.

00:25:01.570 --> 00:25:04.030
And they get feedback
about this IQ test

00:25:04.030 --> 00:25:06.100
score and their
physical attractiveness.

00:25:06.100 --> 00:25:08.308
These two things are
chosen presumably

00:25:08.308 --> 00:25:09.850
because people care
a lot about them.

00:25:09.850 --> 00:25:11.500
People care about
how smart they are.

00:25:11.500 --> 00:25:14.320
People also care about
how good looking they are.

00:25:14.320 --> 00:25:16.060
And then so in the
study, the authors

00:25:16.060 --> 00:25:19.540
elicit people's prior beliefs,
the beliefs at the beginning

00:25:19.540 --> 00:25:23.270
before they receive information,
about a rank between one

00:25:23.270 --> 00:25:23.770
and 10.

00:25:23.770 --> 00:25:27.040
So people are put in groups
of 10 and then asked about,

00:25:27.040 --> 00:25:28.600
in a group of 10
people, how do you

00:25:28.600 --> 00:25:31.900
think you rank
compared to others?

00:25:31.900 --> 00:25:35.090
And then people get
some information,

00:25:35.090 --> 00:25:37.720
and they get, in particular,
bilateral comparisons

00:25:37.720 --> 00:25:39.670
with another participant
in the group.

00:25:39.670 --> 00:25:43.150
And then the authors elicit
their posterior, so beliefs

00:25:43.150 --> 00:25:45.550
after receiving this
information, and also

00:25:45.550 --> 00:25:48.070
the willingness to
pay for true ranks.

00:25:48.070 --> 00:25:51.550
And broadly speaking,
what the authors find

00:25:51.550 --> 00:25:54.520
is there's asymmetric processing
of objective information

00:25:54.520 --> 00:25:55.720
about the self.

00:25:55.720 --> 00:25:58.250
In particular, when people
receive positive news,

00:25:58.250 --> 00:26:00.040
so when somebody says
you've been compared

00:26:00.040 --> 00:26:03.970
to another person in the study
and you're better looking,

00:26:03.970 --> 00:26:07.600
or you have a better IQ
score than this other person,

00:26:07.600 --> 00:26:09.730
people tend to be
roughly Bayesian,

00:26:09.730 --> 00:26:12.550
as in they tend to be pretty
good at actually updating

00:26:12.550 --> 00:26:14.140
based on that information.

00:26:14.140 --> 00:26:18.100
In contrast, when people
receive unfavorable news,

00:26:18.100 --> 00:26:20.740
they tend to essentially
discount the signals,

00:26:20.740 --> 00:26:23.050
and they tend to
essentially not really

00:26:23.050 --> 00:26:24.300
react to that information.

00:26:24.300 --> 00:26:26.050
So that's very much
consistent with people

00:26:26.050 --> 00:26:28.330
who are very happy to
receive positive feedback

00:26:28.330 --> 00:26:30.190
and actually good
at integrating it,

00:26:30.190 --> 00:26:33.250
as in they're good
at doing the math

00:26:33.250 --> 00:26:35.620
as a Bayesian
would want them to.

00:26:35.620 --> 00:26:37.870
But once they get negative
feedback, they essentially

00:26:37.870 --> 00:26:40.690
mostly just discount that
information, presumably

00:26:40.690 --> 00:26:44.890
and very much consistently
with motivated beliefs.

00:26:44.890 --> 00:26:48.387
People tend to want to have
positive images of themselves.

00:26:48.387 --> 00:26:50.470
There's also some evidence
of people's willingness

00:26:50.470 --> 00:26:53.440
to pay for information if they
think the information will

00:26:53.440 --> 00:26:55.990
be good, and not willing
to pay for information

00:26:55.990 --> 00:26:57.820
if they think the
information will be bad,

00:26:57.820 --> 00:26:59.590
so if they're low in the ranks.

00:26:59.590 --> 00:27:03.790
Again, people want to hear
good things about themselves,

00:27:03.790 --> 00:27:07.930
and they learn more or they're
willing to learn more when

00:27:07.930 --> 00:27:09.400
they receive positive news.

00:27:09.400 --> 00:27:11.830
They're also willingness--
willing to pay more

00:27:11.830 --> 00:27:14.950
for information that they
think will be positive.

00:27:14.950 --> 00:27:17.560
There's some other evidence
that's similar to that.

00:27:17.560 --> 00:27:20.230
There's a very nice-- or a
similar paper by Mobius et al

00:27:20.230 --> 00:27:22.550
that's very similar
to Eil and Rao.

00:27:22.550 --> 00:27:27.100
There's some very nice
work by Florian Zimmermann

00:27:27.100 --> 00:27:28.750
in motivated memory.

00:27:28.750 --> 00:27:31.210
So that paper argues
that it's not just

00:27:31.210 --> 00:27:33.040
about when people
update immediately.

00:27:33.040 --> 00:27:35.080
So when people are given
positive or negative

00:27:35.080 --> 00:27:37.580
information, they
update differently.

00:27:37.580 --> 00:27:41.800
But what Florian Zimmermann
shows is that in his work

00:27:41.800 --> 00:27:44.420
that when people are given
this information, in fact,

00:27:44.420 --> 00:27:46.450
in his experiment
there is no asymmetric

00:27:46.450 --> 00:27:47.560
updating in the short run.

00:27:47.560 --> 00:27:49.810
So when people are given
this information about-- this

00:27:49.810 --> 00:27:53.710
is about how they did
in a test in Germany,

00:27:53.710 --> 00:27:58.300
in the short run people seem
to actually be not overly

00:27:58.300 --> 00:28:00.070
optimistic in their updating.

00:28:00.070 --> 00:28:03.130
But then a month later when
the author goes back to people

00:28:03.130 --> 00:28:05.350
and asks them about the
information that they have

00:28:05.350 --> 00:28:07.000
received and their
beliefs, people

00:28:07.000 --> 00:28:09.640
tend to remember
positive signals more

00:28:09.640 --> 00:28:10.960
than negative signals.

00:28:10.960 --> 00:28:13.780
And that suggests
that memory can

00:28:13.780 --> 00:28:16.780
play an important role in the
formation of motivated beliefs.

00:28:16.780 --> 00:28:18.760
When stuff happens,
good stuff, people

00:28:18.760 --> 00:28:22.510
remember more, and bad
things, not so much.

00:28:22.510 --> 00:28:25.180
Now those are all
lab type experiments

00:28:25.180 --> 00:28:28.100
where people are incentivized
to give correct answers.

00:28:28.100 --> 00:28:31.570
But in a way, it's
somewhat contrived.

00:28:31.570 --> 00:28:34.210
There's also quite a bit of
evidence in the real world

00:28:34.210 --> 00:28:37.600
setting, in particular, when
it comes to finance or trading

00:28:37.600 --> 00:28:38.560
behavior.

00:28:38.560 --> 00:28:42.010
For example, there's Barber
and Odean that shows that--

00:28:42.010 --> 00:28:44.650
looks at overconfident
small scale investors.

00:28:44.650 --> 00:28:47.862
Men, for example, tend to
trade a lot more than women.

00:28:47.862 --> 00:28:49.570
We also know from
other settings that men

00:28:49.570 --> 00:28:53.470
tend to be more overconfident
than women, and those men,

00:28:53.470 --> 00:28:57.580
then, also lose more money
or make less money plausibly

00:28:57.580 --> 00:28:59.140
due to overconfidence.

00:28:59.140 --> 00:29:01.390
With this type of evidence,
it's often a little tricky

00:29:01.390 --> 00:29:04.210
because it's hard to say,
is it to due to gender?

00:29:04.210 --> 00:29:06.460
Is it due to overconfidence
and maybe other things

00:29:06.460 --> 00:29:08.710
that are correlated with this?

00:29:08.710 --> 00:29:10.325
But there's quite
a bit of evidence

00:29:10.325 --> 00:29:13.960
on that kind of behavior that
essentially overconfidence

00:29:13.960 --> 00:29:19.090
leads to worse decision here
in this case trading decisions.

00:29:19.090 --> 00:29:22.960
There's also a very nice paper
by Ulrike Malmendier and Tate

00:29:22.960 --> 00:29:24.550
on managerial hubris.

00:29:24.550 --> 00:29:26.200
They have a clever
way of identifying

00:29:26.200 --> 00:29:30.010
overconfident managers, which
essentially is managers that

00:29:30.010 --> 00:29:32.470
are overconfident-- they
are holding stock options,

00:29:32.470 --> 00:29:33.800
if they own stocks.

00:29:33.800 --> 00:29:36.740
So you're not
supposed to do that,

00:29:36.740 --> 00:29:39.910
and you will most likely do that
only if you think your company

00:29:39.910 --> 00:29:43.060
is going to do really great
compared to other companies.

00:29:43.060 --> 00:29:45.640
Instead, you exercise-- you
should exercise your stock

00:29:45.640 --> 00:29:49.540
options and diversify and
not to be overly invested

00:29:49.540 --> 00:29:50.630
in your own firm.

00:29:50.630 --> 00:29:53.980
But if you think that
you're really great

00:29:53.980 --> 00:29:57.580
or your firm is really
great, you [INAUDIBLE]..

00:29:57.580 --> 00:30:00.100
And precisely those
overconfident managers

00:30:00.100 --> 00:30:02.170
engage their business
in more mergers,

00:30:02.170 --> 00:30:05.900
and those mergers, Malmendier
and Tate show, in fact,

00:30:05.900 --> 00:30:07.450
are not good.

00:30:07.450 --> 00:30:09.790
People tend to do
too many mergers that

00:30:09.790 --> 00:30:12.040
essentially destroy
value, presumably

00:30:12.040 --> 00:30:13.430
due to overconfidence.

00:30:13.430 --> 00:30:17.783
So there's that kind
of evidence as well.

00:30:17.783 --> 00:30:19.450
So overall, we think
there's quite a bit

00:30:19.450 --> 00:30:21.520
of evidence of people
systematically holding

00:30:21.520 --> 00:30:23.320
overoptimistic beliefs.

00:30:23.320 --> 00:30:27.730
And those overoptimistic beliefs
are affecting people's choices

00:30:27.730 --> 00:30:29.860
in a way that makes
them worse off.

00:30:29.860 --> 00:30:33.310
In some ways, in particular,
in finance choices

00:30:33.310 --> 00:30:38.170
where people tend to
lose or destroy value.

00:30:38.170 --> 00:30:38.840
OK.

00:30:38.840 --> 00:30:42.640
Any questions on
this before I move

00:30:42.640 --> 00:30:45.460
on to heuristics and biases?

00:30:51.480 --> 00:30:52.140
OK.

00:30:52.140 --> 00:30:59.412
So then we talked about three
of our four issues here already.

00:30:59.412 --> 00:31:01.620
The last thing that we
haven't talked about very much

00:31:01.620 --> 00:31:03.810
is to say Bayesian learning
is just really hard,

00:31:03.810 --> 00:31:05.850
and people might
just not be good

00:31:05.850 --> 00:31:08.520
at it, not for any
motivated reasons

00:31:08.520 --> 00:31:11.790
or because of any utility
reasons [INAUDIBLE]

00:31:11.790 --> 00:31:13.735
anticipatory
[INAUDIBLE] utility.

00:31:13.735 --> 00:31:15.360
But it just might be
really hard to do,

00:31:15.360 --> 00:31:18.060
and people essentially
use heuristics

00:31:18.060 --> 00:31:22.160
because it's too hard to compute
these things on their own.

00:31:22.160 --> 00:31:25.750
Now we talked about this
already quite a bit.

00:31:25.750 --> 00:31:28.630
Lots of economic choices
are made under uncertainty.

00:31:28.630 --> 00:31:32.600
Almost anything in life
involves some uncertainty,

00:31:32.600 --> 00:31:35.260
so we need to know something
about the likelihood

00:31:35.260 --> 00:31:36.310
of relevant events.

00:31:36.310 --> 00:31:40.850
And if you decide which
topic in the course

00:31:40.850 --> 00:31:42.490
to focus on when you
study for an exam,

00:31:42.490 --> 00:31:44.620
and you only have like
one night to do so.

00:31:44.620 --> 00:31:48.220
If a basketball coach decides
whether to leave tired players

00:31:48.220 --> 00:31:50.710
in the game, you kind make
some probabilistic judgment

00:31:50.710 --> 00:31:53.230
about which player
is going to be best.

00:31:53.230 --> 00:31:56.650
Many medical, managerial,
educational, career decisions,

00:31:56.650 --> 00:31:58.390
essentially all those
kinds of decisions

00:31:58.390 --> 00:32:02.183
are involving probabilities
based on probability

00:32:02.183 --> 00:32:04.600
of good things happening, some
probabilities of bad things

00:32:04.600 --> 00:32:06.610
happen, and you need
to make some estimates

00:32:06.610 --> 00:32:08.480
about those probabilities.

00:32:08.480 --> 00:32:14.600
Now how do make
individuals probabilistic

00:32:14.600 --> 00:32:16.740
judgement of this kind?

00:32:16.740 --> 00:32:18.720
So far when we talked
about risk preferences

00:32:18.720 --> 00:32:21.360
earlier in the semester,
we essentially looked at,

00:32:21.360 --> 00:32:24.168
how do people make these choices
for given probabilities, right?

00:32:24.168 --> 00:32:25.710
That was essentially
prospect theory.

00:32:25.710 --> 00:32:28.560
That was about Kahneman
and Tversky, 1979,

00:32:28.560 --> 00:32:30.990
which was how do people
think about risk?

00:32:30.990 --> 00:32:33.270
And what we always assumed
was that the probability

00:32:33.270 --> 00:32:35.398
distribution was given.

00:32:35.398 --> 00:32:37.440
There's a 50% chance of
something good happening.

00:32:37.440 --> 00:32:40.200
There's a 50% chance of
something bad happening,

00:32:40.200 --> 00:32:43.500
and how do people make
those kinds of choices?

00:32:43.500 --> 00:32:46.170
What does their utility
function look like?

00:32:46.170 --> 00:32:48.180
Now we're going to talk
about, how do people

00:32:48.180 --> 00:32:49.472
learn about such probabilities?

00:32:49.472 --> 00:32:52.110
How, in the first
place, do they update

00:32:52.110 --> 00:32:54.060
what probabilities are like?

00:32:54.060 --> 00:32:57.240
Which is, of course,
a very important input

00:32:57.240 --> 00:32:59.360
in their decision making.

00:32:59.360 --> 00:33:03.540
Now, this is based on pioneering
work by Kahneman and Tversky,

00:33:03.540 --> 00:33:05.650
even earlier than the '79 paper.

00:33:05.650 --> 00:33:08.920
This is, in particular,
a paper on 1974,

00:33:08.920 --> 00:33:14.220
which is in the reading list
and on the course website.

00:33:14.220 --> 00:33:15.940
Now first, I want
to start with--

00:33:15.940 --> 00:33:17.820
and this is important
in behavioral economics

00:33:17.820 --> 00:33:21.000
to acknowledge and
try to emphasize

00:33:21.000 --> 00:33:24.900
people are pretty rational and
pretty good in various ways,

00:33:24.900 --> 00:33:28.080
in the sense of people
get lots of things right.

00:33:28.080 --> 00:33:30.030
In particular, people
are pretty good in terms

00:33:30.030 --> 00:33:35.100
of broad directions of
people's probability judgments.

00:33:35.100 --> 00:33:37.290
Here's a very simple example.

00:33:37.290 --> 00:33:40.450
Imagine you're deciding whether
to see the new James Cameron

00:33:40.450 --> 00:33:40.950
movie.

00:33:40.950 --> 00:33:42.450
This could be a
James Cameron movie.

00:33:42.450 --> 00:33:45.270
It could be a Korean
drama or anything else.

00:33:45.270 --> 00:33:47.612
People think about they
want to watch something new.

00:33:47.612 --> 00:33:49.070
Well, how are you
going to do that?

00:33:49.070 --> 00:33:50.880
Well, you can read
some online reviews.

00:33:50.880 --> 00:33:53.340
You hear some opinions
from your friends.

00:33:53.340 --> 00:33:55.590
You look at some Rotten
Tomatoes or other ratings,

00:33:55.590 --> 00:33:58.830
and then you try to make
those kinds of choices.

00:33:58.830 --> 00:34:01.080
And so you start-- maybe you
probably would, like, OK,

00:34:01.080 --> 00:34:04.230
have I seen a James
Cameron movie before,

00:34:04.230 --> 00:34:07.530
or I said Korean drama,
whatever you're interested in.

00:34:07.530 --> 00:34:09.850
have I seen something
like this before?

00:34:09.850 --> 00:34:11.940
How much did I like
the previous one?

00:34:11.940 --> 00:34:13.846
That could be your prior
that you start with.

00:34:13.846 --> 00:34:15.929
And then you get these
other pieces of information

00:34:15.929 --> 00:34:18.872
of online reviews, various
opinions from friends,

00:34:18.872 --> 00:34:21.330
or a Rotten Tomato ratings,
and how do you use those, then?

00:34:21.330 --> 00:34:23.056
What do you do with those?

00:34:23.056 --> 00:34:24.639
So essentially what
you're going to do

00:34:24.639 --> 00:34:26.920
is you take some-- you
got some prior, which

00:34:26.920 --> 00:34:30.340
is about James Cameron movies
or whatever to start with.

00:34:30.340 --> 00:34:33.100
And that's like that--

00:34:33.100 --> 00:34:35.465
that's what you use to start
with in the first place.

00:34:35.465 --> 00:34:37.840
And then you get essentially
these pieces of information.

00:34:37.840 --> 00:34:40.870
And if they're positive
pieces of information

00:34:40.870 --> 00:34:43.840
about those specific movies,
then you're going to update up.

00:34:43.840 --> 00:34:46.330
And if they're bad, if
your friends don't like it,

00:34:46.330 --> 00:34:49.048
as you said, they update that.

00:34:49.048 --> 00:34:51.340
Notice that it could also be
that you have a friend who

00:34:51.340 --> 00:34:54.790
has terrible taste in
movies, and if that friend

00:34:54.790 --> 00:34:58.460
does not like a movie, that you
update the other way and so on.

00:34:58.460 --> 00:35:02.200
So it doesn't need to be that
your friend's movie ratings are

00:35:02.200 --> 00:35:04.537
positively correlated
with yours.

00:35:04.537 --> 00:35:05.870
That could still be informative.

00:35:05.870 --> 00:35:08.245
If somebody has terrible movie
taste and likes something,

00:35:08.245 --> 00:35:11.200
that could be actually good
news for the particular movie.

00:35:11.200 --> 00:35:13.222
But exactly-- this
is what I have here.

00:35:13.222 --> 00:35:14.680
Essentially what
you're going to do

00:35:14.680 --> 00:35:18.940
is a version of this would
be you start with your base

00:35:18.940 --> 00:35:20.530
rate or your prior.

00:35:20.530 --> 00:35:25.090
And then essentially you use the
various pieces of information

00:35:25.090 --> 00:35:27.045
and then adjust your
probability up or down.

00:35:27.045 --> 00:35:29.170
Suppose you have to sort
of [INAUDIBLE] willingness

00:35:29.170 --> 00:35:31.087
to pay or your probability
of seeing the movie

00:35:31.087 --> 00:35:33.760
or doing other things, or
your probability of liking

00:35:33.760 --> 00:35:37.270
the movie, you adjust
it up or down based

00:35:37.270 --> 00:35:39.640
on the different pieces of
information that we get.

00:35:39.640 --> 00:35:42.050
And so generally
people are pretty--

00:35:42.050 --> 00:35:45.250
and this is a pretty
reasonable and mathematical,

00:35:45.250 --> 00:35:47.540
mathematically
well-founded procedure.

00:35:47.540 --> 00:35:50.800
This is actually pretty close
to what a Bayesian would do.

00:35:50.800 --> 00:35:52.965
So people are actually
pretty good at this.

00:35:52.965 --> 00:35:55.570
They're pretty-- they
understand the basics of forming

00:35:55.570 --> 00:35:56.830
likelihood estimates.

00:35:56.830 --> 00:35:59.410
They know the direction
in which features

00:35:59.410 --> 00:36:01.540
a baseline likelihood
and available information

00:36:01.540 --> 00:36:02.680
should affect estimates.

00:36:02.680 --> 00:36:06.770
So overall, people
are pretty good.

00:36:06.770 --> 00:36:11.260
Now what's much harder to do is
not just a direction, but also

00:36:11.260 --> 00:36:13.480
the extent to which
you should update.

00:36:13.480 --> 00:36:18.340
That is to say, people know
which direction to move,

00:36:18.340 --> 00:36:21.940
as in like if you and
I like similar movies

00:36:21.940 --> 00:36:23.650
and I tell you this
movie was great,

00:36:23.650 --> 00:36:25.990
it's pretty clear that
you should move upwards.

00:36:25.990 --> 00:36:27.670
But what's much
harder to understand

00:36:27.670 --> 00:36:29.500
is by how much should
you move upwards

00:36:29.500 --> 00:36:32.080
in your estimate of how good
the movie or the probability

00:36:32.080 --> 00:36:35.260
that you like the movie is.

00:36:35.260 --> 00:36:37.300
And that's true for
many things in life.

00:36:37.300 --> 00:36:39.910
People are pretty good
at the direction in terms

00:36:39.910 --> 00:36:43.000
of where to move up or down.

00:36:43.000 --> 00:36:46.630
What they're much
worse at is how far

00:36:46.630 --> 00:36:48.800
to adjust their estimates.

00:36:48.800 --> 00:36:51.700
And that's where they
should be using Bayes' Rule,

00:36:51.700 --> 00:36:53.980
but that's extremely
cognitively demanding.

00:36:53.980 --> 00:36:59.630
And so in many situations people
tend to not get that right.

00:36:59.630 --> 00:37:03.620
Now, just to sort of remind
you, and I think in recitation

00:37:03.620 --> 00:37:05.580
you discussed this as
well, what is Bayes' Rule

00:37:05.580 --> 00:37:07.360
or how does Bayes' Rule work?

00:37:07.360 --> 00:37:09.540
Let me go very
quickly through that

00:37:09.540 --> 00:37:12.930
and then focus on
that next evidence.

00:37:12.930 --> 00:37:13.910
Here's one example.

00:37:13.910 --> 00:37:17.760
Suppose you have a coin that
you start off thinking as a--

00:37:17.760 --> 00:37:20.060
is fair with probability of 2/3.

00:37:20.060 --> 00:37:22.880
That means the coin is
biased towards heads

00:37:22.880 --> 00:37:25.010
or the coin is heads and tails.

00:37:25.010 --> 00:37:28.430
It's biased towards heads with
probability 1/3, in which case

00:37:28.430 --> 00:37:31.700
heads comes up 75% of the time.

00:37:31.700 --> 00:37:35.840
Now if you flip the coin
and it comes up H, heads,

00:37:35.840 --> 00:37:39.650
what's the probability
of that, that it's fair?

00:37:39.650 --> 00:37:45.380
Clearly, it should be 2/3,
because you've got some signal,

00:37:45.380 --> 00:37:46.940
but by how much
should you adjust

00:37:46.940 --> 00:37:48.140
is actually very hard to do.

00:37:48.140 --> 00:37:49.848
So you can think about
this in your head,

00:37:49.848 --> 00:37:52.460
and you realize it's
actually kind of hard to do,

00:37:52.460 --> 00:37:54.138
unless you can
actually write it down.

00:37:54.138 --> 00:37:56.180
So suppose you had to make
a very quick decision.

00:37:56.180 --> 00:37:57.890
In particular, it's
actually very hard

00:37:57.890 --> 00:37:59.735
to do this quickly on your own.

00:37:59.735 --> 00:38:01.610
You can do this particularly
with one signal,

00:38:01.610 --> 00:38:03.250
but suppose you
get several signals

00:38:03.250 --> 00:38:06.097
and it gets much harder
and harder over time.

00:38:06.097 --> 00:38:07.180
Now how would you do this?

00:38:07.180 --> 00:38:08.750
You can sort of-- in
this particular case,

00:38:08.750 --> 00:38:10.345
you can do a graphical
illustration.

00:38:10.345 --> 00:38:11.720
Again, I'll do
this very quickly,

00:38:11.720 --> 00:38:13.933
but you can read
about it overall.

00:38:13.933 --> 00:38:15.850
You can think about,
there's sort of two types

00:38:15.850 --> 00:38:19.840
of coins or like urns.

00:38:19.840 --> 00:38:22.690
There's a fair coin
that's represented

00:38:22.690 --> 00:38:26.080
by an urn with equal number
of heads and tail balls.

00:38:26.080 --> 00:38:27.670
And there's an
unfair coin, which

00:38:27.670 --> 00:38:31.930
is represented by an urn in
which 75% of the balls are H.

00:38:31.930 --> 00:38:34.060
So you don't know
whether the coin is fair,

00:38:34.060 --> 00:38:36.170
so you don't know which
urn you're drawing from.

00:38:36.170 --> 00:38:37.660
And one way to
think about this is

00:38:37.660 --> 00:38:39.460
imagine you're drawing
from an urn that

00:38:39.460 --> 00:38:41.380
contains both of those urns.

00:38:41.380 --> 00:38:43.780
Of course, you need to
then also take into account

00:38:43.780 --> 00:38:47.390
the probability of the
fair ball being 2/3.

00:38:47.390 --> 00:38:51.460
So the fair urn has twice as
many balls as the unfair one.

00:38:51.460 --> 00:38:53.680
So you can sort of
represent it in that way.

00:38:53.680 --> 00:38:56.070
And then suppose you
draw a ball randomly

00:38:56.070 --> 00:38:57.520
and it's H, what's
the probability

00:38:57.520 --> 00:38:59.230
that it came from the fair urn?

00:38:59.230 --> 00:39:03.430
Then again, if you've taken
probability classes and so on,

00:39:03.430 --> 00:39:07.540
that should be relatively
simple for you to calculate.

00:39:07.540 --> 00:39:10.340
But here in the graphical way,
you can look this very simple.

00:39:10.340 --> 00:39:12.910
There's essentially
seven red balls.

00:39:12.910 --> 00:39:16.480
There's four in the fair coin
and three in the unfair coin.

00:39:16.480 --> 00:39:21.580
So the probability that it
came from the fair coin is 4/7.

00:39:21.580 --> 00:39:24.400
Similarly, if you draw a
T, then the probability

00:39:24.400 --> 00:39:28.000
it came from the fair
urn is 4/5, right?

00:39:28.000 --> 00:39:29.878
And that's, in some sense--

00:39:29.878 --> 00:39:32.420
you can write down the math,
and it's pretty clear and so on.

00:39:32.420 --> 00:39:36.200
But in a way, it's hard to do
this in your head in the sense

00:39:36.200 --> 00:39:38.500
that you know it should
be lower than 2/3,

00:39:38.500 --> 00:39:40.600
but how much lower and
how much should you adjust

00:39:40.600 --> 00:39:41.800
is actually kind of hard.

00:39:41.800 --> 00:39:45.700
And here the answer
happens to be 4/7.

00:39:45.700 --> 00:39:49.820
And so here's sort of the
formal way of doing this.

00:39:49.820 --> 00:39:51.070
I'm not going to go into this.

00:39:51.070 --> 00:39:54.490
But trust me, the answer is 4/7.

00:39:54.490 --> 00:39:57.465
Now, even this simple
example of Bayes' rule,

00:39:57.465 --> 00:39:58.840
maybe it was very
simple for you.

00:39:58.840 --> 00:40:01.150
But it's somewhat
difficult to follow

00:40:01.150 --> 00:40:04.090
in the sense of would you have
gotten the first sentence right

00:40:04.090 --> 00:40:06.040
immediately if I
gave you 10 seconds?

00:40:06.040 --> 00:40:07.090
Maybe, maybe not.

00:40:07.090 --> 00:40:11.810
But probably quite a few of
you would have gotten it wrong.

00:40:11.810 --> 00:40:15.250
Now, in addition,
then, if you try

00:40:15.250 --> 00:40:18.370
to apply these kinds of rules or
the base rule in new situations

00:40:18.370 --> 00:40:20.590
with multiple pieces of
different information,

00:40:20.590 --> 00:40:22.540
it's far more
difficult. Suppose I

00:40:22.540 --> 00:40:25.660
told you to draw like 17
times and get those signals

00:40:25.660 --> 00:40:29.050
and without replacement and
with replacement and so on.

00:40:29.050 --> 00:40:31.810
Things would get way,
way more difficult.

00:40:31.810 --> 00:40:34.510
And the urn example is, in
fact, a much easier one.

00:40:34.510 --> 00:40:37.960
Because there are at least
I'm telling you exactly what

00:40:37.960 --> 00:40:39.430
the probabilities are.

00:40:39.430 --> 00:40:41.020
But in many
real-world situations,

00:40:41.020 --> 00:40:42.130
you don't even know that.

00:40:42.130 --> 00:40:44.150
A friend telling you the
movie is good or bad,

00:40:44.150 --> 00:40:46.900
it's not clear how
to read that signal,

00:40:46.900 --> 00:40:50.290
how do you think about
that signal as a whole.

00:40:50.290 --> 00:40:52.390
So but most people don't
know the precise rule

00:40:52.390 --> 00:40:53.380
of Bayes' rule.

00:40:53.380 --> 00:40:55.680
I guess, at MIT,
most students do.

00:40:55.680 --> 00:40:58.040
But in the general
population, people don't.

00:40:58.040 --> 00:41:00.040
And even if they do,
they can't or don't want

00:41:00.040 --> 00:41:03.140
to think so hard in many cases.

00:41:03.140 --> 00:41:06.130
So what people do instead,
they use intuitive shortcuts

00:41:06.130 --> 00:41:10.140
to make judgments
of likelihoods.

00:41:10.140 --> 00:41:12.080
Now, that's a good thing.

00:41:12.080 --> 00:41:14.590
So we want people to use these
shortcuts because otherwise

00:41:14.590 --> 00:41:16.720
they will just freeze and not
be able to make any choices.

00:41:16.720 --> 00:41:18.553
And so, in some sense,
it's good that people

00:41:18.553 --> 00:41:22.470
are sort of simplifying problems
and they make some choices

00:41:22.470 --> 00:41:24.040
quickly.

00:41:24.040 --> 00:41:25.680
So in some sense, it's good.

00:41:25.680 --> 00:41:29.810
But it also leads to
systematic mistakes.

00:41:29.810 --> 00:41:31.860
Because in some sense,
you can use shortcuts.

00:41:31.860 --> 00:41:34.950
And they help you to get
things approximately right.

00:41:34.950 --> 00:41:38.430
But now, if we can
understand their shortcuts,

00:41:38.430 --> 00:41:40.260
then we can also
understand, potentially,

00:41:40.260 --> 00:41:42.555
their systematic mistakes.

00:41:45.730 --> 00:41:47.730
So what people
tend to do is they

00:41:47.730 --> 00:41:52.680
focus on one or a small
set of the situations

00:41:52.680 --> 00:41:54.120
at hand that seems
most relevant.

00:41:54.120 --> 00:41:56.470
They focus on that to
make their decision.

00:41:56.470 --> 00:41:59.760
And often, then,
they systematically

00:41:59.760 --> 00:42:04.020
neglect other more
complicated other issues

00:42:04.020 --> 00:42:07.200
of the situation, which
makes the likelihood

00:42:07.200 --> 00:42:09.450
of their estimates
typically incorrect.

00:42:09.450 --> 00:42:11.910
And now the question is,
can we sort of understand

00:42:11.910 --> 00:42:13.350
what people focus on?

00:42:13.350 --> 00:42:17.250
And can we understand what
causes these systematic biases

00:42:17.250 --> 00:42:19.740
to sort of make clear
predictions on how

00:42:19.740 --> 00:42:21.135
people do things wrongly.

00:42:21.135 --> 00:42:23.010
And then, perhaps, if
you want to help people

00:42:23.010 --> 00:42:26.100
make better choices,
you can provide them

00:42:26.100 --> 00:42:29.900
with valid information as well.

00:42:29.900 --> 00:42:34.240
Now, one particularly
interesting issue

00:42:34.240 --> 00:42:37.460
is sequences of random outcomes.

00:42:37.460 --> 00:42:39.640
So as I just showed you,
if you sort of drop balls

00:42:39.640 --> 00:42:42.370
from an urn, that's a
very relevant situation,

00:42:42.370 --> 00:42:45.400
not because people in the real
world in real-world situations,

00:42:45.400 --> 00:42:49.690
drop balls from urns, but rather
because, in many situations,

00:42:49.690 --> 00:42:52.510
you do actually get
repeated signals over time

00:42:52.510 --> 00:42:54.715
that you should use to update.

00:42:54.715 --> 00:42:56.590
So an investor might
observe past performance

00:42:56.590 --> 00:42:59.710
of mutual funds before deciding
which one to invest in.

00:42:59.710 --> 00:43:01.900
A patient of a doctor
might observe the outcome

00:43:01.900 --> 00:43:03.580
of prior surgeries
before deciding

00:43:03.580 --> 00:43:07.450
whether to undertake the surgery
or which doctor to choose.

00:43:07.450 --> 00:43:10.090
A coach can observe the recent
performance of a basketball

00:43:10.090 --> 00:43:13.060
player before deciding whether
to put the player in the game.

00:43:13.060 --> 00:43:15.070
There's lots of kinds of
situations where people

00:43:15.070 --> 00:43:17.710
get repeated signals
over time and then

00:43:17.710 --> 00:43:21.130
have to try to infer
likelihoods of probabilities

00:43:21.130 --> 00:43:23.940
of certain events to happen.

00:43:23.940 --> 00:43:25.690
So that's a very common
thing in the world

00:43:25.690 --> 00:43:28.810
that we should try
and understand.

00:43:28.810 --> 00:43:32.210
Now, one systematic pattern
that we see in the world

00:43:32.210 --> 00:43:34.480
is what's called the
gambler's fallacy.

00:43:34.480 --> 00:43:37.720
That's the false belief that, in
a sequence of independent draws

00:43:37.720 --> 00:43:39.520
from a distribution,
an outcome that

00:43:39.520 --> 00:43:43.180
hasn't occurred for a while
is more likely to come up

00:43:43.180 --> 00:43:44.420
on the next block.

00:43:44.420 --> 00:43:46.795
So the important part and
what's doing a lot of work here

00:43:46.795 --> 00:43:48.700
is independent draws.

00:43:48.700 --> 00:43:51.080
Suppose there's a
distribution of outcomes

00:43:51.080 --> 00:43:53.740
and there's independent
draws, which

00:43:53.740 --> 00:43:57.250
means that, essentially, what
happened in the last two,

00:43:57.250 --> 00:43:59.350
three, or four draws
should not have

00:43:59.350 --> 00:44:02.200
any effect on what's going
to happen in the next draw.

00:44:02.200 --> 00:44:05.260
So if you play roulette or
any sort of poker or the like,

00:44:05.260 --> 00:44:09.850
if you get read several
times in a row, that

00:44:09.850 --> 00:44:14.200
has exactly no impact on how
likely it is that red or black

00:44:14.200 --> 00:44:17.500
is coming up in the next draw.

00:44:17.500 --> 00:44:19.900
But people tend to have sort
of this almost like folk

00:44:19.900 --> 00:44:23.200
knowledge of, well, if
red came up a few times,

00:44:23.200 --> 00:44:25.270
now black is due, and
the other way around.

00:44:30.240 --> 00:44:32.330
And that's true in many
different situations.

00:44:32.330 --> 00:44:35.100
But the important part
here is that there's

00:44:35.100 --> 00:44:36.900
a sequence of independent draws.

00:44:36.900 --> 00:44:43.200
So as to say, these
are situations

00:44:43.200 --> 00:44:46.020
where the probability
distribution is actually null.

00:44:46.020 --> 00:44:49.350
That is to say, when you play
roulette or you go to a casino,

00:44:49.350 --> 00:44:52.440
it's not like you need
to learn about what's

00:44:52.440 --> 00:44:55.695
the probability that the
roulette is fair or not.

00:44:55.695 --> 00:44:58.620
You know that there's some
monitoring and so on in those

00:44:58.620 --> 00:44:59.670
situations.

00:44:59.670 --> 00:45:02.640
You know, essentially, that the
chance of getting red or black

00:45:02.640 --> 00:45:04.140
is the same.

00:45:04.140 --> 00:45:07.050
And yet people tend to make
these kinds of updates.

00:45:07.050 --> 00:45:08.800
There's a nice paper
by Gold and Hester,

00:45:08.800 --> 00:45:12.630
an old paper that shows
this quite nicely,

00:45:12.630 --> 00:45:15.870
in which subjects are
told the coin with a black

00:45:15.870 --> 00:45:18.810
and a red side would
be flipped 25 times.

00:45:18.810 --> 00:45:21.570
And the experiment is
slightly sneaky here,

00:45:21.570 --> 00:45:23.730
because essentially
actually they

00:45:23.730 --> 00:45:26.250
reported people a predetermined
sequence of events.

00:45:26.250 --> 00:45:27.940
So there's some
deception involved here,

00:45:27.940 --> 00:45:32.110
which is a little bit
trickier but it's mostly fine.

00:45:32.110 --> 00:45:34.380
So what they see is
essentially the subject

00:45:34.380 --> 00:45:37.650
sees 17 mixed coin flips.

00:45:37.650 --> 00:45:39.870
So they see a sequence
of coin flips.

00:45:39.870 --> 00:45:41.650
It's red and black and so on.

00:45:41.650 --> 00:45:45.180
And then, at the end of
it, you see essentially

00:45:45.180 --> 00:45:47.520
one black and four reds.

00:45:47.520 --> 00:45:49.980
And so, again, this
is supposed to be

00:45:49.980 --> 00:45:53.190
like independent draws,
which to make this case,

00:45:53.190 --> 00:45:58.140
in this experiment, they sort
of rigged the last five draws,

00:45:58.140 --> 00:45:59.760
which is not quite ideal.

00:45:59.760 --> 00:46:02.652
But let's just go
with this for a bit.

00:46:02.652 --> 00:46:04.860
And so what essentially
people see in this experiment

00:46:04.860 --> 00:46:14.670
is mostly evidence
of red and black

00:46:14.670 --> 00:46:16.770
where roughly red
and black is even.

00:46:16.770 --> 00:46:18.765
If anything-- so
the 17 are mixed--

00:46:18.765 --> 00:46:20.130
these are actually draws--

00:46:20.130 --> 00:46:23.940
if anything, people saw more
red than they saw black.

00:46:23.940 --> 00:46:26.460
Then, on the 23rd
flip, after seeing

00:46:26.460 --> 00:46:30.540
these 22 flips to start
with, the participants

00:46:30.540 --> 00:46:33.300
were given a choice
between 70 points

00:46:33.300 --> 00:46:37.368
for sure or 100 points if the
next flip was their color.

00:46:37.368 --> 00:46:38.910
And points are
essentially stuff they

00:46:38.910 --> 00:46:42.060
can get money for eventually.

00:46:42.060 --> 00:46:45.090
And so now it's randomly chosen
whether half of the subjects

00:46:45.090 --> 00:46:48.270
color was red and half
of them was black.

00:46:48.270 --> 00:46:52.170
And so now the propensity to
take the 70 points reveals

00:46:52.170 --> 00:46:55.290
the beliefs about the odds
that the next flip would

00:46:55.290 --> 00:46:56.842
be their color.

00:46:56.842 --> 00:47:00.060
So if you think
the probability is

00:47:00.060 --> 00:47:04.110
50%, which is kind of like,
given the information that you

00:47:04.110 --> 00:47:08.610
got, you should probably
roughly think it's 50%,

00:47:08.610 --> 00:47:10.450
and if you're risk-averse
in particular,

00:47:10.450 --> 00:47:13.650
you should truly choose
the 70 points for sure.

00:47:13.650 --> 00:47:17.520
Because an expectation, if you
get p times 100, if it's 50%,

00:47:17.520 --> 00:47:20.290
you get, essentially, 50.

00:47:20.290 --> 00:47:23.310
So you're only got to choose the
"100 points if the next flip is

00:47:23.310 --> 00:47:29.520
your color" if you really think
your color is likely to occur.

00:47:29.520 --> 00:47:31.980
And in particular if your
probability of that occurring

00:47:31.980 --> 00:47:35.850
is like 70% or higher,
notice that that even

00:47:35.850 --> 00:47:38.070
assumes risk neutrality.

00:47:38.070 --> 00:47:39.840
So if people are
risk-neutral, if they

00:47:39.840 --> 00:47:43.450
think the probability is 70%,
they should be different.

00:47:43.450 --> 00:47:45.120
And again, if
they're risk neutral

00:47:45.120 --> 00:47:49.180
and they choose number 2 here,
their color, well, that's

00:47:49.180 --> 00:47:52.100
only the case if they think
the probability of the color

00:47:52.100 --> 00:47:53.410
occurring is higher than 70%.

00:47:56.070 --> 00:47:58.290
And so given the
evidence that they saw,

00:47:58.290 --> 00:48:00.660
it's going to be
rarely the case that--

00:48:00.660 --> 00:48:03.960
or people should really, given
the evidence that's there,

00:48:03.960 --> 00:48:06.837
the best thing they
can do is essentially--

00:48:06.837 --> 00:48:08.670
or the thing is set up
such that they really

00:48:08.670 --> 00:48:11.500
should choose number one.

00:48:11.500 --> 00:48:14.970
But of course, the
idea is here, if people

00:48:14.970 --> 00:48:18.330
think that it was four times
red, so now black is due,

00:48:18.330 --> 00:48:22.350
the prediction would be that
people would be more likely

00:48:22.350 --> 00:48:28.320
or will choose item number
2 if their color is black.

00:48:28.320 --> 00:48:35.150
And so 24 of 29
red subjects chose

00:48:35.150 --> 00:48:37.970
to take the sure thing, which
essentially is option 1 here

00:48:37.970 --> 00:48:39.110
that I showed you.

00:48:39.110 --> 00:48:42.170
And eight of 30 of the
black subjects did.

00:48:42.170 --> 00:48:46.200
So these are essentially the
people who choose their color.

00:48:46.200 --> 00:48:48.200
So if people have
black as their color,

00:48:48.200 --> 00:48:50.120
they essentially
choose this option

00:48:50.120 --> 00:48:52.880
because they really
think now black is due

00:48:52.880 --> 00:48:57.650
because there's been
essentially four reds in a row.

00:48:57.650 --> 00:49:00.170
Now, again, if you're
told-- and that's

00:49:00.170 --> 00:49:02.120
a little bit in play
in this experiment--

00:49:02.120 --> 00:49:05.900
about these being independent
events, which for coin flips,

00:49:05.900 --> 00:49:07.880
really they should be
independent events,

00:49:07.880 --> 00:49:09.320
that doesn't make any sense.

00:49:09.320 --> 00:49:12.248
You should, if
anything, think red

00:49:12.248 --> 00:49:13.790
is more likely
because the coin might

00:49:13.790 --> 00:49:17.270
be sort more likely to be red.

00:49:17.270 --> 00:49:20.000
But sort of thinking
that black is more likely

00:49:20.000 --> 00:49:23.170
surely is not a good idea.

00:49:23.170 --> 00:49:25.898
And it might essentially
make you lose money.

00:49:25.898 --> 00:49:27.440
Now, there's some
interesting variant

00:49:27.440 --> 00:49:32.030
of this for which they
set, for some variants,

00:49:32.030 --> 00:49:37.890
the 23rd coin flip was delayed
by some time, 24 minutes.

00:49:37.890 --> 00:49:40.110
And what they then
find is, in fact,

00:49:40.110 --> 00:49:43.760
weaker evidence of what's called
the gambler's fallacy, which

00:49:43.760 --> 00:49:48.560
seems to say that once you
let the coin rest for a while,

00:49:48.560 --> 00:49:50.510
people seem to think
that letting the coin

00:49:50.510 --> 00:49:56.210
rest for a while, which sort of
makes it more likely, I guess,

00:49:56.210 --> 00:49:59.780
that the streak continues, or
less likely that essentially

00:49:59.780 --> 00:50:01.560
black now is the case.

00:50:01.560 --> 00:50:04.820
So people seem to these fairly--

00:50:04.820 --> 00:50:07.010
and I think that's fair to
say-- irrational beliefs

00:50:07.010 --> 00:50:10.670
about or biased beliefs about
what's going to happen next.

00:50:10.670 --> 00:50:15.920
And that seems to be easily
swayed by even relatively

00:50:15.920 --> 00:50:16.830
small things.

00:50:16.830 --> 00:50:19.520
It's like the coin
needs to revert.

00:50:19.520 --> 00:50:23.440
But if you wait for 24
minutes, not so much anymore.

00:50:23.440 --> 00:50:25.940
There's quite a bit of work on
the gambler's fallacy overall

00:50:25.940 --> 00:50:26.780
in various settings.

00:50:26.780 --> 00:50:29.652
That's just one setting
to illustrate this.

00:50:29.652 --> 00:50:31.610
There's quite a bit of
other work in that area.

00:50:31.610 --> 00:50:34.505
And it's a fairly robust
finding that people

00:50:34.505 --> 00:50:37.160
have found in the literature.

00:50:37.160 --> 00:50:40.010
Now, a second pattern
that people find

00:50:40.010 --> 00:50:42.590
is what's called the
hot-hand fallacy.

00:50:42.590 --> 00:50:45.720
That's the idea that, in
particular basketball fans

00:50:45.720 --> 00:50:49.100
or other sort of
types of sports fans--

00:50:49.100 --> 00:50:52.040
fans, players, coaches--
believe that there

00:50:52.040 --> 00:50:54.830
is systematic day-to-day
operations in players'

00:50:54.830 --> 00:50:55.760
shooting performance.

00:50:55.760 --> 00:50:57.427
And that's for
basketball, but it's also

00:50:57.427 --> 00:50:59.400
true for other sports.

00:50:59.400 --> 00:51:04.100
So the idea is that the
performance of a player

00:51:04.100 --> 00:51:06.500
may sometimes be predictably
better than expected

00:51:06.500 --> 00:51:09.240
on the basis of the
player's overall record.

00:51:09.240 --> 00:51:11.690
And so what people would
say is, well, the player

00:51:11.690 --> 00:51:14.540
is on fire today or is a
streak shooter and so on.

00:51:14.540 --> 00:51:17.690
And so the idea is
that "on fire" today

00:51:17.690 --> 00:51:23.880
means that he or she is more
likely to hit his or her shots

00:51:23.880 --> 00:51:25.160
than on other days.

00:51:25.160 --> 00:51:27.290
So that's to say
prediction is that made

00:51:27.290 --> 00:51:29.060
shots should cluster together.

00:51:29.060 --> 00:51:31.250
Like on one day you happen
to be really good, maybe,

00:51:31.250 --> 00:51:32.000
in the first half.

00:51:32.000 --> 00:51:37.430
The second half, and conditional
of having made a few shots,

00:51:37.430 --> 00:51:41.247
the next shot should be
more likely than sort

00:51:41.247 --> 00:51:43.820
of the unconditional
probability.

00:51:43.820 --> 00:51:46.280
There's lots of
work on this issue,

00:51:46.280 --> 00:51:49.610
starting by Gilovich and
Vallone and Tversky in 1985.

00:51:49.610 --> 00:51:56.110
People have gone back and
forth between saying in fact

00:51:56.110 --> 00:51:58.870
there is sort of such a
thing as a hot hand or not.

00:51:58.870 --> 00:52:00.340
The initial claims
were essentially

00:52:00.340 --> 00:52:01.360
there is no such thing.

00:52:01.360 --> 00:52:04.480
People believe that
there's a hot hand going on

00:52:04.480 --> 00:52:09.010
and players are really running
hot, but in fact, they're not.

00:52:09.010 --> 00:52:10.660
Some other evidence
later showed maybe

00:52:10.660 --> 00:52:11.952
there is actually such a thing.

00:52:11.952 --> 00:52:13.700
And it was contradicted again.

00:52:13.700 --> 00:52:14.890
So it's kind of complicated.

00:52:14.890 --> 00:52:17.710
But overall it seems to be
that people tend to be--

00:52:17.710 --> 00:52:19.900
players, fans, and
so on-- tend to be

00:52:19.900 --> 00:52:25.270
quite overoptimistic about the
hot-handed streaks happening

00:52:25.270 --> 00:52:28.200
in reality.

00:52:28.200 --> 00:52:31.810
Now, one question is that that
seems kind of odd in some ways.

00:52:31.810 --> 00:52:36.720
And so, on the one hand,
the hot-hand fallacy

00:52:36.720 --> 00:52:40.840
and the gambler's fallacy seem
to be opposites of each other.

00:52:40.840 --> 00:52:42.900
That is to say, the
gambler's fallacy

00:52:42.900 --> 00:52:44.430
is the belief that
the next outcome

00:52:44.430 --> 00:52:47.820
is likely to be different
from the previous ones.

00:52:47.820 --> 00:52:50.100
If I show you, if
you're gambling,

00:52:50.100 --> 00:52:51.690
you've got like four
reds, now you say

00:52:51.690 --> 00:52:52.857
it's going to be black next.

00:52:52.857 --> 00:52:54.510
So if you have three
heads, then you

00:52:54.510 --> 00:52:57.220
think that your coin is going
to show some tails, that's

00:52:57.220 --> 00:53:01.140
essentially saying you saw a
bunch of outcomes of one kind,

00:53:01.140 --> 00:53:03.340
the next one will
likely be different.

00:53:03.340 --> 00:53:05.940
On the other hand, then hot-hand
fallacy is saying, well,

00:53:05.940 --> 00:53:07.560
it's a belief that
the next outcome

00:53:07.560 --> 00:53:10.395
is likely to be similar
to the previous ones.

00:53:10.395 --> 00:53:13.797
Now, what's going on here,
you can think of both of these

00:53:13.797 --> 00:53:15.630
as a consequence of
what's called the belief

00:53:15.630 --> 00:53:17.335
in the law of small numbers.

00:53:17.335 --> 00:53:18.240
Now, what is that?

00:53:18.240 --> 00:53:20.490
You should all know the
law of large numbers, which

00:53:20.490 --> 00:53:23.322
is, in large samples
of independent draws

00:53:23.322 --> 00:53:24.780
from a distribution,
the proportion

00:53:24.780 --> 00:53:27.540
of different outcomes closely
reflects the underlying

00:53:27.540 --> 00:53:28.200
probability.

00:53:28.200 --> 00:53:31.453
So essentially once you
have sufficiently many draws

00:53:31.453 --> 00:53:33.120
from some distribution,
the distribution

00:53:33.120 --> 00:53:38.220
will converge to what
you're drawing from.

00:53:38.220 --> 00:53:41.370
Now, what's the belief in
the law of small numbers?

00:53:41.370 --> 00:53:43.860
Well, it's the belief
that, in small samples,

00:53:43.860 --> 00:53:45.360
the proportion of
different outcomes

00:53:45.360 --> 00:53:47.930
should reflect the
underlying probabilities.

00:53:47.930 --> 00:53:49.680
So usually, the law
of large numbers, it's

00:53:49.680 --> 00:53:51.750
called the law of large
numbers because you

00:53:51.750 --> 00:53:53.550
need the large number of draws.

00:53:53.550 --> 00:53:55.110
That's why it's called that way.

00:53:55.110 --> 00:53:58.140
But people seem to think
that the law of large numbers

00:53:58.140 --> 00:54:01.650
also applies to small samples.

00:54:01.650 --> 00:54:06.720
And so how does it
explain our puzzle?

00:54:06.720 --> 00:54:09.990
Well, then there's a question
of does the person know

00:54:09.990 --> 00:54:12.720
the underlying distribution?

00:54:12.720 --> 00:54:14.970
So if you think you know the
underlying distribution--

00:54:14.970 --> 00:54:18.930
for example, if you think for
sure that the coin is fair

00:54:18.930 --> 00:54:23.290
or the roulette table is not
cheating you, you might say,

00:54:23.290 --> 00:54:26.970
well, roughly, I
should see as many reds

00:54:26.970 --> 00:54:30.390
as I should see blacks or
I should see as many heads

00:54:30.390 --> 00:54:33.750
as I should see tails.

00:54:33.750 --> 00:54:37.050
And so now, if I have a
sample of like four draws,

00:54:37.050 --> 00:54:39.090
if I already have
like four heads,

00:54:39.090 --> 00:54:41.220
then I might sort of think,
in that small sample,

00:54:41.220 --> 00:54:45.720
I need to see, essentially,
a roughly equal fraction

00:54:45.720 --> 00:54:47.070
of heads and tails.

00:54:47.070 --> 00:54:49.110
And therefore, if I
have seen four reds,

00:54:49.110 --> 00:54:51.210
the next one is due to be black.

00:54:51.210 --> 00:54:54.030
And of course that's not
true because essentially

00:54:54.030 --> 00:54:58.567
the large number does not
apply to small samples

00:54:58.567 --> 00:55:00.400
because it's called the
law of large numbers

00:55:00.400 --> 00:55:02.460
and not the law
of small numbers.

00:55:02.460 --> 00:55:05.160
But mistakenly, people
seem to think the law

00:55:05.160 --> 00:55:08.830
of small numbers applies.

00:55:08.830 --> 00:55:13.180
In contrast, if the person
does not know the distribution,

00:55:13.180 --> 00:55:14.860
the belief in the
law of small numbers

00:55:14.860 --> 00:55:17.573
can lead to the
hot-hand fallacy.

00:55:17.573 --> 00:55:19.240
That's to say, if
you're trying to learn

00:55:19.240 --> 00:55:24.760
about a distribution of shots
or the underlying probability,

00:55:24.760 --> 00:55:28.090
if you see some good events in
a row, if you see three heads

00:55:28.090 --> 00:55:31.630
or tails and so on, if you
see a player making three

00:55:31.630 --> 00:55:35.410
shots in a row, you might sort
of try to infer, oh, today

00:55:35.410 --> 00:55:36.970
is a good day or a bad day.

00:55:36.970 --> 00:55:38.758
And people tend to over-infer.

00:55:38.758 --> 00:55:41.050
They tend to think they can
learn more from these three

00:55:41.050 --> 00:55:42.610
shots than they actually do.

00:55:42.610 --> 00:55:47.890
Really, to be able to
estimate whether a player is

00:55:47.890 --> 00:55:51.700
running hot on a given day,
you need quite a few draws.

00:55:51.700 --> 00:55:53.980
But people think, if a
player makes three shots,

00:55:53.980 --> 00:55:57.130
that already means that
his probability of making

00:55:57.130 --> 00:56:00.230
good shots on that day are way
higher than it actually is.

00:56:00.230 --> 00:56:03.970
So essentially people
mistakenly seem to over-infer,

00:56:03.970 --> 00:56:07.090
from a small number
of observations,

00:56:07.090 --> 00:56:09.760
what the underlying
probability distribution is.

00:56:09.760 --> 00:56:13.060
And in that sense, the belief
in the law of small numbers

00:56:13.060 --> 00:56:16.400
can lead to the hot-hand
fallacy as well.

00:56:16.400 --> 00:56:21.250
So these are essentially two
very different conclusions

00:56:21.250 --> 00:56:24.970
that seem to come from
the same underlying issue.

00:56:24.970 --> 00:56:27.920
Then the last pattern
I'm going to show you--

00:56:27.920 --> 00:56:29.920
by the way, there's more
patterns such as those,

00:56:29.920 --> 00:56:32.560
but I'm sort of just showing
you three of them to give you

00:56:32.560 --> 00:56:34.050
a sense of what's going on.

00:56:34.050 --> 00:56:36.340
Remember the example
that I showed you

00:56:36.340 --> 00:56:41.380
in the first lecture, which was
a question about base rates,

00:56:41.380 --> 00:56:45.130
base-rate neglect, which is the
question about 1 in 100 people

00:56:45.130 --> 00:56:45.850
have HIV.

00:56:45.850 --> 00:56:49.570
If we have a test that is 99%
accurate, if a person tests

00:56:49.570 --> 00:56:52.347
positive, what's the probability
of having the disease?

00:56:52.347 --> 00:56:53.680
And so I showed you this before.

00:56:53.680 --> 00:56:55.190
The true answer is 50%.

00:56:55.190 --> 00:56:59.230
But quite a few if
you answered 99%.

00:56:59.230 --> 00:57:04.120
And the plausible explanation
is that people probably

00:57:04.120 --> 00:57:07.600
forgot to take into account how
few people in the population

00:57:07.600 --> 00:57:10.470
have the disease to start with.

00:57:10.470 --> 00:57:13.250
And so this is what's called
the base-rate neglect.

00:57:13.250 --> 00:57:18.140
When people are given some new
information, people tend to--

00:57:18.140 --> 00:57:22.670
in this case, I guess, a
test being 99% accurate,

00:57:22.670 --> 00:57:25.220
people tend to focus on
that piece of information.

00:57:25.220 --> 00:57:28.620
They tend to neglect the
underlying base rate, which,

00:57:28.620 --> 00:57:31.160
in this case, is like
one in 100 people

00:57:31.160 --> 00:57:34.550
tend to have the
disease to start with.

00:57:34.550 --> 00:57:38.360
And again, that's a
natural consequence

00:57:38.360 --> 00:57:41.090
of focusing too much
on one central aspect

00:57:41.090 --> 00:57:45.080
offhand, which is the
99% accuracy of the test.

00:57:45.080 --> 00:57:47.783
In a way, that,
again, can be useful.

00:57:47.783 --> 00:57:49.700
But it's sort of
systematically getting people

00:57:49.700 --> 00:57:51.110
to make wrong choices.

00:57:51.110 --> 00:57:53.930
And there's quite
a bit of evidence

00:57:53.930 --> 00:57:55.650
showing this in the literature.

00:57:55.650 --> 00:57:57.560
And here's, again, what
you should be doing

00:57:57.560 --> 00:57:59.690
and here's the
probability of this.

00:57:59.690 --> 00:58:02.620
Trust me that this
really is 50%.

00:58:02.620 --> 00:58:07.560
Now, the last piece
here, in terms

00:58:07.560 --> 00:58:12.450
of biases or systematic biases,
what I showed you so far was,

00:58:12.450 --> 00:58:17.650
well, people have a hard time
making right choices even

00:58:17.650 --> 00:58:20.810
in simple informational
environments.

00:58:20.810 --> 00:58:24.400
Now, two-dimensional learning
is even more complicated.

00:58:24.400 --> 00:58:27.910
And what I mean by
two-dimensional is to say,

00:58:27.910 --> 00:58:31.780
if you try to learn about
news or the accuracy of news,

00:58:31.780 --> 00:58:34.900
if you watch news
online or anywhere else,

00:58:34.900 --> 00:58:37.900
you don't need to only learn
about somebody gives you

00:58:37.900 --> 00:58:40.060
some information and
you try to update based

00:58:40.060 --> 00:58:42.340
on that information,
but rather you

00:58:42.340 --> 00:58:43.960
also need to learn
at the same time

00:58:43.960 --> 00:58:47.230
about the accuracy of
the piece of information

00:58:47.230 --> 00:58:48.430
that's given to you.

00:58:48.430 --> 00:58:51.580
That is to say, if you watch
something online, any article,

00:58:51.580 --> 00:58:53.860
not only you need to
sort of understand

00:58:53.860 --> 00:58:55.360
what to learn from
that information,

00:58:55.360 --> 00:58:57.280
that piece of
information, but you also

00:58:57.280 --> 00:58:59.380
need to understand
the underlying source

00:58:59.380 --> 00:58:59.880
Of it.

00:58:59.880 --> 00:59:01.547
And there's quite a
bit of work recently

00:59:01.547 --> 00:59:07.230
on fake news and the question
on, can people in fact

00:59:07.230 --> 00:59:08.940
detect fake news?

00:59:08.940 --> 00:59:11.770
Does it have to do with
partisanship in particular?

00:59:11.770 --> 00:59:14.790
So you might think that
Republicans or Democrats one

00:59:14.790 --> 00:59:17.530
certainly news to be true.

00:59:17.530 --> 00:59:19.620
So you might sort of
think that Republicans

00:59:19.620 --> 00:59:23.010
are more likely to want to
believe views that are biased

00:59:23.010 --> 00:59:24.900
towards Republicans,
Democrats are

00:59:24.900 --> 00:59:26.700
more likely to
believe news that are

00:59:26.700 --> 00:59:29.400
more likely to be Democrats.

00:59:29.400 --> 00:59:35.310
And therefore, they might be
more likely to trust fake news

00:59:35.310 --> 00:59:36.880
or fall for fake news.

00:59:36.880 --> 00:59:39.330
David Rand, in fact, seems
like it has actually--

00:59:39.330 --> 00:59:41.910
and at least in their
work seems to have--

00:59:41.910 --> 00:59:47.910
by the way, he's at Sloan, in
marketing, in the marketing

00:59:47.910 --> 00:59:49.170
group--

00:59:49.170 --> 00:59:52.080
he seems to find that it's less
about partisanship but rather

00:59:52.080 --> 00:59:54.178
about people being
not paying attention

00:59:54.178 --> 00:59:56.220
and being somewhat lazy
and making their choices.

00:59:56.220 --> 00:59:59.220
And once you draw
their attention to,

00:59:59.220 --> 01:00:01.140
here's an article
that might be fake,

01:00:01.140 --> 01:00:04.920
people might seem to be
actually quite good at learning

01:00:04.920 --> 01:00:08.450
about the accuracy of it.

01:00:08.450 --> 01:00:10.458
Now let me just
summarize for a bit

01:00:10.458 --> 01:00:12.250
and then see whether
there's any questions.

01:00:12.250 --> 01:00:15.320
So what I've shown you is
essentially using Bayes' rule

01:00:15.320 --> 01:00:17.540
is difficult, or I gave
you some sense of it.

01:00:17.540 --> 01:00:19.880
I showed you three
systematic deviations

01:00:19.880 --> 01:00:22.010
from Bayesian updating,
which is the gambler's

01:00:22.010 --> 01:00:25.430
fallacy, the hot-hand fallacy,
and base rate neglect.

01:00:25.430 --> 01:00:27.620
These are all well known
deviations from Bayesian

01:00:27.620 --> 01:00:28.760
learning.

01:00:28.760 --> 01:00:31.640
Now, 1 and 2, I
have argued, can be

01:00:31.640 --> 01:00:36.010
explained by the belief in
the law of small numbers.

01:00:36.010 --> 01:00:39.190
And there are a really important
real-world implications

01:00:39.190 --> 01:00:41.320
of those types of outcomes.

01:00:41.320 --> 01:00:44.542
For example, one very
nice recent paper--

01:00:44.542 --> 01:00:46.000
Maddie talked about
it a little bit

01:00:46.000 --> 01:00:47.760
in recitation, but
only very briefly--

01:00:47.760 --> 01:00:51.970
is this is paper by Kelly Shue
and co-authors about decision

01:00:51.970 --> 01:00:54.778
making by judges, umpires,
and loan officers,

01:00:54.778 --> 01:00:57.070
where essentially, if you're
a judge or a loan officer,

01:00:57.070 --> 01:00:59.590
you see a bunch of
applications over time.

01:00:59.590 --> 01:01:06.280
And then judges seem
to essentially engage

01:01:06.280 --> 01:01:08.770
in what's the gambler's fallacy.

01:01:08.770 --> 01:01:12.910
That's to say, if they see
three applicants in a row that

01:01:12.910 --> 01:01:15.470
happen to be very
good or very bad,

01:01:15.470 --> 01:01:19.120
that tends to affect
the following applicant.

01:01:19.120 --> 01:01:25.630
That is to say, judges, when
they decide about parole

01:01:25.630 --> 01:01:28.720
and so on, if they have given
parole several times in a row,

01:01:28.720 --> 01:01:30.760
they might think
the next person who

01:01:30.760 --> 01:01:35.090
is in line, if they have granted
parole several times in a row,

01:01:35.090 --> 01:01:37.750
the next person then would
be less likely to get parole,

01:01:37.750 --> 01:01:40.930
even though of course these
are independent events.

01:01:40.930 --> 01:01:43.330
It just happens to be
that one person happens

01:01:43.330 --> 01:01:47.570
to be after or before a certain
applicant in a random way.

01:01:47.570 --> 01:01:49.450
So there's really
important decisions

01:01:49.450 --> 01:01:53.440
that the gambler's
fallacy might affect.

01:01:53.440 --> 01:01:55.933
And more recently, when you
think about base-rate neglect,

01:01:55.933 --> 01:01:57.600
in some sense, the
example that I showed

01:01:57.600 --> 01:02:00.920
you seems to be a fairly
academic example in some ways.

01:02:00.920 --> 01:02:02.620
But when you think about--

01:02:02.620 --> 01:02:05.800
and I don't want to talk too
much about COVID-19 since you

01:02:05.800 --> 01:02:08.260
see that in your life
too much already anyway--

01:02:08.260 --> 01:02:12.730
but if you think about
tests for COVID-19,

01:02:12.730 --> 01:02:14.985
there is now talk
about antibody tests.

01:02:14.985 --> 01:02:18.130
And particularly as tests that
seem to be highly informative.

01:02:18.130 --> 01:02:20.720
In some sense, their
sensitivity and specificity,

01:02:20.720 --> 01:02:23.410
meaning the type 2
errors are pretty lower.

01:02:23.410 --> 01:02:26.960
Their sensitivity and
specificity is very high.

01:02:26.960 --> 01:02:30.350
These tests tend to
be highly accurate.

01:02:30.350 --> 01:02:33.220
But in fact, how much
you can learn from those

01:02:33.220 --> 01:02:36.160
tests is actually very limited
if the overall fraction

01:02:36.160 --> 01:02:38.800
of people who are
infected is not that high.

01:02:38.800 --> 01:02:41.290
And that's exactly the example
that I just showed you.

01:02:41.290 --> 01:02:43.000
If the fraction
of people who are

01:02:43.000 --> 01:02:46.240
positive in the overall
population is relatively low,

01:02:46.240 --> 01:02:47.410
you are doing a test.

01:02:47.410 --> 01:02:51.580
And essentially, receiving
a negative signal actually

01:02:51.580 --> 01:02:56.050
is not giving you a lot
of information overall.

01:02:56.050 --> 01:02:59.110
And people tend to miss
that part precisely

01:02:59.110 --> 01:03:02.130
because of base-rate neglect.

01:03:02.130 --> 01:03:03.380
Let me stop here for a second.

01:03:03.380 --> 01:03:04.650
That was a little bit fast.

01:03:04.650 --> 01:03:07.478
So I want to see if there's
any comments or questions.

01:03:22.930 --> 01:03:25.300
And then, only in
the last 10 minutes,

01:03:25.300 --> 01:03:28.150
talk a little bit about
heuristics and biases.

01:03:28.150 --> 01:03:30.040
And this is what Kahneman
and Tversky really

01:03:30.040 --> 01:03:33.100
are very well known for.

01:03:33.100 --> 01:03:36.640
So the way to think about,
then, biases and probability

01:03:36.640 --> 01:03:38.350
judgments is what we
already discussed.

01:03:38.350 --> 01:03:39.940
The starting point
is that applying

01:03:39.940 --> 01:03:41.620
the laws of probability
and statistics

01:03:41.620 --> 01:03:43.450
is often impossibly hard.

01:03:43.450 --> 01:03:45.670
So people use their quick
and intuitive judgments

01:03:45.670 --> 01:03:47.350
to make likelihood estimates.

01:03:47.350 --> 01:03:50.230
And so there's a seminal
work by Kahneman and Tversky

01:03:50.230 --> 01:03:51.950
and lots of subsequent
work that tries

01:03:51.950 --> 01:03:56.560
to think about these biases.

01:03:56.560 --> 01:03:58.660
If you're interested in
learning more about this,

01:03:58.660 --> 01:04:03.295
Thinking Fast and
Slow by Danny Kahneman

01:04:03.295 --> 01:04:06.860
is really a terrific book
about thinking about this.

01:04:06.860 --> 01:04:08.060
Now, what's a heuristic?

01:04:08.060 --> 01:04:11.020
It's an informal algorithm that
generates an approximate answer

01:04:11.020 --> 01:04:12.910
to a problem, quickly.

01:04:12.910 --> 01:04:15.520
And therefore,
because it's informal,

01:04:15.520 --> 01:04:18.340
it's kind of hard to model.

01:04:18.340 --> 01:04:22.460
Well, the previous things
that I showed you before--

01:04:22.460 --> 01:04:24.110
let me just go
back for a second--

01:04:24.110 --> 01:04:26.090
the gambler's fallacy,
hot-hand fallacy,

01:04:26.090 --> 01:04:27.530
and base-rate neglect--

01:04:27.530 --> 01:04:36.020
here, you can write down models
that capture these phenomena

01:04:36.020 --> 01:04:37.515
pretty well.

01:04:37.515 --> 01:04:39.140
For example, for the
base-rate neglect,

01:04:39.140 --> 01:04:42.770
you could just have a parameter
like how much weight people put

01:04:42.770 --> 01:04:45.740
in the base rate,
how much weight

01:04:45.740 --> 01:04:48.110
do people put in the new
information that they get.

01:04:48.110 --> 01:04:50.000
And then that parameter
will essentially--

01:04:50.000 --> 01:04:51.470
you can estimate that
parameter and you

01:04:51.470 --> 01:04:52.850
can write down a
model that's not

01:04:52.850 --> 01:04:54.920
Bayesian but close to Bayesian.

01:04:54.920 --> 01:04:57.830
Similarly, for the hot-hand
fallacy and gambler's, you

01:04:57.830 --> 01:05:00.050
can model that
people essentially

01:05:00.050 --> 01:05:01.970
apply the law of small numbers.

01:05:01.970 --> 01:05:05.550
And again, that's a model you
can write down and estimate.

01:05:05.550 --> 01:05:08.320
In contrast, some of the stuff
that I'm showing you next

01:05:08.320 --> 01:05:11.490
is much more difficult to
model because in that sense,

01:05:11.490 --> 01:05:15.090
some basic laws
of probability do

01:05:15.090 --> 01:05:16.650
just not apply
anymore in the sense

01:05:16.650 --> 01:05:18.233
that people don't
respect them anymore

01:05:18.233 --> 01:05:20.190
when they make their
belief updating.

01:05:20.190 --> 01:05:22.810
Let me show you, in a
second, what I mean by that.

01:05:22.810 --> 01:05:25.020
But again, heuristics
have a good and bad side.

01:05:25.020 --> 01:05:28.368
So they speed up and
make possible cognition.

01:05:28.368 --> 01:05:30.660
So they help you make decisions
in your everyday lives.

01:05:30.660 --> 01:05:32.520
And without
heuristics, it would be

01:05:32.520 --> 01:05:35.680
really hard to make
any decisions overall.

01:05:35.680 --> 01:05:37.710
But because they're
shortcuts, they occasionally

01:05:37.710 --> 01:05:40.630
produce incorrect
answers or biases.

01:05:40.630 --> 01:05:42.870
So these are essentially
unintended side effects

01:05:42.870 --> 01:05:44.880
of adaptive processes.

01:05:44.880 --> 01:05:48.930
And that makes it very useful
to study heuristics and biases

01:05:48.930 --> 01:05:50.260
together.

01:05:50.260 --> 01:05:52.230
In the same way as
you would study vision

01:05:52.230 --> 01:05:55.350
and optical illusion
together, studying them

01:05:55.350 --> 01:05:59.530
jointly is very helpful because
they're sort of the product

01:05:59.530 --> 01:06:01.200
or the result of the same thing.

01:06:01.200 --> 01:06:01.920
Things are hard.

01:06:01.920 --> 01:06:03.330
Therefore, they use heuristics.

01:06:03.330 --> 01:06:05.460
And because they
use heuristics, that

01:06:05.460 --> 01:06:09.470
leads to systematic biases.

01:06:09.470 --> 01:06:12.230
OK, so now I'm going to show
a few of those heuristics just

01:06:12.230 --> 01:06:14.450
to give you a sense
of what these are.

01:06:14.450 --> 01:06:15.950
One of the most
well-known ones is

01:06:15.950 --> 01:06:18.140
what's called the
representativeness heuristic,

01:06:18.140 --> 01:06:19.880
which is about Linda.

01:06:19.880 --> 01:06:24.000
Linda is 31 years old, single,
outspoken, and very bright.

01:06:24.000 --> 01:06:25.257
She majored in philosophy.

01:06:25.257 --> 01:06:26.840
As a student, she
was deeply concerned

01:06:26.840 --> 01:06:29.120
with issues of discrimination
and social justice

01:06:29.120 --> 01:06:33.650
and has also participated in
anti-nuclear demonstrations.

01:06:33.650 --> 01:06:37.010
The question becomes, then,
rank the following statements

01:06:37.010 --> 01:06:39.440
from most to least probable.

01:06:39.440 --> 01:06:41.900
And so here's a statement.

01:06:41.900 --> 01:06:45.540
You can read about them, all
sorts of different things.

01:06:45.540 --> 01:06:48.510
In particular, what Kahneman
and Tversky are focusing on

01:06:48.510 --> 01:06:51.200
is items 6 and 8.

01:06:51.200 --> 01:06:56.090
And item 6 is "Linda
is a bank teller"

01:06:56.090 --> 01:06:58.400
and item 8 is "Linda
is a bank teller

01:06:58.400 --> 01:07:02.150
and is active in the
feminist movement."

01:07:02.150 --> 01:07:04.880
Now, one thing that
should be clear

01:07:04.880 --> 01:07:07.670
is that 6 is more
likely to an 8,

01:07:07.670 --> 01:07:10.790
because essentially 8 is sort of
conditioning on two statements,

01:07:10.790 --> 01:07:13.280
while 6 is only
conditioning on one.

01:07:13.280 --> 01:07:18.050
So if Linda is a number
8, of course it's

01:07:18.050 --> 01:07:21.020
implied that, if she's a bank
teller and something else,

01:07:21.020 --> 01:07:22.990
of course then she
also is a bank teller.

01:07:22.990 --> 01:07:27.440
So should always be the case
that 6 is more likely than 8.

01:07:27.440 --> 01:07:30.980
But what people tend to
do quite consistently

01:07:30.980 --> 01:07:34.760
is that, in fact, people rank
it more likely that Linda

01:07:34.760 --> 01:07:36.420
is both a bank
teller and a feminist

01:07:36.420 --> 01:07:37.790
than that she's a bank teller.

01:07:37.790 --> 01:07:39.590
And there's quite a
bit of work on this.

01:07:39.590 --> 01:07:41.987
And showed this in various ways.

01:07:41.987 --> 01:07:43.820
But essentially it's
what's it the violation

01:07:43.820 --> 01:07:45.445
of the conjunction
law, which is a very

01:07:45.445 --> 01:07:47.330
basic law of probability.

01:07:47.330 --> 01:07:49.887
And precisely because it's such
a basic law of probability,

01:07:49.887 --> 01:07:52.220
it's hard to actually model
this because essentially you

01:07:52.220 --> 01:07:54.590
can't use simple
things that should

01:07:54.590 --> 01:07:57.900
be true in probability theory.

01:07:57.900 --> 01:07:59.760
There's some potential
concerns about this,

01:07:59.760 --> 01:08:01.133
people misunderstanding this.

01:08:01.133 --> 01:08:03.300
But there's a subsequent
experiment that essentially

01:08:03.300 --> 01:08:05.760
shows that that's not the case.

01:08:05.760 --> 01:08:08.430
Now, what's going on here
is what Kahneman and Tversky

01:08:08.430 --> 01:08:11.340
called the representativeness
heuristic, which people

01:08:11.340 --> 01:08:15.090
essentially use similarity or
representativeness as a proxy

01:08:15.090 --> 01:08:17.040
for probabilistic thinking.

01:08:17.040 --> 01:08:19.770
So based on the
available information,

01:08:19.770 --> 01:08:21.430
people form a
mental image of what

01:08:21.430 --> 01:08:29.100
Linda might be like and then ask
about how likely it is that--

01:08:29.100 --> 01:08:30.600
for example, she's
a schoolteacher--

01:08:30.600 --> 01:08:33.300
they might ask them how
similar is my picture of Linda

01:08:33.300 --> 01:08:35.319
to that of the school teacher?

01:08:35.319 --> 01:08:37.649
And so that turns this
similarity judgment

01:08:37.649 --> 01:08:40.810
into a probability judgment.

01:08:40.810 --> 01:08:43.560
So here, of course, the
example is very much

01:08:43.560 --> 01:08:46.319
rigged in the sense of
they were asking things

01:08:46.319 --> 01:08:48.600
about discrimination,
social justice,

01:08:48.600 --> 01:08:50.819
and anti-nuclear
demonstration and so on, which

01:08:50.819 --> 01:08:53.790
made people think, well, that's
a person that's likely to be

01:08:53.790 --> 01:08:55.529
part of the feminist movement.

01:08:55.529 --> 01:08:57.990
And then people focus on that
when making these choices.

01:08:57.990 --> 01:09:00.720
And they forget the fact
that, essentially, 6

01:09:00.720 --> 01:09:05.292
must be, by definition,
more likely than 8.

01:09:05.292 --> 01:09:06.959
So that's a common
thing that people do.

01:09:06.959 --> 01:09:09.390
So there's quite a bit
of evidence of that.

01:09:12.982 --> 01:09:14.899
And again, that's a very
reasonable heuristic.

01:09:14.899 --> 01:09:17.689
And it probably
works in many cases.

01:09:17.689 --> 01:09:22.220
But it also leads to very
predictably bad choices.

01:09:22.220 --> 01:09:24.740
And it's a poor predictor
of true probability

01:09:24.740 --> 01:09:27.770
in several situations.

01:09:27.770 --> 01:09:30.529
Similarly, what's called
the availability heuristic,

01:09:30.529 --> 01:09:33.560
people assess the
probability of an event

01:09:33.560 --> 01:09:35.720
by ease of which instances
or occurrences can

01:09:35.720 --> 01:09:37.279
be brought to mind.

01:09:37.279 --> 01:09:39.720
So when you ask
people, for example,

01:09:39.720 --> 01:09:44.569
about are there more suicides or
homicides in the US each year,

01:09:44.569 --> 01:09:48.770
people will think about
suicides and homicides

01:09:48.770 --> 01:09:49.970
that they can recall.

01:09:49.970 --> 01:09:52.430
They might think about what
they watched on TV and so on.

01:09:52.430 --> 01:09:54.020
And they judge the
frequency of each

01:09:54.020 --> 01:09:56.450
based on how many
instances they can recall.

01:09:56.450 --> 01:09:59.600
And people will be much more
likely to recall homicides

01:09:59.600 --> 01:10:01.730
because they are much
more salient in the world.

01:10:01.730 --> 01:10:03.800
And that leads people
to think that murders

01:10:03.800 --> 01:10:06.500
are way more common, which
in fact is not the case.

01:10:06.500 --> 01:10:09.710
Again, it's a very sensible
heuristic that people use.

01:10:09.710 --> 01:10:11.960
More often than not, it's
easier to recall things that

01:10:11.960 --> 01:10:14.670
are more common or probable.

01:10:14.670 --> 01:10:16.760
So that's reasonable overall.

01:10:16.760 --> 01:10:18.560
But again, there
are, predictably,

01:10:18.560 --> 01:10:20.420
things that are
then more salient,

01:10:20.420 --> 01:10:22.790
that get more
attention, people tend

01:10:22.790 --> 01:10:27.020
to think they're much more
likely than they in fact are.

01:10:27.020 --> 01:10:30.530
Let me skip the familiarity
and anchoring and adjustment

01:10:30.530 --> 01:10:34.580
and just sort of like conclude.

01:10:34.580 --> 01:10:38.270
So what have you learned
about beliefs overall?

01:10:38.270 --> 01:10:40.010
So we studied
several reasons why

01:10:40.010 --> 01:10:42.200
people might miss
information and fail to learn

01:10:42.200 --> 01:10:43.220
we talk about attention.

01:10:43.220 --> 01:10:44.887
Attention is limited,
and therefore they

01:10:44.887 --> 01:10:45.920
might miss things.

01:10:45.920 --> 01:10:48.410
You talked about, why might
they miss important things?

01:10:48.410 --> 01:10:52.110
Well, because they might have
wrong theories of the world.

01:10:52.110 --> 01:10:54.440
Then we discussed people
might derive utility

01:10:54.440 --> 01:10:56.690
from wrong beliefs and
therefore might actually not

01:10:56.690 --> 01:10:59.420
want to learn, or might
be systematically engaged

01:10:59.420 --> 01:11:01.102
in trying to deceive themselves.

01:11:01.102 --> 01:11:02.810
And then we talked
about people who might

01:11:02.810 --> 01:11:05.990
be bad at Bayesian learning.

01:11:05.990 --> 01:11:08.357
Now, what's important
here, I want

01:11:08.357 --> 01:11:10.940
you to take away it's important
to understand these underlying

01:11:10.940 --> 01:11:12.770
reasons why people
are misinformed

01:11:12.770 --> 01:11:15.200
because that might lead
to vastly different policy

01:11:15.200 --> 01:11:16.340
implications.

01:11:16.340 --> 01:11:19.160
For example, if you were to
make information salient,

01:11:19.160 --> 01:11:23.640
make sense, if you
think people miss

01:11:23.640 --> 01:11:26.670
information and important
things in the world--

01:11:26.670 --> 01:11:29.600
so if you, for example, think
that people just might not

01:11:29.600 --> 01:11:32.210
be aware of the fact that
certain foods have more

01:11:32.210 --> 01:11:35.690
calories than others, then
making that information very

01:11:35.690 --> 01:11:37.820
salient to people when
they purchase things,

01:11:37.820 --> 01:11:39.512
drawing their attention
to that stuff,

01:11:39.512 --> 01:11:40.970
could be really
important and could

01:11:40.970 --> 01:11:44.970
be really powerful in helping
people make better choices.

01:11:44.970 --> 01:11:47.700
Or if they have wrong
theories of the world,

01:11:47.700 --> 01:11:49.590
they think calories
are not that important

01:11:49.590 --> 01:11:52.143
or if they miss the fact
that smoking causes cancer,

01:11:52.143 --> 01:11:54.810
well, then we should really sort
of draw their attention to them

01:11:54.810 --> 01:11:56.070
and help them understand it.

01:11:56.070 --> 01:11:57.750
And that's what
a lot of labeling

01:11:57.750 --> 01:12:01.680
and a lot of making things
salient often is about.

01:12:01.680 --> 01:12:04.980
Now, but, if people
don't want to learn,

01:12:04.980 --> 01:12:06.810
if people actually
have motivated beliefs

01:12:06.810 --> 01:12:11.700
in certain ways, and in fact
they know that, in some ways,

01:12:11.700 --> 01:12:16.770
deep down inside, they know the
relevant information already--

01:12:16.770 --> 01:12:19.290
smokers, for example,
might know exactly

01:12:19.290 --> 01:12:22.350
that smoking is
bad for the health.

01:12:22.350 --> 01:12:24.630
So providing them with
information about smoking

01:12:24.630 --> 01:12:28.560
will not actually
make them learn.

01:12:28.560 --> 01:12:31.650
Or like if you wanted to give
somebody information about how

01:12:31.650 --> 01:12:33.600
to best take care of
your health when it comes

01:12:33.600 --> 01:12:36.780
to Huntington's, well that's
not helpful if the person

01:12:36.780 --> 01:12:39.900
doesn't actually want to believe
that they have Huntington's.

01:12:39.900 --> 01:12:42.030
They're never going to
read what you send them

01:12:42.030 --> 01:12:44.430
if they want to maintain
their positive image

01:12:44.430 --> 01:12:48.620
about their positive
beliefs about their health.

01:12:48.620 --> 01:12:52.910
And in fact, when people
derive actually from beliefs,

01:12:52.910 --> 01:12:56.210
correcting those beliefs
can make them worse off.

01:12:56.210 --> 01:12:58.670
If somebody wants to
believe that for the next--

01:12:58.670 --> 01:13:00.390
they know the
probability of having

01:13:00.390 --> 01:13:02.330
Huntington's is
reasonably high, but they

01:13:02.330 --> 01:13:05.210
want to maintain the
beliefs that they're healthy

01:13:05.210 --> 01:13:08.030
and they want to be happy
for the next 10 or 20 years,

01:13:08.030 --> 01:13:10.550
and then maybe, at some point,
the disease will break out.

01:13:10.550 --> 01:13:14.870
But their choice is they want
to think that they're healthy

01:13:14.870 --> 01:13:18.300
and they do not want to
adjust their behavior.

01:13:18.300 --> 01:13:21.950
So who are we then to tell them
otherwise that's their choice?

01:13:21.950 --> 01:13:24.530
If it makes them less
happy, we might actually

01:13:24.530 --> 01:13:25.885
make them worse off.

01:13:25.885 --> 01:13:27.260
So in some ways,
there's at least

01:13:27.260 --> 01:13:29.870
some argument for
respecting people's choices

01:13:29.870 --> 01:13:33.470
if they want to be deluded, if
they want to delude themselves,

01:13:33.470 --> 01:13:35.540
and the consequences are
not severe in the sense

01:13:35.540 --> 01:13:37.670
of these situations
where they can't really

01:13:37.670 --> 01:13:41.300
do much about that, then in
fact leaving people uninformed

01:13:41.300 --> 01:13:44.740
might be the right thing to do.

01:13:44.740 --> 01:13:47.960
And then understanding
systematic biases

01:13:47.960 --> 01:13:51.110
based on kind of help
improve decisions.

01:13:51.110 --> 01:13:53.150
That's, in some sense,
a lot less controversial

01:13:53.150 --> 01:13:54.942
if you think that people
are systematically

01:13:54.942 --> 01:13:57.200
making wrong choices
because they learned wrongly

01:13:57.200 --> 01:13:59.735
and they just have wrong
information in their heads,

01:13:59.735 --> 01:14:01.860
sort of providing them with
the correct information

01:14:01.860 --> 01:14:06.390
seems like unambiguously
something [INAUDIBLE]..

01:14:06.390 --> 01:14:10.372
That's all I have
on beliefs for you.

01:14:10.372 --> 01:14:12.330
Next time, we're going
to talk about projection

01:14:12.330 --> 01:14:13.500
and attribution bias.

01:14:13.500 --> 01:14:15.720
But I'm also happy to
answer any questions

01:14:15.720 --> 01:14:18.420
in the next few minutes that
you have on this lecture

01:14:18.420 --> 01:14:22.220
or on the summary
that I just showed you