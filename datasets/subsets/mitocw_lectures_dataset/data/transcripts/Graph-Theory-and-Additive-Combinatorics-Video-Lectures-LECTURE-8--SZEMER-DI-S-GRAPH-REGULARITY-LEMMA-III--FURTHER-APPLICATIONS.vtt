WEBVTT

00:00:19.142 --> 00:00:20.600
PROFESSOR: So we've
been discussing

00:00:20.600 --> 00:00:23.900
Szemeredi's regularity level
for the past couple of lectures.

00:00:23.900 --> 00:00:25.580
And one of the
theorems that we proved

00:00:25.580 --> 00:00:30.410
was Roth's theorem, which tells
us how large can a subset of 1

00:00:30.410 --> 00:00:35.220
through N be if it has no
3-term arithmetic progressions.

00:00:35.220 --> 00:00:37.130
And I mentioned at
the end of last time

00:00:37.130 --> 00:00:41.780
that we can construct
fairly large subsets of 1

00:00:41.780 --> 00:00:44.990
through N without 3-term
arithmetic progressions.

00:00:44.990 --> 00:00:48.740
I want to begin today's lecture
with showing you that example,

00:00:48.740 --> 00:00:51.140
showing you that construction.

00:00:51.140 --> 00:00:55.250
But first, let's
try to do something

00:00:55.250 --> 00:00:59.580
that is somewhat more naive,
somewhat more straightforward,

00:00:59.580 --> 00:01:14.240
to try to just construct
greedily 3-AP-free sets of 1

00:01:14.240 --> 00:01:15.830
through n.

00:01:15.830 --> 00:01:19.970
And recall from last time,
we showed Roth's theorem

00:01:19.970 --> 00:01:27.740
that such a set must
have size little o of N.

00:01:27.740 --> 00:01:29.570
So what's one
thing you might do?

00:01:29.570 --> 00:01:33.533
Well, you can try to greedily
construct such a set.

00:01:33.533 --> 00:01:35.450
It will just make our
life a little bit easier

00:01:35.450 --> 00:01:37.740
if we start with 0 instead of 1.

00:01:37.740 --> 00:01:40.310
So you put one
element in, and you

00:01:40.310 --> 00:01:42.200
keep putting in
the next integer,

00:01:42.200 --> 00:01:46.500
as long as it doesn't create a
3-term arithmetic progression.

00:01:46.500 --> 00:01:47.820
So you keep doing this.

00:01:47.820 --> 00:01:51.140
Well, you can't put 2
in, so let's skip 2.

00:01:51.140 --> 00:01:53.654
So we go to the next one, 3.

00:01:53.654 --> 00:01:55.340
So 4 is OK.

00:01:57.870 --> 00:02:00.180
So we skip 5.

00:02:00.180 --> 00:02:05.260
We have to skip 6 as well
because 0, 3, 6, that's a 3-AP.

00:02:05.260 --> 00:02:06.000
So we keep going.

00:02:06.000 --> 00:02:08.904
So what's the next
number we can put in?

00:02:08.904 --> 00:02:11.257
AUDIENCE: 9.

00:02:11.257 --> 00:02:11.840
PROFESSOR: Up.

00:02:11.840 --> 00:02:12.610
Go to 9.

00:02:15.980 --> 00:02:19.010
Then the next one is 10.

00:02:19.010 --> 00:02:21.888
What's the next
one we can put in?

00:02:21.888 --> 00:02:22.930
So we can play this game.

00:02:25.460 --> 00:02:30.490
Find out what is the next
number that you can put in.

00:02:30.490 --> 00:02:32.360
Greedily, if you
could put it in,

00:02:32.360 --> 00:02:37.460
put it in in a way that
generates a subset of integers

00:02:37.460 --> 00:02:41.530
without 3-term
arithmetic progressions.

00:02:41.530 --> 00:02:43.867
So this actually
has a name, so this

00:02:43.867 --> 00:02:45.075
is called a Stanley sequence.

00:02:51.540 --> 00:02:55.530
And there's an easier way
to see what the sequence is.

00:02:55.530 --> 00:03:01.930
Namely, if you write
the sequence in base 3,

00:03:01.930 --> 00:03:09.758
you find that these numbers
are 0; 0, 1; 1, 0; 1, 1.

00:03:12.370 --> 00:03:13.060
1, 0, 0.

00:03:13.060 --> 00:03:14.160
1, 0, 1.

00:03:14.160 --> 00:03:14.950
1, 1, 0.

00:03:14.950 --> 00:03:16.980
1, 1, 1.

00:03:16.980 --> 00:03:25.460
So these are just numbers
whose base 3 representation

00:03:25.460 --> 00:03:30.050
consists of zeros and ones.

00:03:30.050 --> 00:03:32.600
So I'll leave it to
you as an exercise

00:03:32.600 --> 00:03:34.830
to figure out why this is
the case, if you generate

00:03:34.830 --> 00:03:38.570
the sequence greedily this way,
this is exactly the sequence

00:03:38.570 --> 00:03:41.140
that you obtain.

00:03:41.140 --> 00:03:44.040
But once you know that, it's
not too hard to find out

00:03:44.040 --> 00:03:46.300
how many numbers you generate.

00:03:49.240 --> 00:03:57.440
So up to-- suppose N
is equal to 3 to the k.

00:03:57.440 --> 00:04:08.500
We get 2 to the k
terms, which gives you

00:04:08.500 --> 00:04:12.450
N raised to the power
of log base 3 of 2.

00:04:14.852 --> 00:04:17.310
So you can figure out what that
number is, but some numbers

00:04:17.310 --> 00:04:18.279
strictly less than 1.

00:04:28.810 --> 00:04:30.540
And actually, for
a very long time,

00:04:30.540 --> 00:04:32.830
people thought this was
the best construction.

00:04:32.830 --> 00:04:35.620
So this construction was
known even before Stanley,

00:04:35.620 --> 00:04:39.630
so it was something that is
very natural to come up with.

00:04:39.630 --> 00:04:45.658
And it was somewhat of a
surprise when in the '40s,

00:04:45.658 --> 00:04:47.200
it was discovered
that you can create

00:04:47.200 --> 00:04:50.380
much larger subsets of
the positive integers

00:04:50.380 --> 00:04:52.275
without arithmetic progressions.

00:04:52.275 --> 00:04:54.400
So that's the first thing
I want to show you today.

00:04:58.470 --> 00:05:06.180
So these sets were first
discovered by Salem and Spencer

00:05:06.180 --> 00:05:08.310
back in the '40s.

00:05:08.310 --> 00:05:17.080
And a few years later, Behrend
gave a somewhat improved

00:05:17.080 --> 00:05:19.820
and simplified version of the
Salem-Spencer construction.

00:05:19.820 --> 00:05:24.190
So these were all
back in the '40s.

00:05:24.190 --> 00:05:26.160
So these days, we
usually refer, at least

00:05:26.160 --> 00:05:27.910
in the additive
combinatorics community,

00:05:27.910 --> 00:05:30.665
to this construction as
Behrend's construction.

00:05:30.665 --> 00:05:33.040
And this, indeed, what I will
show you is due to Behrend,

00:05:33.040 --> 00:05:36.150
but somehow, this Salem-Spencer
name has been forgotten

00:05:36.150 --> 00:05:38.650
and I just wanted
to point out that it

00:05:38.650 --> 00:05:45.640
was Salem and Spencer who first
demonstrated that there exists

00:05:45.640 --> 00:05:50.590
a subset of 1 through
N that is 3-AP-free

00:05:50.590 --> 00:05:56.710
and has size N to the 1 minus
little o 1, that there's

00:05:56.710 --> 00:05:58.330
no power saving.

00:05:58.330 --> 00:06:01.100
So there exists examples
with no power saving.

00:06:01.100 --> 00:06:04.240
And this is important
because as we

00:06:04.240 --> 00:06:06.150
saw last time, the proof
of Roth's theorem--

00:06:06.150 --> 00:06:08.800
and we basically spent
two full lectures

00:06:08.800 --> 00:06:11.020
proving Roth's theorem.

00:06:11.020 --> 00:06:13.010
And it's somewhat
involved, right.

00:06:13.010 --> 00:06:15.340
It wasn't this one
line of inequality

00:06:15.340 --> 00:06:18.620
you can just use to
deduce the result.

00:06:18.620 --> 00:06:21.700
And that is part
of the difficulty.

00:06:21.700 --> 00:06:24.640
So having such an
example is indication

00:06:24.640 --> 00:06:28.350
that Roth's theorem
should not be so easy.

00:06:28.350 --> 00:06:30.130
That's not a rigorous
demonstration,

00:06:30.130 --> 00:06:32.910
but it's some indication of the
difficulty of Roth's theorem.

00:06:36.430 --> 00:06:39.355
So let's see this
construction due to Behrend.

00:06:47.890 --> 00:06:50.190
So let me write down
the precise statement.

00:06:50.190 --> 00:06:56.760
There exists a constant, C, such
that there exists a subset of 1

00:06:56.760 --> 00:07:07.070
through N that is 3-AP-free
and has size at least N times e

00:07:07.070 --> 00:07:17.320
to the minus C root log N.

00:07:17.320 --> 00:07:19.870
Perhaps somewhat amazingly
enough, this bound

00:07:19.870 --> 00:07:23.290
is still the current best known.

00:07:23.290 --> 00:07:25.920
We do not know any
constructions that

00:07:25.920 --> 00:07:29.040
is essentially better than
Behrend's construction

00:07:29.040 --> 00:07:30.870
from the '40s.

00:07:30.870 --> 00:07:33.180
There were some small
recent improvements

00:07:33.180 --> 00:07:39.983
that clarified what the C
could be, but in this form,

00:07:39.983 --> 00:07:41.650
we do not know anything
better than what

00:07:41.650 --> 00:07:43.050
was known back in the '40s.

00:07:43.050 --> 00:07:46.020
And the construction,
I will show yow--

00:07:46.020 --> 00:07:48.387
hopefully you should believe
that it's quite simple.

00:07:48.387 --> 00:07:50.220
So I will show you what
the construction is.

00:07:50.220 --> 00:07:52.980
It's clever, but
it's not complicated.

00:07:52.980 --> 00:07:56.350
And it's a really interesting
direction to figure out

00:07:56.350 --> 00:08:00.330
is this really the best
construction out there.

00:08:00.330 --> 00:08:01.080
Can you do better?

00:08:04.358 --> 00:08:04.900
So let's see.

00:08:08.590 --> 00:08:12.100
We're going to set some
parameters to be decided later,

00:08:12.100 --> 00:08:14.200
m and d.

00:08:14.200 --> 00:08:22.510
Let me consider x to be this
discrete box in d dimensions.

00:08:22.510 --> 00:08:27.850
So this is the box of lattice
points in d dimensions, 1

00:08:27.850 --> 00:08:30.790
through m raised to d.

00:08:30.790 --> 00:08:34.210
And let me consider
an intersection

00:08:34.210 --> 00:08:44.640
of this box by a sphere
of radius root L.

00:08:44.640 --> 00:08:49.190
So namely, we look
at points in this x,

00:08:49.190 --> 00:08:53.550
such that the sum
of their squares

00:08:53.550 --> 00:08:59.890
is equal to exactly L. You
take a bunch of these spheres,

00:08:59.890 --> 00:09:12.040
they partition your set x,
so the smallest possible sum

00:09:12.040 --> 00:09:16.180
of squares, largest
possible sum of squares.

00:09:16.180 --> 00:09:22.440
So in particular, there
exists some L such that x of L

00:09:22.440 --> 00:09:33.250
is large, just by
pigeonhole principle.

00:09:33.250 --> 00:09:36.790
You can probably do this
step with a bit more finesse,

00:09:36.790 --> 00:09:40.132
but it's not going to change
any of the asymptotics.

00:09:43.366 --> 00:09:46.340
The intuition here-- and we'll
come back to this in a second--

00:09:46.340 --> 00:09:49.950
is that xL lies on a sphere.

00:09:49.950 --> 00:09:51.480
So this is the set
of lattice points

00:09:51.480 --> 00:09:53.070
that lies in a given sphere.

00:09:53.070 --> 00:09:57.180
And because you are looking at
a sphere, it has no 3-term APs.

00:09:59.580 --> 00:10:01.080
So we're going to
use that property,

00:10:01.080 --> 00:10:04.570
but right now it's not yet
a subset of the integers.

00:10:04.570 --> 00:10:09.510
So what we're going to do is to
take this section of a sphere

00:10:09.510 --> 00:10:14.460
and then project it
to one dimension,

00:10:14.460 --> 00:10:17.770
so that it is now going to
be a subset of integers.

00:10:17.770 --> 00:10:19.440
And we're going to
do this in such a way

00:10:19.440 --> 00:10:25.104
that it does not affect
the presence of 3-term APs.

00:10:25.104 --> 00:10:33.090
So let us map x to the
integers by base expansion.

00:10:43.290 --> 00:10:47.660
So this is base 2m expansion.

00:10:57.580 --> 00:10:58.080
All right.

00:10:58.080 --> 00:11:01.356
So what's the point of this
construction here so far?

00:11:01.356 --> 00:11:05.870
Well, you verified a couple
of properties, the first being

00:11:05.870 --> 00:11:13.550
that this construction here
is injective, this map.

00:11:13.550 --> 00:11:20.310
So if you call this map
phi, this phi is injective.

00:11:20.310 --> 00:11:24.480
Well, it's base expansion,
so it's injective.

00:11:24.480 --> 00:11:28.680
But also a somewhat
stronger claim

00:11:28.680 --> 00:11:31.230
is that if you
have three points--

00:11:36.630 --> 00:11:40.050
so if you have
three points in x,

00:11:40.050 --> 00:11:47.770
that map to a 3-AP
in the integers,

00:11:47.770 --> 00:11:58.750
then the three points
originally must be a 3-AP in x.

00:12:02.494 --> 00:12:04.020
Again, this is not a hard claim.

00:12:04.020 --> 00:12:05.670
Just think about it.

00:12:05.670 --> 00:12:08.833
Here, because we're
using base 2m expansion

00:12:08.833 --> 00:12:10.750
and you're only allowed
to use digits up to m,

00:12:10.750 --> 00:12:12.190
you don't have any
wrap around effects.

00:12:12.190 --> 00:12:14.482
You don't have any carryovers
when you do the addition.

00:12:17.760 --> 00:12:26.800
So combining these
two observations,

00:12:26.800 --> 00:12:43.130
we find that the image of
xL is a 3-AP-free set off 1

00:12:43.130 --> 00:12:48.430
through N. So what is N?

00:12:48.430 --> 00:12:53.430
I can take N to be, for
instance, 2m raised to power d,

00:12:53.430 --> 00:12:57.710
so all the numbers up there
are less than this quantity.

00:12:57.710 --> 00:13:04.400
And the size is
the size of x sub

00:13:04.400 --> 00:13:11.512
L, which is N to the d
divided by d m squared.

00:13:11.512 --> 00:13:13.720
And now you just need to
find the appropriate choices

00:13:13.720 --> 00:13:19.430
of the parameters m and d to
maximize the size of x sub L.

00:13:19.430 --> 00:13:23.440
And that's an exercise.

00:13:23.440 --> 00:13:29.690
So, for instance, if you take
m to be e to the root log

00:13:29.690 --> 00:13:37.880
N and d to be root
log N, then you

00:13:37.880 --> 00:13:42.980
find that the size here is
the bound that we claim.

00:13:48.570 --> 00:13:54.920
And that finishes the
construction of Behrend,

00:13:54.920 --> 00:13:57.960
giving you a fairly
large subset of 1

00:13:57.960 --> 00:14:01.480
through N without 3-term
arithmetic progressions.

00:14:01.480 --> 00:14:04.640
And the idea here is you
look at a higher dimensional

00:14:04.640 --> 00:14:07.490
object, namely, a higher
dimensional sphere which

00:14:07.490 --> 00:14:09.380
has this property
of being 3-AP-free,

00:14:09.380 --> 00:14:13.185
and then you project
it onto the integers.

00:14:13.185 --> 00:14:14.060
Any questions so far?

00:14:20.850 --> 00:14:21.610
OK.

00:14:21.610 --> 00:14:24.050
So we have some proofs,
we have some examples.

00:14:24.050 --> 00:14:27.960
So now let's go on to
variations of Roth's theorem.

00:14:27.960 --> 00:14:31.570
And I want to show you
a higher dimensional

00:14:31.570 --> 00:14:33.750
version of Roth's theorem.

00:14:33.750 --> 00:14:35.500
So I mentioned in the
very first lecture--

00:14:35.500 --> 00:14:38.110
so you have this whole host
of theorems and additive

00:14:38.110 --> 00:14:38.880
combinatorics--

00:14:38.880 --> 00:14:42.880
Roth, Szemeredi,
multi-dimensional Szemeredi.

00:14:42.880 --> 00:14:45.640
So this is, in some sense,
the simplest example

00:14:45.640 --> 00:14:48.760
of multi-dimensional
Szemeredi theorem.

00:14:48.760 --> 00:14:50.770
And this example is
known as corners.

00:14:53.330 --> 00:14:54.370
So what's a corner?

00:14:54.370 --> 00:14:58.660
A corner is, well, we're
working inside two dimensions,

00:14:58.660 --> 00:15:04.700
so a corner is simply three
points, such that the two--

00:15:04.700 --> 00:15:07.990
so they're positioned like
that-- such that these two

00:15:07.990 --> 00:15:11.132
segments, they are
parallel to the axes

00:15:11.132 --> 00:15:12.340
and they are the same length.

00:15:12.340 --> 00:15:14.890
So that's, by definition,
what a corner is.

00:15:14.890 --> 00:15:16.810
And the question
is, if you give me

00:15:16.810 --> 00:15:19.840
a subset of a grid
that is corner-free,

00:15:19.840 --> 00:15:21.862
how large can this subset be?

00:15:25.478 --> 00:15:27.170
Here's a theorem.

00:15:27.170 --> 00:15:38.650
So if A is a subset of 1
through N with no corners,

00:15:38.650 --> 00:15:44.320
particular, no three points
of the form, x comma y;

00:15:44.320 --> 00:15:48.680
x plus d comma y;
and x comma y plus d,

00:15:48.680 --> 00:15:56.780
where d is some positive
integer, then the size of A

00:15:56.780 --> 00:15:59.030
has to be little o of N squared.

00:16:02.840 --> 00:16:03.340
Question.

00:16:03.340 --> 00:16:04.965
AUDIENCE: Do we only
care about corners

00:16:04.965 --> 00:16:06.260
oriented in that direction?

00:16:06.260 --> 00:16:07.120
PROFESSOR: OK, good question.

00:16:07.120 --> 00:16:08.495
So that's one of
the first things

00:16:08.495 --> 00:16:10.150
we will address in this proof.

00:16:10.150 --> 00:16:12.880
So your question was, do we
only care about corners oriented

00:16:12.880 --> 00:16:14.510
in the positive direction.

00:16:14.510 --> 00:16:18.300
So you can have a more relaxed
version of this problem

00:16:18.300 --> 00:16:22.150
where you allow d to
be negative as well.

00:16:22.150 --> 00:16:23.650
The first step in
the proof is we'll

00:16:23.650 --> 00:16:26.806
see that that constraint
doesn't actually matter.

00:16:26.806 --> 00:16:27.306
Yes.

00:16:32.790 --> 00:16:33.350
OK.

00:16:33.350 --> 00:16:33.850
Great.

00:16:37.360 --> 00:16:38.110
Let's get started.

00:16:40.640 --> 00:16:47.070
So as Michael mentioned, we do
have this constraint in this

00:16:47.070 --> 00:16:48.950
over here that d is positive.

00:16:48.950 --> 00:16:52.680
And it's somewhat annoying
because if you remember

00:16:52.680 --> 00:16:56.160
in our proof of Roth's theorem,
they are positive, negative,

00:16:56.160 --> 00:16:57.810
they don't play a role.

00:16:57.810 --> 00:17:01.820
So let's try to find a way
to get rid of this constraint

00:17:01.820 --> 00:17:05.650
so that we are in a
more flexible situation.

00:17:05.650 --> 00:17:07.510
So first step is
that we'll get rid

00:17:07.510 --> 00:17:14.190
of this d being
positive requirement.

00:17:17.480 --> 00:17:20.119
So here's a trick.

00:17:20.119 --> 00:17:24.160
Let's consider the sumset
A plus A. So sumset

00:17:24.160 --> 00:17:31.450
here means I'm looking at
all pairwise sums as a set.

00:17:31.450 --> 00:17:35.860
So you don't keep
track of duplicates,

00:17:35.860 --> 00:17:37.720
so you'll keep it as a set.

00:17:37.720 --> 00:17:40.930
And we're living
inside this grid,

00:17:40.930 --> 00:17:43.600
but now somewhat wider in width.

00:17:49.930 --> 00:18:01.670
Then there exists an element
in this domain of the sumset.

00:18:01.670 --> 00:18:03.350
By pigeonhole,
that's represented

00:18:03.350 --> 00:18:05.900
in many different ways.

00:18:05.900 --> 00:18:11.870
So there exists a z
represented as a plus b

00:18:11.870 --> 00:18:19.290
in at least size of
A squared divided

00:18:19.290 --> 00:18:26.640
by size of 2N squared
different ways,

00:18:26.640 --> 00:18:29.190
just because if you look
at how many things come up

00:18:29.190 --> 00:18:34.086
in that representation,
just use pigeonhole.

00:18:34.086 --> 00:18:50.033
And now let's take A prime to
be A intersect z minus A. So

00:18:50.033 --> 00:18:50.950
what's happening here?

00:18:50.950 --> 00:18:55.870
So you have this set
A. And basically what

00:18:55.870 --> 00:18:58.660
I want to do is look at the--

00:18:58.660 --> 00:19:00.130
so suppose A looks like that.

00:19:00.130 --> 00:19:04.900
So look at minus A
and then shift minus A

00:19:04.900 --> 00:19:10.420
over so that they intersect in
as many elements as you can.

00:19:10.420 --> 00:19:12.400
And by pigeonhole,
I can guarantee

00:19:12.400 --> 00:19:14.820
that their intersection
is fairly large.

00:19:21.130 --> 00:19:24.570
So because the size
of A prime, it's

00:19:24.570 --> 00:19:27.600
essentially the number of
ways that z can be represented

00:19:27.600 --> 00:19:31.450
in the aforementioned manner.

00:19:31.450 --> 00:19:44.303
So it suffices to show that A
prime is little o of N squared.

00:19:44.303 --> 00:19:45.970
If you show that,
then you automatically

00:19:45.970 --> 00:19:48.920
show A is little o of N squared.

00:19:48.920 --> 00:19:51.820
But now A prime is symmetric.

00:19:51.820 --> 00:19:54.460
A prime is symmetric
around 0 over 2.

00:19:59.290 --> 00:20:05.370
So this is centrally
symmetric about z over 2,

00:20:05.370 --> 00:20:09.450
meaning that A prime
equals to z minus A prime.

00:20:12.280 --> 00:20:13.900
And so you see now
we've gotten rid

00:20:13.900 --> 00:20:19.090
of this d positive requirement
because if A prime had

00:20:19.090 --> 00:20:23.390
a positive corner, then it has a
negative corner and vice versa.

00:20:27.647 --> 00:20:35.520
So no corner in A
prime with d positive

00:20:35.520 --> 00:20:44.520
implies that no corner
with d negative.

00:20:44.520 --> 00:20:46.170
So now let's forget
about A and A prime

00:20:46.170 --> 00:20:50.640
and just replace A
by A prime and forget

00:20:50.640 --> 00:20:53.890
about this d positive condition.

00:20:56.650 --> 00:21:03.143
So let's forget about this part,
but I do want d not equal to 0.

00:21:03.143 --> 00:21:05.310
Otherwise, you have trivial
corners, and, of course,

00:21:05.310 --> 00:21:07.310
you always have trivial corners.

00:21:07.310 --> 00:21:07.810
All right.

00:21:15.120 --> 00:21:20.080
So let's remember how the
proof of Roth's theorem went.

00:21:20.080 --> 00:21:22.610
And this relates to the
very first lecture where

00:21:22.610 --> 00:21:26.480
I showed you this connection
between additive combinatorics

00:21:26.480 --> 00:21:29.440
on one hand and graph
theory on the other hand,

00:21:29.440 --> 00:21:31.930
or you take some
arithmetic pattern

00:21:31.930 --> 00:21:34.840
and you try to
encode it in a graph

00:21:34.840 --> 00:21:37.240
so that the patterns
in your arithmetic

00:21:37.240 --> 00:21:43.160
set correspond to
patterns in the graph.

00:21:43.160 --> 00:21:45.840
So we're going to do
the same thing here.

00:21:45.840 --> 00:21:50.000
So we're going to encode
the subset of the grid

00:21:50.000 --> 00:21:53.990
as a tripartite
graph in such a way

00:21:53.990 --> 00:21:58.520
that the corners correspond
to triangles in the graph.

00:21:58.520 --> 00:22:03.365
So I'll show you how to do this
and it will be fairly simple.

00:22:03.365 --> 00:22:07.557
And in general, sometimes it
takes a little bit of ingenuity

00:22:07.557 --> 00:22:09.890
to figure out what is the
right way to set up the graph.

00:22:09.890 --> 00:22:12.920
So one of an upcoming
homework problem

00:22:12.920 --> 00:22:14.630
will be for you
to figure out how

00:22:14.630 --> 00:22:18.710
to set up a corresponding
graph when the pattern is not

00:22:18.710 --> 00:22:20.690
a corner, but a square.

00:22:20.690 --> 00:22:24.260
If I add one extra point up
there, how would you do that?

00:22:27.470 --> 00:22:29.690
So what does this
graph look like?

00:22:29.690 --> 00:22:41.784
Let's build a
tripartite graph where--

00:22:41.784 --> 00:22:43.570
so this should be
somewhat reminiscent

00:22:43.570 --> 00:22:48.120
of the proof of Roth's theorem--

00:22:48.120 --> 00:22:56.240
where I give you three sets,
x, y, and z; and x and y

00:22:56.240 --> 00:22:59.600
are both going to
have N elements

00:22:59.600 --> 00:23:03.355
and z is now going
to have 2N elements.

00:23:08.720 --> 00:23:13.000
x is supposed to
enumerate or index

00:23:13.000 --> 00:23:19.680
all the vertical
lines in your grid.

00:23:19.680 --> 00:23:30.130
So you have this grid of
N by N. So x has size 4.

00:23:30.130 --> 00:23:33.160
Each vertex in x corresponds
to a vertical line.

00:23:38.450 --> 00:23:48.350
y corresponds to the horizontal
lines and z corresponds

00:23:48.350 --> 00:23:52.220
to these negatively
sloped diagonal lines--

00:23:52.220 --> 00:23:55.713
slope minus 1 lines.

00:23:55.713 --> 00:23:57.380
And of course you
should only take lines

00:23:57.380 --> 00:24:01.250
that affect your N by N grid.

00:24:01.250 --> 00:24:03.440
So that's why there
are this many of them

00:24:03.440 --> 00:24:06.080
for each direction.

00:24:06.080 --> 00:24:07.760
So what's the graph?

00:24:07.760 --> 00:24:21.685
I join two vertices if
their corresponding lines

00:24:21.685 --> 00:24:39.055
meet in a point of
A. So I might have--

00:24:45.320 --> 00:24:50.060
this may be A, a
point of A. So I

00:24:50.060 --> 00:24:53.570
put an edge between
those two lines--

00:24:53.570 --> 00:24:56.300
one for x, one for z--

00:24:56.300 --> 00:25:02.075
because their intersection lies
in A. And more explicitly--

00:25:02.075 --> 00:25:03.450
I mean, that is
pretty explicit--

00:25:03.450 --> 00:25:06.000
and alternatively,
you could also

00:25:06.000 --> 00:25:12.465
write this graph by
telling us how to put

00:25:12.465 --> 00:25:15.540
in the edge between x and z.

00:25:15.540 --> 00:25:22.280
Namely, you do this if x
comma z minus x lies in A;

00:25:22.280 --> 00:25:28.960
you put an edge between x
and y if x comma y lies in A;

00:25:28.960 --> 00:25:31.710
and likewise, you
put in the final edge

00:25:31.710 --> 00:25:37.680
if z minus y comma y lies in A.

00:25:37.680 --> 00:25:39.760
So two equivalent descriptions
of the same graph.

00:25:44.050 --> 00:25:46.900
Any questions?

00:25:46.900 --> 00:25:47.498
All right.

00:25:47.498 --> 00:25:49.540
So the rest of the proof
is more or less the same

00:25:49.540 --> 00:25:52.940
as the proof of Roth's
theorem we saw last time.

00:25:52.940 --> 00:25:55.930
So we need to figure out
how many edges are there.

00:26:00.710 --> 00:26:05.840
Well, every element of
A gives you three edges.

00:26:10.510 --> 00:26:12.990
So that's one of the edges
corresponding to that element.

00:26:12.990 --> 00:26:17.060
The other two pairs
with two other edges.

00:26:17.060 --> 00:26:24.530
And most importantly, the
triangles in this graph,

00:26:24.530 --> 00:26:28.435
I claim, correspond to corners.

00:26:31.890 --> 00:26:33.680
So what is a triangle?

00:26:33.680 --> 00:26:36.790
A triangle corresponds
to a horizontal line,

00:26:36.790 --> 00:26:40.660
a vertical line, and
a slope 1 minus line

00:26:40.660 --> 00:26:44.520
that pairwise intersect
in elements of A.

00:26:44.520 --> 00:26:47.730
If you look at their
intersections, that's a corner.

00:26:47.730 --> 00:26:51.550
And conversely, if you have a
corner, you build a triangle.

00:26:51.550 --> 00:26:59.040
And because your graph, your
set is corner-free, well,

00:26:59.040 --> 00:27:00.390
you don't have any triangles.

00:27:00.390 --> 00:27:02.340
Actually, no, that's not true.

00:27:02.340 --> 00:27:05.670
So like we saw in
Roth's theorem,

00:27:05.670 --> 00:27:07.860
you have some triangles,
but corresponding

00:27:07.860 --> 00:27:10.410
to trivial corners.

00:27:10.410 --> 00:27:18.880
So triangles correspond
to trivial corners

00:27:18.880 --> 00:27:24.160
because your set
A is corner-free.

00:27:24.160 --> 00:27:28.840
Trivial corners, meaning the
same point with d close to 0,

00:27:28.840 --> 00:27:34.770
so it's not a genuine
corner like that.

00:27:34.770 --> 00:27:39.050
And in particular,
in this graph,

00:27:39.050 --> 00:27:46.515
every edge is in
a unique triangle.

00:27:55.412 --> 00:27:56.870
And so we're in
the same situation.

00:27:56.870 --> 00:28:02.900
By the corollary of
triangle removal lemma,

00:28:02.900 --> 00:28:11.350
we find that the
number of edges must

00:28:11.350 --> 00:28:17.060
be little o of the
number of vertices.

00:28:20.930 --> 00:28:23.400
The number of edges
must be subquadratic,

00:28:23.400 --> 00:28:27.380
so it must be little
o of N squared.

00:28:27.380 --> 00:28:30.360
And so that implies
that the size of A

00:28:30.360 --> 00:28:32.310
is little o of N squared.

00:28:35.256 --> 00:28:39.300
So that proves the
Corners theorem.

00:28:39.300 --> 00:28:40.381
Any questions?

00:28:44.530 --> 00:28:46.270
So once you set up
this graph, the rest

00:28:46.270 --> 00:28:47.540
is the same as Roth's theorem.

00:28:47.540 --> 00:28:49.840
So this connection between
graph theory and additive

00:28:49.840 --> 00:28:52.210
combinatorics, well,
you need to figure out

00:28:52.210 --> 00:28:53.390
how to set up this graph.

00:28:53.390 --> 00:28:57.760
And then sometimes it's
clear, but sometimes you

00:28:57.760 --> 00:29:00.760
have to really think hard
about how to do this.

00:29:04.000 --> 00:29:05.080
All right.

00:29:05.080 --> 00:29:08.680
What does corner have to
do with Roth's theorem,

00:29:08.680 --> 00:29:12.415
other than that they have
very similar looking proofs?

00:29:12.415 --> 00:29:14.290
Well, actually, you can
deduce Roth's theorem

00:29:14.290 --> 00:29:15.550
from the Corners theorem.

00:29:18.990 --> 00:29:24.070
And to show you this precisely,
so let me use r sub 3 of N

00:29:24.070 --> 00:29:37.950
to denote the size of the
largest 3-AP-free set of 1

00:29:37.950 --> 00:29:41.520
through N.

00:29:41.520 --> 00:29:44.790
So this notation, the r sub 3
is actually fairly standard.

00:29:44.790 --> 00:29:47.443
The next one's not so standard,
but let's just do that.

00:29:47.443 --> 00:29:49.110
So that's not an L,
but that's a corner.

00:29:51.690 --> 00:29:58.410
So that is the size of the
largest corner-free subset

00:29:58.410 --> 00:30:02.940
of N squared.

00:30:02.940 --> 00:30:06.890
So we gave bounds
for both quantities,

00:30:06.890 --> 00:30:11.780
but they are actually related to
each other through this fairly

00:30:11.780 --> 00:30:14.300
simple proposition
that if you have

00:30:14.300 --> 00:30:16.370
an upper bound for
corners, then you

00:30:16.370 --> 00:30:20.497
have an upper bound
for Roth's theorem.

00:30:25.270 --> 00:30:39.430
Indeed, given A a 3-AP-free
subset of 1 through N,

00:30:39.430 --> 00:30:42.720
let me build for you
a corner-free subset

00:30:42.720 --> 00:30:47.430
of the grid that is a fairly
large subset of that grid.

00:30:50.220 --> 00:31:04.220
I can form B by setting
it to be the set of pairs

00:31:04.220 --> 00:31:09.930
inside this grid whose
difference, x minus y,

00:31:09.930 --> 00:31:16.530
lies in A. So what
does this look like?

00:31:23.910 --> 00:31:28.450
This is the grid of size 2N.

00:31:28.450 --> 00:31:37.590
And if I start with A that
is 3-AP-free, what I can do--

00:31:37.590 --> 00:31:38.880
so over here--

00:31:38.880 --> 00:31:52.260
is look at the lines like that,
putting all of those points.

00:31:52.260 --> 00:31:54.060
And you see that
this set of points

00:31:54.060 --> 00:32:00.590
should not have any corners
because if they have corners,

00:32:00.590 --> 00:32:02.920
then the corners would
project down to a 3-AP.

00:32:06.740 --> 00:32:08.386
So B is corner-free.

00:32:16.970 --> 00:32:19.680
To recap, if we have
upper bound on corners,

00:32:19.680 --> 00:32:22.630
we have upper bound
on Roth's theorem.

00:32:22.630 --> 00:32:25.425
But you also know
that if you have lower

00:32:25.425 --> 00:32:26.800
bound on Roth's
theorem, then you

00:32:26.800 --> 00:32:28.420
have lower bound on corners.

00:32:28.420 --> 00:32:31.210
So the Behrend construction we
saw at the beginning of today

00:32:31.210 --> 00:32:34.540
extends to, you know, through
exactly this way to a fairly

00:32:34.540 --> 00:32:35.980
large corner-free subset.

00:32:35.980 --> 00:32:38.920
And that's more or less the best
thing that we know how to do.

00:32:38.920 --> 00:32:41.760
In fact, there aren't
that many constructions

00:32:41.760 --> 00:32:43.990
in additive combinatorics
that's known.

00:32:43.990 --> 00:32:45.430
Almost everything
that I know how

00:32:45.430 --> 00:32:47.830
to construct that's
fairly large come

00:32:47.830 --> 00:32:50.160
from Behrend's construction
or some variant

00:32:50.160 --> 00:32:51.850
of Behrend's construction.

00:32:51.850 --> 00:32:53.170
So it looks pretty simple.

00:32:53.170 --> 00:32:55.060
It comes from the '40s,
yet we don't really

00:32:55.060 --> 00:32:59.110
have too many new ideas
besides playing and massaging

00:32:59.110 --> 00:33:00.520
Behrend's construction.

00:33:04.360 --> 00:33:07.180
Let me tell you what is
the best known upper bound

00:33:07.180 --> 00:33:08.680
on the Corners theorem.

00:33:14.440 --> 00:33:15.440
This is due to Shkredov.

00:33:18.430 --> 00:33:22.185
So the proof using
triangle removal lemma,

00:33:22.185 --> 00:33:25.110
it goes to Szemeredi's
regularity lemma.

00:33:25.110 --> 00:33:26.760
It gives you pretty
horrible bounds,

00:33:26.760 --> 00:33:31.070
but using Fourier
analytic methods--

00:33:31.070 --> 00:33:33.010
so you see if you have
upper bound on Roth,

00:33:33.010 --> 00:33:35.200
it doesn't give you an
upper bound on corner,

00:33:35.200 --> 00:33:37.700
so you need to do
something extra.

00:33:37.700 --> 00:33:43.330
And so the best known bound
so far is of the form,

00:33:43.330 --> 00:33:47.825
N squared divided
by polylog log N,

00:33:47.825 --> 00:33:50.305
so log log N raised to
some small constant,

00:33:50.305 --> 00:33:53.748
C. Any questions?

00:34:02.050 --> 00:34:04.730
Last time, we discussed
the triangle counting lemma

00:34:04.730 --> 00:34:06.070
and the triangle removal lemma.

00:34:08.870 --> 00:34:11.020
Well, it shouldn't
be a surprise to you

00:34:11.020 --> 00:34:12.480
that if we can do
it for triangles,

00:34:12.480 --> 00:34:15.929
we may be able to do
it for other subgraphs.

00:34:15.929 --> 00:34:17.790
So that's the next thing
I want to discuss--

00:34:17.790 --> 00:34:22.380
how to generalize the
techniques and results

00:34:22.380 --> 00:34:25.500
that we obtain for
triangles to other graphs,

00:34:25.500 --> 00:34:27.090
and what are some
of the implications

00:34:27.090 --> 00:34:30.000
if you combine it with
Szemeredi's regularity lemma.

00:34:49.179 --> 00:34:56.236
So let's generalize the
triangle counting lemma.

00:35:09.370 --> 00:35:11.650
So the strategy for the
triangle counting lemma,

00:35:11.650 --> 00:35:15.110
let me remind you, was that
we embedded the vertices one

00:35:15.110 --> 00:35:15.610
by one.

00:35:18.280 --> 00:35:20.890
Putting a vertex and
a typical vertex here

00:35:20.890 --> 00:35:27.610
should have many neighbors
to both vertex sets.

00:35:27.610 --> 00:35:32.980
So these two guys should
have sizes typically roughly

00:35:32.980 --> 00:35:36.730
the same as a fraction
of them corresponding

00:35:36.730 --> 00:35:38.920
to the edge density
between the vertex parts.

00:35:41.610 --> 00:35:43.490
And if they're not
too small, then

00:35:43.490 --> 00:35:46.670
from the epsilon regularity
of these two sets,

00:35:46.670 --> 00:35:50.250
you can deduce the number
of edges between them.

00:35:50.250 --> 00:35:54.210
So that was the strategy for
the triangle counting lemma.

00:35:54.210 --> 00:35:58.580
So you can try to extend the
same strategy for other graphs.

00:35:58.580 --> 00:36:01.410
So let me show you how
this would have been done,

00:36:01.410 --> 00:36:04.520
but I don't want to give
too many details because it

00:36:04.520 --> 00:36:07.420
does get somewhat hairy
if you try to execute it.

00:36:11.300 --> 00:36:18.730
So the first strategy is to
embed the vertices of H one

00:36:18.730 --> 00:36:19.230
at a time.

00:36:24.030 --> 00:36:29.100
So my H now is going
to be, let's say, a K4.

00:36:32.770 --> 00:36:38.190
And I wish to embed
this H in this setting

00:36:38.190 --> 00:36:41.940
where you have these four parts
in the regularity partition,

00:36:41.940 --> 00:36:45.570
and they are pairwise epsilon
regular with edge densities

00:36:45.570 --> 00:36:49.700
that are not too small.

00:36:49.700 --> 00:36:52.600
Well, what you can try to
do, mimicking the strategy

00:36:52.600 --> 00:36:58.570
over there, is to first
find a typical image

00:36:58.570 --> 00:36:59.875
for the top vertex.

00:37:04.760 --> 00:37:07.920
And a typical image
over here, minus

00:37:07.920 --> 00:37:14.460
some small bad exceptions,
will have many neighbors

00:37:14.460 --> 00:37:15.600
to each of the three parts.

00:37:19.160 --> 00:37:25.340
Next, I need to figure out
where this vertex can go.

00:37:25.340 --> 00:37:31.020
I'm going to embed this
vertex somewhere here.

00:37:31.020 --> 00:37:34.440
Again, a typical
place for this vertex,

00:37:34.440 --> 00:37:37.440
modulo some small fraction,
which I'm going to throw away.

00:37:37.440 --> 00:37:40.380
So now you see you need
somewhat stronger hypotheses

00:37:40.380 --> 00:37:43.260
on the epsilon regularity,
but they are still

00:37:43.260 --> 00:37:45.720
all polynomial
dependent, so you just

00:37:45.720 --> 00:37:47.830
have to choose your
parameters correctly.

00:37:47.830 --> 00:37:52.120
So this typical green
vertex should have

00:37:52.120 --> 00:37:56.940
lots of neighbors over here.

00:37:59.740 --> 00:38:01.450
So you just keep embedding.

00:38:01.450 --> 00:38:03.280
So it's almost this
greedy strategy.

00:38:03.280 --> 00:38:06.190
You keep embedding
each vertex, but you

00:38:06.190 --> 00:38:10.930
have to guarantee that there
are still lots of options left.

00:38:16.690 --> 00:38:20.170
So embed vertices one at a time.

00:38:20.170 --> 00:38:34.050
And I want to embed each vertex
so that the yet to be embedded

00:38:34.050 --> 00:38:47.550
vertices have many choices left.

00:39:01.690 --> 00:39:05.490
So epsilon regularity guarantees
that you can always do this.

00:39:05.490 --> 00:39:08.290
And you do this all
the way until the end

00:39:08.290 --> 00:39:11.490
and you arrive at
some statement.

00:39:11.490 --> 00:39:13.540
And depending on
how you do this,

00:39:13.540 --> 00:39:15.340
the exact formulation
of the statement

00:39:15.340 --> 00:39:16.798
will be somewhat
different, but let

00:39:16.798 --> 00:39:19.120
me give you one statement
which we will not prove,

00:39:19.120 --> 00:39:22.060
but you can deduce
using this strategy.

00:39:22.060 --> 00:39:25.720
And this is known as a
graph embedding lemma.

00:39:25.720 --> 00:39:28.420
And again, as I
mentioned when I started

00:39:28.420 --> 00:39:29.855
discussing
Szemeredi's regularity

00:39:29.855 --> 00:39:34.870
lemma, the exact statements,
they are fairly robust

00:39:34.870 --> 00:39:39.520
and they are not as important
as the spirit of the ideas.

00:39:39.520 --> 00:39:41.380
So if you have some
application in mind,

00:39:41.380 --> 00:39:44.170
you might have to go into
the proof, tweak a thing

00:39:44.170 --> 00:39:47.210
here and there, but
you get what you want.

00:39:47.210 --> 00:39:50.240
So the graph embedding
lemma says, for example,

00:39:50.240 --> 00:39:59.020
that if H is bipartite, and with
maximum degree and most delta--

00:39:59.020 --> 00:40:03.120
so the H maximum
degrees at most delta--

00:40:03.120 --> 00:40:11.930
and suppose you have vertex
sets V1 through Vr, such

00:40:11.930 --> 00:40:21.970
that each vertex set
is not too small;

00:40:21.970 --> 00:40:29.620
and if these vertex sets
are pairwise epsilon

00:40:29.620 --> 00:40:36.550
regular and the density
is not too small.

00:40:40.890 --> 00:40:44.440
So here I'm assuming
some lower bound

00:40:44.440 --> 00:40:50.220
on the density which
depends on your epsilon.

00:40:50.220 --> 00:41:02.910
Then the conclusion is that
G contains a copy of H.

00:41:02.910 --> 00:41:06.090
I just want to give a few
remarks on the statement

00:41:06.090 --> 00:41:06.900
of this theorem.

00:41:06.900 --> 00:41:10.000
Again, we will not
discuss the proof.

00:41:10.000 --> 00:41:14.250
So what is this
hypothesis on H being

00:41:14.250 --> 00:41:16.200
r-partite have to
do with anything?

00:41:16.200 --> 00:41:19.620
So here, as an example,
when r equals to 4, instead

00:41:19.620 --> 00:41:24.870
of this K4, maybe I also care
about that graph over there.

00:41:29.520 --> 00:41:31.520
Well, maybe some more
vertices, some more edges,

00:41:31.520 --> 00:41:32.840
but if it's 4-partite.

00:41:32.840 --> 00:41:37.640
And the point is that I want
to embed the vertices in such

00:41:37.640 --> 00:41:42.290
a way that that top
vertex goes to the part

00:41:42.290 --> 00:41:45.160
that it's supposed to go into.

00:41:45.160 --> 00:41:47.660
So I'm embedding
this configuration

00:41:47.660 --> 00:41:52.400
in the same way that corresponds
to the proper coloring of H.

00:41:52.400 --> 00:41:56.840
So if you do this,
there's enough room still

00:41:56.840 --> 00:42:01.580
to go, as long as you have
some lower bound on the edge

00:42:01.580 --> 00:42:06.240
density between the
individual parts,

00:42:06.240 --> 00:42:10.170
and you only depend really
on not the number of edges

00:42:10.170 --> 00:42:13.740
of H, but the maximum degree.

00:42:13.740 --> 00:42:18.640
Because if you look at how
many times each vertex,

00:42:18.640 --> 00:42:22.170
its possibilities can be shrunk,
it's in most delta times.

00:42:25.290 --> 00:42:26.993
Now, this graph
embedding lemma--

00:42:26.993 --> 00:42:28.410
so I give you this
statement here,

00:42:28.410 --> 00:42:29.920
but it's a fairly
robust statement.

00:42:29.920 --> 00:42:32.800
And if you want to get, for
example, not just a single copy

00:42:32.800 --> 00:42:35.505
of H, if you want to
get many copies of H,

00:42:35.505 --> 00:42:36.880
you can tweak the
hypotheses, you

00:42:36.880 --> 00:42:38.920
can tweak the proofs
somewhat to get you

00:42:38.920 --> 00:42:42.045
what you want, again, following
what we did for the triangle

00:42:42.045 --> 00:42:42.670
counting lemma.

00:42:45.350 --> 00:42:46.670
Question.

00:42:46.670 --> 00:42:50.226
AUDIENCE: Is the bound on the H
edge density between partitions

00:42:50.226 --> 00:42:51.040
correct?

00:42:51.040 --> 00:42:53.770
So if the maximum
degree increases,

00:42:53.770 --> 00:42:55.240
the lower bound decreases?

00:42:55.240 --> 00:42:58.981
PROFESSOR: If maximum degree
increases, this number goes up.

00:42:58.981 --> 00:42:59.963
AUDIENCE: Oh, OK.

00:43:08.810 --> 00:43:11.300
PROFESSOR: I want to
show you a different way

00:43:11.300 --> 00:43:15.710
to do counting that does not go
through this embedding vertices

00:43:15.710 --> 00:43:18.680
one by one, but
instead we will try

00:43:18.680 --> 00:43:22.250
to analyze what happens if
you take out an edge of H one

00:43:22.250 --> 00:43:24.190
by one.

00:43:24.190 --> 00:43:27.010
And that's an alternative
approach which I like more.

00:43:27.010 --> 00:43:29.270
It's somewhat less
intuitive if you are not

00:43:29.270 --> 00:43:31.880
used to thinking about
it, but the execution

00:43:31.880 --> 00:43:33.350
works out to be much cleaner.

00:43:33.350 --> 00:43:37.460
And it's also in line with
some of the techniques

00:43:37.460 --> 00:43:41.398
that we'll see later on when
we discuss graph limits.

00:43:41.398 --> 00:43:42.440
Let's take a quick break.

00:43:45.620 --> 00:43:46.630
Any questions so far?

00:43:54.100 --> 00:43:58.880
We will see a second strategy
for proving a graph counting

00:43:58.880 --> 00:44:00.650
lemma.

00:44:00.650 --> 00:44:06.100
And the second strategy is
more analytic in nature,

00:44:06.100 --> 00:44:15.740
which is to embed, well, to
analytically somehow we'll

00:44:15.740 --> 00:44:19.670
analyze what happens when
we pick out one edge of H

00:44:19.670 --> 00:44:20.292
at a time.

00:44:26.635 --> 00:44:28.260
So let me give you
the statement first.

00:44:38.050 --> 00:44:41.070
So the graph counting
lemma says that if you

00:44:41.070 --> 00:44:51.030
have a graph H with vertex
set elements of 1 through k--

00:44:51.030 --> 00:44:55.110
I also have an epsilon
parameter and I

00:44:55.110 --> 00:45:04.580
have a graph G and subsets
V1 through Vk of G,

00:45:04.580 --> 00:45:11.900
such that Vi Vj
is epsilon regular

00:45:11.900 --> 00:45:18.370
whenever ij is an edge of H.

00:45:18.370 --> 00:45:21.130
So here, the setup is slightly
different from the picture

00:45:21.130 --> 00:45:22.910
I drew up there.

00:45:22.910 --> 00:45:28.770
So what's going on
here is that you have,

00:45:28.770 --> 00:45:35.390
let's say, H being this graph.

00:45:35.390 --> 00:45:38.990
And suppose you are
in a situation where

00:45:38.990 --> 00:45:41.180
I know that some of these--

00:45:47.700 --> 00:45:53.370
so I know that five of these
pairs are epsilon regular,

00:45:53.370 --> 00:45:58.200
and what I really want
to do is embed this H

00:45:58.200 --> 00:45:59.933
into this configuration.

00:46:06.090 --> 00:46:08.920
So 1, 2; V1, V2; and so on.

00:46:11.800 --> 00:46:15.010
And I want to know how many
ways can you embed this way.

00:46:18.690 --> 00:46:21.240
The conclusion is
that the number

00:46:21.240 --> 00:46:36.900
of tuples, the number of such
embeddings, such that Vi, Vj

00:46:36.900 --> 00:46:45.130
is an edge of G for all
ij being the edge of H--

00:46:45.130 --> 00:46:47.040
so exactly as
showing that picture,

00:46:47.040 --> 00:46:52.390
the number of such
embeddings, the little v's, is

00:46:52.390 --> 00:46:57.040
within a small error, which
is the number of edges

00:46:57.040 --> 00:47:05.660
of H epsilon times the total,
the product of these vertex

00:47:05.660 --> 00:47:16.810
set sizes of this number,
which is what you would predict

00:47:16.810 --> 00:47:21.400
the number of embeddings to be
if all of your bipartite graphs

00:47:21.400 --> 00:47:23.100
were actually random.

00:47:36.350 --> 00:47:38.530
So like in the triangle
counting lemma,

00:47:38.530 --> 00:47:42.520
if you look at the edge
densities in this configuration

00:47:42.520 --> 00:47:46.420
and predict how many
copies of H you would get,

00:47:46.420 --> 00:47:48.400
that's the number you
should write down.

00:47:48.400 --> 00:47:51.730
And this counting
lemma tells you

00:47:51.730 --> 00:47:55.955
that the truth is not so
far from the prediction.

00:48:04.510 --> 00:48:05.679
Any questions?

00:48:14.170 --> 00:48:16.000
So we will prove
the graph counting

00:48:16.000 --> 00:48:21.430
lemma in an analytic manner.

00:48:21.430 --> 00:48:24.130
It helps to-- so it
will be convenient

00:48:24.130 --> 00:48:28.090
for me to rephrase the
result just a little bit

00:48:28.090 --> 00:48:29.940
in this probabilistic form.

00:48:29.940 --> 00:48:34.870
So it has the equivalent
to show that if you

00:48:34.870 --> 00:48:42.325
have uniformly randomly chosen
vertices, little v1 and big V1,

00:48:42.325 --> 00:48:45.770
and so on--

00:48:45.770 --> 00:48:50.700
so little vk and big VK.

00:48:50.700 --> 00:48:53.770
So they're independent,
uniformly, and random,

00:48:53.770 --> 00:48:59.000
then the probability--

00:48:59.000 --> 00:49:02.830
so basically, I am putting
down a potential image

00:49:02.830 --> 00:49:06.225
for each vertex of
H and asking what's

00:49:06.225 --> 00:49:07.600
the probability
that you actually

00:49:07.600 --> 00:49:08.980
have an embedding of H.

00:49:08.980 --> 00:49:12.070
So the probability that
little vi, little vj

00:49:12.070 --> 00:49:18.910
is an actual edge of G for
all ij being an edge of H,

00:49:18.910 --> 00:49:24.150
this number here, we're
saying that it differs

00:49:24.150 --> 00:49:28.500
from the prediction, which is
simply multiplying all the edge

00:49:28.500 --> 00:49:29.670
densities together.

00:49:36.070 --> 00:49:39.710
So the difference between the
actual and the predicted values

00:49:39.710 --> 00:49:43.117
is fairly small.

00:49:43.117 --> 00:49:45.450
So I haven't done anything,
just rephrasing the problem.

00:49:45.450 --> 00:49:48.104
Instead of counting, now we're
looking at probabilities.

00:49:53.050 --> 00:49:56.920
As I mentioned, we'll take
out one edge at a time.

00:49:56.920 --> 00:50:01.510
So relabeling if
necessary, let's assume

00:50:01.510 --> 00:50:09.870
that 1, 2 is an
edge of H. So now we

00:50:09.870 --> 00:50:18.080
will show the following
plane, so I'll denote star.

00:50:18.080 --> 00:50:31.380
That, if you look at
this quantity over here

00:50:31.380 --> 00:50:40.700
compared to if you take out
just the edge density between V1

00:50:40.700 --> 00:50:47.670
and V2, but now you put
in a similar quantity

00:50:47.670 --> 00:50:55.820
where I'm considering all of
the edges of H, except for 1, 2.

00:51:11.750 --> 00:51:14.930
I claim that this difference
is, at most, epsilon.

00:51:24.390 --> 00:51:26.070
So you can think
of this quantity

00:51:26.070 --> 00:51:32.340
here as the same quantity as
the green one, except not on H,

00:51:32.340 --> 00:51:34.990
but on H minus the edge 1, 2.

00:51:43.350 --> 00:51:50.190
To show this star claim, let us
couple the two random processes

00:51:50.190 --> 00:51:51.860
choosing these little vi's.

00:52:07.990 --> 00:52:11.710
By that, I mean here you have
the random little vi's and here

00:52:11.710 --> 00:52:14.590
you have the random little vi's,
but use the same little vi's

00:52:14.590 --> 00:52:17.430
in both probabilities.

00:52:17.430 --> 00:52:21.720
So in both, there are two
different random events,

00:52:21.720 --> 00:52:24.430
but you use the
same little vi's,

00:52:24.430 --> 00:52:35.310
it suffices to show this
inequality star with--

00:52:37.900 --> 00:52:39.770
so what's this process,
this random process?

00:52:39.770 --> 00:52:42.830
You pick V1, you pick V2,
you pick V3, all of them

00:52:42.830 --> 00:52:45.690
independently
uniformly at random.

00:52:45.690 --> 00:52:50.520
But if I show you the inequality
under a further constraint

00:52:50.520 --> 00:52:55.540
of arbitrary V3 through VN,
then that's even better.

00:52:58.540 --> 00:53:10.510
So with V3 through
Vk, fixed arbitrary,

00:53:10.510 --> 00:53:18.420
and only little v1
and little v2 random.

00:53:27.028 --> 00:53:29.570
So you can phrase this in terms
of conditional probabilities,

00:53:29.570 --> 00:53:31.450
if you like.

00:53:31.450 --> 00:53:33.950
You're comparing these
two probabilities.

00:53:33.950 --> 00:53:38.660
Now I fix the V3's
through Vk's, and I just

00:53:38.660 --> 00:53:40.850
let V1 and V2 be random.

00:53:40.850 --> 00:53:47.720
And if condition on V3 through
Vk, you have this inequality,

00:53:47.720 --> 00:53:51.080
then letting V3
through Vk go, letting

00:53:51.080 --> 00:53:53.750
them be random as well,
by triangle inequality,

00:53:53.750 --> 00:53:57.260
you obtain the bound
that we're looking for.

00:54:02.060 --> 00:54:03.340
Any questions about this step?

00:54:06.220 --> 00:54:08.630
If you're confused about
what's happening here,

00:54:08.630 --> 00:54:11.840
another way to bypass all
the probability language

00:54:11.840 --> 00:54:14.720
is to go back to the
counting language.

00:54:14.720 --> 00:54:17.260
So in other words, we're
trying to count embeddings.

00:54:17.260 --> 00:54:23.590
And I'm asking if you
arbitrarily fix V3 through Vk,

00:54:23.590 --> 00:54:26.600
how many different choices
are there to V1 and V2

00:54:26.600 --> 00:54:27.850
in the two different settings.

00:54:32.600 --> 00:54:34.420
OK.

00:54:34.420 --> 00:54:47.950
So let A1 be the set of
places where little v1 can go,

00:54:47.950 --> 00:54:51.880
if you already knew what
V3 through Vk should be.

00:55:05.980 --> 00:55:06.510
OK.

00:55:06.510 --> 00:55:12.600
So you look at all the neighbors
of V1, neighbors of 1 and H,

00:55:12.600 --> 00:55:14.650
except for 2.

00:55:14.650 --> 00:55:19.250
And I want to make
sure that V1, Vi, as i

00:55:19.250 --> 00:55:24.820
ranges over all such neighbors,
is indeed a valid H in G.

00:55:24.820 --> 00:55:26.410
I'll draw a picture in a second.

00:55:26.410 --> 00:55:36.870
And A2, likewise, is
the same quantity,

00:55:36.870 --> 00:55:38.310
but with 2 instead of 1.

00:55:47.200 --> 00:55:51.640
So for example, if you're
trying to embed a K4, what's

00:55:51.640 --> 00:56:00.260
happening here is that you have
this V1, this V2, and somebody

00:56:00.260 --> 00:56:03.130
already arbitrarily
fixed the locations

00:56:03.130 --> 00:56:06.120
where V3 and V4 are embedded.

00:56:06.120 --> 00:56:10.500
And you're asking, well, how
many different choices are now

00:56:10.500 --> 00:56:12.960
left for little v1.

00:56:12.960 --> 00:56:21.830
It's the common
neighborhood of V3 and V4.

00:56:21.830 --> 00:56:24.730
So that's for A1.

00:56:24.730 --> 00:56:32.150
And V3 and V4, also their
common neighborhood in B2 is A2.

00:56:38.550 --> 00:56:39.050
OK.

00:56:39.050 --> 00:56:42.680
So with that notation,
what is it that we're

00:56:42.680 --> 00:56:46.060
trying to show over here?

00:56:46.060 --> 00:56:48.590
So if you rewrite this
inequality with the V3's

00:56:48.590 --> 00:56:56.880
through Vk's fixed, you find
that what we're trying to show

00:56:56.880 --> 00:56:59.116
is the following.

00:57:03.880 --> 00:57:10.680
So we claim-- and this
claim implies the star,

00:57:10.680 --> 00:57:16.040
that if you look at what
the first term should be--

00:57:16.040 --> 00:57:21.820
so this is the number of
edges between A1 and A2

00:57:21.820 --> 00:57:26.590
as a fraction of the
product of V1 and V2.

00:57:33.120 --> 00:57:36.990
And then the second
guy here is if you use

00:57:36.990 --> 00:57:41.890
the prediction d of V1 and V2.

00:57:53.230 --> 00:57:56.620
So this is each of them,
each of these two factors,

00:57:56.620 --> 00:58:01.420
there's a probability
that little v1 lies in A1,

00:58:01.420 --> 00:58:03.790
little v2 lies in
A2, and then you

00:58:03.790 --> 00:58:09.030
tack on this extra constant,
namely, this constant here.

00:58:09.030 --> 00:58:11.580
So we're trying to show that
this difference is small.

00:58:11.580 --> 00:58:14.430
And the claim is that this
difference is, indeed,

00:58:14.430 --> 00:58:15.900
always small.

00:58:15.900 --> 00:58:22.800
There's always, at most, epsilon
for every A1 in V1 and A2

00:58:22.800 --> 00:58:23.400
in V2.

00:58:29.970 --> 00:58:33.950
And here, in particular,
so this statement

00:58:33.950 --> 00:58:35.990
looks somewhat like the
definition of epsilon

00:58:35.990 --> 00:58:38.150
regularity, but
there's no restrictions

00:58:38.150 --> 00:58:42.710
on the sizes of A1 and A2,
and they don't have to be big.

00:58:46.690 --> 00:58:48.670
And as you can imagine,
this statement,

00:58:48.670 --> 00:58:51.250
we're not really
using all that much.

00:58:51.250 --> 00:58:56.170
All we're assuming is
the epsilon regularity

00:58:56.170 --> 00:58:58.150
between B1 and B2.

00:58:58.150 --> 00:59:01.000
So we will deduce
this inequality

00:59:01.000 --> 00:59:07.535
from the hypotheses of epsilon
regularity between B1 and B2.

00:59:07.535 --> 00:59:08.160
So let's check.

00:59:11.730 --> 00:59:19.950
So we know that B1 and B2 is
epsilon regular by hypotheses.

00:59:19.950 --> 00:59:30.770
So if either A1 or
A2 is too small,

00:59:30.770 --> 00:59:40.950
so if A1 is too small
or A2 is too small,

00:59:40.950 --> 00:59:50.920
then we see that
both of these terms

00:59:50.920 --> 00:59:52.780
here are, at most, epsilon.

01:00:00.040 --> 01:00:02.770
So if the A's are too small,
then neither of these terms

01:00:02.770 --> 01:00:03.730
can be too large.

01:00:03.730 --> 01:00:05.920
Here it's bounded by--

01:00:05.920 --> 01:00:08.640
if you took the
product of A1 and

01:00:08.640 --> 01:00:11.743
A2 and likewise over there.

01:00:11.743 --> 01:00:13.660
So in this case, their
difference is, at most,

01:00:13.660 --> 01:00:14.910
epsilon, and we're good to go.

01:00:17.570 --> 01:00:24.750
Otherwise, if A1 and A2 are both
at least an epsilon fraction

01:00:24.750 --> 01:00:34.220
of their [INAUDIBLE]
sets, then we find that--

01:00:37.200 --> 01:00:38.330
so what happens?

01:00:38.330 --> 01:00:45.480
So here, so by the hypothesis
of epsilon regularity,

01:00:45.480 --> 01:00:52.990
we find that d of
V1 and V2 differs

01:00:52.990 --> 01:00:56.500
from the number of
edges between A1 and A2,

01:00:56.500 --> 01:00:59.860
divided by the product
of their sizes.

01:00:59.860 --> 01:01:03.040
So that's just d of A1 and A2.

01:01:03.040 --> 01:01:16.490
So this difference is, at most,
epsilon, which then implies

01:01:16.490 --> 01:01:17.890
the inequality up there.

01:01:21.640 --> 01:01:24.160
So here we're using
that the size of A

01:01:24.160 --> 01:01:35.670
is, at most, the size of V.

01:01:35.670 --> 01:01:38.130
So we have this claim.

01:01:38.130 --> 01:01:42.720
And that claim proves
this inequality in star.

01:01:42.720 --> 01:01:44.850
And basically,
what we've done is

01:01:44.850 --> 01:01:48.630
we showed that if you
took out a single edge,

01:01:48.630 --> 01:01:53.190
you change the desired quantity
by, at most, an epsilon,

01:01:53.190 --> 01:01:54.540
essentially.

01:01:54.540 --> 01:02:00.030
So now you do this for every
edge of H. Alternatively,

01:02:00.030 --> 01:02:06.730
you can do induction on
the number of edges of H.

01:02:06.730 --> 01:02:09.360
So to complete a
proof of the counting

01:02:09.360 --> 01:02:24.980
lemma, so we do induction
on the number of edges of H.

01:02:24.980 --> 01:02:30.520
And when H has exactly one
edge, well, that's pretty easy.

01:02:30.520 --> 01:02:34.210
But now if you have
more edges, well, you

01:02:34.210 --> 01:02:37.480
apply induction
hypothesis to the graph,

01:02:37.480 --> 01:02:41.410
which is H minus the edge 1, 2.

01:02:44.330 --> 01:02:58.880
And you find that
this quantity here

01:02:58.880 --> 01:03:14.850
differs from the
predicted quantity

01:03:14.850 --> 01:03:20.550
by the number of edges of
H minus 1 times epsilon.

01:03:24.760 --> 01:03:27.310
In other words, you run
this prove that we just

01:03:27.310 --> 01:03:28.850
did one edge at a time.

01:03:28.850 --> 01:03:30.610
So each time you
take out an edge,

01:03:30.610 --> 01:03:32.350
you use epsilon
regularity to show

01:03:32.350 --> 01:03:34.660
that the effect of taking
that edge out from H

01:03:34.660 --> 01:03:38.890
does not have too big of an
effect on the actual number

01:03:38.890 --> 01:03:40.570
of embeddings.

01:03:40.570 --> 01:03:42.940
Do this for one edge at a
time, and eventually you

01:03:42.940 --> 01:03:44.745
prove the graph counting lemma.

01:03:49.110 --> 01:03:52.320
So this is one of
those proofs which

01:03:52.320 --> 01:03:57.840
may be less intuitive compared
to the one I showed earlier,

01:03:57.840 --> 01:04:01.120
in the sense that there's not
as nice of a story you can tell

01:04:01.120 --> 01:04:04.180
about putting in one
vertex at a time.

01:04:04.180 --> 01:04:07.560
But on the other hand, if you
were to carry out this proof

01:04:07.560 --> 01:04:11.010
to bound each time how
big the sets have to be,

01:04:11.010 --> 01:04:12.900
it gets much hairier over here.

01:04:12.900 --> 01:04:17.070
And here, the execution is
much cleaner, but maybe less

01:04:17.070 --> 01:04:19.650
intuitive, unless
you're willing to be

01:04:19.650 --> 01:04:21.760
comfortable with
these calculations.

01:04:21.760 --> 01:04:23.010
And it's really not so bad.

01:04:25.810 --> 01:04:29.700
And the strength of these two
results are somewhat different.

01:04:29.700 --> 01:04:33.500
So again, it's not so much the
exact statements that matter,

01:04:33.500 --> 01:04:36.830
but the spirit of
these statements, which

01:04:36.830 --> 01:04:40.550
is that if you have a bunch
of epsilon regular pairs,

01:04:40.550 --> 01:04:45.800
then you can embed and kind
of pretending that everything

01:04:45.800 --> 01:04:47.290
behaved roughly like random.

01:04:50.890 --> 01:04:52.266
Any questions?

01:04:59.410 --> 01:05:02.740
So now we have Szemeredi's
graph regularity lemma,

01:05:02.740 --> 01:05:06.730
we have the graph counting
lemma, embedding lemmas,

01:05:06.730 --> 01:05:11.620
we can use it to derive some
additional applications that

01:05:11.620 --> 01:05:13.540
don't just involve triangles.

01:05:13.540 --> 01:05:15.940
So when we only had a
triangle counting lemma,

01:05:15.940 --> 01:05:18.700
we can only do the
triangle removal lemma,

01:05:18.700 --> 01:05:22.330
but now we can do
other removal lemmas.

01:05:22.330 --> 01:05:25.930
So in particular, there's
the graph removal lemma,

01:05:25.930 --> 01:05:28.490
which generalizes the
triangle removal lemma.

01:05:38.690 --> 01:05:40.870
So in the graph removal
lemma, the statement

01:05:40.870 --> 01:05:46.040
is that for every H
and epsilon, there

01:05:46.040 --> 01:05:55.040
exists a delta, such
that every N vertex

01:05:55.040 --> 01:06:05.450
graph with fewer than
delta N to the vertex

01:06:05.450 --> 01:06:10.980
of H number of copies of H--

01:06:10.980 --> 01:06:15.190
so it has very few copies of H--

01:06:15.190 --> 01:06:29.920
such graph can be made
H-free by removing

01:06:29.920 --> 01:06:33.350
a fairly small number of edges.

01:06:33.350 --> 01:06:37.190
All right, so same statement
as the triangle removal lemma,

01:06:37.190 --> 01:06:41.250
except now for general graph H.

01:06:41.250 --> 01:06:44.460
And as you expect, the proof
is more or less the same

01:06:44.460 --> 01:06:48.140
as that of a triangle
removal lemma,

01:06:48.140 --> 01:06:51.470
once we have the
H counting lemma.

01:06:51.470 --> 01:06:53.880
So let me remind
you how this goes.

01:06:53.880 --> 01:07:01.410
So it's really the same
proof as triangle removal,

01:07:01.410 --> 01:07:05.130
where there was this recipe
for applying the regularity

01:07:05.130 --> 01:07:07.020
lemma from last time.

01:07:07.020 --> 01:07:08.220
So what is it?

01:07:08.220 --> 01:07:14.980
You first-- so what's the first
step when you do regularity?

01:07:14.980 --> 01:07:15.950
You partition.

01:07:15.950 --> 01:07:17.271
So let's do partition.

01:07:20.240 --> 01:07:20.740
OK.

01:07:20.740 --> 01:07:24.020
So apply the regularity
lemma to do partitioning.

01:07:24.020 --> 01:07:26.780
And what's the second step?

01:07:26.780 --> 01:07:28.470
So we clean this graph.

01:07:28.470 --> 01:07:30.870
And you do the same
cleaning procedure

01:07:30.870 --> 01:07:33.720
as in the triangle
removal lemma,

01:07:33.720 --> 01:07:36.210
except maybe you have to adjust
the parameters somewhat, so

01:07:36.210 --> 01:07:44.610
remove edges, remove
low density pairs,

01:07:44.610 --> 01:07:50.200
irregular pairs, and
small vertex sets.

01:07:54.810 --> 01:07:58.070
And the last step--

01:07:58.070 --> 01:07:59.810
OK, so what do we do now?

01:07:59.810 --> 01:08:00.310
OK.

01:08:00.310 --> 01:08:03.020
So you can count.

01:08:03.020 --> 01:08:11.350
So if there were any H left,
then the counting lemma

01:08:11.350 --> 01:08:14.030
shows you that you must have
lots of copies of H left.

01:08:26.600 --> 01:08:28.705
So now let me show you
how to use the strategy.

01:08:31.600 --> 01:08:35.104
Now that we have this
general graph counting lemma,

01:08:35.104 --> 01:08:38.050
we'll prove the
Erdos-Stone-Simonovits theorem,

01:08:38.050 --> 01:08:40.500
which we omitted,
the proof that we

01:08:40.500 --> 01:08:42.250
omitted from the first
part of the course.

01:08:46.020 --> 01:08:54.330
So remind you, the
Erdos-Stone-Simonovits theorem

01:08:54.330 --> 01:09:01.020
says that if you have a graph
H, then the extremal number of H

01:09:01.020 --> 01:09:06.910
is equal to this quantity
which depends only

01:09:06.910 --> 01:09:16.569
on the chromatic number of H.

01:09:16.569 --> 01:09:19.840
The lower bound comes from
taking the Turan graph.

01:09:25.390 --> 01:09:28.140
If you take the Turan graph,
you get this lower bound,

01:09:28.140 --> 01:09:32.149
so it's really the upper bound
that we need to think about.

01:09:34.930 --> 01:09:35.913
All right.

01:09:35.913 --> 01:09:37.080
So what's the strategy here?

01:09:40.760 --> 01:09:42.580
The statement really
is that if you

01:09:42.580 --> 01:09:50.229
have a graph G that's N
vertex whose number of edges

01:09:50.229 --> 01:09:58.230
is at least that much--

01:09:58.230 --> 01:10:02.720
OK, so I fixed an
epsilon bigger than zero,

01:10:02.720 --> 01:10:04.370
fixed a positive epsilon.

01:10:04.370 --> 01:10:06.760
So the claim, what we're
trying to show with

01:10:06.760 --> 01:10:09.500
Erdos-Stone-Simonovits, is that
if you have a graph G with too

01:10:09.500 --> 01:10:12.880
many edges-- too many
meaning this many edges--

01:10:12.880 --> 01:10:19.750
then G contains a copy of H
if N is sufficiently large.

01:10:27.400 --> 01:10:27.900
OK.

01:10:27.900 --> 01:10:30.240
So let's use the
regularity method,

01:10:30.240 --> 01:10:33.000
so applying this
three-step recipe.

01:10:33.000 --> 01:10:34.702
First, we partition.

01:10:40.840 --> 01:10:50.370
So partition the vertex
set of G into m pieces,

01:10:50.370 --> 01:10:54.870
and in such a way that
it is eta regular--

01:10:54.870 --> 01:10:57.840
and for some parameter eta
that we'll decide later.

01:11:14.560 --> 01:11:16.360
The second step is cleaning.

01:11:23.510 --> 01:11:25.920
The cleaning step,
again, it's the same kind

01:11:25.920 --> 01:11:27.820
of cleaning as
we've done before.

01:11:27.820 --> 01:11:37.910
So let's remove an
edge from Vi cross Vj,

01:11:37.910 --> 01:11:43.180
if any of the following hold--

01:11:43.180 --> 01:11:52.630
if Vi Vj is not A to
regular, if the density is

01:11:52.630 --> 01:12:05.000
too small, or either the
two sets is too small.

01:12:09.880 --> 01:12:12.900
So same cleaning as before.

01:12:12.900 --> 01:12:20.500
And we can check that the
number of edges removed

01:12:20.500 --> 01:12:23.980
is not too small.

01:12:23.980 --> 01:12:25.540
So in the first
case, so again, it's

01:12:25.540 --> 01:12:27.360
the same calculation
as last time.

01:12:27.360 --> 01:12:29.440
In the first case, the
number of edges removed

01:12:29.440 --> 01:12:32.250
is, at most, eta N squared.

01:12:32.250 --> 01:12:41.620
And we'll choose eta to be
less than epsilon over 8,

01:12:41.620 --> 01:12:43.753
although it will be actually
significantly smaller,

01:12:43.753 --> 01:12:44.920
as you will see in a second.

01:12:47.490 --> 01:12:51.820
In the second step, same as
what happened in the triangle

01:12:51.820 --> 01:12:56.830
removal stage, the
number of edges

01:12:56.830 --> 01:13:01.700
removed in the second type
is, at most, that amount,

01:13:01.700 --> 01:13:04.010
still very small number.

01:13:04.010 --> 01:13:11.440
And the third one here is
also a very small number.

01:13:11.440 --> 01:13:15.620
So the third type, they
start with one of these sets,

01:13:15.620 --> 01:13:18.580
the m possible--

01:13:18.580 --> 01:13:20.980
so it's a very small number.

01:13:20.980 --> 01:13:28.675
And so the total is, at most,
an epsilon over 2 N squared.

01:13:31.960 --> 01:13:33.465
So maybe I want epsilon over--

01:13:36.140 --> 01:13:42.650
so let's say epsilon over 2
N squared number of edges.

01:13:42.650 --> 01:13:46.690
And I would like that to
be strictly bigger than.

01:13:50.500 --> 01:14:04.640
So now after removing
these edges from G,

01:14:04.640 --> 01:14:11.340
we have this G prime, which has
strictly more than 1 minus 1

01:14:11.340 --> 01:14:13.330
over r.

01:14:13.330 --> 01:14:18.950
Yeah, 1 minus 1 over r times
N squared over 2 edges.

01:14:23.785 --> 01:14:24.660
So now what do we do?

01:14:28.170 --> 01:14:31.980
So we knew from Turan's theorem
that if your graph has strictly

01:14:31.980 --> 01:14:38.270
more than this number of edges,
you must have a K sub r plus 1.

01:14:38.270 --> 01:14:41.700
So even after deleting
all these edges,

01:14:41.700 --> 01:14:45.000
G still has lots of edges
left, in particular,

01:14:45.000 --> 01:14:51.560
Turan's theorem implies
that G prime contains

01:14:51.560 --> 01:14:56.270
a clique on r plus 1 vertices.

01:14:56.270 --> 01:15:04.063
So here I should say that r is
chromatic number of H minus 1.

01:15:08.010 --> 01:15:11.993
So I find one copy
of this clique,

01:15:11.993 --> 01:15:13.410
but what does that
copy look like?

01:15:17.080 --> 01:15:19.120
I find this one
copy of a clique.

01:15:23.310 --> 01:15:26.650
Let's say r equals to 4.

01:15:26.650 --> 01:15:30.400
And the point, now, is that
the counting lemma will allow

01:15:30.400 --> 01:15:44.450
me to amplify that
clique into H.

01:15:44.450 --> 01:15:50.190
So it will allow me to amplify
this clique into a copy of H.

01:15:50.190 --> 01:16:03.230
So, for example, if H
were this graph over here,

01:16:03.230 --> 01:16:07.590
so then you would find a copy of
H in G, which is what we want.

01:16:07.590 --> 01:16:09.090
So why does the
counting lemma allow

01:16:09.090 --> 01:16:12.000
you to do this amplification?

01:16:12.000 --> 01:16:15.500
So it's this point, the
ideas are all there,

01:16:15.500 --> 01:16:18.360
but there's a slight
wrinkle in the calculations.

01:16:18.360 --> 01:16:20.220
I mean, there's,
in the executions,

01:16:20.220 --> 01:16:23.070
I just want to point
out, just in case

01:16:23.070 --> 01:16:26.610
some of the vertices of H end
up in the same vertex in G.

01:16:26.610 --> 01:16:28.780
But that turns out
not to be an issue.

01:16:28.780 --> 01:16:38.020
So by counting lemma, the
number of homomorphisms

01:16:38.020 --> 01:16:43.480
from H to G prime, where
I'm really only considering

01:16:43.480 --> 01:16:47.200
homomorphisms that
map each vertex of H

01:16:47.200 --> 01:16:51.380
to its assigned part.

01:16:51.380 --> 01:16:57.920
It's at least this
quantity where

01:16:57.920 --> 01:17:04.130
I'm looking at the predicted
density of such homomorphisms,

01:17:04.130 --> 01:17:08.480
and all of these edge densities
are at least epsilon over 8.

01:17:08.480 --> 01:17:13.820
So it's at least that amount
minus a small error that

01:17:13.820 --> 01:17:15.110
comes from the counting lemma.

01:17:19.720 --> 01:17:23.720
And, well, all of
the vertex parts

01:17:23.720 --> 01:17:27.875
are quite large, so
all of the vertex parts

01:17:27.875 --> 01:17:35.000
are of size like that.

01:17:38.055 --> 01:17:40.180
So that's the result of
the counting lemma combined

01:17:40.180 --> 01:17:43.030
with information about
the densities of the parts

01:17:43.030 --> 01:17:48.210
and the sizes of the parts
that came out of cleaning.

01:17:48.210 --> 01:18:00.240
So setting eta to be
an appropriate value,

01:18:00.240 --> 01:18:08.660
we see that for
sufficiently large N,

01:18:08.660 --> 01:18:14.400
this quantity here,
is on the order of N

01:18:14.400 --> 01:18:19.590
to the number of vertices
of H. But I'm only

01:18:19.590 --> 01:18:21.780
counting homomorphisms,
and so it

01:18:21.780 --> 01:18:24.720
could be that some
of the vertices of H

01:18:24.720 --> 01:18:29.130
end up in the same vertex
of G. And those would not

01:18:29.130 --> 01:18:31.110
be genuine subgraphs,
so I shouldn't

01:18:31.110 --> 01:18:32.820
consider those as subgraphs.

01:18:32.820 --> 01:18:35.110
Because otherwise, if
you were to allow those,

01:18:35.110 --> 01:18:38.550
then if you found
this K4, then you

01:18:38.550 --> 01:18:41.040
found all four chromatic graphs.

01:18:41.040 --> 01:18:44.400
So you shouldn't consider
copies that are degenerate,

01:18:44.400 --> 01:18:50.080
but that's OK because the number
of maps from the vertex set

01:18:50.080 --> 01:18:55.290
of H to the vertex set of
G that are non-injective

01:18:55.290 --> 01:19:00.420
is of a lower order.

01:19:00.420 --> 01:19:02.833
The number of non-injective
maps from the vertex set,

01:19:02.833 --> 01:19:04.500
well, you have to
pick two vertices of H

01:19:04.500 --> 01:19:07.050
to map to the same vertex, and
then the number of choices,

01:19:07.050 --> 01:19:10.750
you have one order less.

01:19:10.750 --> 01:19:14.307
So there are negligible
fraction of these homomorphisms.

01:19:16.990 --> 01:19:19.310
And the conclusion,
then, is that G prime

01:19:19.310 --> 01:19:27.050
contains a copy of H, which
is what we're looking for.

01:19:27.050 --> 01:19:29.850
If G prime contains copy of H,
then G contains a copy of H,

01:19:29.850 --> 01:19:32.460
and that proves the
Erdos-Stone-Simonovits theorem.

01:19:35.340 --> 01:19:37.950
You get a bit more
out of this proof.

01:19:37.950 --> 01:19:44.780
So you see that not only
does G contain one copy of H,

01:19:44.780 --> 01:19:46.700
but the counting lemma
actually shows you

01:19:46.700 --> 01:19:50.120
it contains many copies of H.

01:19:50.120 --> 01:19:53.360
And this is a phenomenon known
as supersaturation, which

01:19:53.360 --> 01:19:55.340
you already saw in
the first problem set,

01:19:55.340 --> 01:19:59.360
that often when you are
beyond a certain threshold,

01:19:59.360 --> 01:20:04.160
an extremal threshold, you
don't just gain one extra copy,

01:20:04.160 --> 01:20:07.570
but you often gain many copies.

01:20:07.570 --> 01:20:11.790
And you see this
in this proof here.

01:20:11.790 --> 01:20:15.990
So to summarize,
we've seen this proof

01:20:15.990 --> 01:20:19.230
of Erdos-Stone-Simonovits,
which comes

01:20:19.230 --> 01:20:22.470
from applying regularity
and then finding

01:20:22.470 --> 01:20:25.980
a single copy of a clique
from Turan's theorem,

01:20:25.980 --> 01:20:27.840
and then using
counting lemma to boost

01:20:27.840 --> 01:20:35.030
that copy from Turan's theorem
into an actual copy of H.

01:20:35.030 --> 01:20:38.220
So in the second homework,
one of the problems

01:20:38.220 --> 01:20:41.790
is to come up with a different
proof of Erdos-Stone-Simonovits

01:20:41.790 --> 01:20:46.920
that is more similar to the
proof of Kovari-Sos-Turan,

01:20:46.920 --> 01:20:49.400
more through double-counting
like arguments.

01:20:49.400 --> 01:20:51.150
And that is closer in
spirit, although not

01:20:51.150 --> 01:20:56.610
exactly the same as the
original proof in Erdos-Stone.

01:20:56.610 --> 01:21:00.290
So this regularity proof, I
think it's more conceptual.

01:21:00.290 --> 01:21:02.540
You get to see how
to do this boosting,

01:21:02.540 --> 01:21:04.250
but it gets a terrible bound.

01:21:04.250 --> 01:21:06.350
And the other proof that
you see in the homework

01:21:06.350 --> 01:21:08.270
gives you a much
more reasonable bound

01:21:08.270 --> 01:21:13.070
on the dependence
between how N grows

01:21:13.070 --> 01:21:17.710
versus how quickly this
little o has to go to zero.