WEBVTT

00:00:00.000 --> 00:00:00.974
[SQUEAKING]

00:00:02.435 --> 00:00:04.383
[RUSTLING]

00:00:04.383 --> 00:00:09.750
[CLICKING]

00:00:09.750 --> 00:00:13.060
FRANK SCHILLBACH Let me
get started on lecture 20.

00:00:13.060 --> 00:00:15.060
So I'm going to finish
up with what we discussed

00:00:15.060 --> 00:00:16.653
last time, which is lecture 19.

00:00:16.653 --> 00:00:18.570
And then we're going to
talk about lecture 20,

00:00:18.570 --> 00:00:23.687
which is about malleability and
inaccessibility of preferences.

00:00:23.687 --> 00:00:25.770
For now, we're going to
talk for a little bit more

00:00:25.770 --> 00:00:30.290
about defaults and frames,
and nudges in particular.

00:00:30.290 --> 00:00:33.030
So what we left it
off last time was we

00:00:33.030 --> 00:00:34.673
talked about default effects.

00:00:34.673 --> 00:00:36.090
We talked about
that in particular

00:00:36.090 --> 00:00:37.260
in retirement savings.

00:00:37.260 --> 00:00:39.860
Defaults can be sort
of setting essentially

00:00:39.860 --> 00:00:41.790
in one default option.

00:00:41.790 --> 00:00:45.540
That is like, what happens if
you do nothing in a retirement

00:00:45.540 --> 00:00:47.190
savings account.

00:00:47.190 --> 00:00:49.050
Setting a certain
default option can

00:00:49.050 --> 00:00:53.430
have very powerful effects in
affecting people's behavior

00:00:53.430 --> 00:00:55.920
in a domain that's really
important in this case,

00:00:55.920 --> 00:01:01.020
savings, and in a domain where
traditional economic tools have

00:01:01.020 --> 00:01:05.519
not really gone very far, as
in matching contributions,

00:01:05.519 --> 00:01:09.420
or any sort of like types of
financial education, et cetera.

00:01:09.420 --> 00:01:10.920
It's just not doing very much.

00:01:10.920 --> 00:01:12.000
It's expensive to do.

00:01:12.000 --> 00:01:16.110
And it's hard to change
behavior, while using defaults

00:01:16.110 --> 00:01:20.070
is really powerful, and can
cause large behavior change

00:01:20.070 --> 00:01:25.550
or large changes in
outcomes even years later.

00:01:25.550 --> 00:01:28.430
Let me talk a little bit about
what is the optimal default

00:01:28.430 --> 00:01:29.750
decision regime.

00:01:29.750 --> 00:01:34.040
And that's getting
into tricky territory.

00:01:34.040 --> 00:01:36.380
We talked about this
last time a little bit.

00:01:36.380 --> 00:01:38.810
We'll talk about this
again in the last lecture,

00:01:38.810 --> 00:01:41.880
on lecture 23, when
we talk about policy.

00:01:41.880 --> 00:01:44.430
So when do you think about
active choice versus default,

00:01:44.430 --> 00:01:45.680
so what is active choice?

00:01:45.680 --> 00:01:48.440
Active choice to remind you
is like somebody has asked

00:01:48.440 --> 00:01:50.600
at the beginning,
or directly asked,

00:01:50.600 --> 00:01:51.770
you have to make a choice.

00:01:51.770 --> 00:01:55.410
And I'm just sort of forcing
you to do it, more or less.

00:01:55.410 --> 00:01:58.040
[INAUDIBLE] the company hires
somebody and in the hiring

00:01:58.040 --> 00:02:00.860
package, there is just a form
that you need to fill out.

00:02:00.860 --> 00:02:03.483
And otherwise you cannot really
start [INAUDIBLE] company.

00:02:03.483 --> 00:02:05.150
Sometimes that's not
really enforceable,

00:02:05.150 --> 00:02:06.942
but effectively,
essentially just everybody

00:02:06.942 --> 00:02:08.206
makes some choice.

00:02:08.206 --> 00:02:10.789
That's essentially active choice
versus default, which is just

00:02:10.789 --> 00:02:12.247
like, if you do
nothing, you either

00:02:12.247 --> 00:02:15.870
get zero retirement savings,
or some positive amounts.

00:02:15.870 --> 00:02:19.008
Now, if people are really
different, then essentially,

00:02:19.008 --> 00:02:20.300
active choice is a great thing.

00:02:20.300 --> 00:02:22.525
Because if people really
have different preferences,

00:02:22.525 --> 00:02:24.650
then we shouldn't just push
them into one direction

00:02:24.650 --> 00:02:25.250
or the other.

00:02:25.250 --> 00:02:26.090
They should choose.

00:02:26.090 --> 00:02:29.090
Everybody chooses on their
own what's best for them.

00:02:29.090 --> 00:02:32.390
In contrast, if you think most
people are the same anyway,

00:02:32.390 --> 00:02:33.890
and in addition,
people don't really

00:02:33.890 --> 00:02:35.720
know what's best
for them, well then

00:02:35.720 --> 00:02:39.050
it might be better to just
provide one default [INAUDIBLE]

00:02:39.050 --> 00:02:40.930
most people that's
best for most people.

00:02:40.930 --> 00:02:42.680
And then most people
will stick to anyway,

00:02:42.680 --> 00:02:44.990
because these defaults
tend to stick.

00:02:44.990 --> 00:02:46.880
And if people don't
know what they want

00:02:46.880 --> 00:02:48.020
or what's good for
them, then sort

00:02:48.020 --> 00:02:50.062
of letting them choose
actively might make things

00:02:50.062 --> 00:02:53.240
potentially worse anyway.

00:02:53.240 --> 00:02:55.422
And so that's sort of
the fundamental trade

00:02:55.422 --> 00:02:57.908
off when people have come down
on different sides of this.

00:02:57.908 --> 00:02:59.450
And it depends on
your view, kind of,

00:02:59.450 --> 00:03:01.880
about sophistication
and so on of customers

00:03:01.880 --> 00:03:04.010
who are [INAUDIBLE].

00:03:04.010 --> 00:03:07.460
And one big and important
issue with defaults

00:03:07.460 --> 00:03:09.460
and with the nudges,
in general, which

00:03:09.460 --> 00:03:11.630
I'm going to talk about
to you in a second

00:03:11.630 --> 00:03:14.580
is, we kind of want to make
sure that we don't make people

00:03:14.580 --> 00:03:16.860
worse off in some
ways by pushing people

00:03:16.860 --> 00:03:17.970
in certain directions.

00:03:17.970 --> 00:03:19.470
For example, one
thing you might say

00:03:19.470 --> 00:03:22.380
is, well, why not set the
default to like 5% or 10%

00:03:22.380 --> 00:03:24.060
of retirement savings?

00:03:24.060 --> 00:03:27.090
But then you worry about
people potentially over-saving

00:03:27.090 --> 00:03:28.777
in their retirement
savings accounts.

00:03:28.777 --> 00:03:31.110
And then essentially they
have lots of credit card debt,

00:03:31.110 --> 00:03:33.960
potentially, which
might be even larger.

00:03:33.960 --> 00:03:36.163
Sorry, the interest rate
on the credit card debt

00:03:36.163 --> 00:03:37.080
might be really large.

00:03:37.080 --> 00:03:38.163
And people get in trouble.

00:03:38.163 --> 00:03:40.740
Or people then withdraw
prematurely, and so on,

00:03:40.740 --> 00:03:43.830
which is [INAUDIBLE]
potentially worse for them.

00:03:43.830 --> 00:03:46.050
We talked very briefly
about the smart plan,

00:03:46.050 --> 00:03:50.240
which again, I'm going to get
back to in lecture 23 as well.

00:03:50.240 --> 00:03:57.170
Now I also talked briefly
about sort of default effects

00:03:57.170 --> 00:03:59.210
also are quite powerful
in other settings.

00:03:59.210 --> 00:04:01.940
For example, in organ
donations, in some places,

00:04:01.940 --> 00:04:04.830
organ donations are
explicit consent,

00:04:04.830 --> 00:04:06.500
which is like you
have to opt in,

00:04:06.500 --> 00:04:08.660
and otherwise you will
not be an organ donor.

00:04:08.660 --> 00:04:11.060
In other countries-- and
this is from a while ago--

00:04:11.060 --> 00:04:13.813
in other countries,
things are different,

00:04:13.813 --> 00:04:15.980
where there is what's called
presumed consent, which

00:04:15.980 --> 00:04:17.360
is like you have to opt out.

00:04:17.360 --> 00:04:19.160
And otherwise, if
you do not opt out,

00:04:19.160 --> 00:04:21.800
you're presumed to consenting
to organ [INAUDIBLE]..

00:04:21.800 --> 00:04:24.050
We see essentially
huge differences

00:04:24.050 --> 00:04:27.020
in otherwise quite
similar countries,

00:04:27.020 --> 00:04:29.060
which sort of shows you
that the default here

00:04:29.060 --> 00:04:31.370
can have really large effects.

00:04:31.370 --> 00:04:34.070
There's some other examples of
default effects in particular

00:04:34.070 --> 00:04:35.840
when it comes to
voter registration,

00:04:35.840 --> 00:04:37.760
or green energy choices.

00:04:37.760 --> 00:04:39.350
In some cases, in
particular when

00:04:39.350 --> 00:04:41.930
it comes to voter registration,
it's actually quite hard

00:04:41.930 --> 00:04:43.700
to come with arguments
against setting

00:04:43.700 --> 00:04:45.050
a default in a certain way.

00:04:45.050 --> 00:04:47.360
Like in some sense,
getting voters

00:04:47.360 --> 00:04:49.840
to be registered
when you get say

00:04:49.840 --> 00:04:52.550
a driver's license or
the like seems just very

00:04:52.550 --> 00:04:54.400
straightforward and--

00:04:54.400 --> 00:04:57.020
or when they turn 18, that seems
like a very straightforward

00:04:57.020 --> 00:04:58.070
thing to do.

00:04:58.070 --> 00:05:00.950
And there, in some
ways, the defaults

00:05:00.950 --> 00:05:05.210
can really sort of remove some
barriers and inefficiencies

00:05:05.210 --> 00:05:09.950
where we think we should
agree that everybody should

00:05:09.950 --> 00:05:14.270
be able to vote if they would
like to, with some exceptions,

00:05:14.270 --> 00:05:14.990
perhaps.

00:05:14.990 --> 00:05:17.090
And so, if you
can default people

00:05:17.090 --> 00:05:19.730
into getting registered to vote,
that seems like a good thing,

00:05:19.730 --> 00:05:20.690
unambiguously.

00:05:20.690 --> 00:05:24.680
So there sort of default effects
can be really, really powerful.

00:05:24.680 --> 00:05:28.180
Now, let's talk a little
bit about-- not just again,

00:05:28.180 --> 00:05:29.680
we're going to talk
about this again

00:05:29.680 --> 00:05:31.702
in lecture 23, when it
comes, about policy,

00:05:31.702 --> 00:05:33.160
but let me sort of
define the issue

00:05:33.160 --> 00:05:35.210
and sort of be clear about that.

00:05:35.210 --> 00:05:37.960
So you might have sort of come
across the very famous book

00:05:37.960 --> 00:05:40.720
by Richard Thaler,
who recently won

00:05:40.720 --> 00:05:44.350
the Nobel Prize in
economics, and Cass Sunstein,

00:05:44.350 --> 00:05:46.380
called Nudge.

00:05:46.380 --> 00:05:51.030
I'll give you the definition
by Cass Sunstein, who

00:05:51.030 --> 00:05:54.720
says a nudge is a feature of the
social environment that affects

00:05:54.720 --> 00:05:57.270
people's choices without
imposing coercion

00:05:57.270 --> 00:06:01.200
of any kind of
material incentive.

00:06:01.200 --> 00:06:03.912
So that-- default is
sort of like a clear case

00:06:03.912 --> 00:06:05.370
of that, where
essentially you just

00:06:05.370 --> 00:06:07.260
say, OK, if you
don't do anything,

00:06:07.260 --> 00:06:10.020
I'm going to just
pick a choice for you.

00:06:10.020 --> 00:06:13.050
But you can choose, however,
in whichever way you want.

00:06:13.050 --> 00:06:16.540
And there is essentially
no material incentive.

00:06:16.540 --> 00:06:19.272
But I'm changing essentially
your choice environment

00:06:19.272 --> 00:06:20.730
or what people
would call sometimes

00:06:20.730 --> 00:06:22.980
the choice architecture.

00:06:22.980 --> 00:06:24.655
And there's other
versions of that.

00:06:24.655 --> 00:06:28.620
There's simplification,
information disclosure.

00:06:28.620 --> 00:06:31.110
Again, these are all things
that are usually available.

00:06:31.110 --> 00:06:33.360
People can figure
it out on their own.

00:06:33.360 --> 00:06:36.250
But we make it
easier for people.

00:06:36.250 --> 00:06:39.930
And one of the big
mantras [INAUDIBLE]

00:06:39.930 --> 00:06:43.290
about like, what's the
main thing we should do

00:06:43.290 --> 00:06:45.600
to improve people's behavior?

00:06:45.600 --> 00:06:47.850
Dick Thaler would often say,
make it easy for people.

00:06:47.850 --> 00:06:50.100
Essentially, people are often--

00:06:50.100 --> 00:06:52.770
and he said that about
himself-- people are lazy.

00:06:52.770 --> 00:06:54.600
And they make simple choices.

00:06:54.600 --> 00:06:57.550
And if you make it easy for
them to make the right choice,

00:06:57.550 --> 00:07:01.620
they will be more likely to
make good choices for them.

00:07:01.620 --> 00:07:05.040
There's some other things
like warnings, reminders.

00:07:05.040 --> 00:07:07.370
Notice for reminders, for
example, again, you already

00:07:07.370 --> 00:07:09.120
have the information
that somebody gave it

00:07:09.120 --> 00:07:10.480
to you previously.

00:07:10.480 --> 00:07:15.270
So really it's only like
if you have memory issues,

00:07:15.270 --> 00:07:16.680
that might be useful for you.

00:07:16.680 --> 00:07:19.590
There's no material
incentives or [INAUDIBLE]..

00:07:19.590 --> 00:07:20.910
Use of social norms.

00:07:20.910 --> 00:07:24.480
That would be things like I'm
telling you all of your friends

00:07:24.480 --> 00:07:25.110
did x.

00:07:25.110 --> 00:07:27.450
Now would you like to do x or y?

00:07:27.450 --> 00:07:31.110
Or I can send you letters
and say your neighbors used

00:07:31.110 --> 00:07:34.140
so much energy, and you use
more energy than your neighbor.

00:07:34.140 --> 00:07:37.350
And then I put like a sad
face next to it, or something.

00:07:37.350 --> 00:07:40.860
And then you might feel
bad about it because

00:07:40.860 --> 00:07:44.950
of social norms, and sort
of reduce your energy usage,

00:07:44.950 --> 00:07:47.860
which people have in fact done.

00:07:47.860 --> 00:07:50.190
You can also like increase
ease and convenience.

00:07:50.190 --> 00:07:53.550
This is exactly what like
[INAUDIBLE] convenience.

00:07:53.550 --> 00:07:55.740
That's exactly what
Dick Thaler would say.

00:07:55.740 --> 00:07:58.560
Those are things
like, for example, I

00:07:58.560 --> 00:08:04.560
can put Snickers bars next to
the counter at the cashier.

00:08:04.560 --> 00:08:07.300
Or I can put apples next to it.

00:08:07.300 --> 00:08:09.853
And people are more
likely to buy apples

00:08:09.853 --> 00:08:11.520
when it's easy or
simple or when they're

00:08:11.520 --> 00:08:13.320
reminded of eating apples.

00:08:13.320 --> 00:08:16.410
And they're more likely to
eat Snickers bars essentially

00:08:16.410 --> 00:08:18.480
if [INAUDIBLE] to them.

00:08:18.480 --> 00:08:20.160
To the extent that
you want people

00:08:20.160 --> 00:08:24.270
to make what one might
think of as good choices,

00:08:24.270 --> 00:08:27.060
making it easy for them to make
the good choice without sort

00:08:27.060 --> 00:08:29.700
of restricting
anybody's choice sets

00:08:29.700 --> 00:08:33.750
or coercing them in any
way is considered a nudge.

00:08:33.750 --> 00:08:37.320
And then there's also some
things about framing of choices

00:08:37.320 --> 00:08:42.260
as in gains versus losses,
in terms of their contracts.

00:08:42.260 --> 00:08:43.400
Any questions on this?

00:08:49.920 --> 00:08:50.420
OK.

00:08:50.420 --> 00:08:52.513
So then let me give
you, briefly, sort of--

00:08:52.513 --> 00:08:53.930
and again, we're
going to get back

00:08:53.930 --> 00:08:55.202
to all of this in lecture 23.

00:08:55.202 --> 00:08:56.660
But let me briefly
sort of give you

00:08:56.660 --> 00:08:59.140
a couple of examples
of nudges in some cases

00:08:59.140 --> 00:09:03.820
where it's pretty clear that
some nudges are a good idea.

00:09:03.820 --> 00:09:05.820
So one thing that we
talked about quite

00:09:05.820 --> 00:09:07.800
a bit already when it
comes to procrastination

00:09:07.800 --> 00:09:12.990
or self-control problems present
bias, or [? losing focus ?]

00:09:12.990 --> 00:09:17.010
is the health domain, and
what people and in particular,

00:09:17.010 --> 00:09:20.200
individuals and society
often have aligned goals,

00:09:20.200 --> 00:09:23.015
where individuals often
want behavioral change.

00:09:23.015 --> 00:09:24.390
They want to
improve their diets,

00:09:24.390 --> 00:09:27.480
increase physical activity,
stop smoking, get vaccinated,

00:09:27.480 --> 00:09:29.070
use less energy, and so on.

00:09:29.070 --> 00:09:33.190
And often, there are societal
costs of these behaviors,

00:09:33.190 --> 00:09:35.430
or lack of those behaviors.

00:09:35.430 --> 00:09:39.870
Sometimes there's externalities
from smoking or getting or not

00:09:39.870 --> 00:09:41.250
getting vaccinated.

00:09:41.250 --> 00:09:43.590
Sometimes [INAUDIBLE]
health care costs and the

00:09:43.590 --> 00:09:47.640
like from obesity and so on,
where it's bad for everybody

00:09:47.640 --> 00:09:51.120
if the population is sick.

00:09:51.120 --> 00:09:53.340
So these goals are now aligned.

00:09:53.340 --> 00:09:54.840
And the social
planner might want

00:09:54.840 --> 00:09:59.100
people to improve their diet, or
the government might want that,

00:09:59.100 --> 00:10:02.040
or might want to reduce
smoking and so on.

00:10:02.040 --> 00:10:04.680
But individuals, as
we saw previously,

00:10:04.680 --> 00:10:06.750
often fail to follow through.

00:10:06.750 --> 00:10:10.200
And sort of often, education
and information interventions

00:10:10.200 --> 00:10:12.820
are often ineffective.

00:10:12.820 --> 00:10:15.030
Even price interventions
are often ineffective,

00:10:15.030 --> 00:10:17.900
or in the sense of
like increasing prices,

00:10:17.900 --> 00:10:19.900
or incentivizing people,
it doesn't always work.

00:10:19.900 --> 00:10:21.555
Plus, it's quite
expensive to do.

00:10:21.555 --> 00:10:23.430
So one natural question
you might want to ask

00:10:23.430 --> 00:10:29.070
is, can we use some nudges to
align intentions and actions?

00:10:29.070 --> 00:10:31.750
And the reason why this is
really popular, in particular,

00:10:31.750 --> 00:10:35.220
among governments, is that
it's really cheap to do this.

00:10:35.220 --> 00:10:37.710
So it's actually sending
some reminders or the like

00:10:37.710 --> 00:10:40.050
or providing information
in one way or the other,

00:10:40.050 --> 00:10:41.610
is essentially free.

00:10:41.610 --> 00:10:45.240
And if governments don't
have a lot of money,

00:10:45.240 --> 00:10:46.860
that's an easy thing
to do as opposed

00:10:46.860 --> 00:10:51.850
to paying people, which is
quite expensive in many cases.

00:10:51.850 --> 00:10:54.480
So here's an example of a
free intervention which is

00:10:54.480 --> 00:10:56.340
about flu shot communications.

00:10:56.340 --> 00:11:00.270
This is a study by Katy Milkman
and co-authors from 2011.

00:11:00.270 --> 00:11:03.870
So here, the goal
of the intervention

00:11:03.870 --> 00:11:08.220
is to get people to sign up,
or to actually get flu shots.

00:11:08.220 --> 00:11:10.740
So the control group got
an informational mailing.

00:11:10.740 --> 00:11:12.090
I'll show you this in a second.

00:11:12.090 --> 00:11:17.000
The treatment group got the
same, plus an encouragement

00:11:17.000 --> 00:11:18.840
to make a date plan.

00:11:18.840 --> 00:11:23.550
And the treatment group got
the same plus make a date plan,

00:11:23.550 --> 00:11:24.420
and a time plan.

00:11:24.420 --> 00:11:25.987
Let me show you what
this looks like.

00:11:25.987 --> 00:11:27.320
So here's the control condition.

00:11:27.320 --> 00:11:30.368
This is just an
informational mailing.

00:11:30.368 --> 00:11:32.448
It's saying here, the company--

00:11:32.448 --> 00:11:33.990
this is a company
trying to do this--

00:11:33.990 --> 00:11:37.350
is holding a free
flu shot clinic.

00:11:37.350 --> 00:11:40.290
And companies very much want
their workers to get flu shots,

00:11:40.290 --> 00:11:44.070
because that reduces
essentially sort of sick days.

00:11:44.070 --> 00:11:45.690
If everybody gets
their flu shot,

00:11:45.690 --> 00:11:48.190
not only are they healthier and
it's probably good for them.

00:11:48.190 --> 00:11:51.240
They're like less at risk of
dying and infecting each other,

00:11:51.240 --> 00:11:52.692
or just being sick.

00:11:52.692 --> 00:11:54.150
But it's also good
for the company,

00:11:54.150 --> 00:12:00.150
because the company now gets
workers to actually show up

00:12:00.150 --> 00:12:02.533
more, and have fewer sick days.

00:12:02.533 --> 00:12:04.200
So here's sort of the
control condition.

00:12:04.200 --> 00:12:08.510
It looks like a perfectly
reasonable letter,

00:12:08.510 --> 00:12:12.090
where you say, here is these
days, on which you can sign up.

00:12:12.090 --> 00:12:14.820
And you are informed
about the dates and times

00:12:14.820 --> 00:12:17.520
of the workplace flu clinic.

00:12:17.520 --> 00:12:21.210
Now, the treatment condition
is the date plan condition,

00:12:21.210 --> 00:12:24.360
which essentially is
asking people to--

00:12:24.360 --> 00:12:27.660
or invite people to
choose a concrete date

00:12:27.660 --> 00:12:30.000
for getting a flu vaccine.

00:12:30.000 --> 00:12:33.510
And the rest of the
information [? mailer ?]

00:12:33.510 --> 00:12:34.840
is exactly the same.

00:12:34.840 --> 00:12:37.500
So essentially, that's
just asking people,

00:12:37.500 --> 00:12:43.200
pick a day of the week and a
month, or a month and a day,

00:12:43.200 --> 00:12:45.100
and a day of week, and
just write it down.

00:12:45.100 --> 00:12:47.100
And presumably, that also
then encourages people

00:12:47.100 --> 00:12:50.010
to put it into their
calendar or the like

00:12:50.010 --> 00:12:53.070
and sort of acts as
some form of a reminder.

00:12:53.070 --> 00:12:54.527
Notice, there's
no incentive here.

00:12:54.527 --> 00:12:55.860
There's no financial incentives.

00:12:55.860 --> 00:12:56.910
There's no coercion and so on.

00:12:56.910 --> 00:12:59.430
It's just saying why don't
you just pick a day right now.

00:12:59.430 --> 00:13:02.220
Just have a look, check your
calendar, and just figure out.

00:13:02.220 --> 00:13:04.250
Make a plan for this.

00:13:04.250 --> 00:13:08.872
And the second condition is
then make a date and a time plan

00:13:08.872 --> 00:13:10.580
condition, which is
to say, why don't you

00:13:10.580 --> 00:13:15.030
also pick a time, which
is even more concrete.

00:13:15.030 --> 00:13:19.250
That would be Monday,
October 26, at like 9:00 AM

00:13:19.250 --> 00:13:20.590
or whenever I'm going to go.

00:13:20.590 --> 00:13:22.215
Presumably, that also
encourages people

00:13:22.215 --> 00:13:24.240
to put it into their calendar.

00:13:24.240 --> 00:13:26.570
Now what you then get
in this experiment--

00:13:26.570 --> 00:13:28.430
it is a very simple experiment.

00:13:28.430 --> 00:13:30.230
You get, when you
just send a letter,

00:13:30.230 --> 00:13:32.630
33% actually get the flu shot.

00:13:32.630 --> 00:13:36.080
When you also send
a date plan, 34%.

00:13:36.080 --> 00:13:39.122
And when you send a date plan
and a time plan, it's 37%.

00:13:39.122 --> 00:13:40.580
Now, these effects
are a little bit

00:13:40.580 --> 00:13:42.950
underwhelming in the sense
they're kind of small.

00:13:42.950 --> 00:13:48.470
It's like 1.6 percentage points
at 4.2 percentage points.

00:13:48.470 --> 00:13:51.800
And in relative terms
that's not huge effect.

00:13:51.800 --> 00:13:54.050
But notice what's
beautiful about here

00:13:54.050 --> 00:13:57.350
is that the costs
are essentially zero.

00:13:57.350 --> 00:14:00.580
So in some sense, if you value
additional flu shot adherence,

00:14:00.580 --> 00:14:06.770
if you value more people
getting flu shots, then a 4.2

00:14:06.770 --> 00:14:09.230
percentage points increase,
which is about like a 10%

00:14:09.230 --> 00:14:11.450
or whatever, 11%,
12% or something

00:14:11.450 --> 00:14:13.880
increase relative to the
control group, that's

00:14:13.880 --> 00:14:16.460
quite a bit of an
increase for something

00:14:16.460 --> 00:14:18.000
that's essentially free.

00:14:18.000 --> 00:14:20.310
So the cost effectiveness
of this is, of course, very,

00:14:20.310 --> 00:14:20.900
very high.

00:14:20.900 --> 00:14:22.525
Because it's essentially
free, or maybe

00:14:22.525 --> 00:14:25.190
it cost somebody a few
minutes to sort of write this

00:14:25.190 --> 00:14:28.790
down, or design the sheet.

00:14:28.790 --> 00:14:31.220
But really it costs
exactly the same

00:14:31.220 --> 00:14:33.500
to send the sheet
here on the left

00:14:33.500 --> 00:14:35.580
versus the sheet
here on the right.

00:14:35.580 --> 00:14:39.410
So that's kind of an example
of a win-win kind of situation

00:14:39.410 --> 00:14:44.330
where we make things
essentially more effective.

00:14:44.330 --> 00:14:47.580
It's not really hurting anybody,
and it's cheap and free to do.

00:14:47.580 --> 00:14:48.980
And so everybody is happy.

00:14:48.980 --> 00:14:52.190
And it's sort of uncontroversial
in the sense of nobody

00:14:52.190 --> 00:14:53.840
would argue that
this is a bad thing.

00:14:53.840 --> 00:14:55.400
Of course, some
people might sort of

00:14:55.400 --> 00:14:57.990
have some concerns about
the flu shots themselves,

00:14:57.990 --> 00:15:00.710
which I don't think there's any
actual evidence against that

00:15:00.710 --> 00:15:01.548
as far as I know.

00:15:01.548 --> 00:15:03.590
But that's sort of an
example where we say, look,

00:15:03.590 --> 00:15:04.760
this is very cheap to do.

00:15:04.760 --> 00:15:08.457
So presumably you should sort
of pick the right mailing,

00:15:08.457 --> 00:15:10.790
or the right design, if I'm
mailing, to encourage people

00:15:10.790 --> 00:15:14.390
to do desirable things, at least
from the company's perspective,

00:15:14.390 --> 00:15:17.240
while also preserving
people's freedom,

00:15:17.240 --> 00:15:21.530
not spending any money, but
also not encouraging anybody.

00:15:21.530 --> 00:15:22.640
Any questions on this?

00:15:28.220 --> 00:15:28.900
OK.

00:15:28.900 --> 00:15:31.690
So here's another example
of a similar intervention,

00:15:31.690 --> 00:15:34.780
which is signing up for FAFSA.

00:15:34.780 --> 00:15:36.250
This is a paper by
Bettinger et al.

00:15:36.250 --> 00:15:41.950
in 2009, where essentially,
people were provided

00:15:41.950 --> 00:15:44.950
free additional assistance
in completing and filing

00:15:44.950 --> 00:15:47.680
applications for
college financial aid.

00:15:47.680 --> 00:15:51.220
This is FAFSA, the
financial aid for college.

00:15:51.220 --> 00:15:55.660
And it really increased
college enrollment--

00:15:55.660 --> 00:15:59.020
not only FAFSA completion,
but also college enrollment.

00:15:59.020 --> 00:16:01.940
And there's sort of
different conditions.

00:16:01.940 --> 00:16:04.490
One is control versus just
providing people information

00:16:04.490 --> 00:16:04.990
only.

00:16:04.990 --> 00:16:06.990
Here's [INAUDIBLE]
information of like you could

00:16:06.990 --> 00:16:08.703
get some FAFSA help and so on.

00:16:08.703 --> 00:16:10.120
But it seems to
be really what you

00:16:10.120 --> 00:16:15.850
need is some additional
assistance of completing

00:16:15.850 --> 00:16:17.440
and filing this application.

00:16:17.440 --> 00:16:22.240
Just somebody essentially
walking you through the filing

00:16:22.240 --> 00:16:23.730
has a huge impact.

00:16:23.730 --> 00:16:26.230
There's a bit of a question of,
is that really just a nudge,

00:16:26.230 --> 00:16:28.060
or is it like a more
powerful intervention.

00:16:28.060 --> 00:16:29.650
But it's a really
minor intervention

00:16:29.650 --> 00:16:31.570
where some people just
need some level of help

00:16:31.570 --> 00:16:33.670
in filling out this form.

00:16:33.670 --> 00:16:35.530
And it has pretty
large effects, if you

00:16:35.530 --> 00:16:37.690
look at the FAFSA
[INAUDIBLE] completion

00:16:37.690 --> 00:16:41.800
of these forms, reasonably large
effect on college enrollment.

00:16:41.800 --> 00:16:44.470
And you might sort of think this
is a hugely important choice,

00:16:44.470 --> 00:16:49.840
and sort of if you get people
to make that really big change,

00:16:49.840 --> 00:16:53.420
that's pretty remarkable for
doing some very simple things.

00:16:53.420 --> 00:16:57.340
[INAUDIBLE] help me complete
and fill out a simple form,

00:16:57.340 --> 00:17:00.190
something that you
could have done anyway.

00:17:00.190 --> 00:17:04.750
So, and here again, so this is
a relatively cheap thing to do.

00:17:04.750 --> 00:17:07.089
And it's essentially
equivalent to the impact

00:17:07.089 --> 00:17:10.569
of several thousands of
dollars of education subsidy.

00:17:10.569 --> 00:17:14.180
So if you wanted to essentially
pay people to go to college,

00:17:14.180 --> 00:17:15.790
[INAUDIBLE]
subsidize that, which

00:17:15.790 --> 00:17:18.730
you might want to do for
other reasons anyway,

00:17:18.730 --> 00:17:22.780
one key issue with that is
that not only is it perhaps not

00:17:22.780 --> 00:17:24.970
particularly effective,
but also you're

00:17:24.970 --> 00:17:26.800
going to pay a lot of
for marginal people.

00:17:26.800 --> 00:17:28.270
You're going to pay a
lot of people who would

00:17:28.270 --> 00:17:29.530
have gone to college anyway.

00:17:29.530 --> 00:17:30.890
You're going to pay the subsidy.

00:17:30.890 --> 00:17:34.810
So if you have a low budget,
that's really tough to do.

00:17:34.810 --> 00:17:38.410
And sort of then
there's not a lot of--

00:17:38.410 --> 00:17:40.630
the cost effectiveness
tends to be very low.

00:17:40.630 --> 00:17:43.180
Because you have
essentially a low impact,

00:17:43.180 --> 00:17:45.580
but like really high costs.

00:17:45.580 --> 00:17:48.220
And you can read about this
more in the link that I had.

00:17:48.220 --> 00:17:49.720
But essentially the
bottom line here

00:17:49.720 --> 00:17:52.210
is, again, you can do
very small changes that

00:17:52.210 --> 00:17:55.540
can have pretty large effects.

00:17:55.540 --> 00:17:57.970
There's other types of
examples that government use,

00:17:57.970 --> 00:17:59.440
or where governments use--

00:17:59.440 --> 00:18:02.380
not just-- another
example would be

00:18:02.380 --> 00:18:05.440
sending out letters for people
to pay their taxes, which

00:18:05.440 --> 00:18:07.960
essentially are like
reminders, or often appealing

00:18:07.960 --> 00:18:12.460
to people's sort of
prosociality in various ways.

00:18:12.460 --> 00:18:14.080
But the government
sends you letters.

00:18:14.080 --> 00:18:15.670
And there's different
ways in which

00:18:15.670 --> 00:18:17.290
you can write these letters.

00:18:17.290 --> 00:18:18.970
You could say you
should pay your taxes,

00:18:18.970 --> 00:18:20.980
or everybody's
paying their taxes,

00:18:20.980 --> 00:18:23.420
or good citizens pay
their taxes, and so on.

00:18:23.420 --> 00:18:25.900
And people-- there's a whole
industry of nudged units

00:18:25.900 --> 00:18:27.760
where people then try
to figure out what's

00:18:27.760 --> 00:18:29.810
the best way of doing that.

00:18:29.810 --> 00:18:32.170
And by doing that
in different ways,

00:18:32.170 --> 00:18:37.160
that can have a pretty large
effect on people's behaviors.

00:18:37.160 --> 00:18:39.650
And the governments sort
of love this, because it's

00:18:39.650 --> 00:18:40.940
a very cheap thing to do.

00:18:40.940 --> 00:18:43.910
And it can have large impact
and change behavior quite a bit.

00:18:43.910 --> 00:18:46.730
In this case, in the tax
case, increase revenue

00:18:46.730 --> 00:18:50.420
by quite a bit without costing
the government very much.

00:18:50.420 --> 00:18:52.760
Because often, [INAUDIBLE]
send some letters anyway.

00:18:52.760 --> 00:18:55.430
And they send some more in
some less effective ways

00:18:55.430 --> 00:18:56.760
of doing that.

00:18:56.760 --> 00:18:58.700
When you think about
the intervention

00:18:58.700 --> 00:19:00.710
that I have here on the
slide, the FAFSA one

00:19:00.710 --> 00:19:02.240
is essentially just--

00:19:02.240 --> 00:19:03.860
the effects almost
surely will be

00:19:03.860 --> 00:19:07.250
persistent in the sense of
like, this is a one shot choice,

00:19:07.250 --> 00:19:09.720
whether you send your
kid to college or not.

00:19:09.720 --> 00:19:13.625
And once you increase
the college enrollment,

00:19:13.625 --> 00:19:17.865
which might have some other
issues, but once you do that,

00:19:17.865 --> 00:19:19.490
there's going to be
persistent effects.

00:19:19.490 --> 00:19:21.770
Because once kids
go to college, that

00:19:21.770 --> 00:19:23.850
has effects on their
behaviors and so on.

00:19:23.850 --> 00:19:25.040
Now when you're saying
something else, which

00:19:25.040 --> 00:19:26.060
is some other
decisions, [INAUDIBLE]

00:19:26.060 --> 00:19:28.330
just like about paying
taxes where every year

00:19:28.330 --> 00:19:30.770
you get this letter from the
government that sort of tells

00:19:30.770 --> 00:19:33.590
you you should pay your taxes,
maybe the first time it works.

00:19:33.590 --> 00:19:34.840
Maybe the second time as well.

00:19:34.840 --> 00:19:37.130
But at some point,
you're like, whatever.

00:19:37.130 --> 00:19:38.610
I don't care.

00:19:38.610 --> 00:19:40.070
That's a great question.

00:19:40.070 --> 00:19:44.000
And I don't know whether we
have lots of evidence or not.

00:19:44.000 --> 00:19:46.568
I think the governments
would sort of--

00:19:46.568 --> 00:19:48.110
I think there's some
research on this

00:19:48.110 --> 00:19:51.610
that I just am not
sure I'm aware of.

00:19:51.610 --> 00:19:54.830
I think governments or people
would say, well, in some sense,

00:19:54.830 --> 00:19:57.150
we don't care that
much about it.

00:19:57.150 --> 00:19:58.300
Of course we care.

00:19:58.300 --> 00:20:01.317
But it doesn't change the
fact that in the first year--

00:20:01.317 --> 00:20:03.650
so we know that in the first
year, it works pretty well.

00:20:03.650 --> 00:20:05.858
And there's some things that
work better than others.

00:20:05.858 --> 00:20:08.470
Many governments send
some letters or some forms

00:20:08.470 --> 00:20:09.757
of mailers anyway.

00:20:09.757 --> 00:20:12.340
So what you want to do is kind
of like make sure you optimize,

00:20:12.340 --> 00:20:15.670
at least the first or second
time you contact people.

00:20:15.670 --> 00:20:18.100
And that might have
some persistent effects

00:20:18.100 --> 00:20:20.500
by themselves, even if
you don't re-contact them.

00:20:20.500 --> 00:20:22.300
Because once you start
paying your taxes,

00:20:22.300 --> 00:20:25.120
maybe then it becomes a habit,
and people do it anyway.

00:20:25.120 --> 00:20:26.950
But even if that's
not persistent,

00:20:26.950 --> 00:20:29.650
governments would say, well, if
that gets people to pay taxes

00:20:29.650 --> 00:20:32.890
more at least once or
twice, that's worth

00:20:32.890 --> 00:20:34.450
sort of the money spent.

00:20:34.450 --> 00:20:38.770
[INAUDIBLE] somebody essentially
needs to design this letters.

00:20:38.770 --> 00:20:40.810
There's like, templates
of what you can use.

00:20:40.810 --> 00:20:48.060
And that's essentially
very cheap to do

00:20:48.060 --> 00:20:53.400
and has pretty large effects
at least in the short run.

00:20:53.400 --> 00:20:56.490
I think [INAUDIBLE] correct,
which is sometimes it's like,

00:20:56.490 --> 00:20:57.450
some effects--

00:20:57.450 --> 00:21:00.120
it's just information versus
some things about making

00:21:00.120 --> 00:21:02.043
you feel bad, or social norms.

00:21:02.043 --> 00:21:04.710
I think the information things--
if you think people just forget

00:21:04.710 --> 00:21:07.200
stuff-- for example, if people
forget to get their flu shot

00:21:07.200 --> 00:21:09.492
and so on, you should probably
send reminders and so on

00:21:09.492 --> 00:21:11.670
and get them to do that.

00:21:11.670 --> 00:21:14.840
If there's sort of these social
norms, or sort of [INAUDIBLE]

00:21:14.840 --> 00:21:17.070
makes you feel bad in some
ways, maybe those things

00:21:17.070 --> 00:21:20.280
are maybe more likely to
potentially at least go away

00:21:20.280 --> 00:21:22.527
once you do it like 17
times, because at some point,

00:21:22.527 --> 00:21:25.110
people are like, yeah, whatever,
the same letter I got always.

00:21:25.110 --> 00:21:26.700
And I haven't paid
my taxes last time.

00:21:26.700 --> 00:21:29.090
I'm not going to
pay them now either.

00:21:29.090 --> 00:21:32.100
It's additional assistance
in completing and filing

00:21:32.100 --> 00:21:33.480
applications.

00:21:33.480 --> 00:21:34.740
So essentially, it's not--

00:21:34.740 --> 00:21:36.750
so the financial aid
is always available.

00:21:36.750 --> 00:21:39.420
It's just who takes
advantage of financial aid

00:21:39.420 --> 00:21:41.940
depends on how easy
you make it to people,

00:21:41.940 --> 00:21:43.710
and whether you
support them, provide

00:21:43.710 --> 00:21:47.310
some support in completing
and filing applications.

00:21:47.310 --> 00:21:50.550
There's also [INAUDIBLE] with
that where essentially, once

00:21:50.550 --> 00:21:52.440
you do your taxes,
essentially, it already

00:21:52.440 --> 00:21:56.620
prefills you the forms,
which is even easier to do.

00:21:56.620 --> 00:21:59.070
But essentially, this falls
under the category of making

00:21:59.070 --> 00:22:00.930
things easy for people.

00:22:00.930 --> 00:22:05.140
And that can sort of change
behaviors quite a bit.

00:22:05.140 --> 00:22:07.980
But the key part
in the nudges is--

00:22:07.980 --> 00:22:10.500
and let me get back
again to the definition--

00:22:10.500 --> 00:22:16.200
is that there's no
financial incentives.

00:22:16.200 --> 00:22:20.340
So this part here is without
imposing any coercion,

00:22:20.340 --> 00:22:24.210
or without paying any kind
of material incentives,

00:22:24.210 --> 00:22:26.242
or at least, only very,
very small incentives.

00:22:26.242 --> 00:22:28.200
There's a bit of a question
how to define that.

00:22:28.200 --> 00:22:31.800
But once I pay you $1,000 to
go to college or whatever,

00:22:31.800 --> 00:22:34.430
subsidize you, that's
not a nudge anymore.

00:22:34.430 --> 00:22:38.340
Or once I'm taking away certain
options from you, [INAUDIBLE]

00:22:38.340 --> 00:22:40.470
eliminating choices
from your choice set,

00:22:40.470 --> 00:22:41.743
again, that's not a nudge.

00:22:41.743 --> 00:22:42.660
That's something else.

00:22:42.660 --> 00:22:43.993
That's changing your choice set.

00:22:43.993 --> 00:22:45.720
This one is-- nudges
are explicitly

00:22:45.720 --> 00:22:51.600
about keeping people's choices
fully free and available,

00:22:51.600 --> 00:22:56.060
but making it easier for them
to make a certain choice.

00:22:56.060 --> 00:22:57.770
Presumably, a choice
that's desired

00:22:57.770 --> 00:22:59.420
in some ways by
the social planner,

00:22:59.420 --> 00:23:03.030
or about people's
plans for their future.

00:23:03.030 --> 00:23:07.640
So anyway, there's a large
sort of set of examples

00:23:07.640 --> 00:23:09.180
of different nudges and so on.

00:23:09.180 --> 00:23:11.000
And they can be
fairly effective.

00:23:11.000 --> 00:23:12.500
Of course, not for
everybody and not

00:23:12.500 --> 00:23:15.830
for the fraction of people
who are swayed by these nudges

00:23:15.830 --> 00:23:19.020
is not huge, in part because
if you think about it,

00:23:19.020 --> 00:23:21.542
essentially people need to
be marginal in some ways.

00:23:21.542 --> 00:23:23.000
If people just
really, really don't

00:23:23.000 --> 00:23:25.417
want to go to college because
it's too expensive for them,

00:23:25.417 --> 00:23:27.980
or whatever for other
reasons, or if people really

00:23:27.980 --> 00:23:30.240
want to go to college
anyway, the nudge

00:23:30.240 --> 00:23:31.580
will not change that behavior.

00:23:31.580 --> 00:23:33.330
Whether somebody fills
out the form or not

00:23:33.330 --> 00:23:35.000
doesn't really matter.

00:23:35.000 --> 00:23:36.500
But if somebody is
sort of like, ah,

00:23:36.500 --> 00:23:38.030
maybe I should send the
kid to college or not,

00:23:38.030 --> 00:23:39.500
but now [INAUDIBLE]
form and then

00:23:39.500 --> 00:23:41.360
they forget about it
and so on and so forth,

00:23:41.360 --> 00:23:42.818
for that kind of
people, there will

00:23:42.818 --> 00:23:45.530
be potentially large effects.

00:23:45.530 --> 00:23:47.420
But sort of then by
definition, in some ways,

00:23:47.420 --> 00:23:49.420
the fraction of people
who are actually marginal

00:23:49.420 --> 00:23:50.570
will not be huge.

00:23:50.570 --> 00:23:53.930
But again, since the costs
tend to be extremely low,

00:23:53.930 --> 00:23:58.370
it tends to be very much
cost effective to do so.

00:23:58.370 --> 00:24:01.730
Now again, sort of
previewing a little bit what

00:24:01.730 --> 00:24:05.180
comes in the policy lecture
is, so minor interventions

00:24:05.180 --> 00:24:06.960
can have large effects.

00:24:06.960 --> 00:24:10.310
But nudges can often
achieve sort of--

00:24:10.310 --> 00:24:11.810
in some cases,
nudges can achieve

00:24:11.810 --> 00:24:13.057
unambiguous improvements.

00:24:13.057 --> 00:24:15.140
Like, if you send people
reminders and they forget

00:24:15.140 --> 00:24:17.780
to get the flu shot, and
now they do the flu shot,

00:24:17.780 --> 00:24:21.080
or if people don't
take their medication,

00:24:21.080 --> 00:24:23.360
and you remind them,
or provide information,

00:24:23.360 --> 00:24:25.730
or some other forms
[INAUDIBLE] [? tell ?]

00:24:25.730 --> 00:24:27.890
people to take their
medication, it's

00:24:27.890 --> 00:24:30.970
pretty clear that that's
making people better off.

00:24:30.970 --> 00:24:33.530
But there's also a bunch of
challenges and other situations

00:24:33.530 --> 00:24:35.860
which is kind of like--

00:24:35.860 --> 00:24:37.600
in some cases, it's
not at all obvious.

00:24:37.600 --> 00:24:39.310
Like, which nudge to choose--

00:24:39.310 --> 00:24:41.290
are we making
everybody better off?

00:24:41.290 --> 00:24:43.047
Are some people made worse off?

00:24:43.047 --> 00:24:45.130
For example, should everybody
save for retirement?

00:24:45.130 --> 00:24:48.370
Are we pushing people
to save too much?

00:24:48.370 --> 00:24:49.750
Should everybody go to college?

00:24:49.750 --> 00:24:51.100
Maybe in some ways,
for some people,

00:24:51.100 --> 00:24:53.267
it's more suitable than for
others to go to college.

00:24:53.267 --> 00:24:56.210
Often it's a huge financial
expense for the family.

00:24:56.210 --> 00:24:59.560
And if then the job prospects
are not necessarily better

00:24:59.560 --> 00:25:00.950
compared to not
going to college,

00:25:00.950 --> 00:25:03.257
then it's not clear
that one should do that.

00:25:03.257 --> 00:25:05.590
There's also some evidence
or some concerns about nudges

00:25:05.590 --> 00:25:07.503
making people feel bad.

00:25:07.503 --> 00:25:08.920
So you get these
letters that says

00:25:08.920 --> 00:25:11.200
you're destroying
the environment,

00:25:11.200 --> 00:25:13.630
and [INAUDIBLE] all
these sad faces,

00:25:13.630 --> 00:25:15.250
and you're worse
than your neighbors,

00:25:15.250 --> 00:25:16.840
people might just
feel bad about it.

00:25:16.840 --> 00:25:22.450
And we should put
some weight on that.

00:25:22.450 --> 00:25:26.560
If you send these letters
to 1,000 households,

00:25:26.560 --> 00:25:28.640
and 50 households
or 20 households

00:25:28.640 --> 00:25:30.640
change their behavior
in certain ways,

00:25:30.640 --> 00:25:32.740
but 300 households
feel bad about it

00:25:32.740 --> 00:25:35.500
because they feel uncomfortable
getting these nudges,

00:25:35.500 --> 00:25:40.130
or you get all these phone calls
and spam about the environment

00:25:40.130 --> 00:25:42.550
from sending too
many letters, we

00:25:42.550 --> 00:25:43.940
should put some weight on that.

00:25:43.940 --> 00:25:46.280
And then it's like, then
there's some trade offs.

00:25:46.280 --> 00:25:47.560
It's not like entirely free.

00:25:47.560 --> 00:25:50.257
And then there are some
costs and benefits of that.

00:25:50.257 --> 00:25:52.090
There's also some
questions about which self

00:25:52.090 --> 00:25:53.210
should we respect.

00:25:53.210 --> 00:25:54.770
We talked about
this a lot before,

00:25:54.770 --> 00:25:59.070
which is if one self
wants to be very virtuous

00:25:59.070 --> 00:26:01.080
and exercise a lot and
so on, and the other one

00:26:01.080 --> 00:26:03.210
wants to sit on the
couch and watch TV,

00:26:03.210 --> 00:26:06.370
or if one self wants to smoke
and the other one does not,

00:26:06.370 --> 00:26:09.210
it's not obvious that we
should respect the long run

00:26:09.210 --> 00:26:14.610
self compared to the short run
self that wants to just enjoy

00:26:14.610 --> 00:26:15.600
themselves, and so on.

00:26:15.600 --> 00:26:18.360
And sort of now there's
tricky issues [INAUDIBLE]

00:26:18.360 --> 00:26:21.178
are we actually making
people better off or not.

00:26:21.178 --> 00:26:23.470
And we'll get back to these
issues in the last lecture,

00:26:23.470 --> 00:26:26.400
which talks about policy.

00:26:26.400 --> 00:26:28.850
Any questions on this lecture?

00:26:35.030 --> 00:26:38.480
This is actually lecture
20 about malleability

00:26:38.480 --> 00:26:41.930
and inaccessibility
of preferences.

00:26:41.930 --> 00:26:46.430
This is, in a way, a
more radical deviation

00:26:46.430 --> 00:26:48.890
from neoclassical economics.

00:26:48.890 --> 00:26:50.750
Because so far, we
have always said,

00:26:50.750 --> 00:26:52.490
people know their preferences.

00:26:52.490 --> 00:26:53.660
People have certain beliefs.

00:26:53.660 --> 00:26:55.610
Their beliefs might
be wrong, and so on.

00:26:55.610 --> 00:26:59.180
But crucially, people
knew what they were doing,

00:26:59.180 --> 00:27:01.730
and they were deliberately
making certain choices.

00:27:01.730 --> 00:27:06.740
And whether their
preferences were perhaps

00:27:06.740 --> 00:27:09.530
including present bias, or
social preferences, a reference

00:27:09.530 --> 00:27:11.240
dependence or the like--

00:27:11.240 --> 00:27:13.370
but these preferences
were given and fixed.

00:27:13.370 --> 00:27:15.623
And people knew what
those preferences are.

00:27:15.623 --> 00:27:17.790
Now we're going to deviate
from that and think about

00:27:17.790 --> 00:27:22.970
[INAUDIBLE] evidence that
people might not actually A--

00:27:22.970 --> 00:27:24.740
these preferences
might be malleable.

00:27:24.740 --> 00:27:26.990
And B, people might
not even understand

00:27:26.990 --> 00:27:28.640
why they want what they want.

00:27:28.640 --> 00:27:30.230
And sort of the
preferences are sort

00:27:30.230 --> 00:27:32.120
of quite mysterious to people.

00:27:32.120 --> 00:27:35.540
And it's quite easy in some
ways to manipulate people

00:27:35.540 --> 00:27:38.660
in some ways, without them
even understanding it,

00:27:38.660 --> 00:27:40.340
knowing about it.

00:27:40.340 --> 00:27:44.142
So I tell you about some very
fascinating psychology work,

00:27:44.142 --> 00:27:46.600
which is called-- the paper is
called "Telling more than we

00:27:46.600 --> 00:27:48.267
can know" from Nisbett
and Wilson, which

00:27:48.267 --> 00:27:50.172
is a classic paper
in social psychology.

00:27:50.172 --> 00:27:52.630
And then we're going to talk
a little bit about willingness

00:27:52.630 --> 00:27:54.460
to pay, and then
the paper by Ariely

00:27:54.460 --> 00:27:58.540
et al on coherent arbitrariness,
which you were supposed

00:27:58.540 --> 00:28:00.970
to read for today.

00:28:00.970 --> 00:28:01.660
OK.

00:28:01.660 --> 00:28:04.630
So let's step back a little
bit, and talk about the Nisbett

00:28:04.630 --> 00:28:07.220
and Wilson paper.

00:28:07.220 --> 00:28:10.540
So this is-- so
many questions-- you

00:28:10.540 --> 00:28:14.560
might have any questions about
cognitive processes underlying

00:28:14.560 --> 00:28:17.595
our choices, evaluations,
judgments, and behaviors.

00:28:17.595 --> 00:28:18.970
You might wonder,
why do you like

00:28:18.970 --> 00:28:20.143
a certain person versus not?

00:28:20.143 --> 00:28:21.310
What do you like about them?

00:28:21.310 --> 00:28:23.602
And why do you like this
person and not another person?

00:28:23.602 --> 00:28:26.150
And why you're friends with this
person versus somebody else?

00:28:26.150 --> 00:28:27.817
And why do you like
Math versus English,

00:28:27.817 --> 00:28:29.770
and so on and so forth?

00:28:29.770 --> 00:28:31.395
How did you solve
a certain problem

00:28:31.395 --> 00:28:33.520
once you were sort of asked
to solve some problems?

00:28:33.520 --> 00:28:36.162
How did you come
up with a solution?

00:28:36.162 --> 00:28:37.870
Why did you take this
job, or why did you

00:28:37.870 --> 00:28:39.633
take this class,
versus another class?

00:28:39.633 --> 00:28:41.050
And you might sort
of say, well, I

00:28:41.050 --> 00:28:42.460
like this one and not that one.

00:28:42.460 --> 00:28:44.660
And I really like sort
of like computer science,

00:28:44.660 --> 00:28:47.350
versus math versus economics.

00:28:47.350 --> 00:28:49.950
But when you then ask
[INAUDIBLE] yourself

00:28:49.950 --> 00:28:52.810
for others, why exactly do you
like that one, versus something

00:28:52.810 --> 00:28:53.440
else?

00:28:53.440 --> 00:28:57.070
You'll realize quickly that
people often don't quite

00:28:57.070 --> 00:28:59.330
have a good answer
to those questions.

00:28:59.330 --> 00:29:03.100
And Nisbett and Wilson's
fairly provocative paper

00:29:03.100 --> 00:29:05.980
says, essentially, we have no
idea where these preferences

00:29:05.980 --> 00:29:07.070
are coming from.

00:29:07.070 --> 00:29:08.830
And we're just making stuff up.

00:29:08.830 --> 00:29:11.920
And so let me give you
some examples of that.

00:29:11.920 --> 00:29:15.060
The first example is
what's called Maier's

00:29:15.060 --> 00:29:17.640
two-string problem from 1931.

00:29:17.640 --> 00:29:21.180
This is [INAUDIBLE] one of these
classic psychology experiments.

00:29:21.180 --> 00:29:23.160
The experiments
works as follows.

00:29:23.160 --> 00:29:26.280
There's two cords
hung from the ceiling

00:29:26.280 --> 00:29:30.990
of a lab with many objects, such
as poles, ring stands, clamps,

00:29:30.990 --> 00:29:33.150
pliers, and extension cords.

00:29:33.150 --> 00:29:35.640
Subjects are now asked--

00:29:35.640 --> 00:29:38.550
are told that a task is to
tie the two ends of the cords

00:29:38.550 --> 00:29:39.100
together.

00:29:39.100 --> 00:29:41.400
So there's two cords
hanging on the side

00:29:41.400 --> 00:29:44.370
of each side of the room.

00:29:44.370 --> 00:29:48.700
And subjects are supposed to
tie these two cords together.

00:29:48.700 --> 00:29:52.290
The problem is that the
cords are placed far apart

00:29:52.290 --> 00:29:55.020
from each other,
such that subjects

00:29:55.020 --> 00:29:57.943
can't, while holding onto
one cord, reach the other.

00:29:57.943 --> 00:29:59.610
So you can't just
take one cord and then

00:29:59.610 --> 00:30:01.110
sort of like try
to reach the other,

00:30:01.110 --> 00:30:03.300
because it's too far away.

00:30:03.300 --> 00:30:07.080
And so there's all these
different objects in the room.

00:30:07.080 --> 00:30:10.210
And subjects are sort of trying
to figure out how to do this.

00:30:10.210 --> 00:30:14.547
And so then they usually come
up with one or two solutions

00:30:14.547 --> 00:30:17.130
that are not really solutions,
because it doesn't really work.

00:30:17.130 --> 00:30:19.590
So they can get [INAUDIBLE]
like extension cord,

00:30:19.590 --> 00:30:22.350
they use the pliers, and so on,
and the clamp, and some ring

00:30:22.350 --> 00:30:22.900
or whatever.

00:30:22.900 --> 00:30:24.710
And it just doesn't work.

00:30:24.710 --> 00:30:28.880
And then they sort of
try it and keep trying,

00:30:28.880 --> 00:30:30.570
but it doesn't really work out.

00:30:30.570 --> 00:30:33.990
And then they're told
to do it another way.

00:30:33.990 --> 00:30:36.110
And then Maier,
the experimenter,

00:30:36.110 --> 00:30:38.600
walks into the room at some
point, or walks around,

00:30:38.600 --> 00:30:40.617
and essentially,
accidentally-- this

00:30:40.617 --> 00:30:42.950
is sort of like on purpose
of course-- accidentally puts

00:30:42.950 --> 00:30:44.900
some of the cords in motion.

00:30:44.900 --> 00:30:48.230
And subjects then very
quickly afterwards figure out

00:30:48.230 --> 00:30:51.050
the solution within
the next 45 seconds.

00:30:51.050 --> 00:30:52.490
Of course, the
reasoning here is,

00:30:52.490 --> 00:30:54.890
why they figure it out is
because now the cords are

00:30:54.890 --> 00:30:55.730
put in motion.

00:30:55.730 --> 00:30:58.535
And subjects sort of
figure out how to do this.

00:30:58.535 --> 00:31:00.410
And what you're supposed
to do is essentially

00:31:00.410 --> 00:31:01.370
put the cords in motion.

00:31:01.370 --> 00:31:02.870
And once you put
them in motion, you

00:31:02.870 --> 00:31:07.380
can essentially reach them
both and then put it together.

00:31:07.380 --> 00:31:08.360
I have a video here.

00:31:08.360 --> 00:31:10.160
I'm a little traumatized
the last time

00:31:10.160 --> 00:31:11.280
where it didn't work out.

00:31:11.280 --> 00:31:13.730
So I'm not sure [INAUDIBLE].

00:31:13.730 --> 00:31:14.730
Now I don't even see it.

00:31:14.730 --> 00:31:15.960
So maybe let's just skip it.

00:31:15.960 --> 00:31:17.460
But you can watch
the video of that.

00:31:17.460 --> 00:31:20.120
It's the first two and
a half minutes video.

00:31:20.120 --> 00:31:21.390
[INAUDIBLE] see anything.

00:31:21.390 --> 00:31:25.410
But there's a video that sort of
demonstrates this task that you

00:31:25.410 --> 00:31:28.030
can sort of just skip this.

00:31:28.030 --> 00:31:31.920
So now then afterwards,
people are asked, well,

00:31:31.920 --> 00:31:36.780
how did you come up with the
idea of using a pendulum?

00:31:36.780 --> 00:31:39.540
And people then have all
these like explanations.

00:31:39.540 --> 00:31:40.980
Well, it just dawned on me.

00:31:40.980 --> 00:31:42.738
It was the only thing left.

00:31:42.738 --> 00:31:44.530
I thought about all
these different things.

00:31:44.530 --> 00:31:46.655
And at the end, I came up
with this other solution,

00:31:46.655 --> 00:31:48.570
which is using these
strings as a pendulum.

00:31:48.570 --> 00:31:51.112
Or I just realized
the cord would swing

00:31:51.112 --> 00:31:52.320
[INAUDIBLE] the weight on it.

00:31:52.320 --> 00:31:54.067
So what the solution
at the end is like,

00:31:54.067 --> 00:31:56.400
you put the plier on one of
the cords and then swing it.

00:31:56.400 --> 00:31:57.630
And then you sort of
use the other one.

00:31:57.630 --> 00:31:59.430
And then you could
put them together.

00:31:59.430 --> 00:32:02.520
There's one Harvard psychology
faculty subject, in fact,

00:32:02.520 --> 00:32:04.020
had the following
explanation, which

00:32:04.020 --> 00:32:06.320
is, having exhausted
everything else,

00:32:06.320 --> 00:32:07.650
the next thing was to swing it.

00:32:07.650 --> 00:32:10.670
I thought of the situation
of swinging across a river.

00:32:10.670 --> 00:32:13.560
I had an imagery of monkeys
swinging from trees.

00:32:13.560 --> 00:32:17.410
This imagery appeared
simultaneously with a solution.

00:32:17.410 --> 00:32:20.100
The idea appeared complete.

00:32:20.100 --> 00:32:22.500
Now of course, the
problem here is that--

00:32:25.050 --> 00:32:27.510
the problem here is
that's all bullshit.

00:32:27.510 --> 00:32:30.720
Because the reason why people
come up with a solution

00:32:30.720 --> 00:32:35.790
is because, as I told you, Maier
was walking in and accidentally

00:32:35.790 --> 00:32:37.110
putting the cords in motion.

00:32:37.110 --> 00:32:39.990
And there's
differences in timing.

00:32:39.990 --> 00:32:42.390
Sometimes they would come
in earlier, sometimes later.

00:32:42.390 --> 00:32:45.360
And every time the
experimenter walks in,

00:32:45.360 --> 00:32:46.900
people would sort
of accidentally

00:32:46.900 --> 00:32:48.840
then figure it out,
and then come up

00:32:48.840 --> 00:32:51.360
with these elaborate
explanations.

00:32:51.360 --> 00:32:53.160
And the reason why
we know that this

00:32:53.160 --> 00:32:56.820
is because of the
experimenter are coming in

00:32:56.820 --> 00:32:59.790
is because if the experimenter
doesn't accidentally come in,

00:32:59.790 --> 00:33:02.880
and puts the cords in
motion, then essentially,

00:33:02.880 --> 00:33:04.580
people are not figuring out.

00:33:04.580 --> 00:33:07.290
So we know the true
causal effect here

00:33:07.290 --> 00:33:09.840
is the experimenter
coming in and suggesting

00:33:09.840 --> 00:33:11.340
a solution to people.

00:33:11.340 --> 00:33:14.940
But people then make
up all these stories,

00:33:14.940 --> 00:33:16.410
including like the
monkeys swinging

00:33:16.410 --> 00:33:18.990
from trees, how they came
up with a solution, which

00:33:18.990 --> 00:33:21.340
is clearly not how they
came up with the solution.

00:33:21.340 --> 00:33:23.215
The reason why they came
up with the solution

00:33:23.215 --> 00:33:26.980
is because Maier
touched the strings.

00:33:26.980 --> 00:33:27.790
OK.

00:33:27.790 --> 00:33:30.010
Now there's a
second example here,

00:33:30.010 --> 00:33:31.760
or three examples in total.

00:33:31.760 --> 00:33:33.490
So let me tell you
about example number--

00:33:33.490 --> 00:33:38.910
or let me ask first, is there
any questions about this study?

00:33:38.910 --> 00:33:41.990
By the way, you should
lower your hands,

00:33:41.990 --> 00:33:43.670
if you asked
[INAUDIBLE] previously,

00:33:43.670 --> 00:33:46.770
like [? Brian ?]
and [INAUDIBLE]..

00:33:46.770 --> 00:33:48.610
Unless you have have
additional questions.

00:33:53.510 --> 00:33:57.380
Let me tell you the
second study now.

00:33:57.380 --> 00:34:01.460
This is a study by
Latane and Darley,

00:34:01.460 --> 00:34:04.400
which is, I told you
this a little bit

00:34:04.400 --> 00:34:08.150
about [INAUDIBLE] these are
the impacts of bystanders

00:34:08.150 --> 00:34:10.460
and witnesses on
helping behaviors.

00:34:10.460 --> 00:34:12.989
I told you about the story
of the Good Samaritan before.

00:34:12.989 --> 00:34:15.670
This is sort of a similar study.

00:34:15.670 --> 00:34:17.980
And this is
essentially a situation

00:34:17.980 --> 00:34:20.260
that's, again, created
by social psychologists.

00:34:20.260 --> 00:34:22.239
It's not a real
situation, but people are

00:34:22.239 --> 00:34:24.409
meant to think that it's real.

00:34:24.409 --> 00:34:27.550
And so here, the more
people over here, someone

00:34:27.550 --> 00:34:29.139
in another room,
having what sounds

00:34:29.139 --> 00:34:32.440
like an epileptic seizure,
the lower the probability

00:34:32.440 --> 00:34:35.280
that any given individual
will rush to help.

00:34:35.280 --> 00:34:38.699
And you get similar results
for individuals' reaction

00:34:38.699 --> 00:34:40.358
to dangerous-looking
smoke coming out

00:34:40.358 --> 00:34:41.400
of the ceiling of a room.

00:34:41.400 --> 00:34:43.108
So like there's people
in the experiment.

00:34:43.108 --> 00:34:45.630
And they have smoke
coming out in a room.

00:34:45.630 --> 00:34:48.060
And the more people that
are there, the less likely

00:34:48.060 --> 00:34:50.520
is any given person, perhaps
because they're freeriding

00:34:50.520 --> 00:34:53.620
is to actually do
something about it.

00:34:53.620 --> 00:34:55.425
And this looks kind
of very dangerous.

00:34:55.425 --> 00:34:57.300
But nobody does anything,
because essentially

00:34:57.300 --> 00:34:59.550
other people are also
not doing anything.

00:34:59.550 --> 00:35:01.470
Now when you then ask
people afterwards,

00:35:01.470 --> 00:35:05.430
you know, what
influenced your choices?

00:35:05.430 --> 00:35:10.290
And essentially, when you ask
them directly and tactfully

00:35:10.290 --> 00:35:13.540
and bluntly, you always get the
same answer, that essentially,

00:35:13.540 --> 00:35:15.900
they think their behavior
had not been influenced

00:35:15.900 --> 00:35:17.355
by the other people present.

00:35:17.355 --> 00:35:19.230
They think you're like--
for whatever reason,

00:35:19.230 --> 00:35:21.157
they didn't do
anything, or maybe they

00:35:21.157 --> 00:35:23.490
thought it wasn't so bad, or
there wasn't really danger,

00:35:23.490 --> 00:35:26.050
or whatever.

00:35:26.050 --> 00:35:28.280
But of course, these are
randomized experiments.

00:35:28.280 --> 00:35:30.030
We know that some
subjects were influenced

00:35:30.030 --> 00:35:32.155
by the presence of other
people, because precisely,

00:35:32.155 --> 00:35:33.960
the experiment was
creating that variation,

00:35:33.960 --> 00:35:36.090
and it affected
people's behavior.

00:35:36.090 --> 00:35:39.060
[INAUDIBLE] people essentially
just don't understand that,

00:35:39.060 --> 00:35:41.610
and sort of then come up
with other explanations

00:35:41.610 --> 00:35:46.990
of why they do or did
what they, in fact, did.

00:35:46.990 --> 00:35:49.690
And number three is
sort of very similar.

00:35:49.690 --> 00:35:54.890
These are called erroneous
reports about position effects.

00:35:54.890 --> 00:35:58.840
These are studies
where certain items

00:35:58.840 --> 00:36:04.330
are positioned in different
ways, to get people to--

00:36:04.330 --> 00:36:06.680
when people were
evaluating them.

00:36:06.680 --> 00:36:12.310
So passerbys were asked
to evaluate some clothing.

00:36:12.310 --> 00:36:16.630
And they were asked about the
quality and the preferences--

00:36:16.630 --> 00:36:18.460
the quality of certain
goods and people's

00:36:18.460 --> 00:36:20.760
preferences between
different goods.

00:36:20.760 --> 00:36:23.042
And the way this
experiment was set up--

00:36:23.042 --> 00:36:25.500
and this is sort of essentially
like a marketing experiment

00:36:25.500 --> 00:36:26.280
if you want--

00:36:26.280 --> 00:36:29.910
essentially, there's a
pronounced left-to-right

00:36:29.910 --> 00:36:30.990
position effect.

00:36:30.990 --> 00:36:33.510
The rightmost object
was heavily over-chosen

00:36:33.510 --> 00:36:34.600
in these experiments.

00:36:34.600 --> 00:36:36.990
And that's in some sense
irrelevant why that's the case.

00:36:36.990 --> 00:36:39.540
But essentially, from
previous experiments,

00:36:39.540 --> 00:36:41.790
we know that essentially
the rightmost object

00:36:41.790 --> 00:36:45.300
is most likely to be chosen,
perhaps because people

00:36:45.300 --> 00:36:46.320
look at it first.

00:36:46.320 --> 00:36:49.305
[INAUDIBLE] look at it last
and so on, it doesn't matter.

00:36:49.305 --> 00:36:50.680
What's important
for our purposes

00:36:50.680 --> 00:36:53.700
is that the rightmost object
is most heavily chosen.

00:36:53.700 --> 00:36:56.880
People like that object the
best in these experiments.

00:36:56.880 --> 00:36:59.190
And then there's going
to be randomization

00:36:59.190 --> 00:37:02.070
in the positioning
of experiments.

00:37:02.070 --> 00:37:03.630
And people are then
asked, well, why

00:37:03.630 --> 00:37:06.480
did you choose this
sweater versus another?

00:37:06.480 --> 00:37:09.810
And no subject
whatsoever ever mentions

00:37:09.810 --> 00:37:14.040
the position of the
article in the room,

00:37:14.040 --> 00:37:15.720
and virtually all
subjects denied

00:37:15.720 --> 00:37:18.270
it was, when asked directly
about the possible effect

00:37:18.270 --> 00:37:19.802
of the position of the article.

00:37:19.802 --> 00:37:21.760
And people come up with
all these explanations.

00:37:21.760 --> 00:37:24.030
It's like, green is really
always my [INAUDIBLE] favorite

00:37:24.030 --> 00:37:24.540
color.

00:37:24.540 --> 00:37:26.250
And this sweater
is really fluffy.

00:37:26.250 --> 00:37:27.000
And this and that.

00:37:27.000 --> 00:37:30.580
And I really like it
for reason x and y.

00:37:30.580 --> 00:37:33.090
Of course that's
true for some people,

00:37:33.090 --> 00:37:34.860
but we know that at
least some people must

00:37:34.860 --> 00:37:36.390
be swayed by these
position effects,

00:37:36.390 --> 00:37:39.302
because we set up an experiment
to precisely do that.

00:37:39.302 --> 00:37:41.010
And then that essentially
swayed behavior

00:37:41.010 --> 00:37:42.880
in a very predictable way.

00:37:42.880 --> 00:37:44.790
So essentially, there
are these determinants

00:37:44.790 --> 00:37:46.800
on people's preferences
of behavior,

00:37:46.800 --> 00:37:51.500
that people do not
seem to understand.

00:37:51.500 --> 00:37:52.010
OK.

00:37:52.010 --> 00:37:54.527
So let me summarize
what did we learn.

00:37:54.527 --> 00:37:57.110
So there are many instances in
which subjects have no idea why

00:37:57.110 --> 00:37:59.130
they choose what they choose.

00:37:59.130 --> 00:38:00.455
And then people--

00:38:00.455 --> 00:38:02.520
[INAUDIBLE] can read
this more in this paper,

00:38:02.520 --> 00:38:05.010
which I think is a
beautiful paper--

00:38:05.010 --> 00:38:07.940
people appear to
make up stories that

00:38:07.940 --> 00:38:11.450
are based on their a priori,
implicit causal theories.

00:38:11.450 --> 00:38:13.160
What I mean by that,
essentially, they

00:38:13.160 --> 00:38:16.770
have some theory of why
they do certain things.

00:38:16.770 --> 00:38:18.860
And when you ask them,
then, OK, well, why

00:38:18.860 --> 00:38:20.420
did you do what you just did?

00:38:20.420 --> 00:38:22.190
People will sort of
come up with some ways

00:38:22.190 --> 00:38:24.980
of justifying
[INAUDIBLE] in some ways

00:38:24.980 --> 00:38:27.020
their behavior, based
on some theories

00:38:27.020 --> 00:38:28.850
that they had about themselves.

00:38:28.850 --> 00:38:29.735
And sort of--

00:38:29.735 --> 00:38:31.820
I always like fluffy
sweaters, and therefore you

00:38:31.820 --> 00:38:32.820
choose the sweater.

00:38:32.820 --> 00:38:34.820
Of course, it happens to
be the only [INAUDIBLE]

00:38:34.820 --> 00:38:38.270
sweater if it's [INAUDIBLE] on
the right versus on the left.

00:38:38.270 --> 00:38:40.790
But people then sort of have
their ways in which they then

00:38:40.790 --> 00:38:44.510
explain what they do based on
essentially things that they

00:38:44.510 --> 00:38:47.950
sort of essentially make up.

00:38:47.950 --> 00:38:50.831
Any questions on this
before I move on?

00:38:57.900 --> 00:39:02.010
And I encourage you to read
the paper, more for fun

00:39:02.010 --> 00:39:03.265
than for anything.

00:39:03.265 --> 00:39:04.890
It makes you kind of
wonder quite a lot

00:39:04.890 --> 00:39:07.680
in why you do what you
do, and why you prefer

00:39:07.680 --> 00:39:11.540
certain things versus others.

00:39:11.540 --> 00:39:14.180
So let's briefly talk about
two experimental design

00:39:14.180 --> 00:39:17.630
tools, which will be useful
for the coherent arbitrariness

00:39:17.630 --> 00:39:18.642
paper.

00:39:18.642 --> 00:39:20.600
One is what's called the
strategy method, which

00:39:20.600 --> 00:39:22.292
you already came
across a little bit.

00:39:22.292 --> 00:39:23.750
And the second one
is what's called

00:39:23.750 --> 00:39:26.750
the BDM, the
Becker-DeGroot-Marschak

00:39:26.750 --> 00:39:28.850
procedure for
eliciting valuations.

00:39:28.850 --> 00:39:31.010
I'll try to be quick.

00:39:31.010 --> 00:39:38.240
So we're often interested in
behavior in rare contingencies.

00:39:38.240 --> 00:39:40.220
So often, we may
ask the question,

00:39:40.220 --> 00:39:43.370
how would people behave in
many different contingencies?

00:39:43.370 --> 00:39:44.840
Some of them are
often quite rare,

00:39:44.840 --> 00:39:47.730
that don't happen very often.

00:39:47.730 --> 00:39:50.070
And why do we care about
such contingencies?

00:39:50.070 --> 00:39:51.780
Well, sometimes we
just care about them

00:39:51.780 --> 00:39:55.200
like [INAUDIBLE] it's
inherently important

00:39:55.200 --> 00:39:59.640
how people behave or prevent
certain contingencies,

00:39:59.640 --> 00:40:01.995
disasters, earthquakes,
droughts, et cetera.

00:40:01.995 --> 00:40:03.370
We care a lot
about these things,

00:40:03.370 --> 00:40:04.890
even if they're sort
of rarely happening.

00:40:04.890 --> 00:40:06.432
So we kind of want
to know how people

00:40:06.432 --> 00:40:09.510
behave in certain situations.

00:40:09.510 --> 00:40:10.980
But events and
rare contingencies

00:40:10.980 --> 00:40:15.390
also can affect, on top of that,
events in likely contingencies.

00:40:15.390 --> 00:40:16.660
Here's a simple example.

00:40:16.660 --> 00:40:19.173
If your roommates think
you'll punch them in the face

00:40:19.173 --> 00:40:21.090
if they borrow your stuff
without asking them,

00:40:21.090 --> 00:40:23.130
they will not do it.

00:40:23.130 --> 00:40:26.310
Of course, hopefully, you are
not punching your roommates,

00:40:26.310 --> 00:40:30.010
and I very much do not want
to encourage you to do so.

00:40:30.010 --> 00:40:33.270
But here, the key part
here is that punching

00:40:33.270 --> 00:40:38.790
is rare but important, precisely
because you might do so in case

00:40:38.790 --> 00:40:41.910
they misbehave, they will
not borrow your stuff

00:40:41.910 --> 00:40:44.320
or steal your stuff
in the first place.

00:40:44.320 --> 00:40:46.410
So essentially, sort of
these [? equilibrium ?]

00:40:46.410 --> 00:40:49.512
or rare contingencies
might be quite important,

00:40:49.512 --> 00:40:51.720
not because we think they
actually happen very often,

00:40:51.720 --> 00:40:56.540
but they sort of discipline
behavior in other cases.

00:40:56.540 --> 00:40:58.970
Now, what's [? known ?]
as strategy method?

00:40:58.970 --> 00:41:02.480
Well, the strategy
method helps you

00:41:02.480 --> 00:41:06.890
to elicit behavior in many
potentially rare circumstances

00:41:06.890 --> 00:41:08.780
by asking subjects
what they would

00:41:08.780 --> 00:41:13.290
do with a choice implemented
if the circumstances arises.

00:41:13.290 --> 00:41:16.160
That is to say, I'm asking
you-- for many different cases,

00:41:16.160 --> 00:41:18.990
I'm going to implement
one of those cases.

00:41:18.990 --> 00:41:24.320
And that's going to allow me to
be [INAUDIBLE] [? compatible ?]

00:41:24.320 --> 00:41:25.860
in various ways.

00:41:25.860 --> 00:41:30.870
So since the decision does
count if the contingency occurs,

00:41:30.870 --> 00:41:33.180
subjects have an incentive
to choose correctly

00:41:33.180 --> 00:41:35.100
for each contingency.

00:41:35.100 --> 00:41:39.660
That is to say, I could say,
suppose this is what we're

00:41:39.660 --> 00:41:41.580
doing in class in some sense.

00:41:41.580 --> 00:41:43.140
We talked about
social preferences.

00:41:43.140 --> 00:41:46.080
I said, you know, I'm
asking you to make a choice.

00:41:46.080 --> 00:41:50.140
I'm going to only pick one of
you to implement that choice.

00:41:50.140 --> 00:41:57.145
And you have the incentives
now to answer truthfully,

00:41:57.145 --> 00:41:59.520
because it could always be
the case that your choice will

00:41:59.520 --> 00:42:01.420
be implemented.

00:42:01.420 --> 00:42:05.190
And so that allows then the
experimenter, me in that case,

00:42:05.190 --> 00:42:09.030
to generate a lot of data from
one simple experiment, where

00:42:09.030 --> 00:42:10.800
essentially, you
can give a subject

00:42:10.800 --> 00:42:12.900
many different
decisions, and then say,

00:42:12.900 --> 00:42:14.760
only one of your
choices will actually

00:42:14.760 --> 00:42:16.020
be implemented for sure.

00:42:16.020 --> 00:42:18.400
Or you can ask many people
the same question and say,

00:42:18.400 --> 00:42:21.630
[INAUDIBLE] only one of
you, or one of your choices

00:42:21.630 --> 00:42:23.140
will be implemented.

00:42:23.140 --> 00:42:27.480
And so then that's essentially
incentive-compatible,

00:42:27.480 --> 00:42:30.420
because it can always be that
your choice is the one, or one

00:42:30.420 --> 00:42:32.850
of the choices is
the one that counts.

00:42:32.850 --> 00:42:35.680
And when you look at sort
of experimental evidence,

00:42:35.680 --> 00:42:38.370
it seems to suggest that the
strategy methods are asking,

00:42:38.370 --> 00:42:41.372
essentially, if I can ask you--
so there's experiments where

00:42:41.372 --> 00:42:43.830
I can ask you 100 different
questions, and only one of them

00:42:43.830 --> 00:42:46.650
will be implemented, versus
a different group would

00:42:46.650 --> 00:42:49.110
be asked-- a randomized
group would be asked only one

00:42:49.110 --> 00:42:49.630
question.

00:42:49.630 --> 00:42:52.350
It turns out that people
actually answer in experiments

00:42:52.350 --> 00:42:57.250
these questions pretty similarly
in both of these cases.

00:42:57.250 --> 00:42:59.160
So that's to say the
strategy method seems

00:42:59.160 --> 00:43:04.424
to elicit individuals' true
preferences pretty well.

00:43:04.424 --> 00:43:05.103
Sorry.

00:43:05.103 --> 00:43:06.020
[INAUDIBLE] messed up.

00:43:06.020 --> 00:43:07.252
But I think that's fine.

00:43:07.252 --> 00:43:08.960
So what's now the
Becker-DeGroot-Marschak

00:43:08.960 --> 00:43:09.620
procedure?

00:43:09.620 --> 00:43:11.910
It's essentially
a version of that,

00:43:11.910 --> 00:43:17.825
which looks something like this,
which is, people are asked--

00:43:17.825 --> 00:43:19.200
[INAUDIBLE] so
what the goal here

00:43:19.200 --> 00:43:22.523
is to understand people's
willingness to pay for a good.

00:43:22.523 --> 00:43:24.440
So subjects are told
that a price for the good

00:43:24.440 --> 00:43:25.730
will be randomly selected.

00:43:25.730 --> 00:43:29.000
And the price goes from like
$0.50 to like, say, $10.

00:43:29.000 --> 00:43:31.400
For each of these prices,
the person is asked,

00:43:31.400 --> 00:43:35.780
do you prefer buy versus
not-buy in these options.

00:43:35.780 --> 00:43:38.943
And so then, I ask you to
fill out the entire form.

00:43:38.943 --> 00:43:41.360
Tell me, for each of these
prices, what do you want to do?

00:43:41.360 --> 00:43:43.070
If the price is $3,
what do you want?

00:43:43.070 --> 00:43:44.690
If the price is $1,
what do you want?

00:43:44.690 --> 00:43:47.030
If the price is $10,
what do you want?

00:43:47.030 --> 00:43:50.870
And then I'm going to afterwards
then pick a price and say, OK.

00:43:50.870 --> 00:43:52.378
Now the price is $3.50.

00:43:52.378 --> 00:43:54.920
And now I'm going to just look
at like, what did you actually

00:43:54.920 --> 00:43:56.870
say, and whatever
your choice was then

00:43:56.870 --> 00:44:03.440
for that specific price is
going to be implemented.

00:44:03.440 --> 00:44:06.180
Is that clear?

00:44:06.180 --> 00:44:09.270
Now, just to be clear what
the problem is that we're

00:44:09.270 --> 00:44:12.930
trying to overcome here--

00:44:12.930 --> 00:44:17.210
suppose I'm trying
to sell you a mug.

00:44:17.210 --> 00:44:18.758
Or suppose I have this mug here.

00:44:18.758 --> 00:44:20.550
I'm saying, would you
like to buy this mug?

00:44:20.550 --> 00:44:23.220
It's a very beautiful mug
with lots of trees on them.

00:44:23.220 --> 00:44:24.720
I'd like to sell it to you.

00:44:24.720 --> 00:44:27.950
And I'm eliciting your
willingness to pay.

00:44:27.950 --> 00:44:30.470
What's the problem
that arises from just

00:44:30.470 --> 00:44:33.240
asking you this directly,
without the BDM procedure?

00:44:33.240 --> 00:44:37.548
What problem-- what is the BDM
procedure sort of helping with?

00:44:37.548 --> 00:44:39.090
So if you go to a
marketplace, or try

00:44:39.090 --> 00:44:41.820
to bargain with me on
this mug, which again,

00:44:41.820 --> 00:44:45.210
it's a beautiful mug--

00:44:45.210 --> 00:44:47.850
you might want to
shade your valuation.

00:44:47.850 --> 00:44:50.640
You might be willing
to pay $5, but you're

00:44:50.640 --> 00:44:54.030
going to say $2, because you
are hoping to get a bargain

00:44:54.030 --> 00:44:56.340
and get [INAUDIBLE] a
cheap price from me.

00:44:56.340 --> 00:44:57.090
Right?

00:44:57.090 --> 00:44:59.430
And the key part here is
that you are essentially

00:44:59.430 --> 00:45:03.900
hoping that your willingness to
pay whatever you are offering

00:45:03.900 --> 00:45:07.380
me will essentially
change whatever price I'm

00:45:07.380 --> 00:45:09.210
offering the mug to you.

00:45:09.210 --> 00:45:12.240
Notice that that's not the
case here in the BDM procedure.

00:45:12.240 --> 00:45:15.600
In the BDM procedure,
whatever you say

00:45:15.600 --> 00:45:19.103
is independent of the actual
price that's implemented.

00:45:19.103 --> 00:45:20.520
I'm saying,
essentially, I'm going

00:45:20.520 --> 00:45:23.490
to randomly pick one of
those 10 or 20 prices here.

00:45:23.490 --> 00:45:26.700
And you're going to tell me, for
each of those prices, what you

00:45:26.700 --> 00:45:28.710
want, either buy or not buy.

00:45:28.710 --> 00:45:31.538
And your choices are-- since
I'm randomizing afterwards,

00:45:31.538 --> 00:45:32.955
which price selection
you're going

00:45:32.955 --> 00:45:38.810
to pick, your choices
are irrelevant for which

00:45:38.810 --> 00:45:43.247
of the actual
prices are selected.

00:45:43.247 --> 00:45:45.080
And so now essentially
get around this issue

00:45:45.080 --> 00:45:47.480
about people shading or
essentially underreporting

00:45:47.480 --> 00:45:49.220
their willingness to pay.

00:45:49.220 --> 00:45:52.130
Precisely that's what makes
it incentive-compatible.

00:45:52.130 --> 00:45:54.770
And that's essentially what's
called the BDM mechanism.

00:45:54.770 --> 00:45:59.155
That's what many economists
use in many, many experiments.

00:45:59.155 --> 00:46:00.530
There's another
version of what's

00:46:00.530 --> 00:46:07.102
called the BDM which is a more
straightforward version of it.

00:46:07.102 --> 00:46:08.810
Notice here you have
to ask 20 questions.

00:46:08.810 --> 00:46:10.977
Ask would you like to pay--
would you like to buy it

00:46:10.977 --> 00:46:14.910
for $0.50, $1.00, $1.50,
$2.00, and so on and so forth.

00:46:14.910 --> 00:46:20.210
Another version of this is, I'm
asking you straight up what's

00:46:20.210 --> 00:46:21.770
your willingness to pay.

00:46:21.770 --> 00:46:25.100
And the bid is then compared
to a price determined

00:46:25.100 --> 00:46:26.750
by a random number generator.

00:46:26.750 --> 00:46:28.540
I'm just saying, like, tell me
what your willingness to pay

00:46:28.540 --> 00:46:29.210
is.

00:46:29.210 --> 00:46:32.220
Then I'm going to do like
a random number generator.

00:46:32.220 --> 00:46:35.030
If the subject's bid is
greater than the price that's

00:46:35.030 --> 00:46:37.670
generated by the random
number generator,

00:46:37.670 --> 00:46:40.430
he or she pays the price,
not the announced willingness

00:46:40.430 --> 00:46:43.430
to pay, and receives the
item being auctioned.

00:46:43.430 --> 00:46:45.470
If the subject's bid is
lower than the price,

00:46:45.470 --> 00:46:48.740
he or she pays nothing
and receives nothing.

00:46:48.740 --> 00:46:53.210
And so now here again, the
final price the person must pay

00:46:53.210 --> 00:46:55.670
is independent of what
the person indicated

00:46:55.670 --> 00:46:57.380
as your willingness
to pay, which

00:46:57.380 --> 00:47:00.265
essentially solves the
incentive compatibility issue.

00:47:00.265 --> 00:47:01.640
Think of like this
version that I

00:47:01.640 --> 00:47:04.670
have on this line as just a
more efficient way of eliciting

00:47:04.670 --> 00:47:06.720
the question that's
on the previous slide.

00:47:06.720 --> 00:47:08.960
This is essentially
exactly the same thing,

00:47:08.960 --> 00:47:11.370
except for that it's a
more efficient thing to do.

00:47:11.370 --> 00:47:13.700
The problem with this version,
often, is comprehension.

00:47:13.700 --> 00:47:14.533
People get confused.

00:47:14.533 --> 00:47:17.120
Or people tend to still
try to bargain and give

00:47:17.120 --> 00:47:18.780
like lower willingness to pay.

00:47:18.780 --> 00:47:21.308
So sometimes, this
version works better

00:47:21.308 --> 00:47:23.100
because it's much
easier, much transparent,

00:47:23.100 --> 00:47:25.350
when we say, here's a price,
do want to buy it or not,

00:47:25.350 --> 00:47:27.200
versus and so on.

00:47:27.200 --> 00:47:29.822
While here it's people are sort
of [INAUDIBLE] because they're

00:47:29.822 --> 00:47:31.280
used to, essentially,
bargain, they

00:47:31.280 --> 00:47:35.350
tend to sort of under-report
at least sometimes.

00:47:35.350 --> 00:47:36.430
Any questions on this?

00:47:45.920 --> 00:47:49.940
So now I'm also going
to skip this video.

00:47:49.940 --> 00:47:50.805
It's a fun video.

00:47:50.805 --> 00:47:52.805
Who knows the story of
Tom Sawyer and the fence?

00:47:59.130 --> 00:48:02.250
I think it's in
the readings, no?

00:48:02.250 --> 00:48:04.180
Tom Sawyer was misbehaving.

00:48:04.180 --> 00:48:06.880
He was essentially punished
to paint the fence.

00:48:06.880 --> 00:48:08.750
He really didn't want to do it.

00:48:08.750 --> 00:48:10.980
But then his friend comes
along, and essentially,

00:48:10.980 --> 00:48:12.730
he sort of tricks his
friend into thinking

00:48:12.730 --> 00:48:15.115
this is really like
such a fun task,

00:48:15.115 --> 00:48:16.870
and it ends up
being essentially,

00:48:16.870 --> 00:48:21.190
the friend even paying him
for painting the fence,

00:48:21.190 --> 00:48:23.380
and he not having to do it.

00:48:23.380 --> 00:48:25.040
And the point of the
story, of course,

00:48:25.040 --> 00:48:30.080
is that people's willingness to
pay is malleable in the sense,

00:48:30.080 --> 00:48:33.340
like, even to the extent
that people don't even

00:48:33.340 --> 00:48:35.500
know whether they're willing
to pay for something,

00:48:35.500 --> 00:48:39.140
or you have to pay them
something to do it.

00:48:39.140 --> 00:48:41.140
And so essentially,
people's preferences

00:48:41.140 --> 00:48:46.180
are inherently malleable by
the way they're being marketed,

00:48:46.180 --> 00:48:49.732
or the way they appear to
them, or being sold to them.

00:48:49.732 --> 00:48:53.520
You can watch the video in
the slides if you download it.

00:48:53.520 --> 00:48:56.340
For whatever reason this
is not working on Zoom,

00:48:56.340 --> 00:48:57.730
at least for now.

00:48:57.730 --> 00:49:01.940
So stepping back a
little bit-- so overall,

00:49:01.940 --> 00:49:03.990
so far we talked about
two key components

00:49:03.990 --> 00:49:05.250
of individual decision making.

00:49:05.250 --> 00:49:08.340
Utility functions-- what people
want and what they care about,

00:49:08.340 --> 00:49:10.350
and beliefs-- how people
perceive themselves

00:49:10.350 --> 00:49:12.250
and patterns in the world.

00:49:12.250 --> 00:49:13.960
And so understanding
these, of course,

00:49:13.960 --> 00:49:17.580
is important because that
determines people's choices.

00:49:17.580 --> 00:49:19.770
Now so far, we have
always sort of pretended

00:49:19.770 --> 00:49:22.740
and said people's preferences
and beliefs are always

00:49:22.740 --> 00:49:23.670
sharply--

00:49:23.670 --> 00:49:26.400
people are always sharply aware
of what they want and believe

00:49:26.400 --> 00:49:28.770
that costs [INAUDIBLE].

00:49:28.770 --> 00:49:30.790
For example, a
homeowner might have

00:49:30.790 --> 00:49:32.760
reference-dependent
preferences, but they know

00:49:32.760 --> 00:49:34.907
what their preferences are.

00:49:34.907 --> 00:49:36.990
There might be some issues
about how the reference

00:49:36.990 --> 00:49:39.600
point is determined, but the
preference is the function.

00:49:39.600 --> 00:49:42.467
The functional form is
always fixed, and it's known.

00:49:42.467 --> 00:49:44.550
A person might have the
wrong theory of the world,

00:49:44.550 --> 00:49:46.860
but she always has some
beliefs in mind, what

00:49:46.860 --> 00:49:49.020
she uses to make those choices.

00:49:49.020 --> 00:49:51.420
Or a smoker might
act suboptimally,

00:49:51.420 --> 00:49:53.730
but he always has a
fully specific strategy

00:49:53.730 --> 00:49:56.400
in mind for all his current
and future decisions.

00:49:56.400 --> 00:49:58.020
So people-- the
smoker might sort of

00:49:58.020 --> 00:49:59.940
have some issues
with present bias,

00:49:59.940 --> 00:50:02.400
or some wrong beliefs in
some ways or the other.

00:50:02.400 --> 00:50:04.440
But the utility
function was always

00:50:04.440 --> 00:50:08.160
assumed to be fixed and known.

00:50:08.160 --> 00:50:10.510
Now we're going to
deviate from that.

00:50:10.510 --> 00:50:14.610
If you think about it like,
what are your preferences,

00:50:14.610 --> 00:50:16.980
and what do you like and
what do you not like--

00:50:16.980 --> 00:50:20.610
in many cases, we
actually don't know.

00:50:20.610 --> 00:50:23.640
And in many cases,
people sort of just

00:50:23.640 --> 00:50:26.560
make things up as they go along.

00:50:26.560 --> 00:50:29.090
So that's to say, like,
people are essentially

00:50:29.090 --> 00:50:30.630
making some choices.

00:50:30.630 --> 00:50:32.795
And this one of the
examples that I showed you

00:50:32.795 --> 00:50:34.170
from earlier--
the three examples

00:50:34.170 --> 00:50:36.780
that I showed you first,
where people are just

00:50:36.780 --> 00:50:39.127
induced to make certain
choices one way or the other,

00:50:39.127 --> 00:50:40.710
depending on their
choice environment,

00:50:40.710 --> 00:50:42.210
depending on their
social influence.

00:50:42.210 --> 00:50:45.480
In Tom Sawyer's case,
what he tells his friend.

00:50:45.480 --> 00:50:48.420
His friend is now willing to
pay to paint a fence, which

00:50:48.420 --> 00:50:49.840
actually is a punishment.

00:50:49.840 --> 00:50:54.730
So people's preferences
seem inherently malleable.

00:50:54.730 --> 00:50:58.890
And so that's what this paper
about coherent arbitrariness

00:50:58.890 --> 00:51:01.180
is very much about.

00:51:01.180 --> 00:51:07.050
And so if you think about
it, in some ways, almost all

00:51:07.050 --> 00:51:11.730
of economics, and any class that
you have taken, the professor--

00:51:11.730 --> 00:51:15.540
or in any book, people will
write down the utility function

00:51:15.540 --> 00:51:17.980
as if that's some truth that
we know about the world.

00:51:17.980 --> 00:51:20.610
And this is what the
utility function looks like.

00:51:20.610 --> 00:51:22.658
But if you think about
it, in many cases,

00:51:22.658 --> 00:51:24.450
actually, we don't know
what is the utility

00:51:24.450 --> 00:51:24.930
function [INAUDIBLE].

00:51:24.930 --> 00:51:26.597
If somebody asked
[INAUDIBLE] particular

00:51:26.597 --> 00:51:30.240
about new goods, who knows what
our preferences actually are.

00:51:30.240 --> 00:51:33.700
So for example, if you say,
you're trying to buy a monitor,

00:51:33.700 --> 00:51:36.120
and there's like a 30 inch
monitor versus a 24 inch

00:51:36.120 --> 00:51:40.140
monitor, who knows how much
you're willing to pay for that.

00:51:40.140 --> 00:51:43.770
Maybe $50, maybe $100,
maybe $20, maybe nothing.

00:51:43.770 --> 00:51:47.460
It's very hard to sort
of figure this out.

00:51:47.460 --> 00:51:50.190
And sort of when asked to
make these kinds of decisions,

00:51:50.190 --> 00:51:52.877
people tend to sort of
construct their preferences

00:51:52.877 --> 00:51:53.460
on this stuff.

00:51:53.460 --> 00:51:56.010
They essentially make stuff up.

00:51:56.010 --> 00:51:59.167
And so because of
that, because people

00:51:59.167 --> 00:52:00.750
are sort of fundamentally
in some ways

00:52:00.750 --> 00:52:04.980
unsure about their
preferences, this construction

00:52:04.980 --> 00:52:08.070
of their valuation is very
much easily manipulated

00:52:08.070 --> 00:52:12.600
by, often, cues that should
be really irrelevant.

00:52:12.600 --> 00:52:16.470
And so one example that I
showed you a little bit--

00:52:16.470 --> 00:52:19.560
I think in the second
class, or from the survey,

00:52:19.560 --> 00:52:20.910
is what's called anchoring.

00:52:20.910 --> 00:52:23.795
This is also in the Ariely
paper, which essentially

00:52:23.795 --> 00:52:25.170
is, what they were
doing is, they

00:52:25.170 --> 00:52:30.420
were asking people
about their willingness

00:52:30.420 --> 00:52:32.460
to [INAUDIBLE] their
Social Security number.

00:52:32.460 --> 00:52:34.043
And then afterwards,
their willingness

00:52:34.043 --> 00:52:38.310
to pay for different things,
about like, wine, design books,

00:52:38.310 --> 00:52:39.735
chocolates, and so on.

00:52:39.735 --> 00:52:41.610
Notice that these are
somewhat unusual things

00:52:41.610 --> 00:52:44.070
that people wouldn't
necessarily buy every day.

00:52:44.070 --> 00:52:45.930
And essentially what
you see is people

00:52:45.930 --> 00:52:48.690
who are in the highest quintile
of their Social Security

00:52:48.690 --> 00:52:49.350
number--

00:52:49.350 --> 00:52:51.960
this is like year number five--

00:52:51.960 --> 00:52:53.760
have much higher
willingness to pay.

00:52:53.760 --> 00:52:55.590
And this is also all
incentive-compatible

00:52:55.590 --> 00:52:57.840
[INAUDIBLE] methods
essentially of using

00:52:57.840 --> 00:53:00.300
some form of BDM methods.

00:53:00.300 --> 00:53:03.750
People's willingness to pay is
way higher when their Social

00:53:03.750 --> 00:53:04.890
Security number--

00:53:04.890 --> 00:53:08.460
the last digits of their Social
Security number are higher.

00:53:08.460 --> 00:53:10.290
Now of course, that
shouldn't be the case.

00:53:10.290 --> 00:53:13.410
Your Social Security number has
nothing to do with your tastes

00:53:13.410 --> 00:53:14.700
for wine.

00:53:14.700 --> 00:53:17.280
Because that's [INAUDIBLE]
explicitly random.

00:53:17.280 --> 00:53:20.070
Yet, people are
easily manipulable.

00:53:20.070 --> 00:53:23.610
And the difference in sort of
valuations is like, [? huge. ?]

00:53:23.610 --> 00:53:26.340
If you happen to have a
low Social Security number,

00:53:26.340 --> 00:53:33.050
or the last digit are low, then
you are willing to pay $11.73.

00:53:33.050 --> 00:53:35.900
If it's high, you're willing
to pay like three times or even

00:53:35.900 --> 00:53:37.250
more as much.

00:53:37.250 --> 00:53:41.630
So the huge differences show
that people are pretty easily

00:53:41.630 --> 00:53:42.620
manipulable.

00:53:42.620 --> 00:53:45.580
Now notice that these
questions that I asked here

00:53:45.580 --> 00:53:49.040
are sort of somewhat
unusual sort of items.

00:53:49.040 --> 00:53:50.540
This is not asking
like how much are

00:53:50.540 --> 00:53:52.490
you willing to pay for
food at the food truck,

00:53:52.490 --> 00:53:57.660
or whatever stuff that you
do every day, or for a pizza.

00:53:57.660 --> 00:53:59.780
Because there, people
kind of know already

00:53:59.780 --> 00:54:01.200
how much they're paying anyway.

00:54:01.200 --> 00:54:03.090
And if I just ask you
something else that's

00:54:03.090 --> 00:54:05.090
different from the market
price, people probably

00:54:05.090 --> 00:54:09.650
would not want to be
paying like $30 for it,

00:54:09.650 --> 00:54:14.010
even if their Social Security
number tends to be quite high.

00:54:14.010 --> 00:54:17.120
Now, Ariely et al go
further with this.

00:54:17.120 --> 00:54:21.413
In some sense they have sort
of more radical deviations.

00:54:21.413 --> 00:54:23.330
So they elicit people's
willingness to accept.

00:54:23.330 --> 00:54:26.450
And this is how
much they have to be

00:54:26.450 --> 00:54:29.540
paid to endure an
unpleasant sound

00:54:29.540 --> 00:54:32.940
for a different length of time.

00:54:32.940 --> 00:54:36.800
Now, why did they pick
like an unpleasant sound?

00:54:36.800 --> 00:54:38.630
In part, because they
could provide people

00:54:38.630 --> 00:54:39.890
like a sample of it.

00:54:39.890 --> 00:54:43.040
In part because people don't
have any experience with that.

00:54:43.040 --> 00:54:45.330
It's completely unclear
how much you are

00:54:45.330 --> 00:54:47.090
supposed to be willing to pay--

00:54:47.090 --> 00:54:50.060
are willing to accept
for listening to a sound.

00:54:50.060 --> 00:54:52.103
So there's no price or
market price for it.

00:54:52.103 --> 00:54:54.020
And people sort of had
to sort of in some ways

00:54:54.020 --> 00:54:56.250
rely on their own
preferences for that.

00:54:56.250 --> 00:54:58.800
And then they know it's very
easy to change the quantity.

00:54:58.800 --> 00:55:01.320
You can do like 10 seconds,
30 seconds, 50 seconds.

00:55:01.320 --> 00:55:05.320
It's very easy to
manipulate that.

00:55:05.320 --> 00:55:07.030
Now what is the procedure?

00:55:07.030 --> 00:55:12.320
Subjects we're listening to a
30 second sample of the noise.

00:55:12.320 --> 00:55:16.390
Then there were answering
whether they hypothetically

00:55:16.390 --> 00:55:19.210
would be willing to listen
to the noise for another 30

00:55:19.210 --> 00:55:20.970
seconds for x cents.

00:55:20.970 --> 00:55:22.720
And then they were
asked their willingness

00:55:22.720 --> 00:55:27.700
to accept for 10, 30 and
60 seconds afterwards.

00:55:27.700 --> 00:55:32.170
Notice that number two is
only a hypothetical question,

00:55:32.170 --> 00:55:35.870
and is essentially entirely
irrelevant for number three.

00:55:35.870 --> 00:55:36.370
Sorry.

00:55:36.370 --> 00:55:39.040
This is supposed to say
number three, not number one.

00:55:39.040 --> 00:55:42.200
So number two here is like
completely irrelevant.

00:55:42.200 --> 00:55:45.100
This should really have
no effect whatsoever

00:55:45.100 --> 00:55:48.910
in affecting your willingness
to accept for the 10, 30 and 60

00:55:48.910 --> 00:55:49.670
seconds.

00:55:49.670 --> 00:55:51.580
Again, this is just a
hypothetical question

00:55:51.580 --> 00:55:53.890
that really does not
matter, and is not

00:55:53.890 --> 00:55:57.440
going to be implemented at all.

00:55:57.440 --> 00:55:59.050
And then the
experimental variation

00:55:59.050 --> 00:56:04.390
here is that x was
varied across subjects.

00:56:04.390 --> 00:56:05.650
Now, why would x matter?

00:56:05.650 --> 00:56:07.310
What's going on here?

00:56:07.310 --> 00:56:10.380
What's the experiment
trying to do?

00:56:10.380 --> 00:56:13.592
People are very unsure about
what their valuation is.

00:56:13.592 --> 00:56:15.300
So now what they do
is, essentially, they

00:56:15.300 --> 00:56:20.580
use the x-- the amount that's
offered as like an anchor.

00:56:20.580 --> 00:56:24.570
The same way as in, usually in
markets, when you go to a store

00:56:24.570 --> 00:56:27.000
and look at how much
do certain things

00:56:27.000 --> 00:56:29.700
cost, and you don't know the
quality of the underlying

00:56:29.700 --> 00:56:33.060
items, often people sort of
try to infer quality or lack

00:56:33.060 --> 00:56:35.190
of quality from the price.

00:56:35.190 --> 00:56:37.380
And if we sort of say,
if I'm telling you

00:56:37.380 --> 00:56:39.210
I'm going to pay you
like $100 for this,

00:56:39.210 --> 00:56:40.630
it must be really,
really painful.

00:56:40.630 --> 00:56:43.440
So people try to sort of
infer in some ways something

00:56:43.440 --> 00:56:44.340
from that.

00:56:44.340 --> 00:56:47.920
Now notice that this is
explicitly hypothetical.

00:56:47.920 --> 00:56:48.420
[SOUND]

00:56:48.420 --> 00:56:48.990
And so on.

00:56:48.990 --> 00:56:50.700
And so really it
shouldn't matter.

00:56:50.700 --> 00:56:52.200
If people were sure about like--

00:56:52.200 --> 00:56:53.580
I just played you the sound.

00:56:53.580 --> 00:56:55.580
So you shouldn't be able
to tell me how much you

00:56:55.580 --> 00:56:56.620
like it or dislike it.

00:56:56.620 --> 00:56:58.240
So this should
really be irrelevant.

00:56:58.240 --> 00:57:00.490
But people seem to sort of
essentially just not really

00:57:00.490 --> 00:57:02.640
know what's appropriate,
or how much they're

00:57:02.640 --> 00:57:05.820
willing to accept
or not to do that.

00:57:05.820 --> 00:57:08.600
And x essentially is sort of
then anchoring [INAUDIBLE]..

00:57:08.600 --> 00:57:09.100
OK.

00:57:09.100 --> 00:57:11.560
So now we get these
somewhat messy graphs.

00:57:11.560 --> 00:57:15.410
Can somebody explain to
me what this graph shows?

00:57:15.410 --> 00:57:17.920
What do we find?

00:57:17.920 --> 00:57:20.950
These are [INAUDIBLE]
certain subjects.

00:57:20.950 --> 00:57:24.080
And then there's like
10 seconds, 30 seconds,

00:57:24.080 --> 00:57:25.420
and 60 seconds here.

00:57:25.420 --> 00:57:26.950
So people are always--

00:57:26.950 --> 00:57:29.470
for each of these lines,
on average, at least,

00:57:29.470 --> 00:57:31.600
willingness to accept
goes up, in the way

00:57:31.600 --> 00:57:34.250
you expect, in the sense
of, for more time, people

00:57:34.250 --> 00:57:35.890
are asking for more.

00:57:35.890 --> 00:57:37.760
That's very reasonable.

00:57:37.760 --> 00:57:40.360
So 30 seconds, presumably,
are worse than 10 seconds.

00:57:40.360 --> 00:57:43.240
And 60 seconds are
worse than 10 seconds.

00:57:43.240 --> 00:57:46.120
But the levels seem to
be completely arbitrary.

00:57:46.120 --> 00:57:48.430
Essentially, giving
people a high anchor

00:57:48.430 --> 00:57:52.570
increases the levels by
a lot for each duration.

00:57:52.570 --> 00:57:55.130
And giving people a low anchor
decreases their willingness

00:57:55.130 --> 00:57:57.255
to pay at least a little
bit compared to no anchor,

00:57:57.255 --> 00:57:59.890
and surely compared to
the [? high anchor. ?]

00:57:59.890 --> 00:58:00.702
That is to say--

00:58:00.702 --> 00:58:02.660
and this is sort of where
this term comes from,

00:58:02.660 --> 00:58:06.130
coherent arbitrariness,
which is, essentially, people

00:58:06.130 --> 00:58:08.230
seem to be coherent
in the sense of like,

00:58:08.230 --> 00:58:11.890
once you fix a certain
level, based on that level,

00:58:11.890 --> 00:58:14.570
if you ask them, OK, if you
tell me about 10 seconds

00:58:14.570 --> 00:58:17.020
and I ask you about 30
seconds, about 60 seconds,

00:58:17.020 --> 00:58:19.900
this demand curve or
supply curve, if you want,

00:58:19.900 --> 00:58:21.400
looks pretty reasonable.

00:58:21.400 --> 00:58:23.620
But the actual
level to start with

00:58:23.620 --> 00:58:27.520
is completely arbitrary,
because I can essentially

00:58:27.520 --> 00:58:29.380
manipulate you by quite a bit.

00:58:29.380 --> 00:58:32.090
Look at the differences
in magnitudes.

00:58:32.090 --> 00:58:33.460
This is like 50 versus 30.

00:58:33.460 --> 00:58:34.840
That's almost like
twice as much,

00:58:34.840 --> 00:58:37.937
depending on just this
[INAUDIBLE] anchor.

00:58:37.937 --> 00:58:39.770
And there's sort of
different rounds in that

00:58:39.770 --> 00:58:41.853
that you can sort of do
this one way or the other.

00:58:41.853 --> 00:58:45.010
And this is like essentially
increasing or decreasing,

00:58:45.010 --> 00:58:48.360
starting in 10 seconds
versus-- and then going up.

00:58:48.360 --> 00:58:52.170
People's willingness to accept
goes up the longer it is.

00:58:52.170 --> 00:58:54.420
Or if you start with
60 seconds and go down,

00:58:54.420 --> 00:58:56.220
essentially, people's
willingness to accept

00:58:56.220 --> 00:58:57.880
goes down the shorter it is.

00:58:57.880 --> 00:58:59.800
So the direction is
very much coherent.

00:58:59.800 --> 00:59:01.770
The direction of
essentially this sort

00:59:01.770 --> 00:59:05.340
of change in terms of
relative to duration

00:59:05.340 --> 00:59:06.720
is very much coherent.

00:59:06.720 --> 00:59:09.930
The level seems
very much arbitrary.

00:59:09.930 --> 00:59:14.370
And so this is where the
title of the paper comes from.

00:59:14.370 --> 00:59:17.880
Arbitrariness, essentially,
people's willingness to accept

00:59:17.880 --> 00:59:20.390
depended strongly on x.

00:59:20.390 --> 00:59:22.140
For x equals 50, the
willingness to accept

00:59:22.140 --> 00:59:25.350
is like about $0.59 on average.

00:59:25.350 --> 00:59:29.040
For x equals 10,
it's only $0.40.

00:59:29.040 --> 00:59:32.760
But it's also coherent
in very sensible ways.

00:59:32.760 --> 00:59:34.770
Their willingness
to accept are highly

00:59:34.770 --> 00:59:36.390
sensitive to the
duration in the--

00:59:36.390 --> 00:59:38.670
very much in the
expected direction.

00:59:38.670 --> 00:59:43.640
Longer is always perceived
to be more painful.

00:59:43.640 --> 00:59:44.180
OK.

00:59:44.180 --> 00:59:45.430
So how do we think about this?

00:59:45.430 --> 00:59:50.612
Well, so one, preferences can
be influenced by relevant cues.

00:59:50.612 --> 00:59:52.820
For instance, [INAUDIBLE]
arbitrary initial question,

00:59:52.820 --> 00:59:54.862
or the Social Security
number, or whatever that's

00:59:54.862 --> 00:59:56.490
being elicited to start with.

00:59:56.490 --> 01:00:00.440
But once people have
stated a preference,

01:00:00.440 --> 01:00:03.080
related preferences,
they're like, essentially,

01:00:03.080 --> 01:00:07.010
surrounding preferences are
consistent in a sense of like,

01:00:07.010 --> 01:00:10.380
if you think listening
to the sound is painful,

01:00:10.380 --> 01:00:14.910
asking you to listen to it twice
as long will be more painful.

01:00:14.910 --> 01:00:16.460
Therefore, I have
to pay you more

01:00:16.460 --> 01:00:21.860
to do that compared to what
you said in the first place.

01:00:21.860 --> 01:00:23.350
Any questions or is that clear?

01:00:25.980 --> 01:00:27.490
OK.

01:00:27.490 --> 01:00:29.530
So now there are
some concerns about x

01:00:29.530 --> 01:00:32.800
might be viewed as a hint
from the experimenters,

01:00:32.800 --> 01:00:34.390
how bad the sound is.

01:00:34.390 --> 01:00:36.730
But you know, people just
listened to the sound

01:00:36.730 --> 01:00:38.200
for 30 seconds to start with.

01:00:38.200 --> 01:00:40.100
Like, I gave you the
sound for 30 seconds.

01:00:40.100 --> 01:00:41.930
I told you exactly what it is.

01:00:41.930 --> 01:00:47.120
So in some sense, that seems
like not really a concern

01:00:47.120 --> 01:00:47.630
anyway.

01:00:47.630 --> 01:00:49.250
But there's also-- the very
end of this experiment,

01:00:49.250 --> 01:00:51.440
where x was generated
by the last two digits

01:00:51.440 --> 01:00:55.130
of the subject's
Social Security number,

01:00:55.130 --> 01:00:58.190
and explicitly so, and still
there was a correlation between

01:00:58.190 --> 01:01:00.680
essentially x and subjects'
willingness to accept,

01:01:00.680 --> 01:01:03.740
which really shouldn't
be in any other--

01:01:03.740 --> 01:01:06.070
in the absence of such
anchoring effects.

01:01:06.070 --> 01:01:07.280
Are stakes too low?

01:01:07.280 --> 01:01:09.740
I told you about like,
$0.30, and $0.50.

01:01:09.740 --> 01:01:11.592
Maybe people just
don't care, and so on.

01:01:11.592 --> 01:01:13.550
But there's also another
experiment [INAUDIBLE]

01:01:13.550 --> 01:01:14.390
10-fold stakes.

01:01:14.390 --> 01:01:16.350
And they got essentially
the same results.

01:01:20.110 --> 01:01:22.950
So this evidence now is very
much consistent with the idea

01:01:22.950 --> 01:01:25.380
that subjects are searching
for their preferences.

01:01:25.380 --> 01:01:27.240
They don't quite know
their true willingness

01:01:27.240 --> 01:01:30.940
to accept for the sound, and
this is essentially arbitrary.

01:01:30.940 --> 01:01:33.000
But they know their
willingness to accept

01:01:33.000 --> 01:01:35.370
should relate to each
other in a coherent way.

01:01:35.370 --> 01:01:38.310
And in fact, once you fix
the level in some ways,

01:01:38.310 --> 01:01:42.150
they are in fact coherent,
or essentially they're

01:01:42.150 --> 01:01:43.320
sort of making sense.

01:01:43.320 --> 01:01:45.240
They're [INAUDIBLE]
consistent with that,

01:01:45.240 --> 01:01:48.060
once you fix a certain level.

01:01:48.060 --> 01:01:52.930
Now this is now getting
back to Tom Sawyer.

01:01:52.930 --> 01:01:56.300
Now, the paper goes-- so this
essentially is sort of saying,

01:01:56.300 --> 01:01:59.630
your level of willingness
to accept for a certain good

01:01:59.630 --> 01:02:02.510
that's unpleasant is malleable.

01:02:02.510 --> 01:02:04.130
Essentially, I
can manipulate you

01:02:04.130 --> 01:02:08.960
to ask for a lower
or a higher price,

01:02:08.960 --> 01:02:11.450
depending on some irrelevant
question that I ask you,

01:02:11.450 --> 01:02:14.990
for a given thing that is
perceived to be unpleasant.

01:02:14.990 --> 01:02:19.010
Similarly, I can manipulate
you to pay more or less

01:02:19.010 --> 01:02:21.530
for a certain good
that's perceived

01:02:21.530 --> 01:02:23.750
to be a good, something
that you want.

01:02:23.750 --> 01:02:25.880
So the stuff on the
Social Security number

01:02:25.880 --> 01:02:29.130
that I showed you
previously, going back,

01:02:29.130 --> 01:02:30.390
these are all kind of things--

01:02:30.390 --> 01:02:32.760
presumably, there's some
use for these things.

01:02:32.760 --> 01:02:35.550
Belgian chocolates are
supposed to be delicious.

01:02:35.550 --> 01:02:37.230
Red wine, even if
you don't like wine,

01:02:37.230 --> 01:02:39.190
you can sell it to
somebody else or something.

01:02:39.190 --> 01:02:42.600
These are all things that
seem to be worth willing

01:02:42.600 --> 01:02:45.210
to pay for something.

01:02:45.210 --> 01:02:47.460
And now essentially,
given that you

01:02:47.460 --> 01:02:50.430
are willing to pay something,
or some positive amount for it,

01:02:50.430 --> 01:02:53.580
I can manipulate now-- or the
experimenters, in this case,

01:02:53.580 --> 01:02:58.230
can manipulate people into
higher or lower willingness

01:02:58.230 --> 01:02:59.690
to pay.

01:02:59.690 --> 01:03:02.650
And so that's true for
positive or negative things.

01:03:02.650 --> 01:03:05.680
Now, what's amazing about
the Tom Sawyer story,

01:03:05.680 --> 01:03:08.540
however, isn't the--

01:03:08.540 --> 01:03:09.820
it's ain't that work?

01:03:09.820 --> 01:03:12.340
It's sort of like, that's the
expression he sort of says

01:03:12.340 --> 01:03:13.870
towards the end.

01:03:13.870 --> 01:03:16.690
The amazing part
here is that not only

01:03:16.690 --> 01:03:20.770
is Tom Sawyer able to manipulate
his friend's willingness

01:03:20.770 --> 01:03:25.090
to accept, or willingness
to pay in one direction or--

01:03:25.090 --> 01:03:27.670
increase or decrease
it, but he's even able

01:03:27.670 --> 01:03:30.155
to flip it from
something that he

01:03:30.155 --> 01:03:32.530
hates doing, that's essentially
something that you really

01:03:32.530 --> 01:03:34.510
do not want to do, where
essentially one has

01:03:34.510 --> 01:03:37.840
to pay somebody to do it,
but instead, his friend

01:03:37.840 --> 01:03:41.135
is willing to pay Tom Sawyer
for being able to do it, right?

01:03:41.135 --> 01:03:42.760
The friend, now,
instead of like having

01:03:42.760 --> 01:03:45.260
the friend to pay for it, you
can say, I have to pay you $10

01:03:45.260 --> 01:03:48.190
to do it, the friend is
very happy to say, Tom,

01:03:48.190 --> 01:03:49.838
I'm going to pay
you some amount.

01:03:49.838 --> 01:03:51.880
So I think he gives him
some apples or some candy

01:03:51.880 --> 01:03:55.570
or whatever, so that he
can actually do it himself.

01:03:55.570 --> 01:03:57.670
So essentially, what the
manipulation here does

01:03:57.670 --> 01:04:00.280
is not only changing
the level for something

01:04:00.280 --> 01:04:02.890
that's good or bad, but
it's flipping the sign

01:04:02.890 --> 01:04:06.130
from willingness to accept
to willingness to pay, which

01:04:06.130 --> 01:04:08.387
is a more radical deviation.

01:04:08.387 --> 01:04:10.470
You might sort of think
we know what's good for us

01:04:10.470 --> 01:04:11.630
and what's bad for us.

01:04:11.630 --> 01:04:13.630
But it seems to be that
what Tom Sawyer is doing

01:04:13.630 --> 01:04:17.764
is manipulating sort of the
social perception of the item

01:04:17.764 --> 01:04:20.380
or the activity, and that
sort of flips essentially

01:04:20.380 --> 01:04:22.930
the sign of the item.

01:04:22.930 --> 01:04:27.880
So then Ariely now does that
in a very beautiful experiment

01:04:27.880 --> 01:04:28.420
in class.

01:04:28.420 --> 01:04:31.180
And I always have wanted
to sort of replicate this,

01:04:31.180 --> 01:04:34.000
but I'm not sure I should.

01:04:34.000 --> 01:04:37.510
So what he does is, or at
the time, this is with MBAs--

01:04:37.510 --> 01:04:42.340
where he did a poetry
reading from Walt Whitman's

01:04:42.340 --> 01:04:44.020
Leaves of Grass.

01:04:44.020 --> 01:04:47.725
And there, he has an experiment,
where half of the class

01:04:47.725 --> 01:04:49.588
is asked hypothetically--

01:04:49.588 --> 01:04:51.130
again, hypothetically,
whether they'd

01:04:51.130 --> 01:04:53.590
be willing to pay $10
to listen to Ariely

01:04:53.590 --> 01:04:55.450
recite poetry for 10 minutes.

01:04:55.450 --> 01:04:59.440
So he's like, OK, in
class, some of you

01:04:59.440 --> 01:05:01.120
are able to listen to this.

01:05:01.120 --> 01:05:02.320
But it could be like--

01:05:02.320 --> 01:05:05.230
I think it's outside of class,
because otherwise [INAUDIBLE]..

01:05:05.230 --> 01:05:09.460
And are you willing to pay
$10 for attending this poetry

01:05:09.460 --> 01:05:10.220
reading?

01:05:10.220 --> 01:05:12.220
And I think actually,
Ariely is a very good sort

01:05:12.220 --> 01:05:13.120
of story reader.

01:05:13.120 --> 01:05:15.040
So it might actually
be fun to listen to it.

01:05:15.040 --> 01:05:17.050
But some of you know
to ask hypothetically,

01:05:17.050 --> 01:05:19.690
again, are you willing
to pay $10 for it?

01:05:19.690 --> 01:05:21.700
The other half then are
asked hypothetically

01:05:21.700 --> 01:05:25.660
are they're willing to accept
$10 to listen to Ariely recite

01:05:25.660 --> 01:05:26.920
the poetry for 10 minutes.

01:05:26.920 --> 01:05:30.190
That's to say, it sounds
like it's pretty painful

01:05:30.190 --> 01:05:31.570
to listen to Ariely.

01:05:31.570 --> 01:05:33.430
He's going to pay
you $10 to do it.

01:05:33.430 --> 01:05:36.920
And are you willing to do that?

01:05:36.920 --> 01:05:39.110
Notice that here is
only hypothetical.

01:05:39.110 --> 01:05:40.700
These are all
hypothetical choices

01:05:40.700 --> 01:05:42.455
that will not be implemented.

01:05:42.455 --> 01:05:44.330
So these hypothetical
questions should really

01:05:44.330 --> 01:05:47.810
have no effect whatsoever
on what's asked afterwards.

01:05:47.810 --> 01:05:50.510
Because afterwards, people
are in fact indicating

01:05:50.510 --> 01:05:53.210
their monetary valuations
for one, three or six

01:05:53.210 --> 01:05:56.230
minutes of poetry reading.

01:05:56.230 --> 01:05:59.340
Now you can already guess
what's happening here is here,

01:05:59.340 --> 01:06:00.780
the first condition
is essentially

01:06:00.780 --> 01:06:02.460
asking for people's
willingness to pay.

01:06:02.460 --> 01:06:03.930
That's essentially
selling the item

01:06:03.930 --> 01:06:06.180
as like a good, that's
something that you really want.

01:06:06.180 --> 01:06:09.060
And you're sort of
essentially encouraging people

01:06:09.060 --> 01:06:11.880
to say, well, you
should be willing to pay

01:06:11.880 --> 01:06:16.440
something for it, even as much
as like $10 for 10 minutes.

01:06:16.440 --> 01:06:18.270
And that'll [INAUDIBLE]
encourage people,

01:06:18.270 --> 01:06:22.140
since they don't quite know,
listening to your professor

01:06:22.140 --> 01:06:24.480
is like doing some weird
poetry could be really great,

01:06:24.480 --> 01:06:27.060
or could be really bad,
they don't quite know.

01:06:27.060 --> 01:06:30.270
But here, essentially, they're
manipulated into paying for it,

01:06:30.270 --> 01:06:34.440
while here they're manipulated
into being paid for it,

01:06:34.440 --> 01:06:37.890
or asking for money
to be paid for it.

01:06:37.890 --> 01:06:43.765
So now, what they then get
is their willingness to pay,

01:06:43.765 --> 01:06:45.390
and their willingness
to pay conditions

01:06:45.390 --> 01:06:47.160
when the first
condition here, where

01:06:47.160 --> 01:06:51.870
they are given the hypothetical
$10 for 10 minutes.

01:06:51.870 --> 01:06:56.260
Here, the willingness to
pay goes up and is positive.

01:06:56.260 --> 01:06:59.110
And here, it's
negative and goes down.

01:06:59.110 --> 01:07:03.100
So what we see essentially
is that not only is it

01:07:03.100 --> 01:07:06.760
the case that when you make it
essential, when you ask people

01:07:06.760 --> 01:07:08.200
how much are they
willing to pay,

01:07:08.200 --> 01:07:10.890
people are in fact afterwards
willing to pay for it.

01:07:10.890 --> 01:07:13.120
But notice that everybody
could have just said zero

01:07:13.120 --> 01:07:14.650
if they didn't want to pay.

01:07:14.650 --> 01:07:18.190
But in addition,
once you sort of

01:07:18.190 --> 01:07:21.490
fix their willingness
to pay, the demand curve

01:07:21.490 --> 01:07:24.790
is very much
sensible in a sense.

01:07:24.790 --> 01:07:27.195
Once you're willing to pay
some amount for one minute,

01:07:27.195 --> 01:07:28.570
you're going to
be willing to pay

01:07:28.570 --> 01:07:31.240
more money for more minutes.

01:07:31.240 --> 01:07:33.280
Essentially, once we
define this good to be

01:07:33.280 --> 01:07:35.830
a good for you, or like
this item or this activity

01:07:35.830 --> 01:07:37.570
to be like a good
thing for you, people

01:07:37.570 --> 01:07:39.190
say more must be
better, and sort of

01:07:39.190 --> 01:07:42.020
are willing to pay
more for more minutes.

01:07:42.020 --> 01:07:47.320
On the other hand, once the item
is determined as like a bad,

01:07:47.320 --> 01:07:49.450
in the sense of this is
really a bad activity.

01:07:49.450 --> 01:07:52.000
Listening to your
professor reciting

01:07:52.000 --> 01:07:55.993
poetry is really awful, and
you have to be paid for that,

01:07:55.993 --> 01:07:57.910
then they sort of say,
well one minute is bad,

01:07:57.910 --> 01:07:59.680
but like six minutes
surely is really bad.

01:07:59.680 --> 01:08:02.410
And you have to pay me a lot
more for six minutes than

01:08:02.410 --> 01:08:04.270
for one minute.

01:08:04.270 --> 01:08:05.920
And that's essentially
saying subjects

01:08:05.920 --> 01:08:09.430
don't know whether this
reading is good or bad.

01:08:09.430 --> 01:08:11.180
But they do know,
essentially, either way,

01:08:11.180 --> 01:08:13.180
more requires more money.

01:08:13.180 --> 01:08:16.000
And that's essentially exactly
the coherent arbitrariness,

01:08:16.000 --> 01:08:19.180
where essentially,
the level or even like

01:08:19.180 --> 01:08:20.680
the sign is really unclear.

01:08:20.680 --> 01:08:22.180
But once the sign
is sort of fixed,

01:08:22.180 --> 01:08:26.340
people behave in
pretty reasonable ways.

01:08:26.340 --> 01:08:27.670
I actually don't remember.

01:08:27.670 --> 01:08:29.406
I should look it up--

01:08:29.406 --> 01:08:33.569
surely in the paper, whether
it's possible in this condition

01:08:33.569 --> 01:08:34.439
to actually have--

01:08:34.439 --> 01:08:36.147
[INAUDIBLE] so the
[INAUDIBLE] experiment

01:08:36.147 --> 01:08:39.060
would surely always ask
you a whole range where

01:08:39.060 --> 01:08:43.540
you can always say, between
say, minus six or minus 10,

01:08:43.540 --> 01:08:44.790
and plus 10.

01:08:44.790 --> 01:08:47.569
Now tell me your willingness
to accept versus willingness

01:08:47.569 --> 01:08:48.069
to pay.

01:08:48.069 --> 01:08:49.944
So if it's positive,
it's willingness to pay.

01:08:49.944 --> 01:08:52.020
If it's negative, it's
willingness to accept.

01:08:52.020 --> 01:08:53.189
I think that's what's--

01:08:53.189 --> 01:08:54.689
so surely, that's
how the experiment

01:08:54.689 --> 01:08:56.010
should have been done.

01:08:56.010 --> 01:08:58.838
I would like to think
and hope that's also

01:08:58.838 --> 01:08:59.880
how it was actually done.

01:08:59.880 --> 01:09:01.080
I don't remember.

01:09:01.080 --> 01:09:02.543
This is quite a while ago.

01:09:02.543 --> 01:09:04.210
But it's really in
the paper, so I don't

01:09:04.210 --> 01:09:05.609
know if anybody remembers.

01:09:05.609 --> 01:09:07.312
But I think it's exactly right.

01:09:07.312 --> 01:09:09.270
In some sense, the
experiment is very much sort

01:09:09.270 --> 01:09:11.340
of manipulating people in
the way of saying like,

01:09:11.340 --> 01:09:16.319
well, so the prompt here really
gets you into like, OK, you're

01:09:16.319 --> 01:09:17.463
supposed to pay for this.

01:09:17.463 --> 01:09:19.380
Now it's kind of a little
bit of a weird thing

01:09:19.380 --> 01:09:21.630
to say I'm willing to accept.

01:09:21.630 --> 01:09:26.189
Professor says, are you
willing to pay $10 for it?

01:09:26.189 --> 01:09:27.689
And then you say,
well, how much are

01:09:27.689 --> 01:09:30.390
you willing to pay
for like six minutes?

01:09:30.390 --> 01:09:32.580
And then you say, well you
have to pay me like $5,

01:09:32.580 --> 01:09:35.340
is a bit of a weird thing to
say and the other way around.

01:09:35.340 --> 01:09:38.390
So it's very much sort
of like I think a set up

01:09:38.390 --> 01:09:40.950
to sort of generate the effects
that they're looking for.

01:09:40.950 --> 01:09:43.719
I think the underlying-- so I
think the underlying essence--

01:09:43.719 --> 01:09:46.399
and there are some other
experiments that show somewhat

01:09:46.399 --> 01:09:47.580
similar results--

01:09:47.580 --> 01:09:50.600
the underlying essence is
right that particularly

01:09:50.600 --> 01:09:53.240
for unknown goods,
people's preferences

01:09:53.240 --> 01:09:54.770
are very much malleable.

01:09:54.770 --> 01:09:57.182
And people just don't
know what they are.

01:09:57.182 --> 01:09:58.640
And there's also
some other things.

01:09:58.640 --> 01:10:01.970
For example, if you think about,
for example, social activities,

01:10:01.970 --> 01:10:05.090
where maybe your friends like
them, or they don't like them,

01:10:05.090 --> 01:10:07.880
and people are very much
like-- and this is exactly

01:10:07.880 --> 01:10:10.490
the Tom Sawyer example, where
people are very much sort

01:10:10.490 --> 01:10:12.020
of malleable.

01:10:12.020 --> 01:10:14.660
And you can vastly
shape what people

01:10:14.660 --> 01:10:18.080
want versus not in many cases.

01:10:18.080 --> 01:10:20.730
And for example, one
example would be like,

01:10:20.730 --> 01:10:23.060
there's some very nice
work by Loewenstein

01:10:23.060 --> 01:10:27.020
and coauthors that
looks at education.

01:10:27.020 --> 01:10:30.730
And sort of, do students
want to study versus not.

01:10:30.730 --> 01:10:34.130
And do you want to be a
nerd versus not in class.

01:10:34.130 --> 01:10:37.550
And so depending very much
on your environment as a kid,

01:10:37.550 --> 01:10:40.310
when you grow up, if
all of your friends

01:10:40.310 --> 01:10:42.830
are sort of essentially not
working hard, and sort of

01:10:42.830 --> 01:10:45.740
want to be cool, and sort
of do not want to study,

01:10:45.740 --> 01:10:48.380
and it's not a cool thing in
your environment to study,

01:10:48.380 --> 01:10:50.450
people might just not want that.

01:10:50.450 --> 01:10:53.742
But on the other hand, if you
have like five nerd friends who

01:10:53.742 --> 01:10:55.700
are all working really
hard, or like, if you're

01:10:55.700 --> 01:10:57.990
running around MIT,
studying is a cool thing.

01:10:57.990 --> 01:11:00.050
And everybody sort
of is working hard.

01:11:00.050 --> 01:11:04.890
And that becomes suddenly very
much like a desirable activity.

01:11:04.890 --> 01:11:06.560
So I think more
generally, while this

01:11:06.560 --> 01:11:08.660
is a bit of a
contrived experiment,

01:11:08.660 --> 01:11:11.900
surely people's
preferences are very much

01:11:11.900 --> 01:11:16.310
malleable through their
social environments,

01:11:16.310 --> 01:11:19.790
and what they
think others think,

01:11:19.790 --> 01:11:21.380
and how they perceive it.

01:11:21.380 --> 01:11:23.035
And that can be
[INAUDIBLE] shaped by--

01:11:26.470 --> 01:11:29.090
manipulated in certain ways,
and potentially affected.

01:11:29.090 --> 01:11:31.520
That could be very much
like a policy angle

01:11:31.520 --> 01:11:33.710
if you wanted to change
people's behavior profoundly

01:11:33.710 --> 01:11:38.340
without spending
much money, in fact.

01:11:38.340 --> 01:11:41.600
Let me-- I think that's
mostly what I have to say.

01:11:41.600 --> 01:11:44.240
Let me summarize, and then
tell you about next week

01:11:44.240 --> 01:11:46.170
for a second.

01:11:46.170 --> 01:11:48.110
So we asked the
question, whether people

01:11:48.110 --> 01:11:49.340
have stable preferences.

01:11:49.340 --> 01:11:50.870
And it seems to me
that people don't

01:11:50.870 --> 01:11:53.450
have clear preferences
for goods and experiences,

01:11:53.450 --> 01:11:56.330
and sort of construct their
preferences on the spot.

01:11:56.330 --> 01:12:00.020
Now, they're influenced
by environmental cues

01:12:00.020 --> 01:12:03.050
that, in a way, that
doesn't necessarily

01:12:03.050 --> 01:12:05.870
reflect the true utility
from the good or experience.

01:12:05.870 --> 01:12:07.610
And that's very much
also what companies

01:12:07.610 --> 01:12:08.527
are doing [INAUDIBLE].

01:12:08.527 --> 01:12:12.230
And sometimes, some items are
made really, really expensive.

01:12:12.230 --> 01:12:13.820
Like, why is this
thing expensive?

01:12:13.820 --> 01:12:16.640
And somehow companies
are able to create

01:12:16.640 --> 01:12:19.580
some fads or some way of making
things desirable by just making

01:12:19.580 --> 01:12:23.095
them expensive and making
people want them in that way.

01:12:23.095 --> 01:12:24.470
That very much
relies on the fact

01:12:24.470 --> 01:12:26.643
that people are in
fact sort of malleable.

01:12:26.643 --> 01:12:28.310
And you can sort of
introspect and think

01:12:28.310 --> 01:12:31.400
about what things in the world
make you generally happy.

01:12:31.400 --> 01:12:32.827
What things do you really like?

01:12:32.827 --> 01:12:34.910
And what things are sort
of more things that like,

01:12:34.910 --> 01:12:36.860
well, other people like
them, and you kind of

01:12:36.860 --> 01:12:39.193
get sort of manipulated, or
you get sort of like tricked

01:12:39.193 --> 01:12:40.920
into doing that.

01:12:40.920 --> 01:12:42.410
And in some sense,
and we'll talk

01:12:42.410 --> 01:12:44.420
about this next week a
little bit about happiness--

01:12:44.420 --> 01:12:46.712
it's important to sort of
try and figure it out and try

01:12:46.712 --> 01:12:49.910
to sort of not perhaps be as
much influenced by others,

01:12:49.910 --> 01:12:53.960
and rather figuring out what
you generally and truly like.

01:12:53.960 --> 01:12:56.390
So there's a nice
series of experiments

01:12:56.390 --> 01:12:59.510
that sort of demonstrate
this coherent arbitrariness,

01:12:59.510 --> 01:13:02.480
in very sort of like
clean variation,

01:13:02.480 --> 01:13:06.080
and somewhat contrived context.

01:13:06.080 --> 01:13:08.900
So you might wonder then, does
it matter in the real world?

01:13:08.900 --> 01:13:11.772
Well it's perhaps less
important in settings

01:13:11.772 --> 01:13:12.980
where people have experience.

01:13:12.980 --> 01:13:14.897
Again, like if I had
done the same experiments

01:13:14.897 --> 01:13:17.488
with like pizza, you
know how much you're

01:13:17.488 --> 01:13:18.530
willing to pay for pizza.

01:13:18.530 --> 01:13:19.613
You know the market price.

01:13:19.613 --> 01:13:21.260
And you know also, and so on.

01:13:21.260 --> 01:13:23.810
And so there, probably,
your willingness to pay

01:13:23.810 --> 01:13:26.990
and your preferences
are very much like set.

01:13:26.990 --> 01:13:29.630
But in some other cases,
in particular like new

01:13:29.630 --> 01:13:32.310
environments,
[INAUDIBLE] sort of new--

01:13:32.310 --> 01:13:34.057
when preferences are shaped--

01:13:34.057 --> 01:13:36.140
for example, think about
maybe back when you first

01:13:36.140 --> 01:13:40.610
started at MIT, sort of
seeing lots of other students,

01:13:40.610 --> 01:13:42.890
when norms are shaped,
people's preferences

01:13:42.890 --> 01:13:46.820
are very much malleable and
can be influenced profoundly.

01:13:46.820 --> 01:13:52.370
Now, there's not so much
like actual field evidence

01:13:52.370 --> 01:13:55.663
in high stakes settings that
are sort of this clean nature.

01:13:55.663 --> 01:13:57.080
Having said that,
the examples of,

01:13:57.080 --> 01:13:58.970
for example,
environmental-- sorry,

01:13:58.970 --> 01:14:01.982
educational choices is perhaps
the most compelling one

01:14:01.982 --> 01:14:03.440
because that's
really a high stakes

01:14:03.440 --> 01:14:05.160
setting that matters a lot.

01:14:05.160 --> 01:14:08.450
But people's preferences
or their choices

01:14:08.450 --> 01:14:10.940
are very much malleable.

01:14:10.940 --> 01:14:14.920
That's all I have to say
on coherent arbitrariness.

01:14:14.920 --> 01:14:19.370
So next time, Monday, we're
going to talk about poverty.

01:14:19.370 --> 01:14:21.895
Please read the
paper by Mani et al.

01:14:21.895 --> 01:14:23.270
And then on
Wednesday we're going

01:14:23.270 --> 01:14:25.460
to talk about happiness
and mental health.

01:14:25.460 --> 01:14:30.290
Now, I promised you
a guest lecturer.

01:14:30.290 --> 01:14:32.990
And so you know, I was
thinking about what's

01:14:32.990 --> 01:14:33.810
a good thing to do.

01:14:33.810 --> 01:14:37.080
And so you may have heard
about this at UC Berkeley.

01:14:37.080 --> 01:14:40.560
They have llamas come to
campus to make students happy,

01:14:40.560 --> 01:14:42.370
and destress them in some ways.

01:14:42.370 --> 01:14:45.623
Now, of course, I can't
bring any real llamas.

01:14:45.623 --> 01:14:47.040
But what I found,
and you may have

01:14:47.040 --> 01:14:48.460
seen this on The Daily Show.

01:14:48.460 --> 01:14:50.210
There's this thing
called Goat To Meeting,

01:14:50.210 --> 01:14:53.280
where you can essentially
get like a goat or a llama

01:14:53.280 --> 01:14:56.040
to come to your
meeting for 10 minutes.

01:14:56.040 --> 01:15:00.600
And I did ask for like
a llama to arrive.

01:15:00.600 --> 01:15:02.340
But there's apparently
a chance that it

01:15:02.340 --> 01:15:03.990
might be a goat or a cow.

01:15:03.990 --> 01:15:08.490
So we'll have a visitor
at the end of the lecture.

01:15:08.490 --> 01:15:14.280
I think I asked for 2:20,
or between 2:20 and 2:30.

01:15:14.280 --> 01:15:17.400
And which we'll have a guest
lecturer that will hopefully

01:15:17.400 --> 01:15:20.640
either tell us about happiness
or maybe make some of you,

01:15:20.640 --> 01:15:23.370
or at least myself, happy.

01:15:23.370 --> 01:15:27.040
That's all I have for today.

01:15:27.040 --> 01:15:30.740
I'm happy to answer any
questions that you might have.