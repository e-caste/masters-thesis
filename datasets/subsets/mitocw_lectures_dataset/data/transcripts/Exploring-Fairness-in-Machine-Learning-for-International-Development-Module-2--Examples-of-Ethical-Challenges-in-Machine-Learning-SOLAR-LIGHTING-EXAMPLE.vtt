WEBVTT

00:00:04.810 --> 00:00:08.177
[MUSIC PLAYING]

00:00:11.063 --> 00:00:13.030
AMIT GANDHI: Hi, my
name is Amit Gandhi.

00:00:13.030 --> 00:00:15.093
And I'm a graduate
researcher at MIT.

00:00:15.093 --> 00:00:16.510
Welcome to the
series on exploring

00:00:16.510 --> 00:00:19.240
fairness and machine learning
for international development.

00:00:19.240 --> 00:00:21.130
In this module, we
will have an example

00:00:21.130 --> 00:00:23.692
of how an organization would
go about implementing machine

00:00:23.692 --> 00:00:25.900
learning and what some of
the ethical challenges that

00:00:25.900 --> 00:00:28.580
may arise are.

00:00:28.580 --> 00:00:30.830
This module primarily
focuses on decisions

00:00:30.830 --> 00:00:32.820
that are made at the
organizational level.

00:00:32.820 --> 00:00:35.930
But it is important for both
organizational decision makers

00:00:35.930 --> 00:00:37.370
and machine learning
implementers

00:00:37.370 --> 00:00:39.960
to consider these interactions.

00:00:39.960 --> 00:00:41.520
In this case study,
we will be taking

00:00:41.520 --> 00:00:44.670
the role of a chief technology
officer of a social enterprise

00:00:44.670 --> 00:00:47.450
to provide solar lighting
products in East Africa.

00:00:47.450 --> 00:00:49.950
The mission of the company is
to provide affordable lighting

00:00:49.950 --> 00:00:51.793
solutions to people
living in poverty.

00:00:51.793 --> 00:00:53.460
And the company started
off by providing

00:00:53.460 --> 00:00:56.250
high-quality, inexpensive
solar light, so a replacement

00:00:56.250 --> 00:00:59.020
for kerosene lanterns.

00:00:59.020 --> 00:01:02.130
Over time, the company has
grown and increased its product

00:01:02.130 --> 00:01:05.760
offering to include large solar
home systems and along the way

00:01:05.760 --> 00:01:08.250
has implemented pay-as-you-go
models so that households can

00:01:08.250 --> 00:01:10.830
afford to purchase
these larger systems.

00:01:10.830 --> 00:01:13.530
The way pay-as-you-go models
work are that you provide

00:01:13.530 --> 00:01:15.440
the solar lighting
infrastructure as a loaned

00:01:15.440 --> 00:01:16.497
asset to individuals.

00:01:16.497 --> 00:01:19.080
And they pay you back over time
through mobile money payments,

00:01:19.080 --> 00:01:23.350
until the full value of
the asset is recovered.

00:01:23.350 --> 00:01:25.730
The company has been meticulous
about keeping records

00:01:25.730 --> 00:01:27.650
from transactions
from their user base.

00:01:27.650 --> 00:01:30.580
And as a result, you have access
to both demographic information

00:01:30.580 --> 00:01:33.263
and payment history for
all of your clients.

00:01:33.263 --> 00:01:34.930
The information you
have from your users

00:01:34.930 --> 00:01:37.600
includes age, gender,
occupation, location,

00:01:37.600 --> 00:01:39.270
and household income.

00:01:39.270 --> 00:01:41.020
As you look at expanding
the social impact

00:01:41.020 --> 00:01:43.120
of your enterprise, you
realize that this data

00:01:43.120 --> 00:01:45.400
can be analyzed to
determine a creditworthiness

00:01:45.400 --> 00:01:47.300
metric for your customers.

00:01:47.300 --> 00:01:49.300
Additionally, you could
provide this information

00:01:49.300 --> 00:01:51.580
to banks or microfinance
institutions

00:01:51.580 --> 00:01:55.500
so that they can give
loans to your client base.

00:01:55.500 --> 00:01:57.210
Machine learning
is a powerful tool

00:01:57.210 --> 00:01:59.610
that you can use to implement
this credit scoring metric.

00:01:59.610 --> 00:02:01.747
However, you do not have
data scientists or machine

00:02:01.747 --> 00:02:03.330
learning experts
within your team that

00:02:03.330 --> 00:02:05.340
can implement this solution.

00:02:05.340 --> 00:02:07.290
You also do not know
how accurate or powerful

00:02:07.290 --> 00:02:08.836
an algorithm you
developed could be.

00:02:08.836 --> 00:02:10.919
So you do not want to spend
the resources to build

00:02:10.919 --> 00:02:13.290
a full team [INAUDIBLE]
on a small pilot with some

00:02:13.290 --> 00:02:16.010
of your users in Uganda.

00:02:16.010 --> 00:02:18.227
As a resourceful company
with engineering staff,

00:02:18.227 --> 00:02:20.060
you could either have
some of your engineers

00:02:20.060 --> 00:02:21.770
implement a machine
learning solution

00:02:21.770 --> 00:02:25.670
using off-the-shelf products or
work with a third party company

00:02:25.670 --> 00:02:29.670
to implement the solution
for credit scoring.

00:02:29.670 --> 00:02:31.530
Let's pause this case
study for a second

00:02:31.530 --> 00:02:33.660
and examine the pros and
cons of the decisions that

00:02:33.660 --> 00:02:34.830
need to be made.

00:02:34.830 --> 00:02:36.600
It is important to
consider perspectives

00:02:36.600 --> 00:02:38.970
from both the machine
learning implementer as well

00:02:38.970 --> 00:02:40.950
as the organizations to
understand the thoughts

00:02:40.950 --> 00:02:44.010
and complexities that go
into developing a solution.

00:02:44.010 --> 00:02:46.770
Doing it in-house without
a trained data scientist

00:02:46.770 --> 00:02:48.870
will likely involve
implementation of a black box

00:02:48.870 --> 00:02:49.853
solution.

00:02:49.853 --> 00:02:52.020
While someone with no
background in machine learning

00:02:52.020 --> 00:02:54.480
could get a solution up
and running fairly quickly,

00:02:54.480 --> 00:02:58.640
there are several nuances in the
design that may get overlooked.

00:02:58.640 --> 00:03:00.260
Allowing a third
party consultant

00:03:00.260 --> 00:03:03.203
to implement your solution would
solve many of these issues,

00:03:03.203 --> 00:03:05.120
though you may lack both
in-house capabilities

00:03:05.120 --> 00:03:07.520
to understand how your
model is being implemented

00:03:07.520 --> 00:03:10.960
and maintain it moving forward.

00:03:10.960 --> 00:03:13.600
Let's assume that one way or
another, the credit scoring

00:03:13.600 --> 00:03:15.280
algorithm gets built.

00:03:15.280 --> 00:03:17.500
Without paying attention
to fairness in this setup,

00:03:17.500 --> 00:03:19.270
several issues may arise.

00:03:19.270 --> 00:03:22.653
First, you may find that as you
analyze your historical data,

00:03:22.653 --> 00:03:24.820
that certain groups of
people have different default

00:03:24.820 --> 00:03:26.820
rates than others.

00:03:26.820 --> 00:03:30.010
For example, women may have a
lower default rate than men.

00:03:30.010 --> 00:03:32.098
And you may decide that
as an organization,

00:03:32.098 --> 00:03:34.140
you want to be fair and
gender blind in your loan

00:03:34.140 --> 00:03:35.960
determination.

00:03:35.960 --> 00:03:38.500
The slide shows an example of
what different loan rates look

00:03:38.500 --> 00:03:42.130
like for what men and women.

00:03:42.130 --> 00:03:44.320
To implement fairness,
a naive implementer

00:03:44.320 --> 00:03:47.020
may first try to use fairness
through unawareness, which

00:03:47.020 --> 00:03:48.940
means that you simply
hide gender information

00:03:48.940 --> 00:03:51.150
while building your models.

00:03:51.150 --> 00:03:53.280
Depending on correlations
within your data

00:03:53.280 --> 00:03:55.537
and how relevant gender
is to default rates,

00:03:55.537 --> 00:03:57.120
your models could
still predict gender

00:03:57.120 --> 00:03:59.590
and use that in the model.

00:03:59.590 --> 00:04:02.530
Second, since your data shows
a difference in default rates,

00:04:02.530 --> 00:04:05.650
you have to actively decide
how to correct for that.

00:04:05.650 --> 00:04:07.450
In the case of loans,
different approaches

00:04:07.450 --> 00:04:09.950
to implement fairness may have
a trade-off with the accuracy

00:04:09.950 --> 00:04:12.220
of your algorithms.

00:04:12.220 --> 00:04:15.130
Third, the type of algorithm
the implementer uses

00:04:15.130 --> 00:04:16.870
could have trade-offs as well.

00:04:16.870 --> 00:04:19.720
Some algorithms may be faster
at the cost of accuracy.

00:04:19.720 --> 00:04:22.330
Others may be more accurate
at the cost of explainability

00:04:22.330 --> 00:04:26.170
or understandability.

00:04:26.170 --> 00:04:27.910
I won't go more in-depth
on these topics,

00:04:27.910 --> 00:04:30.610
because we will discuss
them more in future modules.

00:04:30.610 --> 00:04:34.220
However, I do want to highlight
a couple of important concepts.

00:04:34.220 --> 00:04:36.580
First, implementing a
machine learning algorithm

00:04:36.580 --> 00:04:38.760
is not an objective process.

00:04:38.760 --> 00:04:41.680
In your implementation, you
are both designing a technology

00:04:41.680 --> 00:04:43.690
and making decisions,
both of which

00:04:43.690 --> 00:04:46.770
introduce your biases
into the system.

00:04:46.770 --> 00:04:49.350
To think that outcomes from
a computer are objective

00:04:49.350 --> 00:04:51.500
is just a fantasy.

00:04:51.500 --> 00:04:53.690
Second, open
communication between you

00:04:53.690 --> 00:04:56.440
and the implementer on your
values as an organization.

00:04:56.440 --> 00:04:59.900
And decisions that they
are making are critical.

00:04:59.900 --> 00:05:03.080
Third, you need a way to audit
your data and your algorithms

00:05:03.080 --> 00:05:06.240
if you want to
have a fair system.

00:05:06.240 --> 00:05:08.870
Let's move on, assuming you were
able to work with a consultant

00:05:08.870 --> 00:05:11.407
to build a satisfactory
solution to your algorithm.

00:05:11.407 --> 00:05:13.490
And you're able to demonstrate
significant success

00:05:13.490 --> 00:05:15.670
with your pilot
in western Uganda.

00:05:15.670 --> 00:05:18.150
You now want to scale your
model to other parts of Uganda

00:05:18.150 --> 00:05:19.730
and East Africa.

00:05:19.730 --> 00:05:21.890
At this point, it is
important to pay attention

00:05:21.890 --> 00:05:24.500
to the representativeness
of your data.

00:05:24.500 --> 00:05:26.750
Are there large differences
between the types of users

00:05:26.750 --> 00:05:29.450
you have in western
Uganda and eastern Uganda?

00:05:29.450 --> 00:05:31.967
How about the users in
Uganda and Tanzania?

00:05:31.967 --> 00:05:34.550
You need to make sure that you
are collecting representatives'

00:05:34.550 --> 00:05:36.740
data as you scale
your solution, which

00:05:36.740 --> 00:05:39.502
involves significant
testing and auditing.

00:05:39.502 --> 00:05:40.960
Additionally, you
want to make sure

00:05:40.960 --> 00:05:42.670
that changes within
your population

00:05:42.670 --> 00:05:44.450
do not suddenly
affect your results.

00:05:44.450 --> 00:05:47.590
For example, if a kerosene tax
were imposed by the government,

00:05:47.590 --> 00:05:49.750
would your model no
longer be accurate?

00:05:49.750 --> 00:05:52.277
How could you build in support
within your organization

00:05:52.277 --> 00:05:53.860
to make sure you can
react to changes?

00:05:57.300 --> 00:05:59.510
Thank you for taking the
time to take this course.

00:05:59.510 --> 00:06:01.427
We hope that you'll
continue to watch the rest

00:06:01.427 --> 00:06:03.650
of the modules in the series.

00:06:03.650 --> 00:06:07.000
[MUSIC PLAYING]