WEBVTT

00:00:17.558 --> 00:00:19.850
PROFESSOR: So we spent the
last few lectures discussing

00:00:19.850 --> 00:00:21.873
Szemerédi's regularity lemma.

00:00:21.873 --> 00:00:23.540
So we saw that this
is an important tool

00:00:23.540 --> 00:00:26.450
with important
applications, allowing

00:00:26.450 --> 00:00:30.080
you to do things like a proof
of Roth's theorem via graph

00:00:30.080 --> 00:00:31.640
theory.

00:00:31.640 --> 00:00:34.820
One of the concepts that came
up when we were discussing

00:00:34.820 --> 00:00:38.210
the statement of Szemerédi's
regularity lemma is that

00:00:38.210 --> 00:00:39.830
of pseudorandomness.

00:00:39.830 --> 00:00:44.480
So the statement of Szemerédi's
graph regularity lemma is that

00:00:44.480 --> 00:00:48.710
you can partition an arbitrary
graph into a bounded number

00:00:48.710 --> 00:00:53.630
of pieces so that the
graph looks random-like,

00:00:53.630 --> 00:00:58.040
as we called it, between
most pairs of parts.

00:00:58.040 --> 00:01:00.017
So what does random-like mean?

00:01:00.017 --> 00:01:02.600
So that's something that I want
to discuss for the next couple

00:01:02.600 --> 00:01:05.239
of lectures.

00:01:05.239 --> 00:01:18.580
And this is the idea
of pseudorandomness,

00:01:18.580 --> 00:01:23.100
which is a concept
that is really

00:01:23.100 --> 00:01:26.880
prevalent in combinatorics, in
theoretical computer science,

00:01:26.880 --> 00:01:28.860
and in many different areas.

00:01:28.860 --> 00:01:33.840
And what pseudorandomness tries
to capture is, in what ways

00:01:33.840 --> 00:01:37.270
can a non-random
object look random?

00:01:37.270 --> 00:01:40.230
So before diving into
some specific mathematics,

00:01:40.230 --> 00:01:43.530
I want to offer some
philosophical remarks.

00:01:43.530 --> 00:01:46.110
So you might know
that, on a computer,

00:01:46.110 --> 00:01:48.420
you want to generate
a random number.

00:01:48.420 --> 00:01:51.510
Well, you type in a "rand," and
it gives you a random number.

00:01:51.510 --> 00:01:56.450
But of course, that's not
necessarily true randomness.

00:01:56.450 --> 00:02:00.870
It came from some
pseudorandom generator.

00:02:00.870 --> 00:02:03.930
Probably there's some seed and
some complex-looking function

00:02:03.930 --> 00:02:06.510
and outputs something
that you couldn't

00:02:06.510 --> 00:02:09.000
distinguish from random.

00:02:09.000 --> 00:02:11.430
But it might not actually
be random but just something

00:02:11.430 --> 00:02:15.730
that looks, in many
different ways, like random.

00:02:15.730 --> 00:02:17.560
So there is this
concept of random.

00:02:17.560 --> 00:02:19.590
You can think about
a random graph,

00:02:19.590 --> 00:02:21.910
right, generate this
Erdos-Renyi random graph.

00:02:21.910 --> 00:02:25.360
Every edge occurs independently
with some probability.

00:02:25.360 --> 00:02:28.660
But I can also show you some
graph, some specific graph,

00:02:28.660 --> 00:02:32.860
which I say, well, it's, for
all intents and purposes,

00:02:32.860 --> 00:02:38.160
just as good as a random graph.

00:02:38.160 --> 00:02:42.380
So in what ways can we
capture that concept?

00:02:42.380 --> 00:02:44.100
So that's what I
want to discuss.

00:02:44.100 --> 00:02:46.520
And that's the topic
of pseudorandomness.

00:02:46.520 --> 00:02:49.250
And of course, well,
this idea extends

00:02:49.250 --> 00:02:51.690
to many areas, number
theory and whatnot,

00:02:51.690 --> 00:02:53.820
but we'll stick
with graph theory.

00:02:53.820 --> 00:02:58.220
In particular, I want to explore
today just one specific notion

00:02:58.220 --> 00:02:59.660
of pseudorandomness.

00:02:59.660 --> 00:03:04.060
And this comes from an important
paper called "Quasi-random

00:03:04.060 --> 00:03:04.560
graphs."

00:03:08.790 --> 00:03:12.350
And this concept is due to
Chung, Graham, and Wilson

00:03:12.350 --> 00:03:13.570
back in the late '80s.

00:03:23.690 --> 00:03:28.980
So they defined various
notions of pseudorandomness,

00:03:28.980 --> 00:03:30.930
and I want to state them.

00:03:30.930 --> 00:03:33.390
And what it turns out--
and the surprising part

00:03:33.390 --> 00:03:36.750
is that these notions,
these definitions,

00:03:36.750 --> 00:03:39.690
although they look
superficially different,

00:03:39.690 --> 00:03:43.960
they are actually all
equivalent to each other.

00:03:43.960 --> 00:03:49.050
So let's see what
the theorem says.

00:03:49.050 --> 00:03:54.090
So the set-up of this theorem
is that you have some fixed

00:03:54.090 --> 00:03:56.640
real p between 0 and 1.

00:03:56.640 --> 00:04:00.810
And this is going to be
your graph edge density.

00:04:00.810 --> 00:04:11.070
So for any sequence
of graphs, Gn--

00:04:13.660 --> 00:04:19.940
so from now, I'm going
to drop the subscript n,

00:04:19.940 --> 00:04:22.600
so G will just be Gn--

00:04:22.600 --> 00:04:27.680
such that the
number of vertices--

00:04:27.680 --> 00:04:37.170
so G is n vertex with
edge density basically p.

00:04:42.090 --> 00:04:44.000
So this is your
sequence of graphs.

00:04:44.000 --> 00:04:49.860
And the claim is that
we're going to state

00:04:49.860 --> 00:04:51.594
some set of properties.

00:04:54.260 --> 00:04:55.750
And these properties
are all going

00:04:55.750 --> 00:04:57.660
to be equivalent to each other.

00:05:03.370 --> 00:05:06.030
So all of these properties
capture some notion

00:05:06.030 --> 00:05:10.140
of pseudorandomness, so in what
ways this is graph G or really

00:05:10.140 --> 00:05:11.640
a sequence of graphs.

00:05:11.640 --> 00:05:13.860
Or you can talk about
a specific graph

00:05:13.860 --> 00:05:15.990
and have some error
parameters and error balance.

00:05:15.990 --> 00:05:19.030
They're all roughly
the same ideas.

00:05:19.030 --> 00:05:21.030
So in what ways can we
talk about this graph G

00:05:21.030 --> 00:05:22.600
being random-like?

00:05:22.600 --> 00:05:26.570
Well, we already saw one
notion when we discussed

00:05:26.570 --> 00:05:28.530
Szemerédi's regularity lemma.

00:05:28.530 --> 00:05:30.700
And let's see that here.

00:05:30.700 --> 00:05:33.930
So this notion is
known as discrepancy.

00:05:33.930 --> 00:05:39.270
And it says that if I restrict
my graph to looking only

00:05:39.270 --> 00:05:42.990
at edges between some
pair of vertex sets,

00:05:42.990 --> 00:05:46.680
then the number of
edges should be roughly

00:05:46.680 --> 00:05:49.800
what you would expect
based on density alone.

00:05:57.730 --> 00:05:59.230
So this is basically
the notion that

00:05:59.230 --> 00:06:02.050
came up in epsilon regularity.

00:06:02.050 --> 00:06:04.720
This is essentially
the same as saying

00:06:04.720 --> 00:06:09.310
that G is epsilon
regular with itself

00:06:09.310 --> 00:06:15.890
where this epsilon now is hidden
in this little o parameter.

00:06:15.890 --> 00:06:18.360
So that's one notion
of pseudorandomness.

00:06:18.360 --> 00:06:23.000
So here's another notion
which is very similar.

00:06:23.000 --> 00:06:25.310
So it's almost just a
semantic difference,

00:06:25.310 --> 00:06:27.530
but, OK, so I have to
do a little bit of work.

00:06:27.530 --> 00:06:29.440
So let me call this DISC prime.

00:06:29.440 --> 00:06:35.730
So it says that if you look at
only edges within this set--

00:06:35.730 --> 00:06:38.840
so instead of taking two
sets, I only look at one set--

00:06:38.840 --> 00:06:41.440
and then look at
how many edges are

00:06:41.440 --> 00:06:45.170
in there versus how many you
should expect based on density

00:06:45.170 --> 00:06:50.940
alone, these two
numbers are also

00:06:50.940 --> 00:06:52.240
very similar to each other.

00:07:00.730 --> 00:07:04.890
So let's get to something that
looks dramatically different.

00:07:04.890 --> 00:07:09.940
The next one, I'm
going to call count.

00:07:09.940 --> 00:07:14.480
So count says that
for every graph

00:07:14.480 --> 00:07:28.380
H, the number of labeled
copies of H in G--

00:07:28.380 --> 00:07:31.620
OK, so labeled copies, I
mean that the vertices of H

00:07:31.620 --> 00:07:33.070
are labeled.

00:07:33.070 --> 00:07:37.350
So for every triangle, there
are six labeled triangles

00:07:37.350 --> 00:07:40.530
that correspond to that
triangle in the graph.

00:07:40.530 --> 00:07:44.790
The number of labeled
copies of H is--

00:07:44.790 --> 00:07:48.110
so what should you expect if
this graph were truly random?

00:07:48.110 --> 00:07:51.900
You would expect p raised
to the number of edges of H

00:07:51.900 --> 00:08:01.530
plus small error times n raised
to number of vertices of H.

00:08:01.530 --> 00:08:09.710
And just as a remark, this
little o term, little o 1 term,

00:08:09.710 --> 00:08:15.600
may depend on H.

00:08:15.600 --> 00:08:19.340
So this condition, count,
says for every graph H,

00:08:19.340 --> 00:08:20.540
this is true.

00:08:20.540 --> 00:08:23.420
And by that, I mean
for every H, there

00:08:23.420 --> 00:08:25.480
is some sequence
of decaying errors.

00:08:25.480 --> 00:08:27.320
But that sequence
of decaying errors

00:08:27.320 --> 00:08:34.789
may depend on your graph H. OK.

00:08:34.789 --> 00:08:38.809
The next one is almost
a special case of count.

00:08:38.809 --> 00:08:41.940
It's called C4.

00:08:41.940 --> 00:08:46.230
And it says that the number
of labeled copies of C4,

00:08:46.230 --> 00:08:59.410
so the fourth cycle, is at
most p raised to power of 4--

00:08:59.410 --> 00:09:05.810
so again, what you should
expect in a random setting just

00:09:05.810 --> 00:09:07.000
for cycle count alone.

00:09:11.740 --> 00:09:15.600
I see, already, some
of you are surprised.

00:09:15.600 --> 00:09:19.590
So we'll discuss that this
is an important constraint.

00:09:19.590 --> 00:09:22.770
It turns out that alone
implies everything, just having

00:09:22.770 --> 00:09:24.030
the correct C4 count.

00:09:27.730 --> 00:09:31.720
The next one, we
will call codegree.

00:09:31.720 --> 00:09:37.550
And the codegree condition
says that if you look at a pair

00:09:37.550 --> 00:09:42.400
of vertices and look at their
number of common neighbors--

00:09:42.400 --> 00:09:45.460
in other words, their codegree--

00:09:45.460 --> 00:09:49.020
then what should you
expect this quantity to be?

00:09:49.020 --> 00:09:51.880
So there are n
vertices that possibly

00:09:51.880 --> 00:09:55.780
could be common neighbors,
and each one of them,

00:09:55.780 --> 00:09:58.730
if this were a random graph
with edge probability p,

00:09:58.730 --> 00:10:00.940
then you expect the
number of common neighbors

00:10:00.940 --> 00:10:03.950
to be around p squared n.

00:10:03.950 --> 00:10:10.240
So the codegree condition
is that this sum is small.

00:10:10.240 --> 00:10:14.800
So most pairs of vertices have
roughly the correct number

00:10:14.800 --> 00:10:16.477
of common neighbors.

00:10:20.213 --> 00:10:27.793
So codegree is number
of common neighbors.

00:10:31.180 --> 00:10:34.990
Next, and the last one,
certainly not the least,

00:10:34.990 --> 00:10:39.130
is eigenvalue condition.

00:10:39.130 --> 00:10:50.410
So here, we are going to denote
by lambda 1 through lambda G

00:10:50.410 --> 00:10:58.130
the eigenvalues of the
adjacency matrix of G.

00:10:58.130 --> 00:11:01.520
So we saw this object
in the last lecture.

00:11:01.520 --> 00:11:03.940
So I include multiplicities.

00:11:03.940 --> 00:11:06.320
If some eigenvalue occurs
with multiple times,

00:11:06.320 --> 00:11:08.920
I include it multiple times.

00:11:08.920 --> 00:11:13.870
So the eigenvalue condition
says that the top eigenvalue

00:11:13.870 --> 00:11:21.720
is around pn and that,
more importantly,

00:11:21.720 --> 00:11:30.770
the other eigenvalues
are all quite small.

00:11:35.810 --> 00:11:39.470
Now, for d regular graph,
the top eigenvalue--

00:11:39.470 --> 00:11:41.990
and it's fine to think
about d regular graphs

00:11:41.990 --> 00:11:44.990
if you want to get some
intuition out of this theorem.

00:11:44.990 --> 00:11:47.360
For d regular graph,
the top eigenvalue

00:11:47.360 --> 00:11:52.860
is equal to d, because
the top eigenvector is d.

00:11:52.860 --> 00:11:56.000
It's the all-one vector.

00:11:56.000 --> 00:12:09.180
So top eigenvector is all-one
vector, which has eigenvalue d.

00:12:09.180 --> 00:12:14.310
And what the eigenvalue
condition says

00:12:14.310 --> 00:12:27.710
is that all the other
eigenvalues are much smaller.

00:12:27.710 --> 00:12:33.010
So here, I'm thinking of d
as on the same order as n.

00:12:33.010 --> 00:12:34.900
OK, so this is the theorem.

00:12:34.900 --> 00:12:36.150
So that's what we'll do today.

00:12:36.150 --> 00:12:39.090
We'll prove that all
of these properties

00:12:39.090 --> 00:12:41.065
are equivalent to each other.

00:12:41.065 --> 00:12:42.690
And all of these
properties, you should

00:12:42.690 --> 00:12:45.812
think of as characterizations
of pseudorandomness.

00:12:45.812 --> 00:12:47.520
And of course, this
theorem guarantees us

00:12:47.520 --> 00:12:50.070
that it doesn't matter
which one you use.

00:12:50.070 --> 00:12:51.890
They're all equivalent
to each other.

00:12:51.890 --> 00:12:54.397
And our proofs are
actually going to be--

00:12:54.397 --> 00:12:56.730
I mean, I'm going to try to
do everything fairly slowly.

00:12:56.730 --> 00:12:58.450
But none of these
proofs are difficult.

00:12:58.450 --> 00:13:00.390
We're not going to use
any fancy tools like

00:13:00.390 --> 00:13:02.100
Szemerédi's regularity lemma.

00:13:02.100 --> 00:13:06.000
In particular, all of
these quantitative errors

00:13:06.000 --> 00:13:09.490
are reasonably
dependent on each other.

00:13:09.490 --> 00:13:12.060
So I've stated
this theorem so far

00:13:12.060 --> 00:13:15.510
in this form where there
is a little 1 error.

00:13:15.510 --> 00:13:26.030
But equivalently, so I can
equivalently state theorem as--

00:13:28.700 --> 00:13:37.770
for example, have DISC with
an epsilon error, which

00:13:37.770 --> 00:13:42.630
is that some inequality is
true with at most epsilon error

00:13:42.630 --> 00:13:44.250
instead of little o.

00:13:44.250 --> 00:13:48.850
And you have a different
epsilon for each one of them.

00:13:48.850 --> 00:13:53.010
And the theorem,
it turns out that--

00:13:53.010 --> 00:13:55.270
OK, so the proof of
this theorem will

00:13:55.270 --> 00:14:00.640
be that these conditions
are true, so all equivalent,

00:14:00.640 --> 00:14:04.780
up to at most a polynomial
change in the epsilons.

00:14:13.970 --> 00:14:18.470
In other words, so
property one is true

00:14:18.470 --> 00:14:22.640
for epsilon implies
that property two is

00:14:22.640 --> 00:14:26.900
true for some epsilon
raised to a constant.

00:14:26.900 --> 00:14:30.240
So the changes in parameters
are quite reasonable.

00:14:30.240 --> 00:14:33.030
And we'll see this
from the proof,

00:14:33.030 --> 00:14:36.865
but I won't say it
again explicitly.

00:14:36.865 --> 00:14:39.240
Any questions so far about
the statement of this theorem?

00:14:46.870 --> 00:14:51.033
So as I mentioned just now,
the most surprising part

00:14:51.033 --> 00:14:52.450
of this theorem
and the one that I

00:14:52.450 --> 00:14:54.480
want you to pay the
most attention to

00:14:54.480 --> 00:14:57.870
is the C4 condition.

00:14:57.870 --> 00:14:59.970
This seems, at
least at face value,

00:14:59.970 --> 00:15:03.660
the weakest condition
among all of them.

00:15:03.660 --> 00:15:07.510
It just says the
correct C4 count.

00:15:07.510 --> 00:15:10.433
But it turns out to be
equivalent to everything else.

00:15:10.433 --> 00:15:12.350
And there's something
special about C4, right?

00:15:12.350 --> 00:15:16.810
If I replace C4 by C3, by just
triangles, then it is not true.

00:15:16.810 --> 00:15:20.200
So I want you to think
about, where does C4

00:15:20.200 --> 00:15:22.300
play this important role?

00:15:22.300 --> 00:15:25.530
How does it play
this important role?

00:15:25.530 --> 00:15:26.140
OK.

00:15:26.140 --> 00:15:29.450
So let's get started
with a proof.

00:15:29.450 --> 00:15:32.340
But before that, let me--

00:15:32.340 --> 00:15:35.288
so in this proof,
one recurring theme

00:15:35.288 --> 00:15:37.830
is that we're going to be using
the Cauchy-Schwarz inequality

00:15:37.830 --> 00:15:38.910
many times.

00:15:38.910 --> 00:15:41.340
And I want to just begin with
an exercise that gives you

00:15:41.340 --> 00:15:44.140
some familiarity with applying
the Cauchy-Schwarz inequality.

00:15:44.140 --> 00:15:47.400
And this is a simple tool,
but it's extremely powerful.

00:15:47.400 --> 00:15:49.650
And it's worthwhile
to master how to use

00:15:49.650 --> 00:15:52.230
a Cauchy-Schwarz inequality.

00:15:52.230 --> 00:15:54.660
So let's get some practice.

00:15:54.660 --> 00:16:01.880
And let me prove a claim
which is not directly related

00:16:01.880 --> 00:16:03.920
to the proof of the
theorem, but it's

00:16:03.920 --> 00:16:08.520
indirect in that it explains
somewhat the C4 condition

00:16:08.520 --> 00:16:11.300
and why we have less than
or equal to over there.

00:16:14.300 --> 00:16:19.100
So the lemma is that if
you have a graph on n

00:16:19.100 --> 00:16:27.500
vertices such that the number
of edges is at least pn

00:16:27.500 --> 00:16:33.950
squared over 2, so edge
density basically p, then

00:16:33.950 --> 00:16:48.500
the number of labeled copies of
C4 is at least p to the 4 minus

00:16:48.500 --> 00:16:51.102
little o 1 n to the 4th.

00:16:56.020 --> 00:16:59.290
So if you have a graph
with each density p--

00:16:59.290 --> 00:17:01.150
p's your constant--
then the number of C4s

00:17:01.150 --> 00:17:04.869
is at least roughly what you
would expect in a random graph.

00:17:07.662 --> 00:17:08.829
So let's see how to do this.

00:17:11.540 --> 00:17:14.530
And I want to show
this inequality as a--

00:17:14.530 --> 00:17:16.660
well, I'll show you how
to prove this inequality.

00:17:16.660 --> 00:17:20.470
But I also want to draw a
sequence of pictures, at least,

00:17:20.470 --> 00:17:22.780
to explain how I think
about applications

00:17:22.780 --> 00:17:26.540
of the Cauchy-Schwarz
inequality.

00:17:26.540 --> 00:17:27.040
OK.

00:17:27.040 --> 00:17:32.810
So the first thing
is that we are

00:17:32.810 --> 00:17:35.090
counting labeled copies of C4.

00:17:35.090 --> 00:17:38.870
And this is basically but
not exactly the same as

00:17:38.870 --> 00:17:44.270
number of homomorphic copies of
C4 and G. So by this guy here,

00:17:44.270 --> 00:17:50.300
I really just mean you are
mapping vertices of C4 to G

00:17:50.300 --> 00:17:53.150
so that the edges
all map to edges.

00:17:53.150 --> 00:18:03.950
But we are allowing not
necessarily injective maps,

00:18:03.950 --> 00:18:08.000
C4 to G. But that's OK.

00:18:08.000 --> 00:18:15.260
So the number of non-injective
maps is at most cubic.

00:18:15.260 --> 00:18:17.770
So we're not really
affecting our count.

00:18:17.770 --> 00:18:20.635
So it's enough to think
about homomorphic copies.

00:18:26.460 --> 00:18:27.570
OK.

00:18:27.570 --> 00:18:28.600
So what's going on here?

00:18:28.600 --> 00:18:30.142
So let me draw a
sequence of pictures

00:18:30.142 --> 00:18:32.680
illustrating this calculation.

00:18:32.680 --> 00:18:38.450
So first, we are thinking
about counting C4s.

00:18:38.450 --> 00:18:39.760
So that's a C4.

00:18:43.030 --> 00:18:51.880
I can rewrite the C4 count as a
sum over pairs of vertices of G

00:18:51.880 --> 00:18:53.980
as the squared codegree.

00:18:59.850 --> 00:19:02.470
And what happens
here-- so this is true.

00:19:02.470 --> 00:19:04.450
I mean, it's not hard
to see why this is true.

00:19:04.450 --> 00:19:07.050
But I want to draw
this in pictures,

00:19:07.050 --> 00:19:09.320
because when you have
larger and bigger graphs,

00:19:09.320 --> 00:19:12.090
it may be more difficult
to think about the algebra

00:19:12.090 --> 00:19:14.740
unless you have
some visualization.

00:19:14.740 --> 00:19:19.820
So what happens here is that
I notice that the C4 has

00:19:19.820 --> 00:19:21.740
a certain reflection.

00:19:21.740 --> 00:19:28.680
Namely, it has a reflection
along this horizontal line.

00:19:28.680 --> 00:19:35.210
And so if I put these
two vertices as u and v,

00:19:35.210 --> 00:19:38.030
then this reflection
tells you that you

00:19:38.030 --> 00:19:42.590
can write this number
of homomorphic copies

00:19:42.590 --> 00:19:43.580
as the sum of squares.

00:19:46.510 --> 00:19:48.850
But once you have this
reflection-- and reflections

00:19:48.850 --> 00:19:50.410
are super useful,
because they allow

00:19:50.410 --> 00:19:52.900
us to get something
into a square

00:19:52.900 --> 00:19:58.260
and then, right after, apply
the Cauchy-Schwarz inequality.

00:19:58.260 --> 00:20:00.160
So we apply Cauchy-Schwarz here.

00:20:00.160 --> 00:20:09.370
And we obtain that this
sum is at most where

00:20:09.370 --> 00:20:18.060
I can pull the square out.

00:20:18.060 --> 00:20:23.210
And I need to think about
what is the correct factor

00:20:23.210 --> 00:20:25.270
to put out here.

00:20:25.270 --> 00:20:26.210
And that should be--

00:20:30.710 --> 00:20:34.835
so what's the correct factor
that I should put out there?

00:20:34.835 --> 00:20:35.960
AUDIENCE: 1 over n squared.

00:20:35.960 --> 00:20:39.190
PROFESSOR: OK, so
1 over n squared.

00:20:39.190 --> 00:20:42.700
So I don't actually like doing
these kind of calculations

00:20:42.700 --> 00:20:45.100
with sums, because then
you have to keep track

00:20:45.100 --> 00:20:47.110
of these normalizing factors.

00:20:47.110 --> 00:20:50.890
One of the upcoming chapters,
when we discuss graph limits--

00:20:50.890 --> 00:20:52.390
or in fact, you
can even do this.

00:20:52.390 --> 00:20:54.805
Instead of taking sums,
if you take an average,

00:20:54.805 --> 00:20:56.680
if you take an expectation,
then it turns out

00:20:56.680 --> 00:21:00.100
you never have to worry about
these normalizing factors.

00:21:00.100 --> 00:21:03.220
So normalizing factors
should never bother you

00:21:03.220 --> 00:21:04.970
if you do it correctly.

00:21:04.970 --> 00:21:08.200
But just to make sure
things are correct, please

00:21:08.200 --> 00:21:10.550
keep me in check.

00:21:10.550 --> 00:21:11.050
All right.

00:21:11.050 --> 00:21:13.180
So what happened in this step?

00:21:13.180 --> 00:21:15.700
In this step, we
pulled out that square.

00:21:15.700 --> 00:21:19.030
And pictorially, what
happens is that we got rid

00:21:19.030 --> 00:21:20.610
of half of this picture.

00:21:26.550 --> 00:21:29.220
So we used Cauchy-Schwarz,
and we wiped out

00:21:29.220 --> 00:21:30.060
half of the picture.

00:21:32.850 --> 00:21:35.730
And now what we can
do is, well, we're

00:21:35.730 --> 00:21:39.540
counting these guys,
this path of length 2.

00:21:39.540 --> 00:21:43.650
But I can reprioritize
this picture

00:21:43.650 --> 00:21:47.410
so that it looks like that.

00:21:47.410 --> 00:21:51.370
And now I notice that there
is one more reflection.

00:21:51.370 --> 00:21:52.710
So there's one more reflection.

00:21:52.710 --> 00:21:54.835
And that's the reflection
around the vertical axis.

00:21:57.890 --> 00:22:01.140
So let me call
this top vertex x.

00:22:01.140 --> 00:22:11.916
And I can rewrite
the sum like that.

00:22:18.220 --> 00:22:18.720
OK.

00:22:18.720 --> 00:22:20.970
So once more, we
do Cauchy-Schwarz,

00:22:20.970 --> 00:22:24.950
which allows us to get rid
of half of the picture.

00:22:24.950 --> 00:22:27.740
And now I'm going to
draw the picture first,

00:22:27.740 --> 00:22:30.600
because then you see that
what we should be left with

00:22:30.600 --> 00:22:32.860
is just a single edge.

00:22:32.860 --> 00:22:37.720
And then you write
down the correct sum,

00:22:37.720 --> 00:22:42.540
making sure that all the
parentheses and normalizations

00:22:42.540 --> 00:22:43.580
are correct.

00:22:43.580 --> 00:22:46.620
But somehow, that
doesn't worry me so much,

00:22:46.620 --> 00:22:48.970
because I know this will
definitely work out.

00:22:53.210 --> 00:22:55.750
But whatever it is, you're just
summing the number of edges.

00:22:55.750 --> 00:22:59.270
So that's just the
number of edges.

00:22:59.270 --> 00:23:09.950
And so we put everything in.

00:23:09.950 --> 00:23:13.790
And we find that the final
quantity is at least p raised

00:23:13.790 --> 00:23:17.150
to 4 n to 4.

00:23:17.150 --> 00:23:18.650
So I did this quite slowly.

00:23:18.650 --> 00:23:23.540
But I'm also emphasizing the
sequence of pictures, partly

00:23:23.540 --> 00:23:25.520
to tell how I think
about these inequalities.

00:23:25.520 --> 00:23:29.940
Because for other similar
looking inequalities-- in fact,

00:23:29.940 --> 00:23:33.260
there is something called
Sidorenko's conjecture, which

00:23:33.260 --> 00:23:36.350
I may discuss more in
a future lecture, that

00:23:36.350 --> 00:23:38.990
says that this
kind of inequality

00:23:38.990 --> 00:23:41.540
should be true
whenever you replace C4

00:23:41.540 --> 00:23:43.520
by any bipartite graph.

00:23:43.520 --> 00:23:47.680
And that's a major open
problem in combinatorics.

00:23:47.680 --> 00:23:49.990
It's kind of hard to keep
track of these calculations

00:23:49.990 --> 00:23:51.910
unless you have a visual anchor.

00:23:51.910 --> 00:23:55.600
And this is my visual anchor,
which I'm trying to explain.

00:23:55.600 --> 00:23:57.610
Of course, it's down to earth.

00:23:57.610 --> 00:23:59.590
It's just the sequence
of inequalities.

00:23:59.590 --> 00:24:04.924
And this is also some
practice with Cauchy-Schwarz.

00:24:04.924 --> 00:24:06.760
All right.

00:24:06.760 --> 00:24:07.924
Any questions?

00:24:10.782 --> 00:24:12.240
But one thing that
this calculation

00:24:12.240 --> 00:24:15.300
told us is that if you
have edge density p,

00:24:15.300 --> 00:24:20.790
then you necessarily have C4
density at least p to the 4th.

00:24:20.790 --> 00:24:25.240
So that partly explains why
you have at most, then, here.

00:24:25.240 --> 00:24:28.030
So you always know that
it's at least this quantity.

00:24:28.030 --> 00:24:30.680
So the C4 quasi
randomness condition

00:24:30.680 --> 00:24:34.260
is really the equivalent
to replacing this less than

00:24:34.260 --> 00:24:35.820
or equal to by an equal sign.

00:24:40.010 --> 00:24:41.660
So let's get
started with proving

00:24:41.660 --> 00:24:44.210
the Chung-Graham-Wilson theorem.

00:24:44.210 --> 00:24:46.220
So the first place
that we'll look at

00:24:46.220 --> 00:24:49.920
is the two versions of DISC.

00:24:49.920 --> 00:24:51.610
So DISC stands for discrepancy.

00:24:54.630 --> 00:25:00.720
So first, the fact that DISC
implies DISC prime, I mean,

00:25:00.720 --> 00:25:02.250
this is pretty easy.

00:25:02.250 --> 00:25:04.590
You take y to equal to x.

00:25:04.590 --> 00:25:10.160
Be slightly careful about the
definitions, but you're OK.

00:25:10.160 --> 00:25:13.590
So not much to do there.

00:25:13.590 --> 00:25:20.050
The other direction,
where you only

00:25:20.050 --> 00:25:21.898
have discrepancies
for a single set

00:25:21.898 --> 00:25:23.440
and you want to
produce discrepancies

00:25:23.440 --> 00:25:25.570
for pairs of sets--

00:25:25.570 --> 00:25:28.440
so this is actually a fairly
common technique in algebra

00:25:28.440 --> 00:25:31.570
that allows you to go
from bilinear forms

00:25:31.570 --> 00:25:33.460
to quadratic forms
and vice versa.

00:25:33.460 --> 00:25:35.090
It's that kind of calculation.

00:25:35.090 --> 00:25:37.640
So let me do it here
concretely in this setting.

00:25:37.640 --> 00:25:39.790
So here, what you
should think of

00:25:39.790 --> 00:25:45.680
is that you have two sets, x
and y, and they might overlap.

00:25:45.680 --> 00:25:52.290
And what they
correspond to in the--

00:25:52.290 --> 00:25:55.220
when you think about the
corresponding Venn diagram,

00:25:55.220 --> 00:26:03.130
where I'm looking at ways
that a pair of vertices

00:26:03.130 --> 00:26:05.290
can fall in x and/or y--

00:26:05.290 --> 00:26:08.410
so if you have x and y.

00:26:14.410 --> 00:26:22.360
And so it's useful to keep
track of which vertices

00:26:22.360 --> 00:26:24.310
are in which set.

00:26:24.310 --> 00:26:27.010
But what the thing
finally comes down

00:26:27.010 --> 00:26:31.240
to is that the number of edges
with one vertex in x and one

00:26:31.240 --> 00:26:35.430
vertex in y, I can write this
bilinear form-type quantity

00:26:35.430 --> 00:26:50.540
as an appropriate sum of just
number of edges in single sets.

00:26:57.040 --> 00:27:00.600
And so there are several ways
to check that this is true.

00:27:00.600 --> 00:27:04.980
One way is to just tally,
keep track of how many edges

00:27:04.980 --> 00:27:07.420
are you counting in each step.

00:27:07.420 --> 00:27:14.330
So if you are trying to count
the number of edges in--

00:27:18.990 --> 00:27:21.140
yeah, so let's say if
you're trying to count

00:27:21.140 --> 00:27:23.840
the number of edges in--

00:27:26.380 --> 00:27:29.320
with one vertex in
x, one vertex in y.

00:27:29.320 --> 00:27:34.570
Then what this corresponds
to is that count.

00:27:34.570 --> 00:27:36.040
But let me do a reflection.

00:27:41.760 --> 00:27:45.780
And then you see that
you can write this sum

00:27:45.780 --> 00:27:49.490
as an alternating sum
of principal squares,

00:27:49.490 --> 00:27:55.070
so this one big square plus the
middle square and minus the two

00:27:55.070 --> 00:28:02.080
sides squares, which is
what that sum comes to.

00:28:02.080 --> 00:28:03.250
All right.

00:28:03.250 --> 00:28:06.400
So if we assume
DISC prime, then I

00:28:06.400 --> 00:28:08.560
know that all of
these individual sets

00:28:08.560 --> 00:28:33.520
have roughly the correct number
of edges up to a little o of n

00:28:33.520 --> 00:28:35.120
squared error.

00:28:35.120 --> 00:28:37.360
And again, I don't have to
do this calculation again,

00:28:37.360 --> 00:28:39.440
because it's the
same calculation.

00:28:39.440 --> 00:28:44.260
So the final thing should be
p times the sizes of x and y

00:28:44.260 --> 00:28:47.420
together plus this same error.

00:28:52.770 --> 00:28:55.070
So that shows you DISC
prime implies DISC.

00:28:55.070 --> 00:28:57.060
So the self version
of discrepancy

00:28:57.060 --> 00:28:59.394
implies the pair
version of discrepancy.

00:29:02.060 --> 00:29:04.850
So let's move on to count.

00:29:10.820 --> 00:29:14.930
To show that DISC
implies count--

00:29:20.180 --> 00:29:22.540
actually, we already did this.

00:29:22.540 --> 00:29:23.908
So this is the counting lemma.

00:29:31.240 --> 00:29:34.050
So the counting
lemma tells us how

00:29:34.050 --> 00:29:38.430
to count labeled copies if you
have these epsilon regularity

00:29:38.430 --> 00:29:40.770
conditions, which is
exactly what DISC is.

00:29:44.600 --> 00:29:47.550
So count is good.

00:29:47.550 --> 00:29:55.210
Another easy implication
is count implies C4.

00:29:55.210 --> 00:29:58.560
Well, this is actually
just tautological.

00:29:58.560 --> 00:30:04.610
C4 condition is a special
case of the count hypothesis.

00:30:08.460 --> 00:30:09.380
All right.

00:30:09.380 --> 00:30:15.090
So let's move on to some
additional implications that

00:30:15.090 --> 00:30:17.190
require a bit more work.

00:30:17.190 --> 00:30:20.030
So what about C4
implies codegree?

00:30:26.730 --> 00:30:28.410
So this is where we
need to do this kind

00:30:28.410 --> 00:30:30.940
of Cauchy-Schwarz exercise.

00:30:30.940 --> 00:30:33.780
So let's start with C4.

00:30:33.780 --> 00:30:38.740
So assume a C4 condition.

00:30:38.740 --> 00:30:42.130
And suppose you have this--

00:30:47.650 --> 00:30:50.840
so I want to deduce that the
codegree condition is true.

00:30:50.840 --> 00:30:56.500
But first, let's
think about just

00:30:56.500 --> 00:30:58.640
what is the sum
of these codegrees

00:30:58.640 --> 00:31:02.050
as I vary u and v over
all pairs of vertices.

00:31:06.980 --> 00:31:11.000
So this is that picture.

00:31:11.000 --> 00:31:18.390
So that is equal to the sums
of degrees squared, which now,

00:31:18.390 --> 00:31:24.090
by Cauchy-Schwarz, you can
deduce to be at least n times 2

00:31:24.090 --> 00:31:25.710
raised to number
of edges-- namely,

00:31:25.710 --> 00:31:28.910
the sum of the degrees--

00:31:28.910 --> 00:31:29.710
that thing squared.

00:31:32.760 --> 00:31:37.400
So now we assume
the C4 condition--

00:31:37.400 --> 00:31:41.540
actually, no, we assume that G
has the density as written up

00:31:41.540 --> 00:31:42.260
there.

00:31:42.260 --> 00:31:51.890
So this quantity is p
squared plus little 1 times

00:31:51.890 --> 00:31:54.170
n cubed, which is
what you should expect

00:31:54.170 --> 00:31:56.480
in a random graph of Gnp.

00:31:59.020 --> 00:32:00.890
But that's not quite
what we're looking for.

00:32:00.890 --> 00:32:03.880
So this is just the
sum of the codegrees.

00:32:03.880 --> 00:32:08.110
What we actually
want is the deviation

00:32:08.110 --> 00:32:14.120
of codegrees from its
expectations, so to speak.

00:32:14.120 --> 00:32:16.360
Now, here's an
important technique

00:32:16.360 --> 00:32:19.360
from probabilistic
combinatorics is

00:32:19.360 --> 00:32:25.920
that if you want to control the
deviation of a random variable,

00:32:25.920 --> 00:32:28.970
one thing you should
look at is the variance.

00:32:28.970 --> 00:32:30.780
So if you can
control the variance,

00:32:30.780 --> 00:32:32.442
then you can control
the deviation.

00:32:32.442 --> 00:32:34.650
And this is a method known
as a second moment method.

00:32:34.650 --> 00:32:36.275
And that's what we're
going to do here.

00:32:36.275 --> 00:32:44.470
So what we'll try to show is
that the second moment of these

00:32:44.470 --> 00:32:45.820
codegrees--

00:32:45.820 --> 00:32:48.030
namely, the sum
of their squares--

00:32:48.030 --> 00:32:53.060
is also what you should expect
as if the random setting.

00:32:53.060 --> 00:32:56.775
And then you can put them
together to show what you want.

00:32:56.775 --> 00:33:01.370
So this quantity here,
well, what is this?

00:33:01.370 --> 00:33:08.790
We just saw-- see, up there,
it's also codegree squared.

00:33:08.790 --> 00:33:12.050
So this quantity
is also the number

00:33:12.050 --> 00:33:13.650
of labeled copies of C4--

00:33:19.280 --> 00:33:23.520
not quite, because you
might have two vertices

00:33:23.520 --> 00:33:25.390
and the same vertex.

00:33:25.390 --> 00:33:30.430
So I incorporate a small error.

00:33:30.430 --> 00:33:37.420
So it's a cubic error, but
it's certainly sub n to the 4.

00:33:37.420 --> 00:33:42.640
And we assume that the number of
labeled copies of C4 by the C4

00:33:42.640 --> 00:33:48.520
condition is no more than
basically p to the 4 times

00:33:48.520 --> 00:33:49.870
n raised to power 4.

00:33:55.495 --> 00:33:55.995
OK.

00:33:55.995 --> 00:33:57.400
So now you have a first moment.

00:33:57.400 --> 00:34:00.300
You have some average,
and you have some control.

00:34:00.300 --> 00:34:02.920
In the second moment,
I can put them together

00:34:02.920 --> 00:34:05.950
to bound the deviation
using this idea

00:34:05.950 --> 00:34:07.035
of controlling variance.

00:34:09.921 --> 00:34:21.060
So the codegree deviation
is upper bounded by--

00:34:21.060 --> 00:34:23.010
so here, using
Cauchy-Schwarz, it's

00:34:23.010 --> 00:34:27.739
upper bounded by basically
the same sum, except I

00:34:27.739 --> 00:34:31.580
want to square the summand.

00:34:42.536 --> 00:34:45.060
This also gets rid of
the pesky absolute value

00:34:45.060 --> 00:34:48.400
side, which is not nicely,
algebraically behaved.

00:34:48.400 --> 00:34:48.900
OK.

00:34:48.900 --> 00:34:51.360
So now I have the square,
and I can expand the square.

00:35:00.810 --> 00:35:04.860
So I expand the square
into these terms.

00:35:14.910 --> 00:35:20.225
And the final term here
is p to the 4 n to the 6.

00:35:24.476 --> 00:35:26.360
No, n to the 4.

00:35:29.790 --> 00:35:32.730
All right.

00:35:32.730 --> 00:35:35.400
But I have controlled
the individual terms

00:35:35.400 --> 00:35:38.630
from the calculations above.

00:35:38.630 --> 00:35:43.660
So I can upper bound
this expression

00:35:43.660 --> 00:35:47.700
by what I'm writing down now.

00:35:54.540 --> 00:35:57.420
And basically, you should
expect that everything

00:35:57.420 --> 00:36:01.760
should cancel out,
because they do cancel out

00:36:01.760 --> 00:36:02.760
in the random case.

00:36:02.760 --> 00:36:05.040
Of course, the
sanity check, it's

00:36:05.040 --> 00:36:07.475
important to write
down this calculation.

00:36:07.475 --> 00:36:08.850
So if everything
works out right,

00:36:08.850 --> 00:36:10.058
everything should cancel out.

00:36:10.058 --> 00:36:11.880
And indeed, they do cancel out.

00:36:11.880 --> 00:36:14.583
And you get that--

00:36:14.583 --> 00:36:20.020
so this is a multiplication.

00:36:20.020 --> 00:36:24.010
This is p squared.

00:36:24.010 --> 00:36:26.800
Is that OK?

00:36:26.800 --> 00:36:28.860
So everything should cancel out.

00:36:28.860 --> 00:36:34.320
And you get a
little o of n cubed.

00:36:34.320 --> 00:36:38.550
To summarize, in this
implication from C4

00:36:38.550 --> 00:36:43.750
to codegree, what
we're doing is we're

00:36:43.750 --> 00:36:48.640
controlling the
variance of codegrees

00:36:48.640 --> 00:36:53.950
using the C4 condition and
the second moment bound,

00:36:53.950 --> 00:36:56.860
showing that the
C4 condition trumps

00:36:56.860 --> 00:36:58.741
over the codegree condition.

00:37:01.830 --> 00:37:02.850
Any questions so far?

00:37:08.100 --> 00:37:10.490
So I'll let you ponder
in this calculation.

00:37:10.490 --> 00:37:15.390
The next one that we'll do
is codegree implies DISC.

00:37:15.390 --> 00:37:20.992
And that will be a calculation
in a very similar flavor.

00:37:20.992 --> 00:37:23.325
But it will be a slightly
longer but with similar flavor

00:37:23.325 --> 00:37:24.370
of calculation.

00:37:24.370 --> 00:37:27.384
So let me do that
after the break.

00:37:27.384 --> 00:37:29.740
All right.

00:37:29.740 --> 00:37:31.200
So what have we done so far?

00:37:31.200 --> 00:37:34.080
So let's summarize the
chain of implications

00:37:34.080 --> 00:37:36.240
that we have already proved.

00:37:36.240 --> 00:37:38.850
So first, we
started with showing

00:37:38.850 --> 00:37:43.545
that the two versions
of DISC are equivalent.

00:37:48.500 --> 00:37:55.610
And then we also noticed
that DISC implies count

00:37:55.610 --> 00:37:58.700
through the counting lemma.

00:37:58.700 --> 00:38:06.090
So we also observed that count
implies C4 tautologically

00:38:06.090 --> 00:38:08.650
and C4 implies codegree.

00:38:14.670 --> 00:38:18.190
So the next natural thing to
do is to complete this circuit

00:38:18.190 --> 00:38:21.400
and show that the
codegree condition implies

00:38:21.400 --> 00:38:24.452
the discrepancy condition.

00:38:24.452 --> 00:38:25.660
So that's what we'll do next.

00:38:28.760 --> 00:38:31.880
And in some sense,
these two steps,

00:38:31.880 --> 00:38:35.660
you should think of them as
going in this natural chain,

00:38:35.660 --> 00:38:37.610
where C4--

00:38:37.610 --> 00:38:43.200
so C4 is like this, C4.

00:38:43.200 --> 00:38:46.680
Codegree condition
is really about that.

00:38:46.680 --> 00:38:49.220
And DISC is really
about single edges.

00:38:49.220 --> 00:38:50.440
So you can go from--

00:38:50.440 --> 00:38:55.520
so double-- if you half,
you get much more power.

00:38:55.520 --> 00:38:57.560
So it's going in
the right direction,

00:38:57.560 --> 00:38:59.940
going downstream, so to speak.

00:38:59.940 --> 00:39:03.470
So that's what we're doing
now, going downstream.

00:39:03.470 --> 00:39:05.769
And then you go upstream
via the counting lemma.

00:39:08.523 --> 00:39:10.360
All right.

00:39:10.360 --> 00:39:13.120
Let's do codegree implies DISC.

00:39:24.670 --> 00:39:29.140
So we want to show the
discrepancy condition, which

00:39:29.140 --> 00:39:30.310
is one written up there.

00:39:30.310 --> 00:39:32.290
But before that, let
me first show you

00:39:32.290 --> 00:39:36.460
that the degrees do
not vary too much,

00:39:36.460 --> 00:39:39.010
show that the degrees are
fairly well distributed,

00:39:39.010 --> 00:39:42.580
which is what you should
expect in a pseudorandom graph.

00:39:42.580 --> 00:39:45.880
So you don't expect the half
the vertices, half in degrees,

00:39:45.880 --> 00:39:48.500
twice the other half.

00:39:48.500 --> 00:39:51.220
So that's the first thing
I want to establish.

00:39:53.920 --> 00:40:02.910
If you look at degrees, this
variance, this deviation,

00:40:02.910 --> 00:40:06.550
is not too big.

00:40:06.550 --> 00:40:07.050
OK.

00:40:07.050 --> 00:40:09.450
So like before, we see
an absolute value sign.

00:40:09.450 --> 00:40:10.420
We see a sum.

00:40:10.420 --> 00:40:12.696
So we'll do Cauchy-Schwarz.

00:40:12.696 --> 00:40:16.630
Cauchy-Schwarz allows us
to bound this quantity,

00:40:16.630 --> 00:40:20.290
replacing the summand
by a sum of squared.

00:40:33.210 --> 00:40:35.550
I have a square, so I
can expand the square.

00:40:41.060 --> 00:40:43.310
So let me expand the square.

00:40:43.310 --> 00:41:02.020
And I get that, so just
expanding this square inside.

00:41:04.708 --> 00:41:10.260
And you see this degree
squared is that picture, so

00:41:10.260 --> 00:41:11.730
that sum of codegrees.

00:41:24.280 --> 00:41:27.400
And sum of the degrees is
just the number of edges.

00:41:39.630 --> 00:41:42.750
But we now assume the
codegree condition,

00:41:42.750 --> 00:41:46.205
which in particular implies
that the sum of the codegrees

00:41:46.205 --> 00:41:47.580
is roughly what
you would expect.

00:41:50.170 --> 00:41:52.170
So the sum of the
codegrees should

00:41:52.170 --> 00:41:58.650
be p squared n cubed plus
a little o n cubed error

00:41:58.650 --> 00:42:01.250
at the end.

00:42:01.250 --> 00:42:05.630
Likewise, the number of
edges is, by assumption,

00:42:05.630 --> 00:42:10.130
what you would expect
in a random graph.

00:42:10.130 --> 00:42:13.480
And then the final term.

00:42:13.480 --> 00:42:17.020
And like before-- and of
course, it's good to do a sanity

00:42:17.020 --> 00:42:17.560
check--

00:42:17.560 --> 00:42:19.750
everything should cancel out.

00:42:19.750 --> 00:42:25.310
So what you end up with
is little o of n squared,

00:42:25.310 --> 00:42:28.070
showing that the degrees
do not vary too much.

00:42:28.070 --> 00:42:31.790
And once you have
that promise, then we

00:42:31.790 --> 00:42:35.487
move onto the actual
discrepancy condition.

00:42:41.570 --> 00:42:47.850
So this discrepancy
can be rewritten

00:42:47.850 --> 00:42:57.760
as the sum over vertices
little x and big X, the degree

00:42:57.760 --> 00:43:08.212
from little x to y minus
p times the size of y,

00:43:08.212 --> 00:43:11.362
so rewriting the sum.

00:43:11.362 --> 00:43:16.090
And of course, what
should we do next?

00:43:16.090 --> 00:43:16.950
Cauchy-Schwarz.

00:43:16.950 --> 00:43:17.560
Great.

00:43:17.560 --> 00:43:21.010
So we'll do a Cauchy-Schwarz.

00:43:21.010 --> 00:43:26.180
OK, so here's an important
step or trick, if you will.

00:43:26.180 --> 00:43:28.990
So we'll do Cauchy-Schwarz.

00:43:28.990 --> 00:43:34.070
And something very nice happens
when you do Cauchy-Schwarz

00:43:34.070 --> 00:43:35.080
here.

00:43:35.080 --> 00:43:35.780
OK.

00:43:35.780 --> 00:43:37.340
So you can write
down the expression

00:43:37.340 --> 00:43:40.070
that you obtain when
you do Cauchy-Schwarz.

00:43:40.070 --> 00:43:41.420
So let me do that first.

00:43:47.490 --> 00:43:47.990
OK.

00:43:47.990 --> 00:43:51.690
So here's a step which is
very easy to gloss over.

00:43:51.690 --> 00:43:53.600
But I want to pause and
emphasize this step,

00:43:53.600 --> 00:43:57.170
because this is actually
really important.

00:43:57.170 --> 00:44:00.250
What I'm going to do now is
to observe that the summand is

00:44:00.250 --> 00:44:01.714
always non-negative.

00:44:07.390 --> 00:44:14.740
Therefore, I can enlarge
the sum from just little x

00:44:14.740 --> 00:44:18.542
and X to the entire vertex set.

00:44:18.542 --> 00:44:19.750
And this is important, right?

00:44:19.750 --> 00:44:21.850
So it's important that we
had to do Cauchy-Schwarz

00:44:21.850 --> 00:44:23.942
first to get a
non-negative summand.

00:44:23.942 --> 00:44:25.525
You couldn't do this
in the beginning.

00:44:29.800 --> 00:44:32.450
So you do that.

00:44:32.450 --> 00:44:35.120
And so I have this
sum of squares.

00:44:35.120 --> 00:44:36.065
I expand.

00:44:43.300 --> 00:44:44.270
I expand.

00:44:44.270 --> 00:44:47.800
I write out all
these expressions.

00:44:57.300 --> 00:45:00.680
And now the little x range
over the entire vertex set.

00:45:15.715 --> 00:45:17.200
All right.

00:45:17.200 --> 00:45:20.780
So what was the
point of all of that?

00:45:20.780 --> 00:45:25.000
So you see this expression
here, the degree

00:45:25.000 --> 00:45:31.870
from little x to big Y
squared, what is that?

00:45:31.870 --> 00:45:34.230
How can we rewrite
this expression?

00:45:38.550 --> 00:45:41.990
So counting little x
and then Y squared--

00:45:45.990 --> 00:45:47.993
AUDIENCE: Sum over u and big Y.

00:45:47.993 --> 00:45:48.660
PROFESSOR: Yeah.

00:45:48.660 --> 00:45:56.064
So sum of codegree
of two vertices in Y,

00:45:56.064 --> 00:46:03.570
so Y, Y prime, and Y codegree
of little y, little y prime.

00:46:09.320 --> 00:46:15.240
And likewise, the
next expression

00:46:15.240 --> 00:46:23.880
can be written as the sum of
the degrees of vertices in Y.

00:46:23.880 --> 00:46:27.670
And the third term,
I leave unchanged.

00:46:27.670 --> 00:46:32.610
So now we've gotten rid
of these funny expressions

00:46:32.610 --> 00:46:35.810
where it's just degree
from the vertex to a set.

00:46:35.810 --> 00:46:40.320
And we could do this because
of this relaxation up here.

00:46:40.320 --> 00:46:41.480
So that was the point.

00:46:41.480 --> 00:46:43.550
We had to use this
relaxation so that we

00:46:43.550 --> 00:46:46.140
get these codegree terms.

00:46:46.140 --> 00:46:48.750
But now, because you
have the codegree terms

00:46:48.750 --> 00:46:51.600
and we assume the
codegree hypothesis,

00:46:51.600 --> 00:46:55.470
we obtain that
this sum is roughly

00:46:55.470 --> 00:47:00.480
what you expect as in a
random case, because all

00:47:00.480 --> 00:47:05.515
the individual deviations do
not add up to more than little

00:47:05.515 --> 00:47:06.780
o n cubed.

00:47:11.150 --> 00:47:15.000
That codegree sum
is what you expect.

00:47:15.000 --> 00:47:19.040
And the next term,
the sum of degrees,

00:47:19.040 --> 00:47:21.875
is also, by what we did
up there, what you expect.

00:47:29.315 --> 00:47:31.380
And finally, the third term.

00:47:34.330 --> 00:47:36.640
And as earlier, if you
did everything correctly,

00:47:36.640 --> 00:47:38.690
everything should cancel.

00:47:38.690 --> 00:47:40.780
And they do.

00:47:40.780 --> 00:47:43.340
And so what you get at the
end is little o of n squared.

00:47:52.020 --> 00:47:54.900
This completes
this fourth cycle.

00:47:59.655 --> 00:48:00.530
Any questions so far?

00:48:06.160 --> 00:48:08.610
So we're missing
one more condition,

00:48:08.610 --> 00:48:11.980
and that's the
eigenvalue condition.

00:48:11.980 --> 00:48:15.928
So far, everything had to do
with counting various things.

00:48:15.928 --> 00:48:17.970
So what does eigenvalue
have to do with anything?

00:48:22.920 --> 00:48:25.360
So the eigenvalue
condition is actually

00:48:25.360 --> 00:48:26.680
a particularly important one.

00:48:26.680 --> 00:48:29.560
And we'll see more of
this in the next lecture.

00:48:29.560 --> 00:48:32.300
But let me first show you
the equivalent implications.

00:48:32.300 --> 00:48:35.600
So what we'll show is that
the eigenvalue condition is

00:48:35.600 --> 00:48:37.660
equivalent to the C4 condition.

00:48:37.660 --> 00:48:38.710
So that's the goal.

00:48:38.710 --> 00:48:42.640
So I'll show equivalence
between EIG and C4.

00:48:46.250 --> 00:48:52.050
So first, it implies a C4
condition, because up to--

00:48:52.050 --> 00:48:53.990
so instead of
counting C4s, which

00:48:53.990 --> 00:48:56.340
is a little bit actually not--

00:48:56.340 --> 00:48:59.780
it's a bit annoying
to do actual C4s.

00:48:59.780 --> 00:49:04.430
Just like earlier, we want to
consider homomorphic copies,

00:49:04.430 --> 00:49:09.420
which are also labeled walks,
so closed walks of length 4.

00:49:09.420 --> 00:49:20.040
So up to a cubic error,
the number of labeled C4s

00:49:20.040 --> 00:49:36.250
is given by the number of
closed walks of length 4, which

00:49:36.250 --> 00:49:41.807
is equal to the trace of the 4th
power of the adjacency matrix

00:49:41.807 --> 00:49:42.390
of this graph.

00:49:53.280 --> 00:49:55.320
And the next thing
is super important.

00:49:55.320 --> 00:49:58.850
So the next thing is sometimes
called a trace method.

00:49:58.850 --> 00:50:02.150
One important way that the
eigenvalue, so the spectrum

00:50:02.150 --> 00:50:06.470
of a graph or matrix, relates to
other combinatorial quantities

00:50:06.470 --> 00:50:07.910
is via this trace.

00:50:07.910 --> 00:50:11.440
So we know that the
trace of the 4th power

00:50:11.440 --> 00:50:14.240
is equal to the fourth
moment of the eigenvalues.

00:50:19.673 --> 00:50:21.590
So if you haven't seen
a proof of this before,

00:50:21.590 --> 00:50:23.507
I encourage you to go
home and think about it.

00:50:23.507 --> 00:50:25.730
So this is an important
identity, of course.

00:50:25.730 --> 00:50:29.330
4 can be replaced by
any number up here.

00:50:29.330 --> 00:50:35.420
And now you have the
eigenvalue condition.

00:50:35.420 --> 00:50:38.720
So I can estimate the sum.

00:50:38.720 --> 00:50:41.660
There's a principle
term-- namely, lambda 1.

00:50:41.660 --> 00:50:42.860
So that's the big term.

00:50:42.860 --> 00:50:44.210
Everything else is small.

00:50:44.210 --> 00:50:46.950
And the smallness is supposed
to capture pseudorandomness.

00:50:46.950 --> 00:50:51.150
But the big term, you have
to analyze separately.

00:50:51.150 --> 00:50:58.380
OK, so let me write
it out like that.

00:50:58.380 --> 00:51:03.030
So the big term, you know that
it is p to the 4 n to the 4

00:51:03.030 --> 00:51:06.500
plus little o of n to the 4.

00:51:06.500 --> 00:51:07.000
OK.

00:51:07.000 --> 00:51:11.220
So next thing is what to
do with the little terms.

00:51:11.220 --> 00:51:13.650
So we want to show that
the contribution in total

00:51:13.650 --> 00:51:15.770
is not too big.

00:51:15.770 --> 00:51:19.010
So what can we do?

00:51:19.010 --> 00:51:22.250
Well, let me first
try something.

00:51:22.250 --> 00:51:27.360
So first, well, you see
that each one of these guys

00:51:27.360 --> 00:51:28.590
is not too big.

00:51:28.590 --> 00:51:35.250
So maybe let's bound each
one of them by little o of n

00:51:35.250 --> 00:51:36.930
raised to 4.

00:51:36.930 --> 00:51:38.880
But then there are
n of them, so you

00:51:38.880 --> 00:51:42.760
have to multiply by an extra n.

00:51:42.760 --> 00:51:44.840
And that's too much.

00:51:44.840 --> 00:51:46.380
That's not good enough.

00:51:46.380 --> 00:51:49.430
So you cannot individually
bound each one of them.

00:51:49.430 --> 00:51:51.793
And this is a novice mistake.

00:51:51.793 --> 00:51:53.210
This is something
that we actually

00:51:53.210 --> 00:51:55.040
will see this type
of calculation

00:51:55.040 --> 00:51:57.630
later on in the term when
we discuss Roth's theorem.

00:51:57.630 --> 00:52:00.767
But you're not supposed to
bound these terms individually.

00:52:00.767 --> 00:52:02.600
The better way to do
this or the correct way

00:52:02.600 --> 00:52:06.520
to do this is to pull
out just a couple--

00:52:06.520 --> 00:52:08.180
some, but not all--

00:52:08.180 --> 00:52:09.750
of these factors.

00:52:09.750 --> 00:52:16.740
So it is upper bounded
by-- you take max of--

00:52:16.740 --> 00:52:19.560
in this case, you can
take out one or two.

00:52:19.560 --> 00:52:22.260
But you take out,
let's say, two factors.

00:52:22.260 --> 00:52:25.610
And then you leave the
remaining sum intact.

00:52:25.610 --> 00:52:30.970
In fact, I can even put lambda
1 back into the remaining sum.

00:52:30.970 --> 00:52:32.130
So that is true.

00:52:32.130 --> 00:52:35.020
So what I've written down is
just true as an inequality.

00:52:35.020 --> 00:52:39.020
And now I apply the
hypothesis on the sizes

00:52:39.020 --> 00:52:39.980
of the other lambdas.

00:52:47.730 --> 00:52:53.370
So the one I pulled out
is little o of n squared.

00:52:53.370 --> 00:52:57.250
And now what's the second sum?

00:52:57.250 --> 00:53:03.360
That sum is the
trace of a squared,

00:53:03.360 --> 00:53:08.290
which is just twice the
number of edges of the graph.

00:53:08.290 --> 00:53:13.330
So that's also at
most n squared.

00:53:13.330 --> 00:53:21.800
So combining everything,
you have the desired bound

00:53:21.800 --> 00:53:22.650
on the C4 count.

00:53:25.352 --> 00:53:27.060
Of course, this gives
you an upper bound.

00:53:27.060 --> 00:53:28.977
But we also did a
calculation before the break

00:53:28.977 --> 00:53:32.240
that shows you that the C4 bound
has a lower bound, as well.

00:53:32.240 --> 00:53:36.240
So really, having the
correct eigenvalue--

00:53:36.240 --> 00:53:38.180
actually, no, this
already shows you

00:53:38.180 --> 00:53:40.580
that the C4 bound is
correct in both directions,

00:53:40.580 --> 00:53:42.753
because this is the main term.

00:53:42.753 --> 00:53:44.170
And then everything
else is small.

00:53:49.070 --> 00:53:50.330
OK.

00:53:50.330 --> 00:53:54.406
The final implication is
C4 implies eigenvalue.

00:54:01.560 --> 00:54:06.470
For this one, I need to
explore the following important

00:54:06.470 --> 00:54:08.520
property of the top eigenvalue.

00:54:08.520 --> 00:54:10.400
So there's something
that we also

00:54:10.400 --> 00:54:13.580
saw last time, which
is the interpretation

00:54:13.580 --> 00:54:19.600
of the top eigenvalue of
a matrix interpreted as--

00:54:22.880 --> 00:54:26.130
so this is sometimes called
the Courant-Fischer criterion.

00:54:26.130 --> 00:54:30.610
Or actually, this is a special
case of Courant-Fischer.

00:54:30.610 --> 00:54:32.930
This is a basic
linear algebra fact.

00:54:32.930 --> 00:54:35.620
If you are not familiar with
it, I recommend looking it up.

00:54:39.110 --> 00:54:49.250
The top eigenvalue of a matrix,
of a real, symmetric matrix,

00:54:49.250 --> 00:54:56.950
is characterized by the maximum
value of this quadratic form.

00:54:59.570 --> 00:55:02.070
Let's say if x is
a non-zero vector.

00:55:05.940 --> 00:55:11.070
So in particular, if I set
x to be a specific vector,

00:55:11.070 --> 00:55:14.540
I can lower bound lambda 1.

00:55:14.540 --> 00:55:28.110
So if we set this boldface 1
to be the all-one vector in R

00:55:28.110 --> 00:55:31.680
raised to the number
of vertices of G,

00:55:31.680 --> 00:55:43.640
then the lambda 1 of
the graph is at least

00:55:43.640 --> 00:55:46.980
this quantity over here.

00:55:46.980 --> 00:55:49.400
The numerator and denominators
are all easy things

00:55:49.400 --> 00:55:50.300
to evaluate.

00:55:50.300 --> 00:55:53.140
The numerator is just
twice the number of edges,

00:55:53.140 --> 00:55:56.140
because you are summing up
all the entries of the matrix.

00:55:56.140 --> 00:56:01.180
And the denominator is just n.

00:56:01.180 --> 00:56:05.920
So the top eigenvalue
is at least roughly pn.

00:56:13.650 --> 00:56:15.505
So what about the
other eigenvalues?

00:56:18.830 --> 00:56:23.420
Well, the other
eigenvalues, I can again

00:56:23.420 --> 00:56:27.930
refer back to this
moment formula relating

00:56:27.930 --> 00:56:31.380
the trace and closed walks.

00:56:31.380 --> 00:56:37.190
It is at most the
trace of the 4th power

00:56:37.190 --> 00:56:42.710
minus the top eigenvalue
raised to the 4th power.

00:56:42.710 --> 00:56:44.370
It's the sum of the
other eigenvalue

00:56:44.370 --> 00:56:45.860
raised to the 4th power.

00:56:45.860 --> 00:56:47.310
And 4 here, we're using the 4.

00:56:47.310 --> 00:56:49.220
It's an even number, right?

00:56:49.220 --> 00:56:52.710
So you have this over here.

00:56:52.710 --> 00:57:08.620
So having a C4 hypothesis and
also knowing what lambda 1 is

00:57:08.620 --> 00:57:11.230
allows you to control
the other lambdas.

00:57:22.530 --> 00:57:27.500
See, lambda 1 cannot be
much greater than pn.

00:57:27.500 --> 00:57:29.270
Also comes out of
the same calculation.

00:57:29.270 --> 00:57:30.246
Yep.

00:57:30.246 --> 00:57:36.102
AUDIENCE: So [INAUDIBLE]
number 1 equal to [INAUDIBLE]??

00:57:36.102 --> 00:57:39.255
PROFESSOR: Yeah, thank you.

00:57:39.255 --> 00:57:40.640
Yeah, so there's a correction.

00:57:40.640 --> 00:57:43.360
So lambda 1 is--

00:57:43.360 --> 00:57:45.160
so in other words,
the little o is always

00:57:45.160 --> 00:57:47.470
respect to the constant density.

00:57:53.288 --> 00:57:53.788
OK, yeah.

00:57:53.788 --> 00:57:54.288
Question.

00:57:54.288 --> 00:57:58.162
AUDIENCE: You said in the
eigenvalue implies C4,

00:57:58.162 --> 00:58:01.515
you somewhere also used the
lower bound to be proved

00:58:01.515 --> 00:58:02.237
[INAUDIBLE].

00:58:02.237 --> 00:58:02.820
PROFESSOR: OK.

00:58:02.820 --> 00:58:06.800
So the question is in
eigenvalue implies C4,

00:58:06.800 --> 00:58:08.480
it says something
about the lower bound.

00:58:08.480 --> 00:58:09.600
So I'm not saying that.

00:58:09.600 --> 00:58:15.760
So as written over here,
this is what we have proved.

00:58:15.760 --> 00:58:18.220
But when you think about the
pseudorandomness condition

00:58:18.220 --> 00:58:22.150
for C4, it shouldn't be just
that the number of C4 count

00:58:22.150 --> 00:58:23.740
is at most something.

00:58:23.740 --> 00:58:26.860
It should be that
it equals to that,

00:58:26.860 --> 00:58:29.770
which would be implied by
the C4 condition itself,

00:58:29.770 --> 00:58:33.180
because we know, always, it
is the case that a C4 count is

00:58:33.180 --> 00:58:36.400
at least what it is
compared to the random case.

00:58:43.380 --> 00:58:47.530
So just one more thing I
said was that lambda 1,

00:58:47.530 --> 00:58:56.290
you also know that it is at
most pn plus little n, because--

00:59:02.286 --> 00:59:04.266
OK.

00:59:04.266 --> 00:59:04.766
Yeah.

00:59:10.240 --> 00:59:13.140
So this finishes the proof of
the Chung-Graham-Wilson theorem

00:59:13.140 --> 00:59:14.740
on quasi-random graphs.

00:59:14.740 --> 00:59:16.960
We stated all of
these hypotheses,

00:59:16.960 --> 00:59:18.940
and they are all
equivalent to each other.

00:59:18.940 --> 00:59:22.030
And I want to emphasize, again,
the most surprising one is

00:59:22.030 --> 00:59:27.610
that C4 implies everything
else, that a fairly seemingly

00:59:27.610 --> 00:59:30.040
weak condition, this just
having the correct number

00:59:30.040 --> 00:59:33.670
of copies of labeled C4s,
is enough to guarantee

00:59:33.670 --> 00:59:37.060
all of these other much more
complicated looking conditions.

00:59:37.060 --> 00:59:40.510
And in particular, just
having the C4 count correct

00:59:40.510 --> 00:59:46.810
implies that the counts of
every other graph H is correct.

00:59:46.810 --> 00:59:49.830
Now, one thing I
want to stress is

00:59:49.830 --> 00:59:52.560
that the Chung-Graham-Wilson
theorem is really

00:59:52.560 --> 00:59:54.540
about dense graphs.

01:00:03.230 --> 01:00:05.335
And by dense, here,
I mean p constant.

01:00:08.940 --> 01:00:11.010
Of course, the theorem
as stated is true

01:00:11.010 --> 01:00:13.650
if you let p equal to 0.

01:00:13.650 --> 01:00:17.140
So there, I said p
strictly between 0 and 1.

01:00:17.140 --> 01:00:22.710
But it is also OK if
you let p be equal to 0.

01:00:25.470 --> 01:00:29.370
You don't get such interesting
theorems, but it is still true.

01:00:32.065 --> 01:00:34.440
But for sparse graphs, what
you really want to care about

01:00:34.440 --> 01:00:40.230
is approximations of the
correct order of magnitude.

01:00:40.230 --> 01:00:44.660
So what I mean is that
you can write down

01:00:44.660 --> 01:00:53.910
some sparse analogs
for p going to 0,

01:00:53.910 --> 01:00:59.505
so p as a function of n going
to 0 as n goes to infinity.

01:01:02.912 --> 01:01:04.870
So let me just write down
a couple of examples,

01:01:04.870 --> 01:01:05.900
but I won't do all of them.

01:01:05.900 --> 01:01:07.760
You can imagine what
they should look like.

01:01:07.760 --> 01:01:13.935
So DISC should say this
quantity over here.

01:01:13.935 --> 01:01:20.200
And the discrepancy
condition is little o

01:01:20.200 --> 01:01:26.080
of pn squared, because
pn squared is the edge

01:01:26.080 --> 01:01:27.080
density overall.

01:01:27.080 --> 01:01:29.680
So that's the quantity you
should compare against and not

01:01:29.680 --> 01:01:30.820
n squared.

01:01:30.820 --> 01:01:33.250
If you're comparing
n squared, you're

01:01:33.250 --> 01:01:36.220
cheating, because n
squared is much bigger

01:01:36.220 --> 01:01:39.160
than the actual edge density.

01:01:39.160 --> 01:01:48.830
Likewise, the number of
labeled copies of H is--

01:01:51.550 --> 01:01:54.880
I want to put the
little o 1 plus little

01:01:54.880 --> 01:02:10.750
in front, so instead of
plus little o of n to the H

01:02:10.750 --> 01:02:11.250
at the end.

01:02:14.230 --> 01:02:15.660
So you understand
the difference.

01:02:15.660 --> 01:02:19.205
So for sparse, this is
the correct normalization

01:02:19.205 --> 01:02:20.830
that you should have,
when p is allowed

01:02:20.830 --> 01:02:23.770
to go to 0 as a function of n.

01:02:23.770 --> 01:02:27.180
And you can write down all
of these conditions, right?

01:02:27.180 --> 01:02:28.710
I'm not saying
there's a theorem.

01:02:28.710 --> 01:02:30.860
You can write out
all these conditions.

01:02:30.860 --> 01:02:35.220
And you can ask, is there also
some notion of equivalence?

01:02:35.220 --> 01:02:37.260
So are these
corresponding conditions

01:02:37.260 --> 01:02:39.400
also equivalent to each other?

01:02:39.400 --> 01:02:42.370
And the answer is emphatically
no, absolutely not.

01:02:42.370 --> 01:02:50.920
So all of these equivalents
fail for sparse.

01:02:50.920 --> 01:02:52.198
Some of them are still true.

01:02:52.198 --> 01:02:53.740
Some of the easier
ones that we did--

01:02:53.740 --> 01:02:56.920
for example, the two versions
of DISC are equivalent.

01:02:56.920 --> 01:02:58.120
That's still OK.

01:02:58.120 --> 01:03:02.860
And some of these calculations
involving Cauchy-Schwarz

01:03:02.860 --> 01:03:06.250
are mostly still OK.

01:03:06.250 --> 01:03:09.370
But the one that really
fails is the counting lemma.

01:03:21.360 --> 01:03:24.090
And let me explain
why with an example.

01:03:24.090 --> 01:03:27.450
So I want to give you an
example of a graph which

01:03:27.450 --> 01:03:36.280
looks pseudorandom in the
sense of DISC but has no,

01:03:36.280 --> 01:03:38.615
let's say, C3 count.

01:03:38.615 --> 01:03:42.180
It also has no C4
count, but it has no--

01:03:42.180 --> 01:03:44.929
has the clean, correct
number of triangles.

01:03:49.820 --> 01:03:51.460
So what's this example?

01:03:51.460 --> 01:03:59.800
So let p be some number which
is little o of 1 over root n

01:03:59.800 --> 01:04:04.920
so some decaying
quantity with n.

01:04:04.920 --> 01:04:06.505
And let's consider Gnp.

01:04:10.820 --> 01:04:14.120
Well, how many triangles
do we expect in Gnp?

01:04:14.120 --> 01:04:18.860
So let's think of p as just
slightly below 1 over root n.

01:04:18.860 --> 01:04:25.430
So the number of triangles
in Gnp in expectation is--

01:04:28.520 --> 01:04:30.975
so that's the expected number.

01:04:30.975 --> 01:04:32.600
And you should expect
the actual number

01:04:32.600 --> 01:04:35.970
to be roughly around that.

01:04:35.970 --> 01:04:43.600
But on the other hand,
the number of edges

01:04:43.600 --> 01:04:48.950
is also expected to
be this quantity here.

01:04:48.950 --> 01:04:51.190
And you expect that the
actual number of edges

01:04:51.190 --> 01:04:53.930
to be very close to it.

01:04:53.930 --> 01:04:59.090
But p is chosen so that
the number of triangles

01:04:59.090 --> 01:05:04.230
is significantly smaller
than the number of edges,

01:05:04.230 --> 01:05:09.630
so asymptotically smaller, fewer
copies of triangles than edges.

01:05:09.630 --> 01:05:17.510
So what we can do
now is remove an edge

01:05:17.510 --> 01:05:28.270
from each copy of a
triangle in this Gnp.

01:05:33.080 --> 01:05:46.630
We removed a tiny
fraction of edges,

01:05:46.630 --> 01:05:48.588
because the number of
triangles is much less

01:05:48.588 --> 01:05:49.630
than the number of edges.

01:05:49.630 --> 01:05:52.370
We removed a tiny
fraction of edges.

01:05:52.370 --> 01:05:55.560
And as a result, we do
not change the discrepancy

01:05:55.560 --> 01:06:01.940
condition up to a small error.

01:06:01.940 --> 01:06:04.910
So the discrepancy
condition still holds.

01:06:08.780 --> 01:06:10.850
However, the graph
has no more triangles.

01:06:27.600 --> 01:06:31.130
So you have this pseudorandom
graph in one sense--

01:06:31.130 --> 01:06:32.840
namely, of having
a discrepancy--

01:06:32.840 --> 01:06:35.510
but fails to be pseudorandom
in a different sense--

01:06:35.510 --> 01:06:38.050
namely, it has no triangles.

01:06:38.050 --> 01:06:39.520
Yep.

01:06:39.520 --> 01:06:41.480
AUDIENCE: Do the
conditions C4 and codegree

01:06:41.480 --> 01:06:45.410
also hold here-- so the issue
being from DISC to count?

01:06:45.410 --> 01:06:47.450
PROFESSOR: Question,
do the conditions C4

01:06:47.450 --> 01:06:49.130
and codegree still hold here?

01:06:49.130 --> 01:06:51.890
Basically, downstream is
OK, but upstream is not.

01:06:54.550 --> 01:06:56.830
So we can go from C4
to codegree to DISC.

01:06:56.830 --> 01:07:00.120
But you can't go upward.

01:07:00.120 --> 01:07:07.670
And understanding how to rectify
the situation, perhaps adding

01:07:07.670 --> 01:07:11.210
additional hypotheses
to make this true so

01:07:11.210 --> 01:07:14.660
that you could have counting
lemmas for triangles

01:07:14.660 --> 01:07:16.790
and other graphs
and sparser graphs,

01:07:16.790 --> 01:07:19.365
that's an important topic.

01:07:19.365 --> 01:07:20.990
And this is something
that I'll discuss

01:07:20.990 --> 01:07:23.870
at greater length
in not next lecture,

01:07:23.870 --> 01:07:25.280
but the one after that.

01:07:25.280 --> 01:07:28.970
And this is, in fact, related
to the Green-Tao theorem,

01:07:28.970 --> 01:07:32.870
which allows you to approve
Szemerédi's theorem among

01:07:32.870 --> 01:07:33.490
the primes.

01:07:33.490 --> 01:07:37.040
The primes contain arbitrarily
long arithmetic progressions,

01:07:37.040 --> 01:07:40.400
because the primes
are also a sparse set.

01:07:40.400 --> 01:07:42.260
So it has density going to 0.

01:07:42.260 --> 01:07:44.360
It's density decaying,
like, 1 over log n,

01:07:44.360 --> 01:07:47.220
according to prime
number theorem.

01:07:47.220 --> 01:07:49.610
But you want to do
regularity method.

01:07:49.610 --> 01:07:53.020
So you have to face
this kind of issues.

01:07:53.020 --> 01:07:57.150
So we'll discuss that more at
length in a couple of lectures.

01:07:57.150 --> 01:08:00.112
But for now, just a warning
that everything here

01:08:00.112 --> 01:08:01.320
is really about dense graphs.

01:08:04.160 --> 01:08:06.830
The next thing I
want to discuss is

01:08:06.830 --> 01:08:10.449
an elaboration of what happens
to these eigenvalue conditions.

01:08:16.939 --> 01:08:19.649
So for dense graphs,
in some sense,

01:08:19.649 --> 01:08:21.520
everything's very clear
from this theorem.

01:08:21.520 --> 01:08:23.319
Once you have this, theorem,
they're all equivalent.

01:08:23.319 --> 01:08:24.640
You can go back and forth.

01:08:24.640 --> 01:08:26.950
And you lose a little bit
of epsilon here and there,

01:08:26.950 --> 01:08:28.617
but everything is
more or less the same.

01:08:28.617 --> 01:08:30.617
But if you go to sparser
world, then you really

01:08:30.617 --> 01:08:31.825
need to be much more careful.

01:08:31.825 --> 01:08:33.450
And we need to think
about other tools.

01:08:36.397 --> 01:08:37.939
And so the remainder
of today, I want

01:08:37.939 --> 01:08:41.990
to just discuss one fairly
simple but powerful tool

01:08:41.990 --> 01:08:46.069
relating eigenvalues on one hand
and the discrepancy condition

01:08:46.069 --> 01:08:47.936
on the other hand.

01:08:47.936 --> 01:08:48.649
All right.

01:08:48.649 --> 01:08:51.109
So you can go from
eigenvalue to discrepancy

01:08:51.109 --> 01:08:52.370
by going down this chain.

01:08:52.370 --> 01:08:56.470
But actually, there's
a much quicker route.

01:08:56.470 --> 01:08:58.490
And this is known as the
expander mixing lemma.

01:09:10.950 --> 01:09:15.450
For simplicity and really will
make our life much simpler,

01:09:15.450 --> 01:09:18.866
we're only going to
consider d-regular graphs.

01:09:18.866 --> 01:09:22.859
So here, d-regular means
every vertex is degree d.

01:09:22.859 --> 01:09:25.899
Same word, but different
meaning from epsilon regular.

01:09:25.899 --> 01:09:28.689
And unfortunately, that's
just the way it is.

01:09:28.689 --> 01:09:33.590
So d regular, and we're
going to have n vertices.

01:09:33.590 --> 01:09:43.170
And the adjacency matrix has
eigenvalues lambda 1, lambda 2,

01:09:43.170 --> 01:09:46.740
and so on, arranged
in decreasing order.

01:09:50.540 --> 01:09:56.600
Let me write lambda
as the maximum

01:09:56.600 --> 01:09:59.780
in absolute value
of the eigenvalues

01:09:59.780 --> 01:10:02.910
except for the top one.

01:10:02.910 --> 01:10:05.550
In particular, this is
either the absolute value

01:10:05.550 --> 01:10:12.030
of the second one
or the last one.

01:10:12.030 --> 01:10:14.120
As I mentioned earlier,
the top eigenvalue

01:10:14.120 --> 01:10:19.020
is necessarily d, because
you have all-ones vector

01:10:19.020 --> 01:10:20.328
as an eigenvector.

01:10:22.960 --> 01:10:30.620
So the expander mixing lemma
says that if I look at two

01:10:30.620 --> 01:10:38.760
vertex subsets, the number of
edges between them compared

01:10:38.760 --> 01:10:40.810
to what you would expect
in a random case--

01:10:40.810 --> 01:10:42.860
so just like in the
disc setting, but here,

01:10:42.860 --> 01:10:45.620
the correct density I
should put is d over n--

01:10:48.460 --> 01:10:55.810
this quantity is upper bounded
by lambda times the root

01:10:55.810 --> 01:10:57.670
of the product of x and y.

01:11:03.400 --> 01:11:07.780
So in particular,
if this lambda--

01:11:07.780 --> 01:11:11.550
so everything except for the
top eigenvalue-- is small,

01:11:11.550 --> 01:11:15.250
then this discrepancy
should be small.

01:11:15.250 --> 01:11:17.980
And you can verify with what
we did, that it's consistent,

01:11:17.980 --> 01:11:19.680
what we just did.

01:11:23.430 --> 01:11:25.640
All right.

01:11:25.640 --> 01:11:27.060
So let's prove the
expander mixing

01:11:27.060 --> 01:11:31.820
lemma, which is pretty simple
given what we've discussed

01:11:31.820 --> 01:11:35.540
so far, relating-- so there was
this spectral characterization

01:11:35.540 --> 01:11:38.067
up there of the top eigenvalue.

01:11:40.810 --> 01:11:45.355
So we can let J be
the all-ones matrix.

01:11:50.722 --> 01:11:53.460
So let J be the all-ones matrix.

01:11:53.460 --> 01:11:58.980
And we know that
the all-ones vector

01:11:58.980 --> 01:12:06.870
is an eigenvector of the
adjacency matrix of G

01:12:06.870 --> 01:12:10.920
with eigenvalue d.

01:12:10.920 --> 01:12:16.860
So the eigendecomposition of
J is also the all-ones vector

01:12:16.860 --> 01:12:18.460
and its complement.

01:12:18.460 --> 01:12:28.690
So we now see that A
sub G minus d over nJ

01:12:28.690 --> 01:12:39.400
has the same eigenvectors as AG.

01:12:39.400 --> 01:12:41.990
So you can choose the
eigenvectors for that.

01:12:41.990 --> 01:12:44.400
It's the same set
of eigenvectors.

01:12:44.400 --> 01:12:46.200
Of course, we consider
this quantity here,

01:12:46.200 --> 01:12:49.080
because this is exactly
the quantity that comes up

01:12:49.080 --> 01:12:51.700
in this expression
once we hit it

01:12:51.700 --> 01:12:56.817
by characteristic vectors of
subsets from left and right.

01:12:56.817 --> 01:12:57.771
All right.

01:13:02.070 --> 01:13:07.140
So what are the eigenvalues?

01:13:14.700 --> 01:13:18.780
So A previously had eigenvalues
lambda 1 through lambda n.

01:13:18.780 --> 01:13:23.100
But now the top one
gets chopped down to 0.

01:13:31.110 --> 01:13:32.970
So you can check
this explicitly.

01:13:32.970 --> 01:13:37.370
So you can check this
explicitly by checking

01:13:37.370 --> 01:13:43.210
that if you take this
matrix multiplied

01:13:43.210 --> 01:13:47.170
by the all-ones
vector, you get 0.

01:13:47.170 --> 01:13:54.400
And if you have a
eigenvector-eigenvalue pair,

01:13:54.400 --> 01:14:00.740
then hitting this by
any of the other ones

01:14:00.740 --> 01:14:07.150
gets you the same as
in A, because you have

01:14:07.150 --> 01:14:08.530
this orthogonality condition.

01:14:08.530 --> 01:14:10.238
All the other eigenvectors
are orthogonal

01:14:10.238 --> 01:14:13.410
to the all-ones vector.

01:14:13.410 --> 01:14:14.190
All right.

01:14:14.190 --> 01:14:20.980
So now we apply the
Courant-Fischer criteria,

01:14:20.980 --> 01:14:27.500
which tells us that the
number in this discrepancy

01:14:27.500 --> 01:14:52.090
quantity, which we can write
in terms of this matrix,

01:14:52.090 --> 01:14:59.440
it is upper bounded by
the product of the length

01:14:59.440 --> 01:15:02.920
of these two vectors,
x and y, multiplied

01:15:02.920 --> 01:15:04.930
by the spectral norm.

01:15:13.570 --> 01:15:16.270
So I'm not quite using
the version up there,

01:15:16.270 --> 01:15:18.760
but I'm using the
spectral norm version,

01:15:18.760 --> 01:15:20.060
which we discussed last time.

01:15:20.060 --> 01:15:21.510
It's essentially
the one up there,

01:15:21.510 --> 01:15:24.970
but you allow not just
single x but x and y.

01:15:24.970 --> 01:15:28.000
And that corresponds to
the largest eigenvalue

01:15:28.000 --> 01:15:32.150
in absolute value,
which we see that.

01:15:32.150 --> 01:15:34.650
It's at most lambda.

01:15:34.650 --> 01:15:42.992
So at most lambda times size
of x, size of y square root.

01:15:45.800 --> 01:15:50.190
And that finishes the proof
of the expander mixing lemma.

01:15:54.790 --> 01:15:59.170
So the moral here is that,
just like what we saw earlier

01:15:59.170 --> 01:16:03.470
in the dense case but for
any parameters-- so here,

01:16:03.470 --> 01:16:05.290
it's a very clean statement.

01:16:05.290 --> 01:16:07.710
You can even have done
the degree graphs.

01:16:07.710 --> 01:16:11.110
d could be a constant.

01:16:11.110 --> 01:16:15.710
If lambda is small
compared to d, then

01:16:15.710 --> 01:16:18.250
you have this
discrepancy condition.

01:16:18.250 --> 01:16:21.080
And the reason why this is
called an expander mixing lemma

01:16:21.080 --> 01:16:24.020
is that there's this
notion of expanders,

01:16:24.020 --> 01:16:27.380
which is not quite the same
but very intimately related

01:16:27.380 --> 01:16:29.300
to pseudorandom graphs.

01:16:29.300 --> 01:16:33.020
So one property of pseudorandom
graphs that is quite useful--

01:16:33.020 --> 01:16:34.692
in particular, in
computer science--

01:16:34.692 --> 01:16:36.650
is that if you take a
small subset of vertices,

01:16:36.650 --> 01:16:38.480
it has lots of neighbors.

01:16:38.480 --> 01:16:40.640
So the graph is now
somehow clustered

01:16:40.640 --> 01:16:42.830
into a few local pieces.

01:16:42.830 --> 01:16:45.790
So there's lots of expansion.

01:16:45.790 --> 01:16:50.140
And that's something
that you can guarantee

01:16:50.140 --> 01:16:54.130
using the expander mixing lemma,
that you have lots of-- you

01:16:54.130 --> 01:16:56.080
take a small subset of vertices.

01:16:56.080 --> 01:16:58.510
You can expand outward.

01:16:58.510 --> 01:17:00.990
So graphs with that
specific property,

01:17:00.990 --> 01:17:02.490
taking a small
subset of vertices

01:17:02.490 --> 01:17:04.440
always gets you
lots of neighbors,

01:17:04.440 --> 01:17:06.030
are called expander graphs.

01:17:06.030 --> 01:17:09.960
And these graphs play an
important role, in particular,

01:17:09.960 --> 01:17:12.780
in computer science in
designing algorithms and proving

01:17:12.780 --> 01:17:15.210
complexity results and
so on but also play

01:17:15.210 --> 01:17:17.715
important roles in graph
theory and combinatorics.

01:17:20.330 --> 01:17:25.310
Well, next time, we'll address
a few questions which are along

01:17:25.310 --> 01:17:33.730
the lines of, one, how small can
lambda be as a function of d?

01:17:33.730 --> 01:17:34.570
So here is this.

01:17:34.570 --> 01:17:38.500
If lambda's small compared to d,
then you have this discrepancy.

01:17:38.500 --> 01:17:43.480
But if d is, let's
say, a million,

01:17:43.480 --> 01:17:46.120
how small can lambda be?

01:17:46.120 --> 01:17:47.980
That's one question.

01:17:47.980 --> 01:17:52.000
Another question is,
considering everything

01:17:52.000 --> 01:17:59.720
that we've said so far,
what can we say about,

01:17:59.720 --> 01:18:03.110
let's say, the
relationship between some

01:18:03.110 --> 01:18:07.580
of these conditions
for sparse graphs

01:18:07.580 --> 01:18:11.360
but that are somewhat special--
for example, kd graphs

01:18:11.360 --> 01:18:13.970
or vertex-transitive graphs?

01:18:13.970 --> 01:18:15.950
And it turns out some
of these relations

01:18:15.950 --> 01:18:19.360
are also equivalent
to each other.