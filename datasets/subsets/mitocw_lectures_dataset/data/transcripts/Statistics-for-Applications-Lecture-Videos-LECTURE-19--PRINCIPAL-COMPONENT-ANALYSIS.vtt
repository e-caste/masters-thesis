WEBVTT

00:00:00.120 --> 00:00:02.460
The following content is
provided under a Creative

00:00:02.460 --> 00:00:03.880
Commons license.

00:00:03.880 --> 00:00:06.090
Your support will help
MIT OpenCourseWare

00:00:06.090 --> 00:00:10.180
continue to offer high quality
educational resources for free.

00:00:10.180 --> 00:00:12.720
To make a donation or to
view additional materials

00:00:12.720 --> 00:00:16.680
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:16.680 --> 00:00:17.620
at ocw.mit.edu.

00:01:14.980 --> 00:01:17.500
PHILIPPE RIGOLLET: --bunch
of x's and a bunch of y's.

00:01:17.500 --> 00:01:20.140
The y's were univariate,
just one real

00:01:20.140 --> 00:01:21.460
valued random variable.

00:01:21.460 --> 00:01:24.760
And the x's were vectors that
described a bunch of attributes

00:01:24.760 --> 00:01:27.730
for each of our individuals
or each of our observations.

00:01:27.730 --> 00:01:30.350
Let's assume now that we're
given essentially only the x's.

00:01:30.350 --> 00:01:33.970
This is sometimes referred
to as unsupervised learning.

00:01:33.970 --> 00:01:35.920
There is just the x's.

00:01:35.920 --> 00:01:38.640
Usually, supervision
is done by the y's.

00:01:38.640 --> 00:01:41.710
And so what you're trying to do
is to make sense of this data.

00:01:41.710 --> 00:01:43.690
You're going to try to
understand this data,

00:01:43.690 --> 00:01:47.062
represent this data,
visualize this data,

00:01:47.062 --> 00:01:48.520
try to understand
something, right?

00:01:48.520 --> 00:01:52.196
So, if I give you a
d-dimensional random vectors,

00:01:52.196 --> 00:01:54.070
and you're going to have
n independent copies

00:01:54.070 --> 00:01:57.310
of this individual-- of
this random vector, OK?

00:01:57.310 --> 00:01:59.530
So you will see that
I'm going to have--

00:01:59.530 --> 00:02:02.200
I'm going to very quickly
run into some limitations

00:02:02.200 --> 00:02:04.270
about what I can actually
draw on the board

00:02:04.270 --> 00:02:05.980
because I'm using
[? boldface ?] here.

00:02:05.980 --> 00:02:08.180
I'm also going to use the
blackboard [? boldface. ?]

00:02:08.180 --> 00:02:09.820
So it's going to
be a bit difficult.

00:02:09.820 --> 00:02:15.430
So tell me if you're actually
a little confused by what

00:02:15.430 --> 00:02:17.710
is a vector, what is a
number, and what is a matrix.

00:02:17.710 --> 00:02:19.720
But we'll get there.

00:02:19.720 --> 00:02:22.450
So I have X in Rd, and
that's a random vector.

00:02:26.230 --> 00:02:30.650
And I have X1 to
Xn that are IID.

00:02:30.650 --> 00:02:37.635
They're independent
copies of X. OK,

00:02:37.635 --> 00:02:40.326
so you can think
of those as being--

00:02:40.326 --> 00:02:41.700
the realization
of these guys are

00:02:41.700 --> 00:02:51.090
going to be a cloud of
n points in R to the d.

00:02:51.090 --> 00:02:54.210
And we're going to think
of d as being fairly large.

00:02:54.210 --> 00:02:55.710
And for this to
start to make sense,

00:02:55.710 --> 00:02:59.760
we're going to think of d
as being at least 4, OK?

00:02:59.760 --> 00:03:01.830
And meaning that you're
going to have a hard time

00:03:01.830 --> 00:03:03.480
visualizing those things.

00:03:03.480 --> 00:03:06.530
If it was 3 or 2, you would
be able to draw these points.

00:03:06.530 --> 00:03:08.040
And that's pretty
much as much sense

00:03:08.040 --> 00:03:09.831
you're going to be
making about those guys,

00:03:09.831 --> 00:03:12.030
just looking at the [INAUDIBLE]

00:03:12.030 --> 00:03:16.860
All right, so I'm going to
write each of those X's, right?

00:03:16.860 --> 00:03:20.520
So this vector, X,
has d coordinate.

00:03:20.520 --> 00:03:25.650
And I'm going to write
them as X1, to Xd.

00:03:30.730 --> 00:03:34.780
And I'm going to stack
them into a matrix, OK?

00:03:34.780 --> 00:03:38.100
So once I have those guys,
I'm going to have a matrix.

00:03:38.100 --> 00:03:40.230
But here, I'm going
to use the double bar.

00:03:40.230 --> 00:03:47.880
And it's X1 transpose,
Xn transpose.

00:03:47.880 --> 00:03:51.250
So what it means is that
the coordinates of this guy,

00:03:51.250 --> 00:03:53.040
of course, are X1,1.

00:03:53.040 --> 00:03:54.710
Here, I have--

00:03:54.710 --> 00:03:57.870
I'm of size d, so I have X1d.

00:03:57.870 --> 00:04:01.290
And here, I have Xn1.

00:04:01.290 --> 00:04:02.940
Xnd.

00:04:02.940 --> 00:04:06.660
And so the i-th, j-th--

00:04:06.660 --> 00:04:10.950
i-th row and j-th column
is the matrix, Xij, right--

00:04:10.950 --> 00:04:12.780
is the entry, Xi to-- sorry.

00:04:23.540 --> 00:04:28.230
OK, so each-- so the rows
here are the observations.

00:04:28.230 --> 00:04:32.040
And the columns are the
covariance over attributes.

00:04:32.040 --> 00:04:32.640
OK?

00:04:32.640 --> 00:04:34.060
So this is an n by d matrix.

00:04:39.220 --> 00:04:41.320
All right, this is really
just some bookkeeping.

00:04:41.320 --> 00:04:43.840
How do we store
this data somehow?

00:04:43.840 --> 00:04:46.257
And the fact that we use a
matrix just like for regression

00:04:46.257 --> 00:04:48.464
is going to be convenient
because we're going to able

00:04:48.464 --> 00:04:50.050
to talk about projections--

00:04:50.050 --> 00:04:53.310
going to be able to talk
about things like this.

00:04:53.310 --> 00:04:56.310
All right, so everything
I'm going to say now

00:04:56.310 --> 00:04:59.190
is about variances
or covariances

00:04:59.190 --> 00:05:01.945
of those things, which means
that I need two moments, OK?

00:05:01.945 --> 00:05:03.570
If the variance does
not exist, there's

00:05:03.570 --> 00:05:05.320
nothing I can say
about this problem.

00:05:05.320 --> 00:05:07.620
So I'm going to assume
that the variance exists.

00:05:07.620 --> 00:05:09.090
And one way to
just put it to say

00:05:09.090 --> 00:05:12.390
that the two norm
of those guys is

00:05:12.390 --> 00:05:15.030
finite, which is another
way to say that each of them

00:05:15.030 --> 00:05:15.690
is finite.

00:05:15.690 --> 00:05:18.210
I mean, you can think
of it the way you want.

00:05:18.210 --> 00:05:21.000
All right, so now,
the mean of X, right?

00:05:21.000 --> 00:05:22.530
So I have a random vector.

00:05:22.530 --> 00:05:26.430
So I can talk about
the expectation of X.

00:05:26.430 --> 00:05:29.040
That's a vector that's in Rd.

00:05:29.040 --> 00:05:33.828
And that's just taking
the expectation entrywise.

00:05:33.828 --> 00:05:34.328
Sorry.

00:05:42.265 --> 00:05:45.540
X1, Xd.

00:05:45.540 --> 00:05:49.640
OK, so I should say it out loud.

00:05:49.640 --> 00:05:51.890
For this, the purpose
of this class,

00:05:51.890 --> 00:05:55.850
I will denote by
subscripts the indices that

00:05:55.850 --> 00:05:57.170
corresponds to observations.

00:05:57.170 --> 00:06:02.690
And superscripts, the
indices that correspond to

00:06:02.690 --> 00:06:04.280
coordinates of a variable.

00:06:04.280 --> 00:06:07.340
And I think that's the
same convention that we

00:06:07.340 --> 00:06:10.599
took for the regression case.

00:06:10.599 --> 00:06:12.390
Of course, you could
use whatever you want.

00:06:12.390 --> 00:06:13.931
If you want to put
commas, et cetera,

00:06:13.931 --> 00:06:16.072
it becomes just a
bit more complicated.

00:06:16.072 --> 00:06:18.070
All right, and so
now, once I have this,

00:06:18.070 --> 00:06:21.380
so this tells me where my cloud
of point is centered, right?

00:06:21.380 --> 00:06:24.380
So if I have a bunch of points--

00:06:24.380 --> 00:06:27.440
OK, so now I have a
distribution on Rd,

00:06:27.440 --> 00:06:29.990
so maybe I should
talk about this--

00:06:29.990 --> 00:06:31.610
I'll talk about
this when we talk

00:06:31.610 --> 00:06:32.960
about the empirical version.

00:06:32.960 --> 00:06:34.460
But if you think
that you have, say,

00:06:34.460 --> 00:06:36.680
a two-dimensional
Gaussian random variable,

00:06:36.680 --> 00:06:38.930
then you have a center
in two dimension, which

00:06:38.930 --> 00:06:41.572
is where it peaks, basically.

00:06:41.572 --> 00:06:43.280
And that's what we're
talking about here.

00:06:43.280 --> 00:06:44.738
But the other thing
we want to know

00:06:44.738 --> 00:06:47.545
is how much does it spread
in every direction, right?

00:06:47.545 --> 00:06:49.670
So in every direction of
the two dimensional thing,

00:06:49.670 --> 00:06:52.220
I can then try to understand
how much spread I'm getting.

00:06:52.220 --> 00:06:54.900
And the way you measure this
is by using covariance, right?

00:06:54.900 --> 00:07:02.150
So the covariance
matrix, sigma--

00:07:02.150 --> 00:07:05.900
that's a matrix which is d by d.

00:07:05.900 --> 00:07:08.150
And it records-- in
the j, k-th entry,

00:07:08.150 --> 00:07:10.620
it records the covariance
between the j-th coordinate

00:07:10.620 --> 00:07:13.490
of X and the k-th
coordinate of X, OK?

00:07:13.490 --> 00:07:14.570
So with entries--

00:07:21.300 --> 00:07:30.510
OK, so I have sigma, which is
sigma 1,1, sigma dd, sigma 1d,

00:07:30.510 --> 00:07:31.175
sigma d1.

00:07:34.750 --> 00:07:39.690
OK, and here I have
sigma jk And sigma jk

00:07:39.690 --> 00:07:48.930
is just the covariance between
Xj, the j-th coordinate

00:07:48.930 --> 00:07:52.160
and the k-th coordinate.

00:07:52.160 --> 00:07:52.869
OK?

00:07:52.869 --> 00:07:55.160
So in particular, it's
symmetric because the covariance

00:07:55.160 --> 00:07:57.780
between Xj and Xk is the same
as the covariance between Xk

00:07:57.780 --> 00:07:58.280
and Xj.

00:07:58.280 --> 00:08:01.230
I should not put those
parentheses here.

00:08:01.230 --> 00:08:05.330
I do not use them in this, OK?

00:08:05.330 --> 00:08:06.900
Just the covariance matrix.

00:08:06.900 --> 00:08:09.050
So that's just something
that records everything.

00:08:09.050 --> 00:08:10.966
And so what's nice about
the covariance matrix

00:08:10.966 --> 00:08:13.040
is that if I actually
give you X as a vector,

00:08:13.040 --> 00:08:15.170
you actually can
build the matrix just

00:08:15.170 --> 00:08:18.140
by looking at vectors
times vectors transpose,

00:08:18.140 --> 00:08:20.210
rather than actually
thinking about building

00:08:20.210 --> 00:08:21.882
it coordinate by coordinate.

00:08:21.882 --> 00:08:23.840
So for example, if you're
used to using MATLAB,

00:08:23.840 --> 00:08:26.006
that's the way you want to
build a covariance matrix

00:08:26.006 --> 00:08:29.600
because MATLAB is good
at manipulating vectors

00:08:29.600 --> 00:08:33.049
and matrices rather than just
entering it entry by entry.

00:08:33.049 --> 00:08:34.820
OK, so, right?

00:08:34.820 --> 00:08:42.590
So, what is the covariance
between Xj and Xk?

00:08:42.590 --> 00:08:51.360
Well by definition, it's
the expectation of Xj and Xk

00:08:51.360 --> 00:09:01.330
minus the expectation of Xj
times the expectation of Xk,

00:09:01.330 --> 00:09:01.830
right?

00:09:01.830 --> 00:09:03.496
That's the definition
of the covariance.

00:09:03.496 --> 00:09:05.770
I hope everybody's seeing that.

00:09:05.770 --> 00:09:08.280
And so, in particular,
I can actually

00:09:08.280 --> 00:09:10.620
see that this thing
can be written as--

00:09:10.620 --> 00:09:14.340
sigma can now be written
as the expectation

00:09:14.340 --> 00:09:21.040
of XX transpose minus
the expectation of X

00:09:21.040 --> 00:09:25.660
times the expectation
of X transpose.

00:09:25.660 --> 00:09:26.500
Why?

00:09:26.500 --> 00:09:29.470
Well, let's look at the jk-th
coefficient of this guy, right?

00:09:29.470 --> 00:09:35.650
So here, if I look at the
jk-th coefficient, I see what?

00:09:35.650 --> 00:09:38.980
Well, I see that
it's the expectation

00:09:38.980 --> 00:09:50.840
of XX transpose jk, which is
equal to the expectation of XX

00:09:50.840 --> 00:09:53.920
transpose jk.

00:09:53.920 --> 00:09:56.570
And what are the
entries of XX transpose?

00:09:56.570 --> 00:10:00.130
Well, they're of the
form, Xj times Xk exactly.

00:10:00.130 --> 00:10:02.940
So this is actually equal to
the expectation of Xj times Xk.

00:10:09.060 --> 00:10:11.250
And this is actually not
the way I want to write it.

00:10:11.250 --> 00:10:12.083
I want to write it--

00:10:15.530 --> 00:10:16.590
OK?

00:10:16.590 --> 00:10:17.590
Is that clear?

00:10:17.590 --> 00:10:20.420
That when I have a rank 1 matrix
of this form, XX transpose,

00:10:20.420 --> 00:10:21.950
the entries are of
this form, right?

00:10:21.950 --> 00:10:23.520
Because if I take--

00:10:23.520 --> 00:10:28.865
for example, think
about x, y, z, and then

00:10:28.865 --> 00:10:32.810
I multiply by x, y, z.

00:10:32.810 --> 00:10:36.380
What I'm getting here is x--

00:10:36.380 --> 00:10:40.350
maybe I should actually
use indices here.

00:10:40.350 --> 00:10:42.735
x1, x2, x3.

00:10:42.735 --> 00:10:44.750
x1, x2, x3.

00:10:44.750 --> 00:10:57.018
The entries are x1x1, x1x2,
x1x3; x2x1, x2x2, x2x3; x3x1,

00:10:57.018 --> 00:11:04.770
x3x2, x3x3, OK?

00:11:04.770 --> 00:11:08.340
So indeed, this is exactly of
the form if you look at jk,

00:11:08.340 --> 00:11:12.566
you get exactly Xj times Xk, OK?

00:11:12.566 --> 00:11:15.685
So that's the beauty
of those matrices.

00:11:15.685 --> 00:11:19.380
So now, once I have this, I
can do exactly the same thing,

00:11:19.380 --> 00:11:23.480
except that here, if I
take the jk-th entry,

00:11:23.480 --> 00:11:25.044
I will get exactly
the same thing,

00:11:25.044 --> 00:11:27.710
except that it's not going to be
the expectation of the product,

00:11:27.710 --> 00:11:29.780
but the product of the
expectation, right?

00:11:29.780 --> 00:11:36.810
So I get that the jk-th entry
of E of X, E of X transpose,

00:11:36.810 --> 00:11:48.310
is just the j-th entry of E of X
times the k-th entry of E of X.

00:11:48.310 --> 00:11:52.540
So if I put those two together,
it's actually telling me

00:11:52.540 --> 00:11:56.990
that if I look at the
j, k-th entry of sigma,

00:11:56.990 --> 00:11:59.690
which I called
little sigma jk, then

00:11:59.690 --> 00:12:01.340
this is actually equal to what?

00:12:01.340 --> 00:12:04.170
It's equal to the first
term minus the second term.

00:12:04.170 --> 00:12:11.420
The first term is the
expectation of Xj, Xk

00:12:11.420 --> 00:12:18.900
minus the expectation of Xj,
expectation of Xk, which--

00:12:18.900 --> 00:12:20.900
oh, by the way, I forgot
to say this is actually

00:12:20.900 --> 00:12:26.022
equal to the expectation of
Xj times the expectation of Xk

00:12:26.022 --> 00:12:28.230
because that's just the
definition of the expectation

00:12:28.230 --> 00:12:28.979
of random vectors.

00:12:28.979 --> 00:12:31.460
So my j and my k are now inside.

00:12:31.460 --> 00:12:37.175
And that's by definition the
covariance between Xj and Xk,

00:12:37.175 --> 00:12:39.550
OK?

00:12:39.550 --> 00:12:43.360
So just if you've seen those
manipulations between vectors,

00:12:43.360 --> 00:12:45.400
hopefully you're bored
out of your mind.

00:12:45.400 --> 00:12:47.800
And if you have not,
then that's something

00:12:47.800 --> 00:12:51.010
you just need to get
comfortable with, right?

00:12:51.010 --> 00:12:52.660
So one thing that's
going to be useful

00:12:52.660 --> 00:12:55.850
is to know very
quickly what's called

00:12:55.850 --> 00:12:57.850
the outer product of a
vector with itself, which

00:12:57.850 --> 00:12:59.997
is the vector of times
the vector transpose, what

00:12:59.997 --> 00:13:01.330
the entries of these things are.

00:13:01.330 --> 00:13:06.510
And that's what we've been using
on this second set of boards.

00:13:06.510 --> 00:13:08.290
OK, so everybody
agrees now that we've

00:13:08.290 --> 00:13:11.860
sort of showed that the
covariance matrix can

00:13:11.860 --> 00:13:14.290
be written in this vector form.

00:13:14.290 --> 00:13:17.500
So expectation of XX
transpose minus expectation

00:13:17.500 --> 00:13:19.312
of X, expectation
of X transpose.

00:13:22.264 --> 00:13:28.060
OK, just like the covariance
can be written in two ways,

00:13:28.060 --> 00:13:30.070
right we know that the
covariance can also

00:13:30.070 --> 00:13:39.460
be written as the expectation
of Xj minus expectation of Xj

00:13:39.460 --> 00:13:45.500
times Xk minus
expectation of Xk, right?

00:13:45.500 --> 00:13:50.220
That's the-- sometimes, this
is the original definition

00:13:50.220 --> 00:13:50.850
of covariance.

00:13:50.850 --> 00:13:52.490
This is the second
definition of covariance.

00:13:52.490 --> 00:13:54.031
Just like you have
the variance which

00:13:54.031 --> 00:13:57.240
is the expectation of the
square of X minus c of X,

00:13:57.240 --> 00:14:00.390
or the expectation X squared
minus the expectation of X

00:14:00.390 --> 00:14:01.160
squared.

00:14:01.160 --> 00:14:03.420
It's the same thing
for covariance.

00:14:03.420 --> 00:14:11.190
And you can actually see this
in terms of vectors, right?

00:14:11.190 --> 00:14:14.270
So this actually implies that
you can also rewrite sigma

00:14:14.270 --> 00:14:21.780
as the expectation of X
minus expectation of X

00:14:21.780 --> 00:14:23.845
times the same thing transpose.

00:14:32.191 --> 00:14:32.690
Right?

00:14:32.690 --> 00:14:35.950
And the reason is because if
you just distribute those guys,

00:14:35.950 --> 00:14:43.760
this is just the
expectation of XX transpose

00:14:43.760 --> 00:14:54.800
minus X, expectation of X
transpose minus expectation

00:14:54.800 --> 00:14:59.750
of XX transpose.

00:14:59.750 --> 00:15:03.608
And then I have plus
expectation of X,

00:15:03.608 --> 00:15:05.628
expectation of X transpose.

00:15:09.930 --> 00:15:13.110
Now, things could go wrong
because the main difference

00:15:13.110 --> 00:15:18.660
between matrices slash
vectors and numbers is

00:15:18.660 --> 00:15:21.930
that multiplication
does not commute, right?

00:15:21.930 --> 00:15:25.610
So in particular, those two
things are not the same thing.

00:15:25.610 --> 00:15:27.860
And so that's the main
difference that we have before,

00:15:27.860 --> 00:15:30.336
but it actually does not
matter for our problem.

00:15:30.336 --> 00:15:32.210
It's because what's
happening is that if when

00:15:32.210 --> 00:15:34.970
I take the expectation
of this guy, then

00:15:34.970 --> 00:15:38.940
it's actually the same as the
expectation of this guy, OK?

00:15:38.940 --> 00:15:43.540
And so just because the
expectation is linear--

00:15:48.230 --> 00:15:50.550
so what we have
is that sigma now

00:15:50.550 --> 00:15:55.560
becomes equal to the
expectation of XX transpose

00:15:55.560 --> 00:15:59.130
minus the expectation
of X, expectation

00:15:59.130 --> 00:16:03.170
of X transpose minus
expectation of X,

00:16:03.170 --> 00:16:07.110
expectation of X transpose.

00:16:07.110 --> 00:16:10.030
And then I have--

00:16:10.030 --> 00:16:14.070
well, really, what
I have is this guy.

00:16:14.070 --> 00:16:15.990
And then I have
plus the expectation

00:16:15.990 --> 00:16:19.680
of X, expectation
of X transpose.

00:16:23.970 --> 00:16:28.570
And now, those three things are
actually equal to each other

00:16:28.570 --> 00:16:30.700
just because the
expectation of X transpose

00:16:30.700 --> 00:16:34.145
is the same as the
expectation of X transpose.

00:16:34.145 --> 00:16:35.520
And so what I'm
left with is just

00:16:35.520 --> 00:16:44.364
the expectation of XX transpose
minus the expectation of X,

00:16:44.364 --> 00:16:49.650
expectation of X transpose, OK?

00:16:49.650 --> 00:16:51.610
So same thing that's
happening when

00:16:51.610 --> 00:16:53.110
you want to prove
that you can write

00:16:53.110 --> 00:16:57.760
the covariance either
this way or that way.

00:16:57.760 --> 00:17:00.980
The same thing happens for
matrices, or for vectors,

00:17:00.980 --> 00:17:02.340
right, or a covariance matrix.

00:17:02.340 --> 00:17:04.609
They go together.

00:17:04.609 --> 00:17:05.920
Is there any questions so far?

00:17:05.920 --> 00:17:09.460
And if you have some, please
tell me, because I want to--

00:17:09.460 --> 00:17:12.490
I don't know to which extent you
guys are comfortable with this

00:17:12.490 --> 00:17:13.420
at all or not.

00:17:16.700 --> 00:17:19.810
OK, so let's move on.

00:17:19.810 --> 00:17:23.460
All right, so of
course, this is what

00:17:23.460 --> 00:17:26.420
I'm describing in terms of
the distribution right here.

00:17:26.420 --> 00:17:28.359
I took expectations.

00:17:28.359 --> 00:17:30.140
Covariances are
also expectations.

00:17:30.140 --> 00:17:32.560
So those depend on some
distribution of X, right?

00:17:32.560 --> 00:17:34.630
If I wanted to compute
that, I would basically

00:17:34.630 --> 00:17:36.601
need to know what the
distribution of X is.

00:17:36.601 --> 00:17:37.975
Now, we're doing
statistics, so I

00:17:37.975 --> 00:17:41.180
need to [INAUDIBLE] my question
is going to be to say, well,

00:17:41.180 --> 00:17:44.380
how well can I estimate the
covariance matrix itself,

00:17:44.380 --> 00:17:47.260
or some properties of
this covariance matrix

00:17:47.260 --> 00:17:48.405
based on data?

00:17:48.405 --> 00:17:50.140
All right, so if I
want to understand

00:17:50.140 --> 00:17:52.990
what my covariance matrix
looks like based on data,

00:17:52.990 --> 00:17:54.940
I'm going to have
to basically form

00:17:54.940 --> 00:17:57.760
its empirical
counterparts, which

00:17:57.760 --> 00:18:02.200
I can do by doing the age-old
statistical trick, which

00:18:02.200 --> 00:18:04.700
is replace your expectation
by an average, all right?

00:18:04.700 --> 00:18:06.658
So let's just-- everything
that's on the board,

00:18:06.658 --> 00:18:09.310
you see expectation, just
replace it by an average.

00:18:09.310 --> 00:18:14.230
OK, so, now I'm going
to be given X1, Xn.

00:18:14.230 --> 00:18:16.551
So, I'm going to define
the empirical mean.

00:18:19.780 --> 00:18:22.290
OK so, really, the idea
is take your expectation

00:18:22.290 --> 00:18:24.970
and replace it by 1
over n sum, right?

00:18:24.970 --> 00:18:28.230
And so the empirical
mean is just 1 over n.

00:18:28.230 --> 00:18:31.510
Some of the Xi's--

00:18:31.510 --> 00:18:34.070
I'm guessing everybody knows
how to average vectors.

00:18:34.070 --> 00:18:36.110
It's just the average
of the coordinates.

00:18:36.110 --> 00:18:39.730
So I will write this as X bar.

00:18:39.730 --> 00:18:51.440
And the empirical covariance
matrix, often called

00:18:51.440 --> 00:18:57.520
sample covariance matrix,
hence the notation, S.

00:18:57.520 --> 00:18:59.800
Well, this is my
covariance matrix, right?

00:18:59.800 --> 00:19:02.650
Let's just replace the
expectations by averages.

00:19:02.650 --> 00:19:12.160
1 over n, sum from i equal 1 to
n, of Xi, Xi transpose, minus--

00:19:12.160 --> 00:19:14.290
this is the expectation
of X. I will replace it

00:19:14.290 --> 00:19:21.380
by the average, which I just
called X bar, X bar transpose,

00:19:21.380 --> 00:19:22.590
OK?

00:19:22.590 --> 00:19:25.480
And that's when I
want to use the--

00:19:25.480 --> 00:19:28.430
that's when I want
to use the notation--

00:19:28.430 --> 00:19:30.670
the second definition,
but I could actually

00:19:30.670 --> 00:19:35.530
do exactly the same thing
using this definition here.

00:19:35.530 --> 00:19:38.750
Sorry, using this
definition right here.

00:19:38.750 --> 00:19:42.340
So this is actually
1 over n, sum from i

00:19:42.340 --> 00:19:55.240
equal 1 to n, of Xi minus X
bar, Xi minus X bar transpose.

00:19:55.240 --> 00:19:56.560
And those are actually--

00:19:56.560 --> 00:19:58.367
I mean, in a way,
it looks like I

00:19:58.367 --> 00:19:59.950
could define two
different estimators,

00:19:59.950 --> 00:20:01.630
but you can actually check.

00:20:01.630 --> 00:20:03.700
And I do encourage
you to do this.

00:20:03.700 --> 00:20:05.920
If you're not comfortable
making those manipulations,

00:20:05.920 --> 00:20:08.294
you can actually check that
those two things are actually

00:20:08.294 --> 00:20:15.216
exactly the same, OK?

00:20:20.540 --> 00:20:25.070
So now, I'm going to want
to talk about matrices, OK?

00:20:25.070 --> 00:20:27.260
And remember, we defined
this big matrix, X,

00:20:27.260 --> 00:20:28.790
with the double bar.

00:20:28.790 --> 00:20:31.160
And the question
is, can I express

00:20:31.160 --> 00:20:35.360
both X bar and the
sample covariance matrix

00:20:35.360 --> 00:20:37.460
in terms of this big matrix, X?

00:20:37.460 --> 00:20:39.740
Because right now,
it's still expressed

00:20:39.740 --> 00:20:40.820
in terms of the vectors.

00:20:40.820 --> 00:20:43.220
I'm summing those vectors,
vectors transpose.

00:20:43.220 --> 00:20:46.050
The question is, can I just
do that in a very compact way,

00:20:46.050 --> 00:20:50.110
in a way that I can actually
remove this sum term,

00:20:50.110 --> 00:20:50.610
all right?

00:20:50.610 --> 00:20:52.990
That's going to be the goal.

00:20:52.990 --> 00:20:54.850
I mean, that's not
a notational goal.

00:20:54.850 --> 00:20:58.091
That's really something
that we want--

00:20:58.091 --> 00:20:59.590
that's going to be
convenient for us

00:20:59.590 --> 00:21:02.740
just like it was convenient
to talk about matrices when

00:21:02.740 --> 00:21:04.199
we did linear regression.

00:21:23.180 --> 00:21:26.340
OK, X bar.

00:21:26.340 --> 00:21:30.000
We just said it's 1 over
n, sum from I equal 1 to n

00:21:30.000 --> 00:21:32.730
of Xi, right?

00:21:32.730 --> 00:21:35.100
Now remember, what does
this matrix look like?

00:21:35.100 --> 00:21:39.010
We said that X bar--

00:21:39.010 --> 00:21:40.270
X is this guy.

00:21:40.270 --> 00:21:45.930
So if I look at X transpose,
the columns of this guy

00:21:45.930 --> 00:21:51.430
becomes X1, my first
observation, X2,

00:21:51.430 --> 00:21:54.840
my second observation, all the
way to Xn, my last observation,

00:21:54.840 --> 00:21:56.280
right?

00:21:56.280 --> 00:21:56.850
Agreed?

00:21:56.850 --> 00:21:58.470
That's what X transpose is.

00:21:58.470 --> 00:22:00.960
So if I want to
sum those guys, I

00:22:00.960 --> 00:22:02.700
can multiply by the
all-ones vector.

00:22:06.284 --> 00:22:08.700
All right, so that's what the
definition of the all-ones 1

00:22:08.700 --> 00:22:09.250
vector is.

00:22:11.840 --> 00:22:19.870
Well, it's just a bunch of
1's in Rn, in this case.

00:22:19.870 --> 00:22:23.620
And so when I do X transpose 1,
what I get is just the sum from

00:22:23.620 --> 00:22:27.690
i equal 1 to n of the Xi's.

00:22:27.690 --> 00:22:36.460
So if I divide by n,
I get my average, OK?

00:22:36.460 --> 00:22:43.200
So here, I definitely
removed the sum term.

00:22:43.200 --> 00:22:47.930
Let's see if with the covariance
matrix, we can do the same.

00:22:47.930 --> 00:22:53.280
Well, and that's actually a
little more difficult to see,

00:22:53.280 --> 00:22:55.280
I guess.

00:22:55.280 --> 00:23:05.510
But let's use this
definition for S, OK?

00:23:05.510 --> 00:23:07.540
And one thing that's
actually going to be--

00:23:07.540 --> 00:23:10.260
so, let's see for
one second, what--

00:23:10.260 --> 00:23:12.510
so it's going to be
something that involves X,

00:23:12.510 --> 00:23:14.377
multiplying X with itself, OK?

00:23:14.377 --> 00:23:15.960
And the question is,
is it going to be

00:23:15.960 --> 00:23:19.032
multiplying X with X transpose,
or X tranpose with X?

00:23:19.032 --> 00:23:20.490
To answer this
question, you can go

00:23:20.490 --> 00:23:23.960
the easy route, which says,
well, my covariance matrix is

00:23:23.960 --> 00:23:24.870
of size, what?

00:23:24.870 --> 00:23:27.682
What is the size of S?

00:23:27.682 --> 00:23:28.670
AUDIENCE: d by d.

00:23:28.670 --> 00:23:30.260
PHILIPPE RIGOLLET: d by d, OK?

00:23:30.260 --> 00:23:34.200
X is of size n by d.

00:23:34.200 --> 00:23:35.760
So if I do X times
X transpose, I'm

00:23:35.760 --> 00:23:37.760
going to have something
which is of size n by n.

00:23:37.760 --> 00:23:39.426
If I do X transpose
X, I'm going to have

00:23:39.426 --> 00:23:40.794
something which is d by d.

00:23:40.794 --> 00:23:41.710
That's the easy route.

00:23:41.710 --> 00:23:44.150
And there's basically
one of the two guys.

00:23:44.150 --> 00:23:46.130
You can actually open
the box a little bit

00:23:46.130 --> 00:23:48.170
and see what's
going on in there.

00:23:48.170 --> 00:23:52.760
If you do X transpose X, which
we know gives you a d by d,

00:23:52.760 --> 00:23:54.920
you'll see that X is
going to have vectors that

00:23:54.920 --> 00:23:57.465
are of the form,
Xi, and X transpose

00:23:57.465 --> 00:24:02.230
is going to have vectors that
are of the form, Xi transpose,

00:24:02.230 --> 00:24:03.710
right?

00:24:03.710 --> 00:24:06.810
And so, this is actually
probably the right way to go.

00:24:06.810 --> 00:24:11.690
So let's look at what's X
transpose X is giving us.

00:24:11.690 --> 00:24:16.850
So I claim that it's actually
going to give us what we want,

00:24:16.850 --> 00:24:19.710
but rather than actually
going there, let's--

00:24:19.710 --> 00:24:22.700
to actually-- I mean, we
could check it entry by entry,

00:24:22.700 --> 00:24:25.400
but there's actually a
nice thing we can do.

00:24:25.400 --> 00:24:28.090
Before we go there,
let's write X transpose

00:24:28.090 --> 00:24:33.260
as the following sum of
variables, X1 and then

00:24:33.260 --> 00:24:36.270
just a bunch of 0's
everywhere else.

00:24:36.270 --> 00:24:39.410
So it's still d by n.

00:24:39.410 --> 00:24:42.470
So n minus 1 of the columns
are equal to 0 here.

00:24:42.470 --> 00:24:45.860
Then I'm going to put
a 0 and then put X2.

00:24:45.860 --> 00:24:48.550
And then just a
bunch of 0's, right?

00:24:48.550 --> 00:24:59.940
So that's just 0, 0 plus 0,
0, all the way to Xn, OK?

00:24:59.940 --> 00:25:01.260
Everybody agrees with it?

00:25:01.260 --> 00:25:03.650
See what I'm doing here?

00:25:03.650 --> 00:25:06.150
I'm just splitting it into
a sum of matrices that

00:25:06.150 --> 00:25:08.730
only have one nonzero columns.

00:25:08.730 --> 00:25:11.210
But clearly, that's true.

00:25:11.210 --> 00:25:15.610
Now let's look at the product
of this guy with itself.

00:25:15.610 --> 00:25:23.396
So, let's call these
matrices M1, M2, Mn.

00:25:26.890 --> 00:25:30.750
So when I do X
transpose X, what I

00:25:30.750 --> 00:25:37.970
do is the sum of the
Mi's for i equal 1 to n,

00:25:37.970 --> 00:25:48.620
times the sum of the
Mi transpose, right?

00:25:48.620 --> 00:25:50.840
Now, the sum of
the Mi's transpose

00:25:50.840 --> 00:25:55.274
is just the sum of each
of the Mi's transpose, OK?

00:25:58.190 --> 00:26:00.620
So now I just have this
product of two sums,

00:26:00.620 --> 00:26:03.290
so I'm just going to
re-index the second one by j.

00:26:03.290 --> 00:26:12.650
So this is sum for i equal
1 to n, j equal 1 to n of Mi

00:26:12.650 --> 00:26:15.600
Mj transpose.

00:26:15.600 --> 00:26:16.100
OK?

00:26:19.036 --> 00:26:20.410
And now what we
want to notice is

00:26:20.410 --> 00:26:26.000
that if i is different
from j, what's happening?

00:26:26.000 --> 00:26:34.380
Well if i is different from j,
let's look at say, M1 times XM2

00:26:34.380 --> 00:26:35.040
transpose.

00:26:54.067 --> 00:26:56.150
So what is the product
between those two matrices?

00:27:04.404 --> 00:27:09.870
AUDIENCE: It's a new
entry and [INAUDIBLE]

00:27:09.870 --> 00:27:11.370
PHILIPPE RIGOLLET:
There's an entry?

00:27:11.370 --> 00:27:12.801
AUDIENCE: Well, it's an entry.

00:27:12.801 --> 00:27:17.116
It's like a dot product in that
form next to [? transpose. ?]

00:27:17.116 --> 00:27:19.490
PHILIPPE RIGOLLET: You mean
a dot product is just getting

00:27:19.490 --> 00:27:20.360
[INAUDIBLE] number, right?

00:27:20.360 --> 00:27:22.068
So I want-- this is
going to be a matrix.

00:27:22.068 --> 00:27:24.550
It's the product of
two matrices, right?

00:27:24.550 --> 00:27:27.100
This is a matrix times a matrix.

00:27:27.100 --> 00:27:31.210
So this should be a matrix,
right, of size d by d.

00:27:35.960 --> 00:27:37.610
Yeah, I should
see a lot of hands

00:27:37.610 --> 00:27:39.060
that look like this, right?

00:27:39.060 --> 00:27:40.200
Because look at this.

00:27:40.200 --> 00:27:42.450
So let's multiply the first--

00:27:42.450 --> 00:27:45.215
let's look at what's going
on in the first column here.

00:27:45.215 --> 00:27:48.840
I'm multiplying this column
with each of those rows.

00:27:48.840 --> 00:27:50.480
The only nonzero
coefficient is here,

00:27:50.480 --> 00:27:54.190
and it only hits
this column of 0's.

00:27:54.190 --> 00:27:57.036
So every time, this is going
to give you 0, 0, 0, 0.

00:27:57.036 --> 00:28:00.020
And it's going to be the same
for every single one of them.

00:28:00.020 --> 00:28:04.420
So this matrix is just
full of 0's, right?

00:28:04.420 --> 00:28:06.130
They never hit each
other when I do

00:28:06.130 --> 00:28:08.350
the matrix-matrix
multiplication.

00:28:08.350 --> 00:28:11.811
There's no-- every
non-zero hits a 0.

00:28:11.811 --> 00:28:13.560
So what it means is--
and this, of course,

00:28:13.560 --> 00:28:16.020
you can check for every
i different from j.

00:28:16.020 --> 00:28:22.290
So this means that Mi times
Mj transpose is actually

00:28:22.290 --> 00:28:27.150
equal to 0 when i is
different from j, Right?

00:28:27.150 --> 00:28:29.370
Everybody is OK with this?

00:28:29.370 --> 00:28:32.670
So what that means is that when
I do this double sum, really,

00:28:32.670 --> 00:28:33.670
it's a simple sum.

00:28:33.670 --> 00:28:37.310
There's only just the
sum from i equal 1

00:28:37.310 --> 00:28:41.820
to n of Mi Mi transpose.

00:28:41.820 --> 00:28:44.820
Because this is the only
terms in this double sum

00:28:44.820 --> 00:28:48.980
that are not going to be 0 when
[INAUDIBLE] [? M1 ?] with M1

00:28:48.980 --> 00:28:50.492
itself.

00:28:50.492 --> 00:28:51.950
Now, let's see
what's going on when

00:28:51.950 --> 00:28:53.930
I do M1 times M1 transpose.

00:28:53.930 --> 00:28:57.890
Well, now, if I do Mi
times and Mi transpose,

00:28:57.890 --> 00:29:00.300
now this guy becomes [? X1 ?]
[INAUDIBLE] it's here.

00:29:00.300 --> 00:29:03.830
And so now, I really have
X1 times X1 transpose.

00:29:03.830 --> 00:29:06.785
So this is really
just the sum from i

00:29:06.785 --> 00:29:20.080
equal 1 to n of Xi Xi transpose,
just because Mi Mi transpose

00:29:20.080 --> 00:29:21.716
is Xi Xi transpose.

00:29:21.716 --> 00:29:22.840
There's nothing else there.

00:29:26.190 --> 00:29:28.520
So that's the good news, right?

00:29:28.520 --> 00:29:37.100
This term here is really just
X transpose X divided by n.

00:29:43.460 --> 00:29:45.740
OK, I can use that
guy again, I guess.

00:29:45.740 --> 00:29:46.260
Well, no.

00:29:46.260 --> 00:30:08.602
Let's just-- OK, so
let me rewrite S.

00:30:08.602 --> 00:30:10.310
All right, that's the
definition we have.

00:30:10.310 --> 00:30:14.990
And we know that this guy
already is equal to 1 over n X

00:30:14.990 --> 00:30:20.960
transpose X. x bar
x bar transpose--

00:30:20.960 --> 00:30:25.950
we know that x bar-- we
just proved that x bar--

00:30:25.950 --> 00:30:31.080
sorry, little x
bar was equal to 1

00:30:31.080 --> 00:30:36.652
over n X bar transpose
times the all-ones vector.

00:30:36.652 --> 00:30:37.860
So I'm just going to do that.

00:30:37.860 --> 00:30:39.340
So that's just
going to be minus.

00:30:39.340 --> 00:30:40.999
I'm going to pull
my two 1 over n's--

00:30:40.999 --> 00:30:42.540
one from this guy,
one from this guy.

00:30:42.540 --> 00:30:44.530
So I'm going to get
1 over n squared.

00:30:44.530 --> 00:30:47.070
And then I'm going
to get X bar--

00:30:47.070 --> 00:30:48.690
sorry, there's no X bar here.

00:30:48.690 --> 00:30:50.908
It's just X. Yeah.

00:30:50.908 --> 00:30:59.861
X transpose all ones times X
transpose all ones transpose,

00:30:59.861 --> 00:31:00.360
right?

00:31:04.580 --> 00:31:07.580
And X transpose all
ones transpose--

00:31:11.800 --> 00:31:14.200
right, the rule-- if I
have A times B transpose,

00:31:14.200 --> 00:31:16.180
it's B transpose times
A transpose, right?

00:31:23.460 --> 00:31:25.060
That's just the rule
of transposition.

00:31:25.060 --> 00:31:31.400
So this is 1
transpose X transpose.

00:31:31.400 --> 00:31:34.120
And so when I put all
these guys together,

00:31:34.120 --> 00:31:38.365
this is actually equal to 1
over n X transpose X minus one

00:31:38.365 --> 00:31:47.670
over n squared X transpose
1, 1 transpose X. Because X

00:31:47.670 --> 00:31:50.466
transpose transposes X, OK?

00:31:53.700 --> 00:31:55.950
So now, I can actually--

00:31:55.950 --> 00:31:59.435
I have something which is
of the form, X transpose X--

00:31:59.435 --> 00:32:01.800
[INAUDIBLE] to the left, X
transpose; to the right, X.

00:32:01.800 --> 00:32:04.930
Here, I have X transpose to
the left, X to the right.

00:32:04.930 --> 00:32:07.690
So it can factor out
whatever's in there.

00:32:07.690 --> 00:32:11.640
So I can write S as 1 over n--

00:32:11.640 --> 00:32:17.230
sorry, X transpose times 1 over
n times the identity of Rd.

00:32:21.610 --> 00:32:33.110
And then I have minus 1
over n, 1, 1 transpose X.

00:32:33.110 --> 00:32:34.490
OK, because if you--

00:32:34.490 --> 00:32:36.770
I mean, you can
distribute it back, right?

00:32:36.770 --> 00:32:38.090
So here, I'm going to get what?

00:32:38.090 --> 00:32:41.810
X transpose identity times X,
the whole thing divided by n.

00:32:41.810 --> 00:32:42.777
That's this term.

00:32:42.777 --> 00:32:45.110
And then the second one is
going to be-- sorry, 1 over n

00:32:45.110 --> 00:32:46.110
squared.

00:32:46.110 --> 00:32:50.840
And then I'm going to get 1 over
n squared times X transpose 1,

00:32:50.840 --> 00:32:53.990
1 transpose which is
this guy, times X,

00:32:53.990 --> 00:32:58.580
and that's the [? right ?]
[? thing, ?] OK?

00:32:58.580 --> 00:33:01.820
So, the way it's written, I
factored out one of the 1 over

00:33:01.820 --> 00:33:02.320
n's.

00:33:02.320 --> 00:33:05.500
So I'm just going to do the
same thing as on this slide.

00:33:05.500 --> 00:33:08.110
So I'm just factoring
out this 1 over n here.

00:33:08.110 --> 00:33:16.280
So it's 1 over n times
X transpose identity

00:33:16.280 --> 00:33:21.010
of our d divided by n
divided by 1 this time,

00:33:21.010 --> 00:33:26.780
minus 1 over n 1, 1
transpose times X, OK?

00:33:26.780 --> 00:33:28.395
So that's just
what's on the slides.

00:33:31.720 --> 00:33:35.874
What does the matrix, 1,
1 transpose, look like?

00:33:35.874 --> 00:33:36.790
AUDIENCE: All 1's.

00:33:36.790 --> 00:33:38.623
PHILIPPE RIGOLLET: It's
just all 1's, right?

00:33:38.623 --> 00:33:41.060
Because the entries are the
products of the all-ones--

00:33:41.060 --> 00:33:42.750
of the coordinates of
the all-ones vectors with

00:33:42.750 --> 00:33:45.208
the coordinates of the all-ones
vectors, so I only get 1's.

00:33:45.208 --> 00:33:49.610
So it's a d by d
matrix with only 1's.

00:33:49.610 --> 00:33:52.170
So this matrix, I can
actually write exactly, right?

00:33:52.170 --> 00:33:55.710
H, this matrix that
I called H which

00:33:55.710 --> 00:33:59.430
is what's sandwiched in-between
this X transpose and X.

00:33:59.430 --> 00:34:02.760
By definition, I said this
is the definition of H. Then

00:34:02.760 --> 00:34:06.060
this thing, I can write
its coordinates exactly.

00:34:18.880 --> 00:34:23.110
We know it's identity
divided by n minus--

00:34:23.110 --> 00:34:25.330
sorry, I don't know
why I keep [INAUDIBLE]..

00:34:25.330 --> 00:34:29.110
Minus 1 over n 1, 1 transpose--

00:34:29.110 --> 00:34:30.940
so it's this matrix
with the only 1's

00:34:30.940 --> 00:34:34.389
on the diagonals and 0's and
elsewhere-- minus a matrix that

00:34:34.389 --> 00:34:36.487
only has 1 over n everywhere.

00:34:41.469 --> 00:34:49.820
OK, so the whole thing is 1
minus 1 over n on the diagonals

00:34:49.820 --> 00:34:57.430
and then minus 1
over n here, OK?

00:34:57.430 --> 00:35:01.920
And now I claim that this matrix
is an orthogonal projector.

00:35:01.920 --> 00:35:05.580
Now, I'm writing this, but
it's completely useless.

00:35:05.580 --> 00:35:08.190
This is just a way for you to
see that it's actually very

00:35:08.190 --> 00:35:11.430
convenient now to think
about this problem

00:35:11.430 --> 00:35:14.850
as being a matrix
problem, because things

00:35:14.850 --> 00:35:17.890
are much nicer when you
think about the actual form

00:35:17.890 --> 00:35:18.890
of your matrices, right?

00:35:18.890 --> 00:35:21.090
They could tell you,
here is the matrix.

00:35:21.090 --> 00:35:23.340
I mean, imagine you're
sitting at a midterm,

00:35:23.340 --> 00:35:25.910
and I say, here's the
matrix that has 1 minus 1

00:35:25.910 --> 00:35:28.640
over n on the diagonals
and minus 1 over n

00:35:28.640 --> 00:35:30.010
on the [INAUDIBLE] diagonal.

00:35:30.010 --> 00:35:32.855
Prove to me that it's
a projector matrix.

00:35:32.855 --> 00:35:34.230
You're going to
have to basically

00:35:34.230 --> 00:35:35.520
take this guy times itself.

00:35:35.520 --> 00:35:37.497
It's going to be really
complicated, right?

00:35:37.497 --> 00:35:38.580
So we know it's symmetric.

00:35:38.580 --> 00:35:39.930
That's for sure.

00:35:39.930 --> 00:35:42.120
But the fact that it
has this particular way

00:35:42.120 --> 00:35:44.100
of writing it is
going to make my life

00:35:44.100 --> 00:35:45.599
super easy to check this.

00:35:45.599 --> 00:35:47.140
That's the definition
of a projector.

00:35:47.140 --> 00:35:48.930
It has to be
symmetric and it has

00:35:48.930 --> 00:35:51.270
to square to itself
because we just

00:35:51.270 --> 00:35:54.300
said in the chapter
on linear regression

00:35:54.300 --> 00:35:57.360
that once you project, if you
apply the projection again,

00:35:57.360 --> 00:35:59.610
you're not moving because
you're already there.

00:35:59.610 --> 00:36:04.469
OK, so why is H
squared equal to H?

00:36:04.469 --> 00:36:05.760
Well let's just write H square.

00:36:05.760 --> 00:36:09.300
It's the identity
minus 1 over n 1, 1

00:36:09.300 --> 00:36:16.610
transpose times the
identity minus 1 over n 1, 1

00:36:16.610 --> 00:36:19.370
transpose, right?

00:36:19.370 --> 00:36:22.490
Let's just expand this now.

00:36:22.490 --> 00:36:25.350
This is equal to
the identity minus--

00:36:25.350 --> 00:36:29.280
well, the identity times 1, 1
transpose is just the identity.

00:36:29.280 --> 00:36:31.900
So it's 1, 1 transpose, sorry.

00:36:31.900 --> 00:36:38.840
So 1 over n 1, 1 transpose
minus 1 over n 1, 1 transpose.

00:36:38.840 --> 00:36:40.400
And then there's
going to be what

00:36:40.400 --> 00:36:42.710
makes the deal is that
I get this 1 over n

00:36:42.710 --> 00:36:44.750
squared this time.

00:36:44.750 --> 00:36:46.950
And then I get the product
of 1 over n trans--

00:36:46.950 --> 00:36:48.200
oh, let's write it completely.

00:36:48.200 --> 00:36:58.010
I get 1, 1 transpose
times 1, 1 transpose, OK?

00:36:58.010 --> 00:37:01.260
But this thing here--

00:37:01.260 --> 00:37:03.840
what is this?

00:37:03.840 --> 00:37:06.359
n, right, is the end product
of the all-ones vector

00:37:06.359 --> 00:37:07.400
with the all-ones vector.

00:37:07.400 --> 00:37:10.740
So I'm just summing n times
1 squared, which is n.

00:37:10.740 --> 00:37:11.980
So this is equal to n.

00:37:11.980 --> 00:37:13.920
So I pull it out,
cancel one of the ends,

00:37:13.920 --> 00:37:15.870
and I'm back to
what I had before.

00:37:15.870 --> 00:37:21.720
So I had identity minus 2
over n 1, 1 transpose plus 1

00:37:21.720 --> 00:37:27.530
over n 1, 1 transpose
which is equal to H.

00:37:27.530 --> 00:37:30.700
Because one of the 1
over n's cancel, OK?

00:37:36.264 --> 00:37:37.430
So it's a projection matrix.

00:37:37.430 --> 00:37:41.030
It's projecting onto
some linear space, right?

00:37:41.030 --> 00:37:42.450
It's taking a matrix.

00:37:42.450 --> 00:37:44.480
Sorry, it's taking
a vector and it's

00:37:44.480 --> 00:37:46.535
projecting onto a
certain space of vectors.

00:37:49.255 --> 00:37:50.229
What is this space?

00:37:53.160 --> 00:37:54.920
Right, so, how do
you-- so I'm only

00:37:54.920 --> 00:37:57.500
asking the answer to this
question in words, right?

00:37:57.500 --> 00:37:59.830
So how would you
describe the vectors

00:37:59.830 --> 00:38:02.950
onto which this
matrix is projecting?

00:38:02.950 --> 00:38:05.050
Well, if you want to
answer this question,

00:38:05.050 --> 00:38:07.870
the way you would tackle
it is first by saying, OK,

00:38:07.870 --> 00:38:13.690
what does a vector which is of
the form, H times something,

00:38:13.690 --> 00:38:14.960
look like, right?

00:38:14.960 --> 00:38:16.870
What can I say about
this vector that's

00:38:16.870 --> 00:38:19.540
going to be definitely
giving me something

00:38:19.540 --> 00:38:21.760
about the space on
which it projects?

00:38:21.760 --> 00:38:24.800
I need to know a little more to
know that it projects exactly

00:38:24.800 --> 00:38:25.820
onto this.

00:38:25.820 --> 00:38:29.050
But one way we can
do this is just

00:38:29.050 --> 00:38:30.440
see how it acts on a vector.

00:38:30.440 --> 00:38:32.370
What does it do to a
vector to apply H, right?

00:38:32.370 --> 00:38:44.550
So I take v. And let's see what
taking v and applying H to it

00:38:44.550 --> 00:38:46.410
looks like.

00:38:46.410 --> 00:38:48.750
Well, it's the identity
minus something.

00:38:48.750 --> 00:38:50.640
So it takes v and
it removes something

00:38:50.640 --> 00:38:54.160
from v. What does it remove?

00:38:54.160 --> 00:39:00.590
Well, it's 1 over n
times v transpose 1 times

00:39:00.590 --> 00:39:03.861
the all-ones vector, right?

00:39:03.861 --> 00:39:04.360
Agreed?

00:39:04.360 --> 00:39:13.570
I just wrote v transpose 1
instead of 1 transpose v,

00:39:13.570 --> 00:39:16.250
which are the same thing.

00:39:16.250 --> 00:39:17.310
What is this thing?

00:39:25.160 --> 00:39:27.765
What should I call it in
mathematical notation?

00:39:30.720 --> 00:39:31.460
v bar, right?

00:39:31.460 --> 00:39:35.150
I should all it v bar because
this is exactly the average

00:39:35.150 --> 00:39:38.840
of the entries of v, agreed?

00:39:38.840 --> 00:39:41.560
This is summing the entries
of v's, and this is dividing

00:39:41.560 --> 00:39:43.170
by the number of those v's.

00:39:43.170 --> 00:39:44.860
Sorry, now v is in our--

00:39:49.162 --> 00:39:51.074
sorry, why do I divide by--

00:39:53.950 --> 00:39:59.070
I'm just-- OK, I need to check
what my dimensions are now.

00:39:59.070 --> 00:40:00.390
No, it's in Rd, right?

00:40:00.390 --> 00:40:02.660
So why do I divide by n?

00:40:05.520 --> 00:40:07.720
So it's not really v bar.

00:40:07.720 --> 00:40:13.910
It's the sum of the
v's divided by--

00:40:13.910 --> 00:40:14.870
right, so it's v bar.

00:40:24.024 --> 00:40:25.163
AUDIENCE: [INAUDIBLE]

00:40:25.163 --> 00:40:25.996
[INTERPOSING VOICES]

00:40:25.996 --> 00:40:27.968
AUDIENCE: Yeah, v
has to be [INAUDIBLE]

00:40:27.968 --> 00:40:29.450
PHILIPPE RIGOLLET: Oh, yeah.

00:40:29.450 --> 00:40:31.120
OK, thank you.

00:40:31.120 --> 00:40:34.750
So everywhere I wrote
Hd, that was actually Hn.

00:40:34.750 --> 00:40:35.290
Oh, man.

00:40:35.290 --> 00:40:37.220
I wish I had a computer now.

00:40:37.220 --> 00:40:37.720
All right.

00:40:37.720 --> 00:40:43.230
So-- yeah, because the--

00:40:43.230 --> 00:40:43.740
yeah, right?

00:40:43.740 --> 00:40:45.775
So why it's not--

00:40:45.775 --> 00:40:48.150
well, why I thought it was
this is because I was thinking

00:40:48.150 --> 00:40:49.890
about the outer
dimension of X, really

00:40:49.890 --> 00:40:51.780
of X transpose, which is
really the inner dimension,

00:40:51.780 --> 00:40:52.914
didn't matter to me, right?

00:40:52.914 --> 00:40:55.080
So the thing that I can
sandwich between X transpose

00:40:55.080 --> 00:40:56.790
and X has to be n by n.

00:40:56.790 --> 00:40:58.800
So this was actually n by n.

00:40:58.800 --> 00:41:00.480
And so that's actually n by n.

00:41:00.480 --> 00:41:03.330
Everything is n by n.

00:41:03.330 --> 00:41:04.308
Sorry about that.

00:41:08.220 --> 00:41:09.400
So this is n.

00:41:09.400 --> 00:41:10.440
This is n.

00:41:10.440 --> 00:41:12.130
This is-- well, I
didn't really tell you

00:41:12.130 --> 00:41:16.290
what the all-ones vector
was, but it's also in our n.

00:41:16.290 --> 00:41:18.430
Yeah, OK.

00:41:22.190 --> 00:41:23.730
Thank you.

00:41:23.730 --> 00:41:27.939
And n-- actually, I used the
fact that this was of size n

00:41:27.939 --> 00:41:28.480
here already.

00:41:31.690 --> 00:41:33.340
OK, and so that's indeed v bar.

00:41:38.996 --> 00:41:40.870
So what is this projection
doing to a vector?

00:41:47.470 --> 00:41:51.930
It's removing its average
on each coordinate, right?

00:41:51.930 --> 00:41:54.570
And the effect of this
is that v is a vector.

00:41:54.570 --> 00:41:58.355
What is the average of Hv?

00:41:58.355 --> 00:41:59.340
AUDIENCE: 0.

00:41:59.340 --> 00:42:00.840
PHILIPPE RIGOLLET:
Right, so it's 0.

00:42:00.840 --> 00:42:04.050
It's the average of v, which
is v bar, minus the average

00:42:04.050 --> 00:42:07.230
of something that only has v
bar's entry, which is v bar.

00:42:07.230 --> 00:42:08.490
So this thing is actually 0.

00:42:11.560 --> 00:42:12.840
So let me repeat my question.

00:42:12.840 --> 00:42:15.700
Onto what subspace
does H project?

00:42:22.700 --> 00:42:26.670
Onto the subspace of
vectors that have mean 0.

00:42:26.670 --> 00:42:30.010
A vector that has
mean 0 is a vector.

00:42:30.010 --> 00:42:34.970
So if you want to talk more
linear algebra, v bar--

00:42:34.970 --> 00:42:36.750
for a vector you
have mean 0, it means

00:42:36.750 --> 00:42:43.440
that v is orthogonal to the
span of the all-ones vector.

00:42:43.440 --> 00:42:44.280
That's it.

00:42:44.280 --> 00:42:46.080
It projects to this space.

00:42:46.080 --> 00:42:47.930
So in words, it
projects onto the space

00:42:47.930 --> 00:42:49.880
of vectors that have 0 mean.

00:42:49.880 --> 00:42:52.380
In linear algebra,
it says it projects

00:42:52.380 --> 00:42:55.760
onto the hyperplane
which is orthogonal

00:42:55.760 --> 00:42:58.360
to the all-ones vector, OK?

00:42:58.360 --> 00:43:01.860
So that's all.

00:43:01.860 --> 00:43:04.760
Can you guys still
see the screen?

00:43:04.760 --> 00:43:05.940
Are you good over there?

00:43:05.940 --> 00:43:07.420
OK.

00:43:07.420 --> 00:43:12.030
All right, so now, what it
means is that, well, I'm

00:43:12.030 --> 00:43:13.280
doing this weird thing, right?

00:43:13.280 --> 00:43:15.360
I'm taking the inner product--

00:43:15.360 --> 00:43:20.030
so S is taking X. And then
it's removing its mean of each

00:43:20.030 --> 00:43:21.440
of the columns of X, right?

00:43:21.440 --> 00:43:24.530
When I take H times X, I'm
basically applying this

00:43:24.530 --> 00:43:26.780
projection which consists
in removing the mean of all

00:43:26.780 --> 00:43:28.430
the X's.

00:43:28.430 --> 00:43:31.340
And then I multiply
by H transpose.

00:43:31.340 --> 00:43:33.550
But what's actually
nice is that, remember,

00:43:33.550 --> 00:43:35.930
H is a projector.

00:43:35.930 --> 00:43:38.000
Sorry, I don't
want to keep that.

00:43:38.000 --> 00:43:47.010
Which means that when I
look at X transpose HX,

00:43:47.010 --> 00:43:52.410
it's the same as looking
at X transpose H squared X.

00:43:52.410 --> 00:43:54.420
But since H is equal
to its transpose,

00:43:54.420 --> 00:43:58.020
this is actually the same
as looking at X transpose H

00:43:58.020 --> 00:44:07.146
transpose HX, which is the
same as looking at HX transpose

00:44:07.146 --> 00:44:11.000
HX, OK?

00:44:11.000 --> 00:44:14.300
So what it's doing, it's
first applying this projection

00:44:14.300 --> 00:44:18.950
matrix, H, which removes the
mean of each of your columns,

00:44:18.950 --> 00:44:23.000
and then looks at the inner
products between those guys,

00:44:23.000 --> 00:44:23.586
right?

00:44:23.586 --> 00:44:25.460
Each entry of this guy
is just the covariance

00:44:25.460 --> 00:44:27.320
between those centered things.

00:44:27.320 --> 00:44:28.910
That's all it's doing.

00:44:28.910 --> 00:44:35.450
All right, so those are actually
going to be the key statements.

00:44:35.450 --> 00:44:37.270
So everything we've
done so far is really

00:44:37.270 --> 00:44:38.920
mainly linear algebra, right?

00:44:38.920 --> 00:44:41.950
I mean, looking at expectations
and covariances was just--

00:44:41.950 --> 00:44:44.200
we just used the fact that
the expectation was linear.

00:44:44.200 --> 00:44:45.520
We didn't do much.

00:44:45.520 --> 00:44:47.450
But now there's a nice
thing that's happening.

00:44:47.450 --> 00:44:50.050
And that's why we're
going to switch

00:44:50.050 --> 00:44:51.550
from the language
of linear algebra

00:44:51.550 --> 00:44:53.710
to more statistical,
because what's happening

00:44:53.710 --> 00:44:57.010
is that if I look at this
quadratic form, right?

00:44:57.010 --> 00:44:59.080
So I take sigma.

00:44:59.080 --> 00:45:00.462
So I take a vector, u.

00:45:03.630 --> 00:45:09.180
And I'm going to look at
u-- so let's say, in Rd.

00:45:09.180 --> 00:45:14.796
And I'm going to look
at u transpose sigma u.

00:45:14.796 --> 00:45:15.295
OK?

00:45:18.510 --> 00:45:19.720
What is this doing?

00:45:19.720 --> 00:45:24.630
Well, we know that u transpose
sigma u is equal to what?

00:45:24.630 --> 00:45:31.720
Well, sigma is the
expectation of XX transpose

00:45:31.720 --> 00:45:35.610
minus the expectation of X
expectation of X transpose,

00:45:35.610 --> 00:45:36.110
right?

00:45:39.460 --> 00:45:40.948
So I just substitute in there.

00:45:46.100 --> 00:45:49.370
Now, u is deterministic.

00:45:49.370 --> 00:45:52.250
So in particular, I can push
it inside the expectation

00:45:52.250 --> 00:45:55.180
here, agreed?

00:45:55.180 --> 00:45:57.200
And I can do the
same from the right.

00:45:57.200 --> 00:46:00.800
So here, when I push u
transpose here, and u here,

00:46:00.800 --> 00:46:06.170
what I'm left with is the
expectation of u transpose X

00:46:06.170 --> 00:46:09.990
times X transpose u.

00:46:09.990 --> 00:46:11.436
OK?

00:46:11.436 --> 00:46:14.050
And now, I can do the
same thing for this guy.

00:46:14.050 --> 00:46:17.410
And this tells me that this is
the expectation of u transpose

00:46:17.410 --> 00:46:21.340
X times the expectation
of X transpose u.

00:46:24.640 --> 00:46:29.260
Of course, u transpose X
is equal to X transpose u.

00:46:29.260 --> 00:46:31.330
And u-- yeah.

00:46:31.330 --> 00:46:33.910
So what it means is
that this is actually

00:46:33.910 --> 00:46:43.700
equal to the expectation
of u transpose X squared

00:46:43.700 --> 00:46:48.020
minus the expectation
of u transpose X,

00:46:48.020 --> 00:46:49.065
the whole thing squared.

00:46:56.900 --> 00:46:58.900
But this is something
that should look familiar.

00:46:58.900 --> 00:47:01.316
This is really just the variance
of this particular random

00:47:01.316 --> 00:47:03.360
variable which is of
the form, u transpose X,

00:47:03.360 --> 00:47:06.900
right? u transpose
X is a number.

00:47:06.900 --> 00:47:10.110
It involves a random vector,
so it's a random variable.

00:47:10.110 --> 00:47:11.580
And so it has a variance.

00:47:11.580 --> 00:47:15.430
And this variance is exactly
given by this formula.

00:47:15.430 --> 00:47:19.595
So this is just the
variance of u transpose X.

00:47:19.595 --> 00:47:21.720
So what we've proved is
that if I look at this guy,

00:47:21.720 --> 00:47:29.772
this is really just the
variance of u transpose X, OK?

00:47:37.580 --> 00:47:40.930
I can do the same thing
for the sample variance.

00:47:40.930 --> 00:47:41.770
So let's do this.

00:47:48.240 --> 00:47:52.140
And as you can
see, spoiler alert,

00:47:52.140 --> 00:47:56.334
this is going to be
the sample variance.

00:47:59.590 --> 00:48:09.430
OK, so remember, S is 1 over n,
sum of Xi Xi transpose minus X

00:48:09.430 --> 00:48:12.100
bar X bar transpose.

00:48:12.100 --> 00:48:16.060
So when I do u
transpose, Su, what

00:48:16.060 --> 00:48:19.400
it gives me is 1 over
n sum from i equal 1

00:48:19.400 --> 00:48:25.780
to n of u transpose Xi times
Xi transpose u, all right?

00:48:25.780 --> 00:48:27.880
So those are two numbers
that multiply each other

00:48:27.880 --> 00:48:30.370
and that happen to be
equal to each other,

00:48:30.370 --> 00:48:36.430
minus u transpose X
bar X bar transpose u,

00:48:36.430 --> 00:48:38.770
which is also the product
of two numbers that happen

00:48:38.770 --> 00:48:39.997
to be equal to each other.

00:48:39.997 --> 00:48:41.455
So I can rewrite
this with squares.

00:48:55.120 --> 00:48:57.390
So we're almost there.

00:48:57.390 --> 00:49:00.360
All I need to know to check
is that this thing is actually

00:49:00.360 --> 00:49:02.010
the average of
those guys, right?

00:49:02.010 --> 00:49:04.530
So u transpose X bar.

00:49:04.530 --> 00:49:05.030
What is it?

00:49:05.030 --> 00:49:10.980
It's 1 over n sum from i equal
1 to n of u transpose Xi.

00:49:10.980 --> 00:49:17.050
So it's really something that I
can write as u transpose X bar,

00:49:17.050 --> 00:49:17.550
right?

00:49:17.550 --> 00:49:19.383
That's the average of
those random variables

00:49:19.383 --> 00:49:21.240
of the form, u transpose Xi.

00:49:23.880 --> 00:49:29.910
So what it means is that u
transpose Su, I can write as 1

00:49:29.910 --> 00:49:38.060
over n sum from i equal 1 to
n of u transpose Xi squared

00:49:38.060 --> 00:49:46.720
minus u transpose X
bar squared, which

00:49:46.720 --> 00:49:51.660
is the empirical variance
that we need noted by small

00:49:51.660 --> 00:49:54.600
s squared, right?

00:49:54.600 --> 00:50:06.850
So that's the empirical variance
of u transpose X1 all the way

00:50:06.850 --> 00:50:08.209
to u transpose Xn.

00:50:12.430 --> 00:50:13.910
OK, and here, same thing.

00:50:13.910 --> 00:50:15.210
I use exactly the same thing.

00:50:15.210 --> 00:50:17.990
I just use the fact that here,
the only thing I use is really

00:50:17.990 --> 00:50:20.790
the linearity of this
guy, of 1 over n sum

00:50:20.790 --> 00:50:24.020
or the linearity of expectation,
that I can push things

00:50:24.020 --> 00:50:26.740
in there, OK?

00:50:30.224 --> 00:50:31.640
AUDIENCE: So what
you have written

00:50:31.640 --> 00:50:33.844
at the end of that
sum for uT Su?

00:50:33.844 --> 00:50:35.010
PHILIPPE RIGOLLET: This one?

00:50:35.010 --> 00:50:35.380
AUDIENCE: Yeah.

00:50:35.380 --> 00:50:37.290
PHILIPPE RIGOLLET: Yeah, I
said it's equal to small s,

00:50:37.290 --> 00:50:39.430
and I want to make a
difference between the big S

00:50:39.430 --> 00:50:40.660
that I'm using here.

00:50:40.660 --> 00:50:42.650
So this is equal to small--

00:50:42.650 --> 00:50:45.190
I don't know, I'm
trying to make it look

00:50:45.190 --> 00:50:47.550
like a calligraphic s squared.

00:50:56.870 --> 00:51:00.040
OK, so this is nice, right?

00:51:00.040 --> 00:51:04.120
This covariance matrix-- so
let's look at capital sigma

00:51:04.120 --> 00:51:05.210
itself right now.

00:51:05.210 --> 00:51:07.070
This covariance matrix,
we know that if we

00:51:07.070 --> 00:51:11.690
read its entries, what
we get is the covariance

00:51:11.690 --> 00:51:15.260
between the coordinates
of the X's, right,

00:51:15.260 --> 00:51:19.140
of the random vector, X.
And the coordinates, well,

00:51:19.140 --> 00:51:22.530
by definition, are attached
to a coordinate system.

00:51:22.530 --> 00:51:25.830
So I only know
what the covariance

00:51:25.830 --> 00:51:30.570
of X in of those two things are,
or the covariance of those two

00:51:30.570 --> 00:51:31.320
things are.

00:51:31.320 --> 00:51:33.570
But what if I want to find
coordinates between linear

00:51:33.570 --> 00:51:35.076
combination of the X's?

00:51:35.076 --> 00:51:37.200
Sorry, if I want to find
covariances between linear

00:51:37.200 --> 00:51:38.566
combination of those X's.

00:51:38.566 --> 00:51:40.440
And that's exactly what
this allows me to do.

00:51:40.440 --> 00:51:44.640
It says, well, if I pre-
and post-multiply by u,

00:51:44.640 --> 00:51:47.010
this is actually telling
me what the variance

00:51:47.010 --> 00:51:51.950
of X along direction u is, OK?

00:51:51.950 --> 00:51:53.944
So there's a lot of
information in there,

00:51:53.944 --> 00:51:55.610
and it's just really
exploiting the fact

00:51:55.610 --> 00:52:00.600
that there is some linearity
going on in the covariance.

00:52:00.600 --> 00:52:02.060
So, why variance?

00:52:02.060 --> 00:52:03.870
Why is variance
interesting for us, right?

00:52:03.870 --> 00:52:04.370
Why?

00:52:04.370 --> 00:52:05.760
I started by saying,
here, we're going

00:52:05.760 --> 00:52:07.050
to be interested
in having something

00:52:07.050 --> 00:52:08.151
to do dimension reduction.

00:52:08.151 --> 00:52:10.650
We have-- think of your points
as [? being in a ?] dimension

00:52:10.650 --> 00:52:13.990
larger than 4, and we're going
to try to reduce the dimension.

00:52:13.990 --> 00:52:15.480
So let's just think
for one second,

00:52:15.480 --> 00:52:19.320
what do we want about a
dimension reduction procedure?

00:52:19.320 --> 00:52:23.427
If I have all my points that
live in, say, three dimensions,

00:52:23.427 --> 00:52:25.260
and I have one point
here and one point here

00:52:25.260 --> 00:52:28.020
and one point here and one
point here and one point here,

00:52:28.020 --> 00:52:30.090
and I decide to project
them onto some plane--

00:52:30.090 --> 00:52:32.132
that I take a plane that's
just like this, what's

00:52:32.132 --> 00:52:34.673
going to happen is that those
points are all going to project

00:52:34.673 --> 00:52:36.030
to the same point, right?

00:52:36.030 --> 00:52:38.070
I'm just going to
not see anything.

00:52:38.070 --> 00:52:40.410
However, if I take a
plane which is like this,

00:52:40.410 --> 00:52:42.932
they're all going to
project into some nice line.

00:52:42.932 --> 00:52:44.640
Maybe I can even
project them onto a line

00:52:44.640 --> 00:52:47.160
and they will still be
far apart from each other.

00:52:47.160 --> 00:52:48.160
So that's what you want.

00:52:48.160 --> 00:52:51.930
You want to be able to
say, when I take my points

00:52:51.930 --> 00:52:54.610
and I say I project them
onto lower dimensions,

00:52:54.610 --> 00:52:57.270
I do not want them to collapse
into one single point.

00:52:57.270 --> 00:53:00.540
I want them to be spread as
possible in the direction

00:53:00.540 --> 00:53:02.251
on which I project.

00:53:02.251 --> 00:53:04.000
And this is what we're
going to try to do.

00:53:04.000 --> 00:53:06.510
And of course, measuring
spread between points

00:53:06.510 --> 00:53:08.160
can be done in many ways, right?

00:53:08.160 --> 00:53:09.960
I mean, you could
look at, I don't know,

00:53:09.960 --> 00:53:12.900
sum of pairwise distances
between those guys.

00:53:12.900 --> 00:53:14.790
You could look at
some sort of energy.

00:53:14.790 --> 00:53:16.380
You can look at
many ways to measure

00:53:16.380 --> 00:53:18.199
of spread in a direction.

00:53:18.199 --> 00:53:19.740
But variance is a
good way to measure

00:53:19.740 --> 00:53:21.150
of spread between points.

00:53:21.150 --> 00:53:23.727
If you have a lot of
variance between your points,

00:53:23.727 --> 00:53:25.560
then chances are they're
going to be spread.

00:53:25.560 --> 00:53:27.720
Now, this is not
always the case, right?

00:53:27.720 --> 00:53:30.480
If I have a direction in which
all my points are clumped

00:53:30.480 --> 00:53:33.234
onto one big point and
one other big point,

00:53:33.234 --> 00:53:34.900
it's going to choose
this because that's

00:53:34.900 --> 00:53:37.180
the direction that
has a lot of variance.

00:53:37.180 --> 00:53:39.030
But hopefully, the
variance is going

00:53:39.030 --> 00:53:41.560
to spread things out nicely.

00:53:41.560 --> 00:53:47.730
So the idea of principal
component analysis

00:53:47.730 --> 00:53:51.330
is going to try to
identify those variances--

00:53:51.330 --> 00:53:55.740
those directions along which
we have a lot of variance.

00:53:55.740 --> 00:53:57.870
Reciprocally, we're
going to try to eliminate

00:53:57.870 --> 00:54:01.890
the directions along which we do
not have a lot of variance, OK?

00:54:01.890 --> 00:54:02.640
And let's see why.

00:54:02.640 --> 00:54:08.130
Well, if-- so here's
the first claim.

00:54:08.130 --> 00:54:14.000
If you transpose Su is equal
to 0, what's happening?

00:54:14.000 --> 00:54:17.159
Well, I know that an empirical
variance is equal to 0.

00:54:17.159 --> 00:54:18.950
What does it mean for
an empirical variance

00:54:18.950 --> 00:54:22.056
to be equal to 0?

00:54:22.056 --> 00:54:23.680
So I give you a bunch
of points, right?

00:54:23.680 --> 00:54:26.420
So those points are those
points-- u transpose

00:54:26.420 --> 00:54:29.090
X1, u transpose-- those
are a bunch of numbers.

00:54:29.090 --> 00:54:31.090
What does it mean to have
the empirical variance

00:54:31.090 --> 00:54:33.279
of those points
being equal to 0?

00:54:33.279 --> 00:54:34.570
AUDIENCE: They're all the same.

00:54:34.570 --> 00:54:36.590
PHILIPPE RIGOLLET:
They're all the same.

00:54:36.590 --> 00:54:43.680
So what it means is that
when I have my points, right?

00:54:43.680 --> 00:54:46.470
So, can you find a direction
for those points in which they

00:54:46.470 --> 00:54:48.850
project to all the same point?

00:54:51.400 --> 00:54:52.360
No, right?

00:54:52.360 --> 00:54:53.590
There's no such thing.

00:54:53.590 --> 00:54:55.870
For this to happen, you have
to have your points which

00:54:55.870 --> 00:54:57.849
are perfectly aligned.

00:54:57.849 --> 00:54:59.390
And then when you're
going to project

00:54:59.390 --> 00:55:01.830
onto the orthogonal
of this guy, they're

00:55:01.830 --> 00:55:03.690
going to all project
to the same point

00:55:03.690 --> 00:55:06.450
here, which means that
the empirical variance is

00:55:06.450 --> 00:55:08.790
going to be 0.

00:55:08.790 --> 00:55:10.270
Now, this is an extreme case.

00:55:10.270 --> 00:55:11.760
This will never
happen in practice,

00:55:11.760 --> 00:55:13.840
because if that
happens, well, I mean,

00:55:13.840 --> 00:55:16.850
you can basically figure
that out very quickly.

00:55:16.850 --> 00:55:21.520
So in the same way,
it's very unlikely

00:55:21.520 --> 00:55:23.710
that you're going to have
u transpose sigma u, which

00:55:23.710 --> 00:55:26.230
is equal to 0, which means
that, essentially, all

00:55:26.230 --> 00:55:28.510
your points are [INAUDIBLE]
or let's say all of them

00:55:28.510 --> 00:55:30.069
are orthogonal to u, right?

00:55:30.069 --> 00:55:31.360
So it's exactly the same thing.

00:55:31.360 --> 00:55:33.330
It just says that in
the population case,

00:55:33.330 --> 00:55:36.960
there's no probability that your
points deviate from this guy

00:55:36.960 --> 00:55:37.510
here.

00:55:37.510 --> 00:55:41.142
This happens with
zero probability, OK?

00:55:41.142 --> 00:55:42.600
And that's just
because if you look

00:55:42.600 --> 00:55:46.690
at the variance of this
guy, it's going to be 0.

00:55:46.690 --> 00:55:48.910
And then that means that
there's no deviation.

00:55:48.910 --> 00:55:51.430
By the way, I'm using
the name projection

00:55:51.430 --> 00:55:55.510
when I talk about u
transpose X, right?

00:55:55.510 --> 00:55:59.170
So let's just be
clear about this.

00:55:59.170 --> 00:56:04.090
If you-- so let's say I
have a bunch of points,

00:56:04.090 --> 00:56:06.050
and u is a vector
in this direction.

00:56:06.050 --> 00:56:08.650
And let's say that u has the--

00:56:08.650 --> 00:56:10.120
so this is 0.

00:56:10.120 --> 00:56:10.720
This is u.

00:56:10.720 --> 00:56:17.560
And let's say that
u has norm, 1, OK?

00:56:17.560 --> 00:56:21.140
When I look, what is the
coordinate of the projection?

00:56:21.140 --> 00:56:23.860
So what is the length
of this guy here?

00:56:23.860 --> 00:56:25.569
Let's call this guy X1.

00:56:25.569 --> 00:56:26.860
What is the length of this guy?

00:56:31.150 --> 00:56:32.330
In terms of inner products?

00:56:35.990 --> 00:56:39.678
This is exactly u transpose X1.

00:56:39.678 --> 00:56:42.730
This length here,
if this is X2, this

00:56:42.730 --> 00:56:46.580
is exactly u transpose X2, OK?

00:56:46.580 --> 00:56:52.430
So those-- u transpose X
measure exactly the distance

00:56:52.430 --> 00:56:55.700
to the origin of those--

00:56:55.700 --> 00:56:58.310
I mean, it's really--

00:56:58.310 --> 00:57:00.887
think of it as being
just an x-axis thing.

00:57:00.887 --> 00:57:02.220
You just have a bunch of points.

00:57:02.220 --> 00:57:02.960
You have an origin.

00:57:02.960 --> 00:57:04.520
And it's really just
telling you what

00:57:04.520 --> 00:57:07.670
the coordinate on this
axis is going to be, right?

00:57:07.670 --> 00:57:10.820
So in particular, if the
empirical variance is 0,

00:57:10.820 --> 00:57:12.470
it means that all
these points project

00:57:12.470 --> 00:57:14.840
to the same point, which
means that they have

00:57:14.840 --> 00:57:16.912
to be orthogonal to this guy.

00:57:16.912 --> 00:57:19.370
And you can think of it as
being also maybe an entire plane

00:57:19.370 --> 00:57:23.990
that's orthogonal
to this line, OK?

00:57:23.990 --> 00:57:26.590
So that's why I talk
about projection,

00:57:26.590 --> 00:57:29.560
because the inner
products, u transpose X,

00:57:29.560 --> 00:57:36.220
is really measuring
the coordinates of X

00:57:36.220 --> 00:57:39.410
when u becomes the x-axis.

00:57:39.410 --> 00:57:42.820
Now, if u does not have
norm 1, then you just

00:57:42.820 --> 00:57:44.365
have a change of scale here.

00:57:44.365 --> 00:57:46.790
You just have a
change of unit, right?

00:57:46.790 --> 00:57:51.560
So this is really u times X1.

00:57:51.560 --> 00:57:54.044
The coordinates should really
be divided by the norm of u.

00:57:59.150 --> 00:58:04.970
OK, so now, just in
the same way-- so

00:58:04.970 --> 00:58:07.160
we're never going
to have exactly 0.

00:58:07.160 --> 00:58:08.810
But if we [INAUDIBLE]
the other end,

00:58:08.810 --> 00:58:12.050
if u transpose Su is
large, what does it mean?

00:58:14.990 --> 00:58:17.740
It means that when
I look at my points

00:58:17.740 --> 00:58:22.194
as projected onto the
axis generated by u,

00:58:22.194 --> 00:58:23.860
they're going to have
a lot of variance.

00:58:23.860 --> 00:58:25.930
They're going to be far away
from each other in average,

00:58:25.930 --> 00:58:26.430
right?

00:58:26.430 --> 00:58:28.900
That's what large variance
means, or at least

00:58:28.900 --> 00:58:31.310
large empirical variance means.

00:58:31.310 --> 00:58:34.690
And same thing for u.

00:58:34.690 --> 00:58:36.130
So what we're going
to try to find

00:58:36.130 --> 00:58:39.870
is a u that maximizes this.

00:58:39.870 --> 00:58:42.230
If I can find a u
that maximizes this

00:58:42.230 --> 00:58:44.790
so I can look in
every direction,

00:58:44.790 --> 00:58:48.320
and suddenly I find a direction
in which the spread is massive,

00:58:48.320 --> 00:58:50.070
then that's a point
on which I'm basically

00:58:50.070 --> 00:58:52.260
the less likely
to have my points

00:58:52.260 --> 00:58:54.824
project onto each other
and collide, right?

00:58:54.824 --> 00:58:56.490
At least I know they're
going to project

00:58:56.490 --> 00:58:59.710
at least onto two points.

00:58:59.710 --> 00:59:02.290
So the idea now is
to say, OK, let's try

00:59:02.290 --> 00:59:04.630
to maximize this spread, right?

00:59:04.630 --> 00:59:09.130
So we're going to try to
find the maximum over all u's

00:59:09.130 --> 00:59:12.886
of u transpose Su.

00:59:12.886 --> 00:59:15.010
And that's going to be the
direction that maximizes

00:59:15.010 --> 00:59:15.968
the empirical variance.

00:59:15.968 --> 00:59:22.075
Now of course, if I read it
like that for all u's in Rd,

00:59:22.075 --> 00:59:23.666
what is the value
of this maximum?

00:59:28.060 --> 00:59:29.220
It's infinity, right?

00:59:29.220 --> 00:59:32.160
Because I can always
multiply u by 10,

00:59:32.160 --> 00:59:34.662
and this entire thing is
going to multiplied by 100.

00:59:34.662 --> 00:59:36.620
So I'm just going to take
u as large as I want,

00:59:36.620 --> 00:59:38.661
and this thing is going
to be as large as I want,

00:59:38.661 --> 00:59:40.050
and so I need to constrain u.

00:59:40.050 --> 00:59:42.840
And as I said, I need
to have u of size 1

00:59:42.840 --> 00:59:45.990
to talk about coordinates
in the system generated

00:59:45.990 --> 00:59:47.340
by u like this.

00:59:47.340 --> 00:59:50.730
So I'm just going to
constrain u to have

00:59:50.730 --> 00:59:55.467
Euclidean norm equal to 1, OK?

00:59:55.467 --> 00:59:57.050
So that's going to
be my goal-- trying

00:59:57.050 --> 01:00:01.100
to find the largest
possible u transpose Su,

01:00:01.100 --> 01:00:03.680
or in other words, empirical
variance of the points

01:00:03.680 --> 01:00:07.520
projected onto the direction
u when u is of norm 1,

01:00:07.520 --> 01:00:11.039
which justifies to use
the word, "direction,"

01:00:11.039 --> 01:00:12.830
and because there's no
magnitude to this u.

01:00:17.770 --> 01:00:22.410
OK, so how am I
going to do this?

01:00:22.410 --> 01:00:25.230
I could just fold and
say, let's just optimize

01:00:25.230 --> 01:00:26.700
this thing, right?

01:00:26.700 --> 01:00:28.540
Let's just take this problem.

01:00:28.540 --> 01:00:32.250
It says maximize a function
onto some constraints.

01:00:32.250 --> 01:00:34.125
Immediately, the constraint
is sort of nasty.

01:00:34.125 --> 01:00:37.212
I'm on a sphere, and I'm trying
to move points on the sphere.

01:00:37.212 --> 01:00:38.670
And I'm maximizing
this thing which

01:00:38.670 --> 01:00:40.182
actually happens to be convex.

01:00:40.182 --> 01:00:42.390
And we know we know how to
minimize convex functions,

01:00:42.390 --> 01:00:45.280
but maximize them is
a different question.

01:00:45.280 --> 01:00:47.340
And so this problem
might be super hard.

01:00:47.340 --> 01:00:49.020
So I can just say,
OK, here's what

01:00:49.020 --> 01:00:52.950
I want to do, and let me
give that to an optimizer

01:00:52.950 --> 01:00:56.010
and just hope that the optimizer
can solve this problem for me.

01:00:56.010 --> 01:00:57.630
That's one thing we can do.

01:00:57.630 --> 01:01:00.092
Now as you can imagine, PCA
is so well spread, right?

01:01:00.092 --> 01:01:01.800
Principal component
analysis is something

01:01:01.800 --> 01:01:03.700
that people do constantly.

01:01:03.700 --> 01:01:06.190
And so that means that we
know how to do this fast.

01:01:06.190 --> 01:01:07.600
So that's one thing.

01:01:07.600 --> 01:01:10.740
The other thing that you should
probably question about why--

01:01:10.740 --> 01:01:13.110
if this thing is actually
difficult, why in the world

01:01:13.110 --> 01:01:16.200
would you even choose the
variance as a measure of spread

01:01:16.200 --> 01:01:19.020
if there's so many
measures of spread, right?

01:01:19.020 --> 01:01:21.222
The variance is one
measure of spread.

01:01:21.222 --> 01:01:22.680
It's not guaranteed
that everything

01:01:22.680 --> 01:01:26.366
is going to project nicely
far apart from each other.

01:01:26.366 --> 01:01:27.990
So we could choose
the variance, but we

01:01:27.990 --> 01:01:28.800
could choose something else.

01:01:28.800 --> 01:01:30.990
If the variance does
not help, why choose it?

01:01:30.990 --> 01:01:32.520
Turns out the variance helps.

01:01:32.520 --> 01:01:35.555
So this is indeed a
non-convex problem.

01:01:35.555 --> 01:01:38.340
I'm maximizing, so
it's actually the same.

01:01:38.340 --> 01:01:41.850
I can make this
constraint convex

01:01:41.850 --> 01:01:43.920
because I'm maximizing
a convex function,

01:01:43.920 --> 01:01:45.720
so it's clear that
the maximum is going

01:01:45.720 --> 01:01:47.220
to be attained at the boundary.

01:01:47.220 --> 01:01:51.540
So I can actually just fill
this ball into some convex ball.

01:01:51.540 --> 01:01:53.430
However, I'm still
maximizing, so this

01:01:53.430 --> 01:01:55.170
is a non-convex problem.

01:01:55.170 --> 01:01:57.550
And this turns out to be the
fanciest non-convex problem

01:01:57.550 --> 01:01:59.001
we know how to solve.

01:01:59.001 --> 01:02:00.750
And the reason why we
know how to solve it

01:02:00.750 --> 01:02:04.410
is not because of optimization
or using gradient-type things

01:02:04.410 --> 01:02:06.690
or anything of the
algorithms that I mentioned

01:02:06.690 --> 01:02:09.350
during the maximum likelihood.

01:02:09.350 --> 01:02:11.000
It's because of linear algebra.

01:02:11.000 --> 01:02:13.980
Linear algebra guarantees that
we know how to solve this.

01:02:13.980 --> 01:02:17.885
And to understand this, we
need to go a little deeper

01:02:17.885 --> 01:02:22.360
in linear algebra, and we
need to understand the concept

01:02:22.360 --> 01:02:24.590
of diagonalization of a matrix.

01:02:24.590 --> 01:02:29.850
So who has ever seen the
concept of an eigenvalue?

01:02:29.850 --> 01:02:30.790
Oh, that's beautiful.

01:02:30.790 --> 01:02:31.880
And if you're not
raising your hand,

01:02:31.880 --> 01:02:33.588
you're just playing
"Candy Crush," right?

01:02:33.588 --> 01:02:35.930
All right, so, OK.

01:02:44.930 --> 01:02:46.640
This is great.

01:02:46.640 --> 01:02:48.160
Everybody's seen it.

01:02:48.160 --> 01:02:51.230
For my live audience of
millions, maybe you have not,

01:02:51.230 --> 01:02:53.600
so I will still go through it.

01:02:53.600 --> 01:02:58.840
All right, so one
of the basic facts--

01:02:58.840 --> 01:03:02.490
and I remember when
I learned this in--

01:03:02.490 --> 01:03:04.090
I mean, when I was
an undergrad, I

01:03:04.090 --> 01:03:05.860
learned about the
spectral decomposition

01:03:05.860 --> 01:03:07.450
and this diagonalization
of matrices.

01:03:07.450 --> 01:03:09.070
And for me, it was just
a structural property

01:03:09.070 --> 01:03:11.445
of matrices, but it turns out
that it's extremely useful,

01:03:11.445 --> 01:03:13.294
and it's useful for
algorithmic purposes.

01:03:13.294 --> 01:03:14.710
And so what this
theorem tells you

01:03:14.710 --> 01:03:16.765
is that if you take
a symmetric matrix--

01:03:22.860 --> 01:03:24.340
well, with real
entries, but that

01:03:24.340 --> 01:03:28.220
really does not matter so much.

01:03:28.220 --> 01:03:30.730
And here, I'm
going to actually--

01:03:30.730 --> 01:03:33.200
so I take a symmetric matrix,
and actually S and sigma

01:03:33.200 --> 01:03:36.190
are two such symmetric
matrices, right?

01:03:36.190 --> 01:03:44.500
Then there exists P
and D, which are both--

01:03:44.500 --> 01:03:47.000
so let's say d by d.

01:03:47.000 --> 01:03:55.960
Which are both d by d
such that P is orthogonal.

01:03:58.960 --> 01:04:02.420
That means that P transpose
P is equal to PP transpose

01:04:02.420 --> 01:04:06.360
is equal to the identity.

01:04:06.360 --> 01:04:07.630
And D is diagonal.

01:04:11.840 --> 01:04:20.130
And sigma, let's say, is
equal to PDP transpose, OK?

01:04:20.130 --> 01:04:22.080
So it's a diagonalization
because it's

01:04:22.080 --> 01:04:23.970
finding a nice transformation.

01:04:23.970 --> 01:04:25.260
P has some nice properties.

01:04:25.260 --> 01:04:28.050
It's really just the change
of coordinates in which

01:04:28.050 --> 01:04:31.044
your matrix is diagonal, right?

01:04:31.044 --> 01:04:32.460
And the way you
want to see this--

01:04:32.460 --> 01:04:35.610
and I think it sort of helps
to think about this problem

01:04:35.610 --> 01:04:36.720
as being--

01:04:36.720 --> 01:04:38.276
sigma being a covariance matrix.

01:04:38.276 --> 01:04:39.900
What does a covariance
matrix tell you?

01:04:39.900 --> 01:04:41.490
Think of a
multivariate Gaussian.

01:04:41.490 --> 01:04:43.660
Can everybody visualize a
three-dimensional Gaussian

01:04:43.660 --> 01:04:45.150
density?

01:04:45.150 --> 01:04:48.200
Right, so it's going to be some
sort of a bell-shaped curve,

01:04:48.200 --> 01:04:51.870
but it might be more elongated
in one direction than another.

01:04:51.870 --> 01:04:54.310
And then going to chop
it like that, all right?

01:04:54.310 --> 01:04:56.120
So I'm going to chop it off.

01:04:56.120 --> 01:05:00.070
And I'm going to look at
how it bleeds, all right?

01:05:00.070 --> 01:05:02.287
So I'm just going to look
at where the blood is.

01:05:02.287 --> 01:05:03.620
And what it's going to look at--

01:05:03.620 --> 01:05:08.720
it's going to look like some
sort of ellipsoid, right?

01:05:08.720 --> 01:05:11.652
In high dimension, it's
just going to be an olive.

01:05:11.652 --> 01:05:13.610
And that is just going
to be bigger and bigger.

01:05:13.610 --> 01:05:16.460
And then I chop it
off a little lower,

01:05:16.460 --> 01:05:20.150
and I get something a
little bigger like this.

01:05:20.150 --> 01:05:23.070
And so it turns out that sigma
is capturing exactly this,

01:05:23.070 --> 01:05:23.570
right?

01:05:23.570 --> 01:05:27.320
The matrix sigma-- so the
center of your covariance matrix

01:05:27.320 --> 01:05:29.240
of your Gaussian is
going to be this thing.

01:05:29.240 --> 01:05:33.690
And sigma is going to tell you
which direction it's elongated.

01:05:33.690 --> 01:05:36.140
And so in particular, if you
look, if you knew an ellipse,

01:05:36.140 --> 01:05:38.160
you know there's something
called principal axis, right?

01:05:38.160 --> 01:05:39.743
So you could actually
define something

01:05:39.743 --> 01:05:43.190
that looks like this, which is
this axis, the one along which

01:05:43.190 --> 01:05:44.390
it's the most elongated.

01:05:44.390 --> 01:05:47.345
Then the axis along which
is orthogonal to it,

01:05:47.345 --> 01:05:49.370
along which it's
slightly less elongated,

01:05:49.370 --> 01:05:52.880
and you go again and again
along the orthogonal ones.

01:05:52.880 --> 01:05:56.500
It turns out that
those things here

01:05:56.500 --> 01:05:59.620
is the new coordinate system
in which this transformation, P

01:05:59.620 --> 01:06:03.190
and P transpose, is
putting you into.

01:06:03.190 --> 01:06:06.390
And D has entries
on the diagonal

01:06:06.390 --> 01:06:09.979
which are exactly this length
and this length, right?

01:06:09.979 --> 01:06:11.270
So that's just what it's doing.

01:06:11.270 --> 01:06:12.920
It's just telling
you, well, if you

01:06:12.920 --> 01:06:16.760
think of having this Gaussian
or this high-dimensional

01:06:16.760 --> 01:06:19.990
ellipsoid, it's elongated
along certain directions.

01:06:19.990 --> 01:06:23.020
And these directions are
actually maybe not well aligned

01:06:23.020 --> 01:06:25.270
with your original coordinate
system, which might just

01:06:25.270 --> 01:06:27.430
be the usual one, right--

01:06:27.430 --> 01:06:29.740
north, south, and east, west.

01:06:29.740 --> 01:06:30.800
Maybe I need to turn it.

01:06:30.800 --> 01:06:33.174
And that's exactly what this
orthogonal transformation is

01:06:33.174 --> 01:06:36.820
doing for you, all right?

01:06:36.820 --> 01:06:39.627
So, in a way, this is actually
telling you even more.

01:06:39.627 --> 01:06:41.710
It's telling you that any
matrix that's symmetric,

01:06:41.710 --> 01:06:45.190
you can actually
turn it somewhere.

01:06:45.190 --> 01:06:47.530
And that'll start to dilate
things in the directions

01:06:47.530 --> 01:06:49.060
that you have, and
then turn it back

01:06:49.060 --> 01:06:50.800
to what you originally had.

01:06:50.800 --> 01:06:53.110
And that's actually
exactly the effect

01:06:53.110 --> 01:06:57.180
of applying a symmetric matrix
through a vector, right?

01:06:57.180 --> 01:06:58.920
And it's pretty impressive.

01:06:58.920 --> 01:07:04.650
It says if I take sigma
times v. Any sigma that's

01:07:04.650 --> 01:07:07.560
of this form, what I'm
doing is-- that's symmetric.

01:07:07.560 --> 01:07:09.360
What I'm really
doing to v is I'm

01:07:09.360 --> 01:07:12.150
changing its coordinate
system, so I'm rotating it.

01:07:12.150 --> 01:07:14.970
Then I'm changing-- I'm
multiplying its coordinates,

01:07:14.970 --> 01:07:16.956
and then I'm rotating it back.

01:07:16.956 --> 01:07:18.330
That's all it's
doing, and that's

01:07:18.330 --> 01:07:21.550
what all symmetric
matrices do, which

01:07:21.550 --> 01:07:24.070
means that this is doing a lot.

01:07:24.070 --> 01:07:27.130
All right, so OK.

01:07:27.130 --> 01:07:29.237
So, what do I know?

01:07:29.237 --> 01:07:30.820
So I'm not going to
prove that this is

01:07:30.820 --> 01:07:32.140
the so-called spectral theorem.

01:07:39.270 --> 01:07:45.850
And the diagonal entries of
D is of the form, lambda 1,

01:07:45.850 --> 01:07:49.980
lambda 2, lambda d, 0, 0.

01:07:49.980 --> 01:08:01.800
And the lambda j's are
called eigenvalues of D.

01:08:01.800 --> 01:08:05.170
Now in general, those numbers
can be positive, negative,

01:08:05.170 --> 01:08:06.660
or equal to 0.

01:08:06.660 --> 01:08:12.000
But here, I know that
sigma and S are--

01:08:12.000 --> 01:08:15.290
well, they're
symmetric for sure,

01:08:15.290 --> 01:08:17.467
but they are positive
semidefinite.

01:08:23.939 --> 01:08:25.840
What does it mean?

01:08:25.840 --> 01:08:30.930
It means that when I take u
transpose sigma u for example,

01:08:30.930 --> 01:08:33.192
this number is
always non-negative.

01:08:35.910 --> 01:08:36.720
Why is this true?

01:08:42.770 --> 01:08:43.609
What is this number?

01:08:47.670 --> 01:08:49.850
It's the variance of--
and actually, I don't even

01:08:49.850 --> 01:08:51.229
need to finish this sentence.

01:08:51.229 --> 01:08:53.957
As soon as I say that
this is a variance, well,

01:08:53.957 --> 01:08:55.040
it has to be non-negative.

01:08:55.040 --> 01:08:57.990
We know that a variance
is not negative.

01:08:57.990 --> 01:09:00.532
And so, that's also a
nice way you can use that.

01:09:00.532 --> 01:09:02.240
So it's just to say,
well, OK, this thing

01:09:02.240 --> 01:09:04.680
is positive semidefinite because
it's a covariance matrix.

01:09:04.680 --> 01:09:06.920
So I know it's a variance, OK?

01:09:06.920 --> 01:09:08.779
So I get this.

01:09:08.779 --> 01:09:10.560
Now, if I had some
negative numbers--

01:09:10.560 --> 01:09:15.350
so the effect of that is that
when I draw this picture,

01:09:15.350 --> 01:09:19.040
those axes are always positive,
which is kind of a weird thing

01:09:19.040 --> 01:09:19.950
to say.

01:09:19.950 --> 01:09:23.840
But what it means is that when
I take a vector, v, I rotate it,

01:09:23.840 --> 01:09:28.250
and then I stretch it in the
directions of the coordinate,

01:09:28.250 --> 01:09:30.260
I cannot flip it.

01:09:30.260 --> 01:09:34.260
I can only stretch or shrink,
but I cannot flip its sign,

01:09:34.260 --> 01:09:34.760
all right?

01:09:34.760 --> 01:09:37.370
But in general, for
any symmetric matrices,

01:09:37.370 --> 01:09:38.840
I could do this.

01:09:38.840 --> 01:09:40.910
But when it's positive
symmetric definite,

01:09:40.910 --> 01:09:43.020
actually what turns out
is that all the lambda

01:09:43.020 --> 01:09:48.350
j's are non-negative.

01:09:48.350 --> 01:09:51.370
I cannot flip it, OK?

01:09:51.370 --> 01:09:53.778
So all the eigenvalues
are non-negative.

01:09:56.590 --> 01:09:58.469
That's a property
of positive semidef.

01:09:58.469 --> 01:10:00.510
So when it's symmetric,
you have the eigenvalues.

01:10:00.510 --> 01:10:01.670
They can be any number.

01:10:01.670 --> 01:10:03.780
And when it's positive
semidefinite, in particular

01:10:03.780 --> 01:10:05.220
that's the case of
the covariance matrix

01:10:05.220 --> 01:10:07.110
and the empirical
covariance matrix, right?

01:10:07.110 --> 01:10:08.940
Because the empirical
covariance matrix

01:10:08.940 --> 01:10:12.150
is an empirical variance,
which itself is non-negative.

01:10:12.150 --> 01:10:17.900
And so I get that the
eigenvalues are non-negative.

01:10:17.900 --> 01:10:23.030
All right, so principal
component analysis is saying,

01:10:23.030 --> 01:10:32.370
OK, I want to find
the direction, u,

01:10:32.370 --> 01:10:38.830
that maximizes u
transpose Su, all right?

01:10:38.830 --> 01:10:40.420
I've just introduced
in one slide

01:10:40.420 --> 01:10:41.690
something about eigenvalues.

01:10:41.690 --> 01:10:44.740
So hopefully, they should help.

01:10:44.740 --> 01:10:47.560
So what is it that I'm
going to be getting?

01:10:47.560 --> 01:10:51.446
Well, let's just
see what happens.

01:10:51.446 --> 01:10:53.570
Oh, I forgot to mention
that-- and I will use this.

01:10:53.570 --> 01:10:56.020
So the lambda j's are
called eigenvectors.

01:10:56.020 --> 01:11:08.690
And then the matrix, P,
has columns v1 to vd, OK?

01:11:08.690 --> 01:11:13.370
The fact that it's orthogonal--
that P transpose P is equal

01:11:13.370 --> 01:11:15.470
to the identity--

01:11:15.470 --> 01:11:20.810
means that those guys
satisfied that vi transpose

01:11:20.810 --> 01:11:27.485
vj is equal to 0 if i
is different from j.

01:11:27.485 --> 01:11:31.040
And vi transpose vi is
actually equal to 1,

01:11:31.040 --> 01:11:33.920
right, because the
entries of PP transpose

01:11:33.920 --> 01:11:38.990
are exactly going to be of
the form, vi transpose vj, OK?

01:11:38.990 --> 01:11:40.890
So those v's are
called eigenvectors.

01:11:46.000 --> 01:11:52.020
And v1 is attached to lambda 1,
and v2 is attached to lambda 2,

01:11:52.020 --> 01:11:53.180
OK?

01:11:53.180 --> 01:11:56.280
So let's see what's
happening with those things.

01:11:56.280 --> 01:11:58.045
What happens if I take sigma--

01:11:58.045 --> 01:12:00.170
so if you know eigenvalues,
you know exactly what's

01:12:00.170 --> 01:12:01.580
going to happen.

01:12:01.580 --> 01:12:06.920
If I look at, say, sigma
times v1, well, what is sigma?

01:12:06.920 --> 01:12:15.440
We know that sigma
is PDP transpose v1.

01:12:15.440 --> 01:12:17.420
What is P transpose times v1?

01:12:17.420 --> 01:12:21.560
Well, P transpose has
rows v1 transpose,

01:12:21.560 --> 01:12:26.850
v2 transpose, all the
way to vd transpose.

01:12:26.850 --> 01:12:30.910
So when I multiply
this by v1, what

01:12:30.910 --> 01:12:32.820
I'm left with is
the first coordinate

01:12:32.820 --> 01:12:38.010
is going to be equal to 1
and the second coordinate is

01:12:38.010 --> 01:12:40.980
going to be equal to 0, right?

01:12:40.980 --> 01:12:42.910
Because they're
orthogonal to each other--

01:12:42.910 --> 01:12:45.810
0 all the way to the end.

01:12:45.810 --> 01:12:48.890
So that's when I
do P transpose v1.

01:12:48.890 --> 01:12:55.250
Now I multiply by
D. Well, I'm just

01:12:55.250 --> 01:12:58.950
multiplying this guy by lambda
1, this guy by lambda 2,

01:12:58.950 --> 01:13:02.150
and this guy by lambda d, so
this is really just lambda 1.

01:13:04.720 --> 01:13:12.080
And now I need to
post-multiply by P.

01:13:12.080 --> 01:13:14.190
So what is P times this guy?

01:13:14.190 --> 01:13:19.730
Well, P is v1 all the way to vd.

01:13:19.730 --> 01:13:21.290
And now I multiply
by a vector that

01:13:21.290 --> 01:13:24.620
only has 0's except
lambda 1 on the first guy.

01:13:24.620 --> 01:13:26.510
So this is just
lambda 1 times v1.

01:13:29.470 --> 01:13:34.630
So what we've proved is that
sigma times v1 is lambda 1 v1,

01:13:34.630 --> 01:13:37.330
and that's probably the
notion of eigenvalue you're

01:13:37.330 --> 01:13:39.010
most comfortable with, right?

01:13:39.010 --> 01:13:41.620
So just when I
multiply by v1, I get

01:13:41.620 --> 01:13:45.440
v1 back multiplied by something,
which is the eigenvalue.

01:13:45.440 --> 01:13:54.450
So in particular, if I look
at v1, transpose sigma v1,

01:13:54.450 --> 01:13:55.180
what do I get?

01:13:55.180 --> 01:13:58.800
Well, I get lambda
1 v1 transpose v1,

01:13:58.800 --> 01:14:00.180
which is 1, right?

01:14:00.180 --> 01:14:04.050
So this is actually
lambda 1 v1 transpose v1,

01:14:04.050 --> 01:14:08.360
which is lambda 1, OK?

01:14:08.360 --> 01:14:10.940
And if I do the same
with v2, clearly I'm

01:14:10.940 --> 01:14:13.450
going to get v2 transpose sigma.

01:14:13.450 --> 01:14:16.910
v2 is equal to lambda 2.

01:14:16.910 --> 01:14:19.910
So for each of the
vj's, I know that if I

01:14:19.910 --> 01:14:21.650
look at the variance
along the vj,

01:14:21.650 --> 01:14:27.760
it's actually exactly given by
those eigenvalues, all right?

01:14:27.760 --> 01:14:38.490
Which proves this, because the
variance along the eigenvectors

01:14:38.490 --> 01:14:40.270
is actually equal
to the eigenvalues.

01:14:40.270 --> 01:14:43.760
So since they're variances,
they have to be non-negative.

01:14:43.760 --> 01:14:47.960
So now, I'm looking for
the one direction that

01:14:47.960 --> 01:14:50.450
has the most variance, right?

01:14:50.450 --> 01:14:53.040
But that's not only
among the eigenvectors.

01:14:53.040 --> 01:14:55.520
That's also among
the other directions

01:14:55.520 --> 01:14:57.200
that are in-between
the eigenvectors.

01:14:57.200 --> 01:14:59.390
If I were to look only
at the eigenvectors,

01:14:59.390 --> 01:15:02.420
it would just tell me, well,
just pick the eigenvector, vj,

01:15:02.420 --> 01:15:05.990
that's associated to the
largest of the lambda j's.

01:15:05.990 --> 01:15:09.080
But it turns out that that's
also true for any vector--

01:15:09.080 --> 01:15:11.810
that the maximum direction is
actually one direction which

01:15:11.810 --> 01:15:13.809
is among the eigenvectors.

01:15:13.809 --> 01:15:16.100
And among the eigenvectors,
we know that the one that's

01:15:16.100 --> 01:15:17.080
the largest--

01:15:17.080 --> 01:15:18.740
that carries the
largest variance is

01:15:18.740 --> 01:15:23.780
the one that's associated to the
largest eigenvalue, all right?

01:15:23.780 --> 01:15:26.990
And so this is what PCA is
going to try to do for me.

01:15:26.990 --> 01:15:29.420
So in practice, that's what
I mentioned already, right?

01:15:29.420 --> 01:15:31.970
We're trying to
project the point cloud

01:15:31.970 --> 01:15:34.730
onto a low-dimensional
space, D prime,

01:15:34.730 --> 01:15:36.800
by keeping as much
information as possible.

01:15:36.800 --> 01:15:39.230
And by "as much information,"
I mean we do not

01:15:39.230 --> 01:15:41.540
want points to collide.

01:15:41.540 --> 01:15:45.530
And so what PCA is
going to do is just

01:15:45.530 --> 01:15:48.231
going to try to project
[? on two ?] directions.

01:15:48.231 --> 01:15:49.730
So there's going
to be a u, and then

01:15:49.730 --> 01:15:52.021
there's going to be something
orthogonal to u, and then

01:15:52.021 --> 01:15:55.550
the third one, et cetera, so
that once we project on those,

01:15:55.550 --> 01:15:59.600
we're keeping as much of the
covariance as possible, OK?

01:15:59.600 --> 01:16:02.859
And in particular,
those directions

01:16:02.859 --> 01:16:04.400
that we're going to
pick are actually

01:16:04.400 --> 01:16:06.920
a subset of the vj's that
are associated to the largest

01:16:06.920 --> 01:16:08.580
eigenvalues.

01:16:08.580 --> 01:16:11.300
So I'm going to
stop here for today.

01:16:11.300 --> 01:16:15.020
We'll finish this on Tuesday.

01:16:15.020 --> 01:16:18.260
But basically, the idea is
it's just the following.

01:16:18.260 --> 01:16:22.590
You're just going to--
well, let me skip one more.

01:16:22.590 --> 01:16:24.812
Yeah, this is the idea.

01:16:24.812 --> 01:16:27.020
You're first going to pick
the eigenvector associated

01:16:27.020 --> 01:16:30.290
to the largest eigenvalue.

01:16:30.290 --> 01:16:33.890
Then you're going to pick
the direction that orthogonal

01:16:33.890 --> 01:16:37.130
to the vector that
you've picked,

01:16:37.130 --> 01:16:38.984
and that's carrying
the most variance.

01:16:38.984 --> 01:16:40.650
And that's actually
the second largest--

01:16:40.650 --> 01:16:44.030
the eigenvector associated to
the second largest eigenvalue.

01:16:44.030 --> 01:16:46.520
And you're going to go all
the way to the number of them

01:16:46.520 --> 01:16:50.120
that you actually want to pick,
which is in this case, d, OK?

01:16:50.120 --> 01:16:53.180
And wherever you choose
to chop this process,

01:16:53.180 --> 01:16:56.390
not going all the way to d,
is going to actually give you

01:16:56.390 --> 01:16:57.890
a lower-dimensional
representation

01:16:57.890 --> 01:17:01.238
in the coordinate system
that's given by v1, v2, v3, et

01:17:01.238 --> 01:17:02.420
cetera, OK?

01:17:02.420 --> 01:17:04.591
So we'll see that in
more details on Tuesday.

01:17:04.591 --> 01:17:06.090
But I don't want
to get into it now.

01:17:06.090 --> 01:17:07.500
We don't have enough time.

01:17:07.500 --> 01:17:10.000
Are there any questions?