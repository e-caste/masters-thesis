WEBVTT

00:00:00.050 --> 00:00:01.770
The following
content is provided

00:00:01.770 --> 00:00:04.010
under a Creative
Commons license.

00:00:04.010 --> 00:00:06.860
Your support will help MIT
OpenCourseWare continue

00:00:06.860 --> 00:00:10.720
to offer high quality
educational resources for free.

00:00:10.720 --> 00:00:13.320
To make a donation or
view additional materials

00:00:13.320 --> 00:00:17.207
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:17.207 --> 00:00:17.832
at ocw.mit.edu.

00:00:21.130 --> 00:00:23.630
PROFESSOR: We're going to start
a brand new, exciting topic,

00:00:23.630 --> 00:00:25.287
dynamic programming.

00:00:25.287 --> 00:00:25.870
AUDIENCE: Yes!

00:00:25.870 --> 00:00:26.830
PROFESSOR: Yeah!

00:00:26.830 --> 00:00:28.245
So exciting.

00:00:28.245 --> 00:00:30.620
Actually, I am really excited
because dynamic programming

00:00:30.620 --> 00:00:33.820
is my favorite thing in
the world, in algorithms.

00:00:33.820 --> 00:00:37.070
And it's going to be
the next four lectures,

00:00:37.070 --> 00:00:38.640
it's so exciting.

00:00:38.640 --> 00:00:40.380
It has lots of different facets.

00:00:40.380 --> 00:00:44.720
It's a very general,
powerful design technique.

00:00:44.720 --> 00:00:47.390
We don't talk a lot about
algorithm design in this class,

00:00:47.390 --> 00:00:50.060
but dynamic programming is
one that's so important.

00:00:50.060 --> 00:00:52.000
And also takes a little
while to settle in.

00:00:52.000 --> 00:00:57.510
We like to injected it
into you now, in 006.

00:00:57.510 --> 00:01:05.920
So in general, our motivation
is designing new algorithms

00:01:05.920 --> 00:01:10.600
and dynamic programming,
also called DP,

00:01:10.600 --> 00:01:17.840
is a great way-- or a
very general, powerful way

00:01:17.840 --> 00:01:18.365
to do this.

00:01:33.860 --> 00:01:37.540
It's especially good, and
intended for, optimization

00:01:37.540 --> 00:01:39.100
problems, things
like shortest paths.

00:01:39.100 --> 00:01:41.274
You want to find the
best way to do something.

00:01:41.274 --> 00:01:43.190
Shortest path is you
want to find the shortest

00:01:43.190 --> 00:01:46.600
path, the minimum-length path.

00:01:46.600 --> 00:01:48.610
You want to minimize,
maximize something, that's

00:01:48.610 --> 00:01:51.320
an optimization
problem, and typically

00:01:51.320 --> 00:01:54.090
good algorithms to solve them
involve dynamic programming.

00:01:54.090 --> 00:01:56.390
It's a bit of a broad statement.

00:01:56.390 --> 00:01:58.320
You can also think of
dynamic programming

00:01:58.320 --> 00:02:01.700
as a kind of exhaustive search.

00:02:01.700 --> 00:02:03.270
Which is usually
a bad thing to do

00:02:03.270 --> 00:02:05.260
because it leads to
exponential time.

00:02:05.260 --> 00:02:09.449
But if you do it in a clever
way, via dynamic programming,

00:02:09.449 --> 00:02:13.670
you typically get
polynomial time.

00:02:13.670 --> 00:02:17.430
So one perspective is
that dynamic programming

00:02:17.430 --> 00:02:23.170
is approximately
careful brute force.

00:02:23.170 --> 00:02:25.650
It's kind of a
funny combination.

00:02:25.650 --> 00:02:29.340
A bit of an oxymoron.

00:02:29.340 --> 00:02:31.600
But we take the idea of
brute force, which is,

00:02:31.600 --> 00:02:35.690
try all possibilities
and you do it carefully

00:02:35.690 --> 00:02:37.110
and you get it to
polynomial time.

00:02:37.110 --> 00:02:38.985
There are a lot of
problems where essentially

00:02:38.985 --> 00:02:41.650
the only known polynomial
time algorithm is

00:02:41.650 --> 00:02:42.650
via dynamic programming.

00:02:42.650 --> 00:02:44.440
It doesn't always work,
there's some problems

00:02:44.440 --> 00:02:46.856
where we don't think there are
polynomial time algorithms,

00:02:46.856 --> 00:02:50.770
but when it's possible
DP is a nice, sort of,

00:02:50.770 --> 00:02:54.037
general approach to it.

00:02:54.037 --> 00:02:56.620
And we're going to be talking a
lot about dynamic programming.

00:02:56.620 --> 00:02:59.850
There's a lot of different
ways to think about it.

00:02:59.850 --> 00:03:02.690
We'll look at a few today.

00:03:02.690 --> 00:03:05.754
We're going to warm up today
with some fairly easy problems

00:03:05.754 --> 00:03:07.170
that we already
know how to solve,

00:03:07.170 --> 00:03:09.690
namely computing
Fibonacci numbers.

00:03:09.690 --> 00:03:11.210
It's pretty easy.

00:03:11.210 --> 00:03:13.092
And computing shortest paths.

00:03:13.092 --> 00:03:14.550
And then in the
next three lectures

00:03:14.550 --> 00:03:16.530
we're going to get to
more interesting examples

00:03:16.530 --> 00:03:20.210
where it's pretty
surprising that you can even

00:03:20.210 --> 00:03:24.160
solve the problem
in polynomial time.

00:03:24.160 --> 00:03:26.470
Probably the first burning
question on your mind,

00:03:26.470 --> 00:03:29.280
though, is why is it
called dynamic programming?

00:03:29.280 --> 00:03:31.240
What does that even mean?

00:03:31.240 --> 00:03:34.760
And I used to have this
spiel about, well, you

00:03:34.760 --> 00:03:37.480
know, programming
refers to the-- I think

00:03:37.480 --> 00:03:39.550
it's the British
notion of the word,

00:03:39.550 --> 00:03:42.040
where it's about optimization.

00:03:42.040 --> 00:03:44.750
Optimization in American
English is something

00:03:44.750 --> 00:03:47.690
like programming
in British English,

00:03:47.690 --> 00:03:50.350
where you want to
set up the program--

00:03:50.350 --> 00:03:52.800
the schedule for your
trains or something,

00:03:52.800 --> 00:03:54.850
where programming
comes from originally.

00:03:54.850 --> 00:03:57.675
But I looked up the
actual history of,

00:03:57.675 --> 00:04:00.730
why is it called
dynamic programming.

00:04:00.730 --> 00:04:04.960
Dynamic programming was invented
by a guy named Richard Bellman.

00:04:04.960 --> 00:04:07.850
You may have heard of Bellman
in the Bellman-Ford algorithm.

00:04:07.850 --> 00:04:10.360
So this is actually the
precursor to Bellman-Ford.

00:04:10.360 --> 00:04:13.290
And we're going to see
Bellman-Ford come up naturally

00:04:13.290 --> 00:04:15.480
in this setting.

00:04:15.480 --> 00:04:17.519
So here's a quote about him.

00:04:17.519 --> 00:04:19.300
It says, Bellman
explained that he

00:04:19.300 --> 00:04:22.250
invented the name dynamic
programming to hide the fact

00:04:22.250 --> 00:04:25.840
that he was doing
mathematical research.

00:04:25.840 --> 00:04:27.950
He was working at this
place called Rand,

00:04:27.950 --> 00:04:31.890
and under a secretary of defense
who had a pathological fear

00:04:31.890 --> 00:04:35.096
and hatred for
the term research.

00:04:35.096 --> 00:04:36.970
So he settled on the
term dynamic programming

00:04:36.970 --> 00:04:39.450
because it would be
difficult to give

00:04:39.450 --> 00:04:42.000
a pejorative meaning to it.

00:04:42.000 --> 00:04:43.870
And because it was
something not even

00:04:43.870 --> 00:04:45.830
a congressman could object to.

00:04:45.830 --> 00:04:49.430
Basically, it sounded cool.

00:04:49.430 --> 00:04:52.540
So that's the origin of the
name dynamic programming.

00:04:52.540 --> 00:04:53.630
So why is the called that?

00:04:53.630 --> 00:04:55.450
Who knows.

00:04:55.450 --> 00:04:56.780
I mean, now you know.

00:04:56.780 --> 00:04:59.430
But it's not--
it's a weird term.

00:04:59.430 --> 00:05:00.720
Just take it for what it is.

00:05:04.150 --> 00:05:06.820
It may make some
kind of sense, but--

00:05:06.820 --> 00:05:07.500
All right.

00:05:07.500 --> 00:05:14.570
So we are going to start
with this example of how

00:05:14.570 --> 00:05:16.800
to compute Fibonacci numbers.

00:05:16.800 --> 00:05:18.700
And maybe before
we actually start

00:05:18.700 --> 00:05:23.250
I'm going to give you
a sneak peak of what

00:05:23.250 --> 00:05:26.480
you can think of
dynamic programming as.

00:05:26.480 --> 00:05:37.830
And this equation,
so to speak, is

00:05:37.830 --> 00:05:42.400
going to change throughout
today's lecture.

00:05:42.400 --> 00:05:43.970
In the end we'll
settle on a sort

00:05:43.970 --> 00:05:46.760
of more accurate perspective.

00:05:46.760 --> 00:05:49.020
The basic idea of
dynamic programming

00:05:49.020 --> 00:05:53.120
is to take a problem,
split it into subproblems,

00:05:53.120 --> 00:05:55.780
solve those subproblems,
and reuse the solutions

00:05:55.780 --> 00:05:56.720
to your subproblems.

00:05:56.720 --> 00:05:59.520
It's like a lesson in recycling.

00:05:59.520 --> 00:06:03.290
So we'll see that in
Fibonacci numbers.

00:06:03.290 --> 00:06:06.270
So you remember
Fibonacci numbers, right?

00:06:06.270 --> 00:06:10.445
The number of rabbits you have
on day n, if they reproduce.

00:06:15.020 --> 00:06:18.630
We've mentioned them before,
we're talking about AVL trees,

00:06:18.630 --> 00:06:19.910
I think.

00:06:19.910 --> 00:06:23.290
So this is the
usual-- you can think

00:06:23.290 --> 00:06:26.710
of it as a recursive definition
or recurrence on Fibonacci

00:06:26.710 --> 00:06:27.770
numbers.

00:06:27.770 --> 00:06:30.890
It's the definition of what
the nth Fibonacci number is.

00:06:30.890 --> 00:06:34.810
So let's suppose our goal--
an algorithmic problem is,

00:06:34.810 --> 00:06:39.220
compute the nth
Fibonacci number.

00:06:39.220 --> 00:06:42.710
And I'm going to assume here
that that fits in a word.

00:06:42.710 --> 00:06:44.980
And so basic
arithmetic, addition,

00:06:44.980 --> 00:06:48.100
whatever's constant
time per operation.

00:06:48.100 --> 00:06:50.590
So how do we do it?

00:06:50.590 --> 00:06:52.200
You all know how to do it.

00:06:52.200 --> 00:06:56.610
Anyways-- but I'm going to give
you the dynamic programming

00:06:56.610 --> 00:06:57.790
perspective on things.

00:06:57.790 --> 00:07:00.500
So this will seem
kind of obvious,

00:07:00.500 --> 00:07:03.610
but it is-- we're going to apply
exactly the same principles

00:07:03.610 --> 00:07:06.610
that we will apply over and
over in dynamic programming.

00:07:06.610 --> 00:07:11.910
But here it's in a
very familiar setting.

00:07:11.910 --> 00:07:16.430
So we're going to start with
the naive recursive algorithm.

00:07:24.850 --> 00:07:29.150
And that is, if you want to
compute the nth Fibonacci

00:07:29.150 --> 00:07:32.010
number, you check whether
you're in the base case.

00:07:35.890 --> 00:07:38.520
I'm going to write it this way.

00:07:55.230 --> 00:07:57.320
So f is just our return value.

00:07:57.320 --> 00:07:59.470
You'll see why I write
it this way in a moment.

00:07:59.470 --> 00:08:01.270
Then you return f.

00:08:01.270 --> 00:08:03.030
In the base case
it's 1, otherwise

00:08:03.030 --> 00:08:05.860
you recursively call
Fibonacci of n minus 1.

00:08:05.860 --> 00:08:08.520
You recursively call
Fibonacci of n minus 2.

00:08:08.520 --> 00:08:10.600
Add them together, return that.

00:08:10.600 --> 00:08:12.762
This is a correct algorithm.

00:08:12.762 --> 00:08:15.160
Is it a good algorithm?

00:08:15.160 --> 00:08:16.255
No.

00:08:16.255 --> 00:08:17.230
It's very bad.

00:08:17.230 --> 00:08:18.260
Exponential time.

00:08:23.450 --> 00:08:25.480
How do we know it's
exponential time,

00:08:25.480 --> 00:08:27.730
other than from experience?

00:08:27.730 --> 00:08:31.880
Well, we can write the
running time as recurrence.

00:08:31.880 --> 00:08:34.990
T of n represents the time
to compute the nth Fibonacci

00:08:34.990 --> 00:08:35.780
number.

00:08:35.780 --> 00:08:39.440
How can I write the recurrence?

00:08:39.440 --> 00:08:41.630
You're gonna throwback to
the early lectures, divide

00:08:41.630 --> 00:08:42.606
and conquer.

00:08:48.950 --> 00:08:50.890
I hear whispers.

00:08:50.890 --> 00:08:51.390
Yeah?

00:08:51.390 --> 00:08:52.366
AUDIENCE: [INAUDIBLE].

00:08:55.320 --> 00:08:57.081
PROFESSOR: Yeah.

00:08:57.081 --> 00:09:03.340
T of n minus 1 plus t of
n minus 2 plus constant.

00:09:07.190 --> 00:09:08.865
I don't know how
many you have by now.

00:09:11.510 --> 00:09:13.540
So to create the
nth Fibonacci number

00:09:13.540 --> 00:09:15.790
we have to compute the n
minus first Fibonacci number,

00:09:15.790 --> 00:09:17.456
and the n minus second
Fibonacci number.

00:09:17.456 --> 00:09:19.890
That's these two recursions.

00:09:19.890 --> 00:09:21.760
And then we take
constant time otherwise.

00:09:21.760 --> 00:09:24.910
We do constant number of
additions, comparisons.

00:09:24.910 --> 00:09:27.920
Return all these operations--
take constant time.

00:09:27.920 --> 00:09:29.550
So that's a recurrence.

00:09:29.550 --> 00:09:31.390
How do we solve this recurrence?

00:09:31.390 --> 00:09:35.720
Well one way is to see this
is the Fibonacci recurrence.

00:09:35.720 --> 00:09:37.350
So it's the same thing.

00:09:37.350 --> 00:09:38.970
There's this plus whatever.

00:09:38.970 --> 00:09:41.720
But in particular, this is at
least the nth Fibonacci number.

00:09:41.720 --> 00:09:43.350
And if you know
Fibonacci stuff, that's

00:09:43.350 --> 00:09:48.440
about the golden ratio
to the nth power.

00:09:48.440 --> 00:09:50.650
Which is bad.

00:09:50.650 --> 00:09:52.930
We had a similar
recurrence in AVL trees.

00:09:52.930 --> 00:09:55.850
And so another way
to solve it-- it's

00:09:55.850 --> 00:09:59.100
just good review--
say, oh well, that's

00:09:59.100 --> 00:10:03.600
at least 2 times t of n minus 2.

00:10:03.600 --> 00:10:05.020
Because it's going
to be monotone.

00:10:05.020 --> 00:10:07.331
The bigger n is, the
more work you have to do.

00:10:07.331 --> 00:10:08.830
Because to do the
nth thing you have

00:10:08.830 --> 00:10:10.770
to do the n minus first thing.

00:10:10.770 --> 00:10:14.260
So we could just reduce t of
n minus 1 to t of n minus 2.

00:10:14.260 --> 00:10:16.620
That will give us a lower bound.

00:10:16.620 --> 00:10:19.700
And now these two terms-- now
this is sort of an easy thing.

00:10:19.700 --> 00:10:22.950
You see that you're
multiplying by 2 each time.

00:10:22.950 --> 00:10:24.670
You're subtracting
2 from n each time.

00:10:24.670 --> 00:10:27.270
How many times can
I subtract 2 from n?

00:10:27.270 --> 00:10:30.390
N/2 times, before I
get down to a constant.

00:10:30.390 --> 00:10:37.740
And so this is equal
to 2 to the n over 2--

00:10:37.740 --> 00:10:39.460
I mean, times some
constant, which

00:10:39.460 --> 00:10:41.900
is what you get
in the base case.

00:10:41.900 --> 00:10:45.970
So I guess I should say theta.

00:10:45.970 --> 00:10:48.890
This thing is theta that.

00:10:48.890 --> 00:10:49.390
OK.

00:10:49.390 --> 00:10:51.320
So it's at least that big.

00:10:51.320 --> 00:10:56.302
And the right constant is phi.

00:10:56.302 --> 00:10:58.620
And the base of the exponent.

00:10:58.620 --> 00:10:59.120
OK.

00:10:59.120 --> 00:11:00.203
So that's a bad algorithm.

00:11:00.203 --> 00:11:03.690
We all know it's
a bad algorithm.

00:11:03.690 --> 00:11:06.670
But I'm going to give you a
general approach for making

00:11:06.670 --> 00:11:09.000
bad algorithms like this good.

00:11:09.000 --> 00:11:11.036
And that general approach
is called memoization.

00:11:14.170 --> 00:11:16.600
We'll go over here.

00:11:19.910 --> 00:11:21.900
And this is a technique
of dynamic programming.

00:11:24.708 --> 00:11:28.370
So I'm going to call this the
memoized dynamic programming

00:11:28.370 --> 00:11:28.870
algorithm.

00:11:36.840 --> 00:11:44.270
So did I settle on
using memo in the notes?

00:11:44.270 --> 00:11:44.770
Yeah.

00:11:48.570 --> 00:11:50.520
The idea is simple.

00:11:50.520 --> 00:11:52.870
Whenever we compute
a Fibonacci number

00:11:52.870 --> 00:11:55.167
we put it in a dictionary.

00:11:55.167 --> 00:11:57.250
And then when we need to
compute the nth Fibonacci

00:11:57.250 --> 00:11:58.825
number we check, is it
already in the dictionary?

00:11:58.825 --> 00:12:00.241
Did we already
solve this problem?

00:12:00.241 --> 00:12:03.020
If so, return that answer.

00:12:03.020 --> 00:12:05.085
Otherwise, computer it.

00:12:05.085 --> 00:12:06.960
You'll see the transformation
is very simple.

00:12:57.810 --> 00:12:59.040
OK.

00:12:59.040 --> 00:13:04.650
These two lines are
identical to these two lines.

00:13:04.650 --> 00:13:06.320
So you can see how
the transformation

00:13:06.320 --> 00:13:07.730
works in general.

00:13:07.730 --> 00:13:10.856
You could do this with
any recursive algorithm.

00:13:10.856 --> 00:13:14.700
The memoization transformation
on that algorithm--

00:13:14.700 --> 00:13:19.050
which is, we initially make an
empty dictionary called memo.

00:13:19.050 --> 00:13:22.950
And before we actually do
the computation we say,

00:13:22.950 --> 00:13:26.820
well, check whether this version
of the Fibonacci problem,

00:13:26.820 --> 00:13:31.432
computing f of n, is
already in our dictionary.

00:13:31.432 --> 00:13:33.265
So if that key is already
in the dictionary,

00:13:33.265 --> 00:13:35.935
we return the corresponding
value in the dictionary.

00:13:38.960 --> 00:13:41.590
And then once we've computed
the nth Fibonacci number,

00:13:41.590 --> 00:13:44.780
if we bothered to do this,
if this didn't apply,

00:13:44.780 --> 00:13:47.140
then we store it
in the memo table.

00:13:47.140 --> 00:13:50.900
So we say well, if you ever
need to compute f of n again,

00:13:50.900 --> 00:13:52.370
here it is.

00:13:52.370 --> 00:13:54.820
And then we return that value.

00:13:54.820 --> 00:13:57.192
So this is a general procedure.

00:13:57.192 --> 00:14:00.910
It can apply to any
recursive algorithm

00:14:00.910 --> 00:14:04.900
with no side effects
I guess, technically.

00:14:04.900 --> 00:14:07.789
And it turns out, this makes
the algorithm efficient.

00:14:07.789 --> 00:14:09.955
Now there's a lot of ways
to see why it's efficient.

00:14:12.680 --> 00:14:15.260
In general, maybe it's helpful
to think about the recursion

00:14:15.260 --> 00:14:16.220
tree.

00:14:16.220 --> 00:14:19.660
So if you want to compute
fn in the old algorithm,

00:14:19.660 --> 00:14:22.550
we compute fn minus
1 and fn minus two

00:14:22.550 --> 00:14:23.940
completely separately.

00:14:23.940 --> 00:14:29.820
To compute fn minus 1 we compute
fn minus 2 and fn minus 3.

00:14:29.820 --> 00:14:34.720
To compute fn minus 2 we compute
fn minus 3 and fn minus 4.

00:14:34.720 --> 00:14:35.620
And so on.

00:14:35.620 --> 00:14:39.490
And you can see why
that's exponential in n.

00:14:39.490 --> 00:14:44.250
Because we're only decrementing
n by one or two each time.

00:14:44.250 --> 00:14:48.200
But then you observe, hey,
these fn minus 3's are the same.

00:14:48.200 --> 00:14:50.935
I should really only have
to compute them once.

00:14:50.935 --> 00:14:52.310
And that's what
we're doing here.

00:14:52.310 --> 00:14:55.660
The first time you call
fn minus 3, you do work.

00:14:55.660 --> 00:14:58.780
But once it's done and you go
over to this other recursive

00:14:58.780 --> 00:15:00.760
call, this will
just get cut off.

00:15:00.760 --> 00:15:02.040
There's no tree here.

00:15:02.040 --> 00:15:04.180
Here we might have
some recursive calling.

00:15:04.180 --> 00:15:08.700
Here we won't, because it's
already in the memo table.

00:15:08.700 --> 00:15:11.930
In fact, this already
happens with fn minus 2.

00:15:11.930 --> 00:15:15.600
This whole tree disappears
because fn minus 2

00:15:15.600 --> 00:15:17.610
has already been done.

00:15:17.610 --> 00:15:18.110
OK.

00:15:18.110 --> 00:15:21.090
So it's clear why
it improves things.

00:15:21.090 --> 00:15:24.750
So in fact you can argue
that this call will be free

00:15:24.750 --> 00:15:27.540
because you already
did the work in here.

00:15:27.540 --> 00:15:30.700
But I want to give you a very
particular way of thinking

00:15:30.700 --> 00:15:40.015
about why this is efficient,
which is following.

00:16:06.550 --> 00:16:09.800
So you could write down a
recurrence for the running time

00:16:09.800 --> 00:16:10.300
here.

00:16:10.300 --> 00:16:13.050
But in some sense recurrences
aren't quite the right way

00:16:13.050 --> 00:16:15.240
of thinking about
this because recursion

00:16:15.240 --> 00:16:17.220
is kind of a rare thing.

00:16:17.220 --> 00:16:20.840
If you're calling
Fibonacci of some value, k,

00:16:20.840 --> 00:16:23.700
you're only going to make
recursive calls the first time

00:16:23.700 --> 00:16:24.930
you call Fibonacci of k.

00:16:24.930 --> 00:16:28.250
Because henceforth,
you've put it

00:16:28.250 --> 00:16:30.860
in the memo table
you will not recurse.

00:16:30.860 --> 00:16:35.370
So you can think of
there being two versions

00:16:35.370 --> 00:16:36.640
of calling Fibonacci of k.

00:16:36.640 --> 00:16:40.950
There's the first time, which
is the non-memoized version that

00:16:40.950 --> 00:16:42.960
does recursion-- does some work.

00:16:42.960 --> 00:16:46.090
And then every time
henceforth you're

00:16:46.090 --> 00:16:48.080
doing memoized calls
of Fibonacci of k,

00:16:48.080 --> 00:16:51.620
and those cost constant time.

00:16:51.620 --> 00:16:59.820
So the memoized calls
cost constant time.

00:16:59.820 --> 00:17:03.120
So we can think of
them as basically free.

00:17:03.120 --> 00:17:06.210
That's when you call
Fibonacci of n minus 2,

00:17:06.210 --> 00:17:09.210
because that's a
memoized call, you really

00:17:09.210 --> 00:17:10.990
don't pay anything for it.

00:17:10.990 --> 00:17:12.781
I mean, you're already
paying constant time

00:17:12.781 --> 00:17:14.250
to do addition and whatever.

00:17:14.250 --> 00:17:16.089
So you don't have to
worry about the time.

00:17:16.089 --> 00:17:18.900
There's no recursion here.

00:17:18.900 --> 00:17:21.670
And then what we care
about is that the number

00:17:21.670 --> 00:17:26.950
of non-memorized calls,
which is the first time you

00:17:26.950 --> 00:17:33.925
call Fibonacci of k, is n.

00:17:33.925 --> 00:17:35.640
No theta is even necessary.

00:17:35.640 --> 00:17:38.990
We are going to
call Fibonacci of 1.

00:17:38.990 --> 00:17:41.860
At some point we're going
to call Fibonacci of 2

00:17:41.860 --> 00:17:46.039
at some point, and the original
call is Fibonacci of n.

00:17:46.039 --> 00:17:48.080
All of those things will
be called at some point.

00:17:48.080 --> 00:17:49.650
That's pretty easy to see.

00:17:49.650 --> 00:17:52.050
But in particular,
certainly at most this,

00:17:52.050 --> 00:17:53.900
we never call
Fibonacci of n plus 1

00:17:53.900 --> 00:17:55.820
to compute Fibonacci of n.

00:17:55.820 --> 00:17:57.460
So it's at most n calls.

00:17:57.460 --> 00:18:00.880
Indeed it will be exactly n
calls that are not memoized.

00:18:00.880 --> 00:18:02.130
Those ones we have to pay for.

00:18:02.130 --> 00:18:03.650
How much do we have to pay?

00:18:03.650 --> 00:18:07.610
Well, if you don't count
the recursion-- which

00:18:07.610 --> 00:18:09.840
is what this recurrence
does-- if you ignore

00:18:09.840 --> 00:18:13.235
recursion then the total amount
of work done here is constant.

00:18:16.110 --> 00:18:30.630
So I will say the non-recursive
work per call is constant.

00:18:30.630 --> 00:18:34.010
And therefore I claim
that the running time is

00:18:34.010 --> 00:18:37.870
constant-- I'm sorry, is linear.

00:18:37.870 --> 00:18:41.390
Constant would be
pretty amazing.

00:18:41.390 --> 00:18:44.310
This is actually not the
best algorithm-- as an aside.

00:18:44.310 --> 00:18:46.720
The best algorithm for computing
the nth Fibonacci number

00:18:46.720 --> 00:18:50.299
uses log n arithmetic
operations.

00:18:50.299 --> 00:18:51.840
So you can do better,
but if you want

00:18:51.840 --> 00:18:55.430
to see that you
should take 6046.

00:18:55.430 --> 00:18:55.950
OK.

00:18:55.950 --> 00:18:57.866
We're just going to get
to linear today, which

00:18:57.866 --> 00:19:01.430
is a lot better
than exponential.

00:19:01.430 --> 00:19:03.890
So why linear?

00:19:03.890 --> 00:19:07.700
Because there's n non-memoize
calls, and each of them

00:19:07.700 --> 00:19:08.690
cost constant.

00:19:08.690 --> 00:19:12.330
So it's the product
of those two numbers.

00:19:12.330 --> 00:19:14.820
This is an important idea.

00:19:14.820 --> 00:19:18.320
And it's so important
I'm going to write it

00:19:18.320 --> 00:19:25.080
down again in a slightly
more general framework.

00:19:25.080 --> 00:19:37.400
In general, in
dynamic programming--

00:19:37.400 --> 00:19:40.540
I didn't say why it's
called memoization.

00:19:40.540 --> 00:19:42.363
The idea is you have
this memo pad where

00:19:42.363 --> 00:19:44.810
you write down all
your scratch work.

00:19:44.810 --> 00:19:46.495
That's this memo dictionary.

00:19:46.495 --> 00:19:49.390
And to memoize is to write
down on your memo pad.

00:19:49.390 --> 00:19:50.530
I didn't make it up.

00:19:50.530 --> 00:19:52.630
Another crazy term.

00:19:52.630 --> 00:19:55.450
It means remember.

00:19:55.450 --> 00:19:59.610
And then you remember all the
solutions that you've done.

00:19:59.610 --> 00:20:02.380
And then you reuse
those solutions.

00:20:02.380 --> 00:20:04.460
Now these solutions are
not really a solution

00:20:04.460 --> 00:20:06.470
to the problem
that I care about.

00:20:06.470 --> 00:20:08.970
The problem I care about is
computing the nth Fibonacci

00:20:08.970 --> 00:20:09.850
number.

00:20:09.850 --> 00:20:13.230
To get there I had to compute
other Fibonacci numbers.

00:20:13.230 --> 00:20:15.060
Why?

00:20:15.060 --> 00:20:16.730
Because I had a
recursive formulation.

00:20:16.730 --> 00:20:19.220
This is not always the
way to solve a problem.

00:20:19.220 --> 00:20:22.550
But usually when you're
solving something

00:20:22.550 --> 00:20:28.030
you can split it into parts,
into subproblems, we call them.

00:20:28.030 --> 00:20:29.700
They're not always
of the same flavor

00:20:29.700 --> 00:20:31.450
as your original goal
problem, but there's

00:20:31.450 --> 00:20:34.290
some kind of related parts.

00:20:34.290 --> 00:20:38.040
And this is the big challenge
in designing a dynamic program,

00:20:38.040 --> 00:20:41.410
is to figure out what
are the subproblems.

00:20:41.410 --> 00:20:43.080
Let's say, the
first thing I want

00:20:43.080 --> 00:20:45.621
to know about a dynamic program,
is what are the subproblems.

00:20:47.610 --> 00:20:51.640
Somehow they are designed to
help solve your actual problem.

00:20:58.650 --> 00:21:01.650
And the idea of memoization is,
once you solve a subproblem,

00:21:01.650 --> 00:21:02.650
write down the answer.

00:21:02.650 --> 00:21:05.350
If you ever need to solve
that same problem again

00:21:05.350 --> 00:21:07.400
you reuse the answer.

00:21:07.400 --> 00:21:09.134
So that is the core idea.

00:21:09.134 --> 00:21:10.800
And so in this sense
dynamic programming

00:21:10.800 --> 00:21:16.025
is essentially recursion
plus memoization.

00:21:23.490 --> 00:21:27.450
And so in this case these
are the subproblems.

00:21:27.450 --> 00:21:29.950
Fibonacci of 1 through
Fibonacci of n.

00:21:29.950 --> 00:21:31.677
The one we care about
is Fibonacci of n.

00:21:31.677 --> 00:21:33.760
But to get there we solve
these other subproblems.

00:21:36.290 --> 00:21:40.180
In all cases, if this
is the situation-- so

00:21:40.180 --> 00:21:43.280
for any dynamic program,
the running time

00:21:43.280 --> 00:21:46.560
is going to be equal to the
number of different subproblems

00:21:46.560 --> 00:21:50.590
you might have to solve,
or that you do solve,

00:21:50.590 --> 00:21:54.480
times the amount of time
you spend per subproblem.

00:22:00.950 --> 00:22:01.450
OK.

00:22:01.450 --> 00:22:06.960
In this situation we
had n subproblems.

00:22:06.960 --> 00:22:10.900
And for each of them
we spent constant time.

00:22:10.900 --> 00:22:13.580
And when I measure the
time per subproblem

00:22:13.580 --> 00:22:15.890
which, in the Fibonacci
case I claim is constant,

00:22:15.890 --> 00:22:18.790
I ignore recursive calls.

00:22:18.790 --> 00:22:19.914
That's the key.

00:22:19.914 --> 00:22:21.330
We don't have to
solve recurrences

00:22:21.330 --> 00:22:22.630
with dynamic programming.

00:22:22.630 --> 00:22:23.355
Yay.

00:22:23.355 --> 00:22:25.250
No recurrences necessary.

00:22:25.250 --> 00:22:25.750
OK.

00:22:25.750 --> 00:22:29.840
Don't count recursions.

00:22:34.280 --> 00:22:36.800
Obviously, don't count
memoized recursions.

00:22:36.800 --> 00:22:39.750
The reason is, I only
need to count them once.

00:22:39.750 --> 00:22:42.540
After the first time
I do it, it's free.

00:22:42.540 --> 00:22:45.835
So I count how many different
subproblems do I need to do?

00:22:45.835 --> 00:22:48.210
These are they going to be
the expensive recursions where

00:22:48.210 --> 00:22:49.810
I do work, I do
some amount of work,

00:22:49.810 --> 00:22:52.060
but I don't count the
recursions because otherwise I'd

00:22:52.060 --> 00:22:53.600
be double counting.

00:22:53.600 --> 00:22:56.930
I only want to count
each subproblem once,

00:22:56.930 --> 00:22:59.010
and then this will solve it.

00:22:59.010 --> 00:23:00.840
So a simple idea.

00:23:00.840 --> 00:23:03.860
In general, dynamic programming
is a super simple idea.

00:23:03.860 --> 00:23:05.390
It's nothing fancy.

00:23:05.390 --> 00:23:07.440
It's basically just memoization.

00:23:07.440 --> 00:23:10.730
There is one extra trick
we're going to pull out,

00:23:10.730 --> 00:23:12.770
but that's the idea.

00:23:12.770 --> 00:23:14.110
All right.

00:23:14.110 --> 00:23:24.010
Let me tell you
another perspective.

00:23:24.010 --> 00:23:28.050
This is the one maybe
most commonly taught.

00:23:28.050 --> 00:23:31.260
Is to think of-- but I'm
not a particular fan of it.

00:23:31.260 --> 00:23:32.470
I really like memoization.

00:23:32.470 --> 00:23:35.240
I think it's a simple idea.

00:23:35.240 --> 00:23:38.500
And as long as you
remember this formula here,

00:23:38.500 --> 00:23:39.900
it's really easy to work with.

00:23:43.240 --> 00:23:45.560
But some people like to
think of it this way.

00:23:45.560 --> 00:23:49.315
And so you can pick whichever
way you find most intuitive.

00:23:49.315 --> 00:23:51.440
Instead of thinking of a
recursive algorithm, which

00:23:51.440 --> 00:23:55.940
in some sense starts at the
top of what you want to solve

00:23:55.940 --> 00:23:58.660
and works its way down,
you could do the reverse.

00:23:58.660 --> 00:24:00.829
You could start at the
bottom and work your way up.

00:24:00.829 --> 00:24:02.370
And this is probably
how you normally

00:24:02.370 --> 00:24:03.994
think about computing
Fibonacci numbers

00:24:03.994 --> 00:24:05.754
or how you learned it before.

00:24:05.754 --> 00:24:07.670
I'm going to write it
in a slightly funny way.

00:24:31.655 --> 00:24:33.780
The point I want to make
is that the transformation

00:24:33.780 --> 00:24:38.840
I'm doing from the naive
recursive algorithm,

00:24:38.840 --> 00:24:42.080
to the memoized algorithm,
to the bottom-up algorithm

00:24:42.080 --> 00:24:44.030
is completely automated.

00:24:44.030 --> 00:24:46.650
I'm not thinking,
I'm just doing.

00:24:46.650 --> 00:24:47.470
OK.

00:24:47.470 --> 00:24:49.500
It's easy.

00:24:49.500 --> 00:24:52.470
This code is exactly
the same as this code

00:24:52.470 --> 00:24:56.590
and as that code, except
I replaced n by k.

00:24:56.590 --> 00:24:59.850
Just because I needed a couple
of different n values here.

00:24:59.850 --> 00:25:01.380
Or I want to iterate
over n values.

00:25:04.110 --> 00:25:06.490
And then there's this
stuff around that code

00:25:06.490 --> 00:25:07.690
which is just formulaic.

00:25:11.000 --> 00:25:13.460
A little bit of thought
goes into this for loop,

00:25:13.460 --> 00:25:15.160
but that's it.

00:25:15.160 --> 00:25:15.660
OK.

00:25:15.660 --> 00:25:19.110
This does exactly the same
thing as the memoized algorithm.

00:25:22.525 --> 00:25:24.150
Maybe it takes a
little bit of thinking

00:25:24.150 --> 00:25:26.840
to realize, if you unroll all
the recursion that's happening

00:25:26.840 --> 00:25:29.350
here and just write
it out sequentially,

00:25:29.350 --> 00:25:31.830
this is exactly
what's happening.

00:25:31.830 --> 00:25:34.580
This code does exactly the
same additions, exactly

00:25:34.580 --> 00:25:37.020
the same computations as this.

00:25:37.020 --> 00:25:39.690
The only difference
is how you get there.

00:25:39.690 --> 00:25:42.310
Here we're using a loop,
here we're using recursion.

00:25:42.310 --> 00:25:45.559
But the same things
happen in the same order.

00:25:45.559 --> 00:25:47.350
It's really no difference
between the code.

00:25:47.350 --> 00:25:49.683
This code's probably going
to be more efficient practice

00:25:49.683 --> 00:25:52.980
because you don't make
function calls so much.

00:25:52.980 --> 00:25:55.840
In fact I made a
little mistake here.

00:25:55.840 --> 00:25:57.240
This is not a
function call, it's

00:25:57.240 --> 00:25:59.470
just a lookup into a table.

00:25:59.470 --> 00:26:01.180
Here I'm using a hash
table to be simple,

00:26:01.180 --> 00:26:02.721
but of course you
could use an array.

00:26:05.920 --> 00:26:10.330
But they're both constant
time with good hashing.

00:26:10.330 --> 00:26:10.830
All right.

00:26:10.830 --> 00:26:12.380
So is it clear
what this is doing?

00:26:12.380 --> 00:26:13.580
I think so.

00:26:13.580 --> 00:26:16.410
I think I made a little typo.

00:26:16.410 --> 00:26:21.630
So we have to compute--
oh, another typo.

00:26:21.630 --> 00:26:27.010
We have to compute f1 up to
fn, which in python is that.

00:26:27.010 --> 00:26:31.860
And we compute it
exactly how we used to.

00:26:31.860 --> 00:26:34.020
Except now, instead
of recursing,

00:26:34.020 --> 00:26:38.170
I know that when I'm computing
the k Fibonacci number-- man.

00:26:38.170 --> 00:26:39.135
So many typos.

00:26:39.135 --> 00:26:40.504
AUDIENCE: [LAUGHTER]

00:26:40.504 --> 00:26:42.920
PROFESSOR: You
guys are laughing.

00:26:42.920 --> 00:26:45.230
When I compute the
kth Fibonacci number

00:26:45.230 --> 00:26:48.230
I know that I've already
computed the previous two.

00:26:48.230 --> 00:26:48.730
Why?

00:26:48.730 --> 00:26:50.550
Because I'm doing them
in increasing order.

00:26:50.550 --> 00:26:52.620
Nothing fancy.

00:26:52.620 --> 00:26:55.230
Then I can just do
this and the solutions

00:26:55.230 --> 00:26:56.750
will just be waiting there.

00:26:56.750 --> 00:26:58.450
If they work, I'd
get a key error.

00:26:58.450 --> 00:27:00.120
So I'd know that there's a bug.

00:27:00.120 --> 00:27:01.670
But in fact, I won't
get a key error.

00:27:01.670 --> 00:27:05.090
I will have always computed
these things already.

00:27:05.090 --> 00:27:06.610
Then I store it in my table.

00:27:06.610 --> 00:27:07.290
Then I iterate.

00:27:07.290 --> 00:27:11.250
Eventually I've solved all the
subproblems, f1 through fn.

00:27:11.250 --> 00:27:14.140
And the one I cared
about was the nth one.

00:27:14.140 --> 00:27:14.640
OK.

00:27:14.640 --> 00:27:17.490
So straightforward.

00:27:17.490 --> 00:27:19.432
I do this because
I don't really want

00:27:19.432 --> 00:27:21.140
to have to go through
this transformation

00:27:21.140 --> 00:27:22.870
for every single problem we do.

00:27:22.870 --> 00:27:25.150
I'm doing it in Fibonacci
because it's super easy

00:27:25.150 --> 00:27:27.080
to write the code
out explicitly.

00:27:27.080 --> 00:27:29.200
But you can do it for all
of the dynamic programs

00:27:29.200 --> 00:27:33.130
that we cover in the
next four lectures.

00:27:33.130 --> 00:27:33.630
OK.

00:27:33.630 --> 00:27:35.810
I'm going to give you
now the general case.

00:27:35.810 --> 00:27:39.260
This was the special
Fibonacci version.

00:27:39.260 --> 00:27:44.230
In general, the bottom-up does
exactly the same computation

00:27:44.230 --> 00:27:45.565
as the memoized version.

00:27:51.270 --> 00:27:59.920
And what we're doing is
actually a topological sort

00:27:59.920 --> 00:28:05.195
of the subproblem
dependency DAG.

00:28:13.840 --> 00:28:17.240
So in this case, the
dependency DAG is very simple.

00:28:17.240 --> 00:28:21.330
In order to compute--
I'll do it backwards.

00:28:21.330 --> 00:28:26.450
In order to compute fn,
I need to know fn minus 1

00:28:26.450 --> 00:28:28.820
and fn minus 2.

00:28:28.820 --> 00:28:32.090
If I know those
I can compute fn.

00:28:32.090 --> 00:28:35.080
Then there's fn
minus 3, which is

00:28:35.080 --> 00:28:38.450
necessary to compute this
one, and that one, and so on.

00:28:38.450 --> 00:28:40.470
So you see what
this DAG looks like.

00:28:40.470 --> 00:28:42.160
Now, I've drawn
it conveniently so

00:28:42.160 --> 00:28:43.510
all the edges go left to right.

00:28:43.510 --> 00:28:46.420
So this is a topological
order from left to right.

00:28:46.420 --> 00:28:50.760
And so I just need to do
f1, f2, up to fn in order.

00:28:50.760 --> 00:28:53.850
Usually it's totally
obvious what order

00:28:53.850 --> 00:28:55.560
to solve the subproblems in.

00:28:55.560 --> 00:28:58.320
But in general, what
you should have in mind

00:28:58.320 --> 00:29:00.430
is that we are doing
a topological sort.

00:29:00.430 --> 00:29:03.070
Here we just did it in our
heads because it's so easy.

00:29:03.070 --> 00:29:04.220
And usually it's so easy.

00:29:04.220 --> 00:29:05.480
It's just a for loop.

00:29:05.480 --> 00:29:06.430
Nothing fancy.

00:29:09.900 --> 00:29:10.500
All right.

00:29:13.404 --> 00:29:14.450
I'm missing an arrow.

00:29:20.030 --> 00:29:22.554
All right.

00:29:22.554 --> 00:29:24.845
Let's do something a little
more interesting, shall we?

00:29:37.971 --> 00:29:38.470
All right.

00:29:38.470 --> 00:29:41.770
One thing you can do from
this bottom-up perspective

00:29:41.770 --> 00:29:42.775
is you can save space.

00:29:46.310 --> 00:29:48.572
Storage space in the algorithm.

00:29:48.572 --> 00:29:51.110
We don't usually worry
about space in this class,

00:29:51.110 --> 00:29:55.910
but it matters in reality.

00:29:55.910 --> 00:29:57.500
So here we're
building a table size,

00:29:57.500 --> 00:29:59.830
n, but in fact we
really only need

00:29:59.830 --> 00:30:02.280
to remember the last two values.

00:30:02.280 --> 00:30:04.190
So you could just store
the last two values,

00:30:04.190 --> 00:30:07.620
and each time you make a
new one delete the oldest.

00:30:07.620 --> 00:30:10.100
so by thinking a
little bit here you

00:30:10.100 --> 00:30:12.470
realize you only
need constant space.

00:30:12.470 --> 00:30:15.790
Still linear time,
but constant space.

00:30:15.790 --> 00:30:17.444
And that's often the case.

00:30:17.444 --> 00:30:18.860
From the bottom-up
perspective you

00:30:18.860 --> 00:30:20.699
see what you really
need to store,

00:30:20.699 --> 00:30:21.990
what you need to keep track of.

00:30:24.830 --> 00:30:26.359
All right.

00:30:26.359 --> 00:30:28.400
I guess another nice thing
about this perspective

00:30:28.400 --> 00:30:30.710
is, the running time
is totally obvious.

00:30:30.710 --> 00:30:32.430
This is clearly constant time.

00:30:32.430 --> 00:30:34.730
So this is clearly linear time.

00:30:34.730 --> 00:30:37.030
Whereas, in this
memoized algorithm

00:30:37.030 --> 00:30:40.110
you have to think
about, when's it

00:30:40.110 --> 00:30:42.150
going to be memoized,
when is it not?

00:30:42.150 --> 00:30:44.979
I still like this perspective
because, with this rule,

00:30:44.979 --> 00:30:46.520
just multiply a
number of subproblems

00:30:46.520 --> 00:30:49.110
by time per subproblem,
you get the answer.

00:30:49.110 --> 00:30:54.300
But it's a little less
obvious than code like this.

00:30:54.300 --> 00:30:57.060
So choose however you
like to think about it.

00:31:00.340 --> 00:31:00.840
All right.

00:31:00.840 --> 00:31:02.700
We move onto shortest paths.

00:31:28.890 --> 00:31:32.310
So I'm again, as usual, thinking
about single-source shortest

00:31:32.310 --> 00:31:33.980
paths.

00:31:33.980 --> 00:31:37.040
So we want to compute the
shortest pathway from s

00:31:37.040 --> 00:31:41.550
to v for all v. OK.

00:31:41.550 --> 00:31:46.040
I'd like to write this initially
as a naive recursive algorithm,

00:31:46.040 --> 00:31:49.900
which I can then memoize,
which I can then bottom-upify.

00:31:49.900 --> 00:31:53.040
I just made that up.

00:31:53.040 --> 00:31:57.080
So how could I write this as
a naive recursive algorithm?

00:31:57.080 --> 00:31:59.710
It's not so obvious.

00:31:59.710 --> 00:32:06.850
But first I'm going to tell you
how, just as an oracle tells

00:32:06.850 --> 00:32:08.470
you, here's what you should do.

00:32:08.470 --> 00:32:10.950
But then we're going to think
about-- go back, step back.

00:32:10.950 --> 00:32:11.992
Actually, it's up to you.

00:32:11.992 --> 00:32:13.491
I could tell you
the answer and then

00:32:13.491 --> 00:32:15.070
we could figure out
how we got there,

00:32:15.070 --> 00:32:18.381
or we could just
figure out the answer.

00:32:18.381 --> 00:32:18.880
Preferences?

00:32:22.350 --> 00:32:22.934
Figure it out.

00:32:22.934 --> 00:32:23.433
All right.

00:32:23.433 --> 00:32:24.050
Good.

00:32:24.050 --> 00:32:26.140
No divine inspiration allowed.

00:32:26.140 --> 00:32:37.584
So let me give you a tool.

00:32:37.584 --> 00:32:38.760
The tool is guessing.

00:32:42.660 --> 00:32:45.880
This may sound silly, but
it's a very powerful tool.

00:32:48.500 --> 00:32:51.320
The general idea is, suppose
you don't know something

00:32:51.320 --> 00:32:53.360
but you'd like to know it.

00:32:53.360 --> 00:32:55.350
So what's the answer
to this question?

00:32:55.350 --> 00:32:56.630
I don't know.

00:32:56.630 --> 00:32:57.860
Man, I really want a cushion.

00:32:57.860 --> 00:32:59.650
How am I going to
answer the question?

00:32:59.650 --> 00:33:00.440
Guess.

00:33:00.440 --> 00:33:00.940
OK?

00:33:00.940 --> 00:33:02.530
AUDIENCE: [LAUGHTER]

00:33:02.530 --> 00:33:04.310
PROFESSOR: It's a
tried and tested method

00:33:04.310 --> 00:33:05.490
for solving any problem.

00:33:16.920 --> 00:33:19.720
I'm kind of belaboring
the point here.

00:33:19.720 --> 00:33:23.610
The algorithmic concept is,
don't just try any guess.

00:33:23.610 --> 00:33:25.960
Try them all.

00:33:25.960 --> 00:33:26.747
OK?

00:33:26.747 --> 00:33:29.490
AUDIENCE: [LAUGHTER]

00:33:29.490 --> 00:33:30.810
PROFESSOR: Also pretty simple.

00:33:30.810 --> 00:33:32.881
I said dynamic
programming was simple.

00:33:32.881 --> 00:33:33.380
OK.

00:33:38.400 --> 00:33:43.030
Try all guesses.

00:33:43.030 --> 00:33:45.430
This is central to the
dynamic programming.

00:33:45.430 --> 00:33:52.470
I know it sounds obvious, but if
I want to fix my equation here,

00:33:52.470 --> 00:33:58.260
dynamic programming is roughly
recursion plus memoization.

00:33:58.260 --> 00:34:01.800
This should really
be, plus guessing.

00:34:01.800 --> 00:34:06.340
Memoization, which is obvious,
guessing which is obvious,

00:34:06.340 --> 00:34:08.928
are the central concepts
to dynamic programming.

00:34:08.928 --> 00:34:11.219
I'm trying to make it sound
easy because usually people

00:34:11.219 --> 00:34:12.802
have trouble with
dynamic programming.

00:34:12.802 --> 00:34:15.310
It is easy.

00:34:15.310 --> 00:34:16.792
Try all the guesses.

00:34:16.792 --> 00:34:18.500
That's something a
computer can do great.

00:34:18.500 --> 00:34:20.111
This is the brute force part.

00:34:20.111 --> 00:34:20.610
OK.

00:34:20.610 --> 00:34:23.690
But we're going to
do it carefully.

00:34:23.690 --> 00:34:24.689
Not that carefully.

00:34:24.689 --> 00:34:26.690
I mean, we're just
trying all the guesses.

00:34:26.690 --> 00:34:27.590
Take the best one.

00:34:33.960 --> 00:34:35.980
That's kind of important
that we can choose one

00:34:35.980 --> 00:34:36.864
to be called best.

00:34:36.864 --> 00:34:38.239
That's why dynamic
programming is

00:34:38.239 --> 00:34:39.610
good for optimization problems.

00:34:39.610 --> 00:34:42.110
You want to maximize
something, minimize something,

00:34:42.110 --> 00:34:45.130
you try them all and then you
can forget about all of them

00:34:45.130 --> 00:34:47.020
and just reduce it
down to one thing which

00:34:47.020 --> 00:34:50.890
is the best one, or a best one.

00:34:50.890 --> 00:34:51.389
OK.

00:34:51.389 --> 00:34:53.830
So now I want you
to try to apply

00:34:53.830 --> 00:34:56.639
this principle to
shortest paths.

00:34:56.639 --> 00:34:59.380
Now I'm going to draw a
picture which may help.

00:34:59.380 --> 00:35:08.790
We have the source, s,
we have some vertex,

00:35:08.790 --> 00:35:11.370
v. We'd like to
find the shortest--

00:35:11.370 --> 00:35:13.870
a shortest path from s to v.

00:35:13.870 --> 00:35:16.250
Suppose I want to know
what this shortest path is.

00:35:16.250 --> 00:35:18.092
Suppose this was it.

00:35:18.092 --> 00:35:19.790
You have an idea already?

00:35:19.790 --> 00:35:21.245
Yeah.

00:35:21.245 --> 00:35:27.550
AUDIENCE: What you could do is
you could look at everywhere

00:35:27.550 --> 00:35:29.490
you can go from s.

00:35:29.490 --> 00:35:33.074
[INAUDIBLE] shortest path
of each of those notes.

00:35:33.074 --> 00:35:33.740
PROFESSOR: Good.

00:35:33.740 --> 00:35:37.920
So I can look at all the
places I could go from s,

00:35:37.920 --> 00:35:41.790
and then look at the shortest
paths from there to v.

00:35:41.790 --> 00:35:45.600
So we could call this s prime.

00:35:45.600 --> 00:35:46.730
So here's the idea.

00:35:46.730 --> 00:35:50.720
There's some hypothetical
shortest path.

00:35:50.720 --> 00:35:53.460
I don't know where
it goes first,

00:35:53.460 --> 00:35:56.080
so I will guess
where it goes first.

00:35:56.080 --> 00:35:58.800
I know the first
edge must be one

00:35:58.800 --> 00:36:00.090
of the outgoing edges from s.

00:36:00.090 --> 00:36:01.100
I don't know which one.

00:36:01.100 --> 00:36:03.210
Try them all.

00:36:03.210 --> 00:36:04.479
Very simple idea.

00:36:04.479 --> 00:36:06.020
Then from each of
those, if somehow I

00:36:06.020 --> 00:36:10.590
can compute the shortest
path from there to v,

00:36:10.590 --> 00:36:13.740
just do that and
take the best choice

00:36:13.740 --> 00:36:15.650
for what that first edge was.

00:36:15.650 --> 00:36:18.670
So this would be the
guess first edge approach.

00:36:22.340 --> 00:36:23.690
It's a very good idea.

00:36:23.690 --> 00:36:28.470
Not quite the one I wanted
because unfortunately

00:36:28.470 --> 00:36:30.589
that changes s.

00:36:30.589 --> 00:36:32.130
And so this would
work, it would just

00:36:32.130 --> 00:36:33.504
be slightly less
efficient if I'm

00:36:33.504 --> 00:36:36.090
solving single-source
shortest paths.

00:36:36.090 --> 00:36:38.010
So I'm going to tweak
that idea slightly

00:36:38.010 --> 00:36:40.310
by guessing the last edge
instead of the first edge.

00:36:40.310 --> 00:36:41.962
They're really equivalent.

00:36:41.962 --> 00:36:43.420
If I was doing this
I'd essentially

00:36:43.420 --> 00:36:46.480
be solving a single-target
shortest paths,

00:36:46.480 --> 00:36:49.820
which we talked about before.

00:36:49.820 --> 00:36:51.645
So I'm going to draw
the same picture.

00:36:56.510 --> 00:36:59.430
I want to get to v. I'm
going to guess the last edge,

00:36:59.430 --> 00:37:01.560
call it uv.

00:37:01.560 --> 00:37:06.260
I know it's one of the incoming
edges to v-- unless s equals v,

00:37:06.260 --> 00:37:07.780
then there's a special case.

00:37:07.780 --> 00:37:09.910
As long as this path has
length of at least 1,

00:37:09.910 --> 00:37:11.290
there's some last edge.

00:37:11.290 --> 00:37:12.040
What is it?

00:37:12.040 --> 00:37:12.840
I don't know.

00:37:12.840 --> 00:37:14.070
Guess.

00:37:14.070 --> 00:37:19.010
Guess all the possible
incoming edges to v, and then

00:37:19.010 --> 00:37:23.130
recursively compute the
shortest path from s to u.

00:37:23.130 --> 00:37:25.871
And then add on the edge v.

00:37:25.871 --> 00:37:26.370
OK.

00:37:26.370 --> 00:37:27.750
So what is this shortest path?

00:37:27.750 --> 00:37:32.160
It's delta of s comma
u, which looks the same.

00:37:32.160 --> 00:37:34.910
It's another subproblem
that I want to solve.

00:37:34.910 --> 00:37:37.780
There's v subproblems
here I care about. .

00:37:37.780 --> 00:37:38.660
So that's good.

00:37:38.660 --> 00:37:39.780
I take that.

00:37:39.780 --> 00:37:41.805
I add on the weight
of the edge uv.

00:37:44.800 --> 00:37:50.020
And that should hopefully
give me delta of s comma v.

00:37:50.020 --> 00:37:53.460
Well, if I was lucky and I
guessed the right choice of u.

00:37:53.460 --> 00:37:55.490
In reality, I'm not lucky.

00:37:55.490 --> 00:38:02.790
So I have to minimize
over all edges uv.

00:38:02.790 --> 00:38:05.370
So this is the--
we're minimizing

00:38:05.370 --> 00:38:06.580
over the choice of u.

00:38:06.580 --> 00:38:08.420
V is already given here.

00:38:08.420 --> 00:38:12.130
So I take the minimum over
all edges of the shortest

00:38:12.130 --> 00:38:17.324
path from s to u, plus
the weight of the edge uv.

00:38:17.324 --> 00:38:19.740
That should give me the shortest
path because this gave me

00:38:19.740 --> 00:38:21.580
the shortest path from s to u.

00:38:21.580 --> 00:38:24.110
Then I added on the edge
I need to get there.

00:38:24.110 --> 00:38:30.120
And wherever the shortest path
is, it uses some last edge, uv.

00:38:30.120 --> 00:38:32.650
There's got to be some choice
of u that is the right one.

00:38:32.650 --> 00:38:35.245
That's the good guess
that we're hoping for.

00:38:35.245 --> 00:38:36.620
We don't know what
the good guess

00:38:36.620 --> 00:38:38.650
is so we just try them all.

00:38:38.650 --> 00:38:43.265
But whatever it is, this will
be the weight of that path.

00:38:43.265 --> 00:38:44.890
It's going to take
the best path from s

00:38:44.890 --> 00:38:46.650
to u because sub
paths are shortest

00:38:46.650 --> 00:38:47.691
paths are shortest paths.

00:38:47.691 --> 00:38:48.900
Optimal substructure.

00:38:48.900 --> 00:38:51.456
So this part will
be delta of su.

00:38:51.456 --> 00:38:53.950
This part is obviously w of uv.

00:38:53.950 --> 00:38:57.760
So this will give
the right answer.

00:38:57.760 --> 00:38:59.610
Hopefully.

00:38:59.610 --> 00:39:00.865
OK.

00:39:00.865 --> 00:39:02.240
It's certainly
going to-- I mean,

00:39:02.240 --> 00:39:05.100
this is the analog of the
naive recursive algorithm

00:39:05.100 --> 00:39:05.830
for Fibonacci.

00:39:05.830 --> 00:39:08.190
So it's not going to be
efficient if I-- I mean,

00:39:08.190 --> 00:39:09.850
this is an algorithm, right?

00:39:09.850 --> 00:39:13.720
You could say-- this
is a recursive call.

00:39:13.720 --> 00:39:17.520
We're going to treat this
as recursive call instead

00:39:17.520 --> 00:39:19.910
of just a definition.

00:39:19.910 --> 00:39:23.290
Then this is a
recursive algorithm.

00:39:23.290 --> 00:39:27.086
How good or bad is this
recursive algorithm?

00:39:27.086 --> 00:39:28.010
AUDIENCE: Terrible.

00:39:28.010 --> 00:39:28.850
PROFESSOR: Terrible.

00:39:28.850 --> 00:39:29.960
Very good.

00:39:29.960 --> 00:39:31.240
Very bad, I should say.

00:39:34.000 --> 00:39:38.080
It's definitely going
to be exponential

00:39:38.080 --> 00:39:39.120
without memoization.

00:39:39.120 --> 00:39:39.957
But we know.

00:39:39.957 --> 00:39:41.540
We know how to make
algorithms better.

00:39:41.540 --> 00:39:42.640
We memoize.

00:39:42.640 --> 00:39:43.140
OK.

00:39:43.140 --> 00:39:46.850
So I think you know how to write
this as a memoized algorithm.

00:39:46.850 --> 00:39:51.820
To define the function delta
of sv, you first check,

00:39:51.820 --> 00:39:54.080
is s comma v in the memo table?

00:39:54.080 --> 00:39:55.560
If so return that value.

00:39:55.560 --> 00:39:59.450
Otherwise, do this computation
where this is a recursive call

00:39:59.450 --> 00:40:02.820
and then stored it
in the memo table.

00:40:02.820 --> 00:40:03.460
OK.

00:40:03.460 --> 00:40:05.126
I don't think I need
to write that down.

00:40:05.126 --> 00:40:07.360
It's just like the
memoized code over there.

00:40:07.360 --> 00:40:11.190
Just there's now two
arguments instead of one.

00:40:11.190 --> 00:40:12.390
In fact, s isn't changing.

00:40:12.390 --> 00:40:18.500
So I only need to store
with v instead of s comma v.

00:40:18.500 --> 00:40:19.845
Is that a good algorithm?

00:40:19.845 --> 00:40:23.570
I claim memoization
makes everything faster.

00:40:23.570 --> 00:40:25.700
Is that a fast algorithm?

00:40:36.560 --> 00:40:37.640
Not so obvious, I guess.

00:40:51.080 --> 00:40:52.570
Yes?

00:40:52.570 --> 00:40:54.990
How many people think, yes,
that's a good algorithm?

00:40:54.990 --> 00:40:55.750
AUDIENCE: Better.

00:40:55.750 --> 00:40:56.030
PROFESSOR: Better.

00:40:56.030 --> 00:40:57.090
Definitely better.

00:40:57.090 --> 00:40:58.340
Can't be worse.

00:40:58.340 --> 00:41:00.960
How many people think it's
a bad algorithm still?

00:41:00.960 --> 00:41:01.460
OK.

00:41:01.460 --> 00:41:04.760
So three for yes, zero for no.

00:41:04.760 --> 00:41:08.840
How many people aren't sure?

00:41:08.840 --> 00:41:09.871
Including the yes votes?

00:41:09.871 --> 00:41:10.370
Good.

00:41:12.970 --> 00:41:13.570
All right.

00:41:13.570 --> 00:41:14.410
It's not so tricky.

00:41:14.410 --> 00:41:15.415
Let me draw you a graph.

00:41:27.372 --> 00:41:30.350
Something like that.

00:41:30.350 --> 00:41:32.330
So we wanted to commit
delta of s comma

00:41:32.330 --> 00:41:36.670
v. Let me give these
guys names, a and b.

00:41:36.670 --> 00:41:39.380
So we compute delta of
s comma v. To compute

00:41:39.380 --> 00:41:45.830
that we need to know delta
of s comma a and delta

00:41:45.830 --> 00:41:48.110
of s comma v. All right?

00:41:48.110 --> 00:41:51.990
Those are the two ways-- sorry,
actually we just need one.

00:41:51.990 --> 00:41:57.640
Only one incoming edge to v.
So its delta of s comma a.

00:42:00.950 --> 00:42:04.090
Sorry-- I should have
put a base case here too.

00:42:04.090 --> 00:42:07.150
Delta of s comma s equals 0.

00:42:10.200 --> 00:42:10.700
OK.

00:42:10.700 --> 00:42:12.801
Delta of s comma
a plus the edge.

00:42:12.801 --> 00:42:13.300
OK.

00:42:13.300 --> 00:42:14.940
There is some
shortest path to a.

00:42:14.940 --> 00:42:16.520
To compute the
shortest path to a we

00:42:16.520 --> 00:42:18.020
look at all the
incoming edges to a.

00:42:18.020 --> 00:42:19.400
There's only one.

00:42:19.400 --> 00:42:22.320
So delta of s comma b.

00:42:22.320 --> 00:42:24.620
Now I want to compute the
shortest paths from b.

00:42:24.620 --> 00:42:26.220
Well, there's two
ways to get to b.

00:42:26.220 --> 00:42:33.730
One of them is delta of s
comma b-- sorry, s comma s.

00:42:33.730 --> 00:42:35.470
Came from s.

00:42:35.470 --> 00:42:42.760
The other way is delta of s
comma v. Do you see a problem?

00:42:42.760 --> 00:42:44.540
Yeah.

00:42:44.540 --> 00:42:47.420
Delta of s comma v is what
we were trying to figure out.

00:42:50.080 --> 00:42:51.990
Now you might say, oh,
it's OK because we're

00:42:51.990 --> 00:42:54.060
going to memoize our
answer to delta s comma v

00:42:54.060 --> 00:42:55.309
and then we can reuse it here.

00:42:55.309 --> 00:42:57.530
Except, we haven't finished
computing delta of s

00:42:57.530 --> 00:43:02.340
comma v. We can only put it in
the memo table once we're done.

00:43:02.340 --> 00:43:06.430
So when this call happens the
memo table has not been set.

00:43:06.430 --> 00:43:07.930
And we're going to
do the same thing

00:43:07.930 --> 00:43:09.840
over and over and over again.

00:43:09.840 --> 00:43:12.890
This is an infinite algorithm.

00:43:12.890 --> 00:43:14.740
Oops.

00:43:14.740 --> 00:43:15.560
Not so hot.

00:43:19.410 --> 00:43:30.160
So it's going to be infinite
time on graphs with cycles.

00:43:35.155 --> 00:43:35.655
OK.

00:43:35.655 --> 00:43:42.400
For DAGs, for acyclic graphs, it
actually runs in v plus e time.

00:43:42.400 --> 00:43:43.670
This is the good case.

00:43:43.670 --> 00:43:46.930
In this situation we
can use this formula.

00:43:46.930 --> 00:43:48.940
The time is equal to the
number of subproblems

00:43:48.940 --> 00:43:52.320
times the time per subproblem.

00:43:52.320 --> 00:43:55.040
So I guess we have to think
about that a little bit.

00:43:55.040 --> 00:43:55.720
Where's my code?

00:43:55.720 --> 00:43:57.190
Here's my code.

00:43:57.190 --> 00:44:00.660
Number of subproblems
is v. There's

00:44:00.660 --> 00:44:03.110
v different subproblems
that I'm using here.

00:44:03.110 --> 00:44:05.360
I'm always reusing
subproblems of the form delta

00:44:05.360 --> 00:44:06.400
s comma something.

00:44:06.400 --> 00:44:10.310
The something could be
any of the v vertices.

00:44:10.310 --> 00:44:14.620
How much time do I
spend per subproblem?

00:44:14.620 --> 00:44:15.790
That's a little tricky.

00:44:15.790 --> 00:44:17.640
It's the number
of incoming edges

00:44:17.640 --> 00:44:30.710
to v. So time for a
sub problem delta of sv

00:44:30.710 --> 00:44:36.850
is the indegree of v. The
number of incoming edges to v.

00:44:36.850 --> 00:44:39.230
So this depends on
v. So I can't just

00:44:39.230 --> 00:44:41.050
take a straightforward
product here.

00:44:41.050 --> 00:44:42.466
What this is really
saying is, you

00:44:42.466 --> 00:44:44.130
should sum up over
all sub problems

00:44:44.130 --> 00:44:46.600
of the time per sub problem.

00:44:46.600 --> 00:44:58.360
So total time is the sum over
all v and v, the indegree of v.

00:44:58.360 --> 00:45:02.920
And we know this
is number of edges.

00:45:02.920 --> 00:45:06.650
It's really-- so indegree
plus 1, indegree plus 1.

00:45:06.650 --> 00:45:11.000
So this is v plus v. OK.

00:45:11.000 --> 00:45:14.060
Handshaking again.

00:45:14.060 --> 00:45:14.680
OK.

00:45:14.680 --> 00:45:17.340
Now we already knew an algorithm
for shortest paths and DAGs.

00:45:17.340 --> 00:45:18.650
And it ran a v plus e time.

00:45:18.650 --> 00:45:21.370
So it's another way
to do the same thing.

00:45:21.370 --> 00:45:23.930
If you think about
it long enough,

00:45:23.930 --> 00:45:27.340
this algorithm
memoized, is essentially

00:45:27.340 --> 00:45:30.770
doing a depth first search
to do a topological sort

00:45:30.770 --> 00:45:33.440
to run one round
of Bellman-Ford.

00:45:33.440 --> 00:45:36.120
So we had topological sort
plus one round of Bellman-Ford.

00:45:36.120 --> 00:45:38.160
This is kind of it
all rolled into one.

00:45:38.160 --> 00:45:40.630
This should look kind of like
the Bellman Ford relaxation

00:45:40.630 --> 00:45:42.960
step, or shortest
paths relaxation step.

00:45:42.960 --> 00:45:44.150
It is.

00:45:44.150 --> 00:45:46.724
This min is really
doing the same thing.

00:45:46.724 --> 00:45:48.140
So it's really the
same algorithm.

00:45:48.140 --> 00:45:50.098
But we come at it from
a different perspective.

00:45:52.400 --> 00:45:53.470
OK.

00:45:53.470 --> 00:45:55.870
But I claim I can use
this same approach

00:45:55.870 --> 00:45:58.830
to solve shortest paths in
general graphs, even when they

00:45:58.830 --> 00:46:01.540
have cycles.

00:46:01.540 --> 00:46:04.170
How am I going to do that?

00:46:04.170 --> 00:46:08.980
DAGs seem fine-- oh, what
was the lesson learned here?

00:46:08.980 --> 00:46:18.400
Lesson learned is that
subproblem dependencies

00:46:18.400 --> 00:46:19.450
should be acyclic.

00:46:19.450 --> 00:46:22.240
Otherwise, we get an
infinite algorithm.

00:46:22.240 --> 00:46:24.990
For memoization to work
this is what you need.

00:46:24.990 --> 00:46:26.050
It's all you need.

00:46:29.590 --> 00:46:30.090
OK.

00:46:30.090 --> 00:46:32.460
We've almost seen this already.

00:46:32.460 --> 00:46:35.090
Because I said that, to
do a bottom up algorithm

00:46:35.090 --> 00:46:39.140
you do a topological sort of
this subproblem dependency DAG.

00:46:39.140 --> 00:46:40.641
I already said it
should be acyclic.

00:46:40.641 --> 00:46:41.140
OK.

00:46:41.140 --> 00:46:42.490
We just forgot.

00:46:42.490 --> 00:46:43.872
I didn't tell you yet.

00:46:43.872 --> 00:46:45.580
So for that to work
it better be acyclic.

00:46:45.580 --> 00:46:48.870
For DP to work, for memoization
to work, it better be acyclic.

00:46:48.870 --> 00:46:55.530
If you're acyclic then
this is the running time.

00:46:55.530 --> 00:46:59.590
So that's all general.

00:46:59.590 --> 00:47:00.090
OK.

00:47:00.090 --> 00:47:02.060
So somehow I need to
take a cyclic graph

00:47:02.060 --> 00:47:03.005
and make it acyclic.

00:47:08.047 --> 00:47:10.005
We've actually done this
already in recitation.

00:47:18.740 --> 00:47:22.830
So if I have a
graph-- let's take

00:47:22.830 --> 00:47:25.850
a very simple cyclic graph.

00:47:25.850 --> 00:47:26.350
OK.

00:47:26.350 --> 00:47:29.570
One thing I could do is explode
it into multiple layers.

00:47:29.570 --> 00:47:32.400
We did this on quiz
two in various forms.

00:47:32.400 --> 00:47:35.020
It's like the only cool thing
you can do with shortest paths,

00:47:35.020 --> 00:47:37.882
I feel like.

00:47:37.882 --> 00:47:40.200
If you want to make a
shortest path problem harder,

00:47:40.200 --> 00:47:45.597
require that you reduce your
graph to k copies of the graph.

00:47:45.597 --> 00:47:47.180
I'm going to do it
in a particular way

00:47:47.180 --> 00:47:50.580
here-- which I think you've
seen in recitation-- which

00:47:50.580 --> 00:47:54.360
is to think of this axis as
time, or however you want,

00:47:54.360 --> 00:47:57.090
and make all of the
edges go from each layer

00:47:57.090 --> 00:47:58.050
to the next layer.

00:48:01.080 --> 00:48:03.470
This should be a
familiar technique.

00:48:03.470 --> 00:48:05.340
So the idea is,
every time I follow

00:48:05.340 --> 00:48:07.440
an edge I go down
to the next layer.

00:48:07.440 --> 00:48:09.740
This makes any graph acyclic.

00:48:09.740 --> 00:48:11.510
Done.

00:48:11.510 --> 00:48:13.640
What in the world
does this mean?

00:48:13.640 --> 00:48:14.550
What is it doing?

00:48:20.070 --> 00:48:21.960
What does it mean?

00:48:21.960 --> 00:48:23.100
Double rainbow.

00:48:23.100 --> 00:48:23.600
All right.

00:48:23.600 --> 00:48:24.433
AUDIENCE: [LAUGHTER]

00:48:24.433 --> 00:48:26.694
PROFESSOR: So-- I
don't know how I've

00:48:26.694 --> 00:48:28.610
gone so long in the
semester without referring

00:48:28.610 --> 00:48:29.360
to double rainbow.

00:48:29.360 --> 00:48:30.680
It used to be my favorite.

00:48:30.680 --> 00:48:31.180
All right.

00:48:31.180 --> 00:48:34.570
So here's what it means.

00:48:34.570 --> 00:48:37.019
Delta sub k of sv.

00:48:37.019 --> 00:48:38.560
I'm going to define
this first-- this

00:48:38.560 --> 00:48:41.640
is a new kind of
subproblem-- which

00:48:41.640 --> 00:48:49.220
is, what is the shortest-- what
is the weight of the shortest

00:48:49.220 --> 00:48:54.525
s to v path that uses,
at most, k edges.

00:48:58.732 --> 00:49:00.940
So I want it to be shortest
in terms of total weight,

00:49:00.940 --> 00:49:03.090
but I also want it to
use few edges total.

00:49:03.090 --> 00:49:05.200
So this is going to be 0.

00:49:05.200 --> 00:49:08.580
In some sense, if you
look at-- so here's s

00:49:08.580 --> 00:49:11.890
and I'm always going
to make s this.

00:49:11.890 --> 00:49:15.370
And then this is going to
be v in the zero situation.

00:49:15.370 --> 00:49:17.460
This is going to be v
in the one situation,

00:49:17.460 --> 00:49:20.600
v-- so if I look at this
v, I look at the shortest

00:49:20.600 --> 00:49:24.040
path from s to v, that
is delta sub 0 of sv.

00:49:24.040 --> 00:49:28.460
So maybe I'll call this v
sub 0, v sub 1, v sub 2.

00:49:28.460 --> 00:49:28.960
OK.

00:49:28.960 --> 00:49:30.585
Shortest path from
here to here is,

00:49:30.585 --> 00:49:32.390
there's no way to
get there on 0 edges.

00:49:32.390 --> 00:49:33.970
Shortest path from
here to here, that

00:49:33.970 --> 00:49:38.166
is the best way to get there
with, at most, one edge.

00:49:38.166 --> 00:49:39.540
Shortest path from
here to here--

00:49:39.540 --> 00:49:41.770
well, if I add some
vertical edges too,

00:49:41.770 --> 00:49:43.436
I guess, cheating a little bit.

00:49:43.436 --> 00:49:45.060
Then this is the best
way to get from s

00:49:45.060 --> 00:49:46.960
to v using at most two edges.

00:49:46.960 --> 00:49:51.000
And then you get
a recurrence which

00:49:51.000 --> 00:49:54.750
is the min over all last edges.

00:49:54.750 --> 00:49:56.570
So I'm just copying
that recurrence,

00:49:56.570 --> 00:50:03.040
but realizing that the s to
u part uses one fewer edge.

00:50:03.040 --> 00:50:04.331
And then I use the edge uv.

00:50:07.198 --> 00:50:07.698
OK.

00:50:07.698 --> 00:50:09.330
That's our new recurrence.

00:50:09.330 --> 00:50:11.380
By adding this k
parameter I've made

00:50:11.380 --> 00:50:14.180
this recurrence on
subproblems acyclic.

00:50:14.180 --> 00:50:17.050
Unfortunately, I've increased
the number of subproblems.

00:50:17.050 --> 00:50:28.250
The number of subproblems
now is v squared.

00:50:28.250 --> 00:50:30.840
Technically, v times v minus 1.

00:50:30.840 --> 00:50:32.880
Because I really--
actually, v squared.

00:50:32.880 --> 00:50:34.130
Sorry.

00:50:34.130 --> 00:50:36.860
I start at 0.

00:50:36.860 --> 00:50:46.210
And what I care about, my goal,
is delta sub v minus 1 of sv.

00:50:46.210 --> 00:50:47.900
Because by
Bellman-Ford analysis I

00:50:47.900 --> 00:50:51.270
know that I only care about
simple paths, paths of length

00:50:51.270 --> 00:50:52.040
at most v minus 1.

00:50:52.040 --> 00:50:53.873
I'm assuming here no
negative weight cycles.

00:50:53.873 --> 00:50:55.450
I should've said that earlier.

00:50:55.450 --> 00:50:58.620
If you assume that, then
this is what I care about.

00:50:58.620 --> 00:51:00.380
So k ranges from 0 to v minus 1.

00:51:00.380 --> 00:51:02.160
So there are v choices for k.

00:51:02.160 --> 00:51:04.860
There are v choices for v.
So the number of subproblems

00:51:04.860 --> 00:51:05.510
is v squared.

00:51:05.510 --> 00:51:07.230
How much time do I
spend per subproblem?

00:51:07.230 --> 00:51:08.271
Well, the same as before.

00:51:08.271 --> 00:51:09.890
The indegree-- where
did I write it?

00:51:09.890 --> 00:51:13.012
Up here-- the indegree
of that problem.

00:51:13.012 --> 00:51:14.470
So what I'm really
doing is summing

00:51:14.470 --> 00:51:17.400
over all v of the indegree.

00:51:17.400 --> 00:51:19.850
And then I multiply it by
v. So the running time,

00:51:19.850 --> 00:51:25.240
total running time is ve.

00:51:25.240 --> 00:51:26.740
Sound familiar?

00:51:26.740 --> 00:51:29.660
This is Bellman-Ford's
algorithm again.

00:51:29.660 --> 00:51:32.410
And this is actually where
Bellman-Ford algorithm

00:51:32.410 --> 00:51:35.810
came from is this view
on dynamic programming.

00:51:35.810 --> 00:51:38.640
So we're seeing yet another
way to do Bellman-Ford.

00:51:38.640 --> 00:51:39.660
It may seem familiar.

00:51:39.660 --> 00:51:41.160
But in the next
three lectures we're

00:51:41.160 --> 00:51:42.743
going to see a whole
bunch of problems

00:51:42.743 --> 00:51:44.650
that can succumb to
the same approach.

00:51:44.650 --> 00:51:47.040
And that's super cool.