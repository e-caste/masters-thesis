WEBVTT

00:00:07.000 --> 00:00:11.000
OK, good morning.
So today, we're going to

00:00:11.000 --> 00:00:16.000
continue our exploration of
multithreaded algorithms.

00:00:16.000 --> 00:00:21.000
Last time we talked about some
aspects of scheduling,

00:00:21.000 --> 00:00:26.000
and a little bit about
linguistics to describe a

00:00:26.000 --> 00:00:30.000
multithreaded competition.
And today, we're going to

00:00:30.000 --> 00:00:32.000
actually deal with some
algorithms.

00:00:47.000 --> 00:00:50.000
So, we're going to start out
with a really simple,

00:00:50.000 --> 00:00:53.000
actually, what's fun about
this, actually,

00:00:53.000 --> 00:00:57.000
is that everything I'm going to
teach you today I could have

00:00:57.000 --> 00:01:01.000
taught you in week two,
OK, because basically it's just

00:01:01.000 --> 00:01:05.000
taking the divide and conquer
hammer, and just smashing

00:01:05.000 --> 00:01:10.000
problem after problem with it.
OK, and so, actually next

00:01:10.000 --> 00:01:13.000
week's lectures on caching,
also very similar.

00:01:13.000 --> 00:01:18.000
So, everybody should bone up on
their master theorem and

00:01:18.000 --> 00:01:22.000
substitution methods for
occurrences, and so forth

00:01:22.000 --> 00:01:25.000
because that's our going to be
doing.

00:01:25.000 --> 00:01:28.000
And of course,
all the stuff will be on the

00:01:28.000 --> 00:01:31.000
final.
So let's start with matrix

00:01:31.000 --> 00:01:32.000
multiplication.

00:01:40.000 --> 00:01:45.000
And we'll do n by n.
So, our problem is to do C

00:01:45.000 --> 00:01:50.000
equals A times B.
And the way we'll do that is

00:01:50.000 --> 00:01:55.000
using divide and conquer,
as we saw before,

00:01:55.000 --> 00:02:02.000
although we're not going to use
Strassen's method.

00:02:02.000 --> 00:02:08.000
OK, we'll just use the ordinary
thing, and I'll leave Strassen's

00:02:08.000 --> 00:02:12.000
as an exercise.
So, the idea is we're going to

00:02:12.000 --> 00:02:18.000
look at matrix multiplication in
terms of an n by n matrix,

00:02:18.000 --> 00:02:22.000
in terms of n over 2 by n over
2 matrices.

00:02:22.000 --> 00:02:28.000
So, I partition C into four
blocks, and likewise with A and

00:02:28.000 --> 00:02:30.000
B.

00:02:50.000 --> 00:02:58.000
OK, and we multiply those out,
and that gives us the

00:02:58.000 --> 00:03:03.000
following.
Make sure I get all my indices

00:03:03.000 --> 00:03:04.000
right.

00:03:40.000 --> 00:03:45.000
OK, so it gives us the sum of
these two n by n matrices.

00:03:45.000 --> 00:03:49.000
OK, so for example,
if I multiply the first row by

00:03:49.000 --> 00:03:54.000
the first column,
I'm putting the first term,

00:03:54.000 --> 00:03:58.000
A_1-1 times B_1-1 in this
matrix, in the second one,

00:03:58.000 --> 00:04:03.000
A_1-2 times B_2-1 gets placed
here.

00:04:03.000 --> 00:04:06.000
So, when I sum them,
and so forth,

00:04:06.000 --> 00:04:10.000
for the other entries,
and when I sum them,

00:04:10.000 --> 00:04:16.000
I'm going to get my result.
So, we can write that out as a,

00:04:16.000 --> 00:04:22.000
let's see, I'm not sure this is
going to all fit on one board,

00:04:22.000 --> 00:04:28.000
but we'll see we can do.
OK, so we can write that out as

00:04:28.000 --> 00:04:35.000
a multithreaded program.
So this, we're going to see

00:04:35.000 --> 00:04:41.000
that n is an exact power of two
for simplicity.

00:04:41.000 --> 00:04:49.000
And since we're going to have
two matrices that we have to

00:04:49.000 --> 00:04:57.000
add, we're going to basically
put one of them in our output,

00:04:57.000 --> 00:05:04.000
C; that'll be the first one,
and we're going to use a

00:05:04.000 --> 00:05:11.000
temporary matrix,
T, which is also n by n.

00:05:11.000 --> 00:05:21.000
OK, and the code looks
something like this,

00:05:21.000 --> 00:05:32.000
OK, n equals one,
and C of one gets A of 1-1

00:05:32.000 --> 00:05:41.000
times B of 1-1.
Otherwise, what we do then is

00:05:41.000 --> 00:05:49.000
we partition the matrices.
OK, so we partition them into

00:05:49.000 --> 00:05:55.000
the block.
So, how long does it take me to

00:05:55.000 --> 00:06:05.000
partition at matrix into blocks
if I'm clever at my programming?

00:06:05.000 --> 00:06:07.000
Yeah?
No time, or it actually does

00:06:07.000 --> 00:06:10.000
take a little bit of time.
Yeah, order one,

00:06:10.000 --> 00:06:13.000
basically, OK,
because all it is is just index

00:06:13.000 --> 00:06:15.000
calculations.
You have to change what the

00:06:15.000 --> 00:06:18.000
index is.
You have to pass in what you're

00:06:18.000 --> 00:06:22.000
passing these in addition to A,
B, and C for example,

00:06:22.000 --> 00:06:26.000
pass and arrange which would
have essentially a constant

00:06:26.000 --> 00:06:28.000
overhead.
But it's basically order one

00:06:28.000 --> 00:06:30.000
time.

00:06:36.000 --> 00:06:39.000
Basically order one time,
OK, to partition the matrices

00:06:39.000 --> 00:06:43.000
because all we are doing is
index calculations.

00:06:43.000 --> 00:06:46.000
And all we have to do is just
as we go through,

00:06:46.000 --> 00:06:50.000
is just make sure we keep track
of the indices,

00:06:50.000 --> 00:06:52.000
OK?
Any questions about that?

00:06:52.000 --> 00:06:55.000
People follow?
OK, that's sort of standard

00:06:55.000 --> 00:07:05.000
programming.
So then, what I do is I spawn

00:07:05.000 --> 00:07:17.000
multiplication of,
woops, the sub-matrices,

00:07:17.000 --> 00:07:22.000
and spawn --

00:07:36.000 --> 00:07:43.000
-- and continue,
C_2-1, gets A_2-1,

00:07:43.000 --> 00:07:53.000
B_1-1, two, and let's see,
2-2, yeah, it's 2-1.

00:07:53.000 --> 00:08:02.000
OK, and continuing onto the
next page.

00:08:02.000 --> 00:08:10.000
Let me just make sure I somehow
get the indentation right.

00:08:10.000 --> 00:08:18.000
This is my level of
indentation, and I'm continuing

00:08:18.000 --> 00:08:25.000
right along.
And now what I do is put the

00:08:25.000 --> 00:08:30.000
results in T,
and then --

00:08:58.000 --> 00:09:01.000
OK, so I've spawn off all these
multiplications.

00:09:01.000 --> 00:09:06.000
So that means when I spawn,
I get to, after I spawn

00:09:06.000 --> 00:09:11.000
something I can go onto the next
statement, and execute that even

00:09:11.000 --> 00:09:15.000
as this is executing.
OK, so that's our notion of

00:09:15.000 --> 00:09:20.000
multithreaded programming.
I spawn off these eight things.

00:09:20.000 --> 00:09:24.000
What do I do next?
What's the next step in this

00:09:24.000 --> 00:09:25.000
code?
Sync.

00:09:25.000 --> 00:09:28.000
Yeah.
OK, I've got to wait for them

00:09:28.000 --> 00:09:33.000
to be done before I can use
their results.

00:09:33.000 --> 00:09:38.000
OK, so I put a sync in,
say wait for all those things I

00:09:38.000 --> 00:09:42.000
spawned off to be done,
and then what?

00:09:48.000 --> 00:09:51.000
Yeah.
That you have to add T and C.

00:09:51.000 --> 00:09:55.000
So let's do that with a
subroutine call.

00:09:55.000 --> 00:10:02.000
OK, and then we are done.
We do a return at the end.

00:10:02.000 --> 00:10:08.000
OK, so let's write the code for
add, because add,

00:10:08.000 --> 00:10:14.000
we also would like to do in
parallel if we can.

00:10:14.000 --> 00:10:21.000
And what we are doing here is
doing C gets C plus T,

00:10:21.000 --> 00:10:25.000
OK?
So, we're going to add T into

00:10:25.000 --> 00:10:29.000
C.
So, we have some code here to

00:10:29.000 --> 00:10:35.000
do our base case,
and partitioning because we're

00:10:35.000 --> 00:10:43.000
going to do it divide and
conquer as before.

00:10:43.000 --> 00:10:49.000
And this one's actually a lot
easier.

00:10:49.000 --> 00:10:54.000
We just spawn,
add a C_1-1,

00:10:54.000 --> 00:11:00.000
T_1-1, n over 2,
C_1-2, T_1-2,

00:11:00.000 --> 00:11:06.000
n over 2, C_2-1,
T_2-1, n over 2,

00:11:06.000 --> 00:11:13.000
C_2-2, 2-2-2,
n over 2, and then sync,

00:11:13.000 --> 00:11:21.000
and return the result.
OK, so all we're doing here is

00:11:21.000 --> 00:11:26.000
just dividing it into four
pieces, spawning them off.

00:11:26.000 --> 00:11:30.000
That's it.
OK, wait until they're all

00:11:30.000 --> 00:11:32.000
done, then we return with the
result.

00:11:32.000 --> 00:11:36.000
OK, so any questions about how
this code works?

00:11:36.000 --> 00:11:39.000
So, remember that,
here, we're going to have a

00:11:39.000 --> 00:11:43.000
scheduler underneath which is
scheduling this onto our

00:11:43.000 --> 00:11:46.000
processors.
And we're going to have to

00:11:46.000 --> 00:11:50.000
worry about how well that
scheduler is doing.

00:11:50.000 --> 00:11:53.000
And, from last time,
we learned that there were two

00:11:53.000 --> 00:11:56.000
important measures,
OK, that can be used

00:11:56.000 --> 00:12:01.000
essentially to predict the
performance on any number of

00:12:01.000 --> 00:12:03.000
processors.
And what are those two

00:12:03.000 --> 00:12:08.000
measures?
Yeah, T_1 and T infinity so

00:12:08.000 --> 00:12:12.000
that we had some names.
T_1 is the work,

00:12:12.000 --> 00:12:16.000
good, and T infinity is
critical path length,

00:12:16.000 --> 00:12:19.000
good.
So, you have to work in the

00:12:19.000 --> 00:12:23.000
critical path length.
If we know the work in the

00:12:23.000 --> 00:12:28.000
critical path length,
we can do things like say what

00:12:28.000 --> 00:12:33.000
the parallelism is of our
program, and from that,

00:12:33.000 --> 00:12:38.000
understand how many processors
it makes sense to run this

00:12:38.000 --> 00:12:47.000
program on.
OK, so let's do that analysis.

00:12:47.000 --> 00:12:59.000
OK, so let's let M_P of n be
the p processor execution time

00:12:59.000 --> 00:13:09.000
for our mult code,
and A_P of n be the same thing

00:13:09.000 --> 00:13:19.000
for our matrix addition code.
So, the first thing we're going

00:13:19.000 --> 00:13:23.000
to analyze is work.
And, what do we hope our answer

00:13:23.000 --> 00:13:26.000
to our work is?
What we analyze work,

00:13:26.000 --> 00:13:29.000
what do we hope it's going to
be?

00:13:39.000 --> 00:13:41.000
Well, we hope it's going to be
small.

00:13:41.000 --> 00:13:45.000
I'll grant you that.
What could we benchmark it

00:13:45.000 --> 00:13:48.000
against?
Yeah, if we wrote just

00:13:48.000 --> 00:13:52.000
something that didn't used to
have any parallelism.

00:13:52.000 --> 00:13:57.000
We'd like our parallel code
when run on one processor to be

00:13:57.000 --> 00:14:02.000
just as fast as our serial code,
the normal code that we would

00:14:02.000 --> 00:14:08.000
use to write to do this problem.
That's generally the way that

00:14:08.000 --> 00:14:11.000
we would like these things to
operate, OK?

00:14:11.000 --> 00:14:16.000
So, what is that for matrix
multiplication in the naÔve way?

00:14:16.000 --> 00:14:19.000
Yeah, it's n^3.
Of course, we use Strassen's

00:14:19.000 --> 00:14:24.000
algorithm, or one of the other,
faster algorithms beat n^3.

00:14:24.000 --> 00:14:28.000
But, for this problem,
we are just going to focus on

00:14:28.000 --> 00:14:32.000
n^3.
I'm going to let you do the

00:14:32.000 --> 00:14:37.000
Strassen as an exercise.
So, let's analyze the work.

00:14:37.000 --> 00:14:43.000
OK, since we have a subroutine
for add that we are using in the

00:14:43.000 --> 00:14:47.000
multiply code,
OK, we start by analyzing the

00:14:47.000 --> 00:14:49.000
add.
So, we have A_1 of n,

00:14:49.000 --> 00:14:52.000
OK, is, well,
can somebody give me a

00:14:52.000 --> 00:14:56.000
recurrence here?
What's the recurrence for

00:14:56.000 --> 00:15:02.000
understanding the running time
of this code?

00:15:10.000 --> 00:15:16.000
OK, this is basically week two.
This is lecture one actually.

00:15:16.000 --> 00:15:22.000
This is like lecture two or,
at worst, lecture three.

00:15:22.000 --> 00:15:25.000
Well, A of 1 of n.

00:15:34.000 --> 00:15:35.000
Plus order one,
right.

00:15:35.000 --> 00:15:39.000
OK, that's right.
So, we have four problems of

00:15:39.000 --> 00:15:41.000
size n over 2 that we are
solving.

00:15:41.000 --> 00:15:45.000
OK, so to see this,
you don't even have to know

00:15:45.000 --> 00:15:49.000
that we are doing this in
parallel, because the work is

00:15:49.000 --> 00:15:54.000
basically what would happen if
it executed on a serial machine.

00:15:54.000 --> 00:15:59.000
So, we have four problems of
size n over 2 plus order one is

00:15:59.000 --> 00:16:05.000
the total work.
Any questions about how I got

00:16:05.000 --> 00:16:10.000
that recurrence?
Is that pretty straightforward?

00:16:10.000 --> 00:16:15.000
If not, let me know.
OK, and so, what's the solution

00:16:15.000 --> 00:16:19.000
to this recurrence?
Yeah, order n^2.

00:16:19.000 --> 00:16:23.000
How do we know that?
Yeah, master method,

00:16:23.000 --> 00:16:30.000
so n to the log base two of
four, right, is n^2.

00:16:30.000 --> 00:16:33.000
Compare that with order one.
This dramatically dominates.

00:16:33.000 --> 00:16:37.000
So this is the answer,
the n to the log base two of

00:16:37.000 --> 00:16:39.000
four, n^2.
OK, everybody remember that?

00:16:39.000 --> 00:16:43.000
OK, so I want people to bone up
because this is going to be

00:16:43.000 --> 00:16:47.000
recurrences, and divide and
conquer and stuff is going to be

00:16:47.000 --> 00:16:50.000
on the final,
OK, even though we haven't seen

00:16:50.000 --> 00:16:52.000
it in many moons.
OK, so that's good.

00:16:52.000 --> 00:16:56.000
That's the same as the serial.
If I had to add 2N by n

00:16:56.000 --> 00:16:59.000
matrices, how long does it take
me to do it?

00:16:59.000 --> 00:17:04.000
n^2 time.
OK, so the input is size n^2.

00:17:04.000 --> 00:17:12.000
So, you're not going to be the
size of the input if you have to

00:17:12.000 --> 00:17:16.000
look at every piece of the
input.

00:17:16.000 --> 00:17:23.000
OK, let's now do the work of
the matrix multiplication.

00:17:23.000 --> 00:17:28.000
So once again,
we want to get a recurrence

00:17:28.000 --> 00:17:36.000
here.
So, what's our recurrence here?

00:17:36.000 --> 00:17:39.000
Yeah?
Not quite.

00:17:39.000 --> 00:17:42.000
Eight, right,
good.

00:17:42.000 --> 00:17:48.000
OK, eight, M1,
n over 2, plus,

00:17:48.000 --> 00:18:00.000
yeah, there's theta n^2 for the
addition, and then there's extra

00:18:00.000 --> 00:18:11.000
theta one that we can absorb
into theta n^2.

00:18:11.000 --> 00:18:19.000
Isn't asymptotics great?
OK, it's just great.

00:18:19.000 --> 00:18:27.000
And so, what's the solution to
that one?

00:18:27.000 --> 00:18:35.000
Theta n^3, why is that?
Man, we are exercising old

00:18:35.000 --> 00:18:36.000
muscles.
Aren't we?

00:18:36.000 --> 00:18:39.000
And they're just creaking.
I can hear them.

00:18:39.000 --> 00:18:42.000
Why is that?
Yeah, master method because

00:18:42.000 --> 00:18:46.000
we're looking at,
what are we comparing?

00:18:46.000 --> 00:18:50.000
Yeah, n to the log base two of
eight, or n^3 versus n^2,

00:18:50.000 --> 00:18:55.000
this one dominates order n^3.
OK, so this is same as serial.

00:18:55.000 --> 00:18:59.000
This was the same as serial.
This was the same as serial.

00:18:59.000 --> 00:19:02.000
That's good.
OK, we know we have a program

00:19:02.000 --> 00:19:06.000
that on one processor,
will execute the same as our

00:19:06.000 --> 00:19:12.000
serial code on which it's based.
Namely, we could have done

00:19:12.000 --> 00:19:15.000
this.
If I had just got rid of all

00:19:15.000 --> 00:19:19.000
the spawns and syncs,
that would have just been a

00:19:19.000 --> 00:19:22.000
perfectly good piece of
pseudocode describing the

00:19:22.000 --> 00:19:26.000
runtime of the algorithm,
describing the serial

00:19:26.000 --> 00:19:29.000
algorithm.
And its run time ends up being

00:19:29.000 --> 00:19:33.000
exactly the same,
not too surprising.

00:19:33.000 --> 00:19:38.000
OK?
OK, so now we do the new stuff,

00:19:38.000 --> 00:19:47.000
critical path length.
OK, so here we have A infinity

00:19:47.000 --> 00:19:54.000
of n.
Ooh, OK, so we're going to add

00:19:54.000 --> 00:20:02.000
up the critical path of this
code here.

00:20:02.000 --> 00:20:07.000
Hmm, how do I figure out the
critical path on a piece of code

00:20:07.000 --> 00:20:08.000
like that?

00:20:26.000 --> 00:20:30.000
So, it's going to expand into
one of those DAG's.

00:20:30.000 --> 00:20:33.000
What's the DAG going to look
like?

00:20:33.000 --> 00:20:37.000
How do I reason?
So, it's actually easier not to

00:20:37.000 --> 00:20:42.000
think about the DAG,
but to simply think about

00:20:42.000 --> 00:20:45.000
what's going on in the code.
Yeah?

00:20:45.000 --> 00:20:49.000
Yeah, so it's basically,
since all four spawns are

00:20:49.000 --> 00:20:54.000
spawning off the same thing,
and they're operating in

00:20:54.000 --> 00:20:59.000
parallel, I could just look at
one.

00:20:59.000 --> 00:21:01.000
Or in general,
if I spawned off several

00:21:01.000 --> 00:21:06.000
things, I look at which everyone
is going to have the maximum

00:21:06.000 --> 00:21:09.000
critical path for the things
that I've spawned off.

00:21:09.000 --> 00:21:13.000
So, when we do work,
we're usually doing plus when I

00:21:13.000 --> 00:21:17.000
have multiple subroutines.
When we do critical path,

00:21:17.000 --> 00:21:20.000
I'm doing max.
It's going to be the max over

00:21:20.000 --> 00:21:23.000
the critical paths of the
subroutines that I call.

00:21:23.000 --> 00:21:26.000
OK, and here they are all
equal.

00:21:26.000 --> 00:21:30.000
So what's the recurrence that I
get?

00:21:41.000 --> 00:21:47.000
What's the recurrence I'm going
to get out of this one?

00:21:47.000 --> 00:21:51.000
Yeah, A infinity,
n over 2, plus constant,

00:21:51.000 --> 00:21:58.000
OK, because this is what the
worst is of any of those four

00:21:58.000 --> 00:22:05.000
because they're all the same.
They're all a problem looking

00:22:05.000 --> 00:22:11.000
at the critical path of
something that's half the size,

00:22:11.000 --> 00:22:15.000
for a problem that's half the
size.

00:22:15.000 --> 00:22:21.000
OK, people with me?
OK, so what's the solution to

00:22:21.000 --> 00:22:24.000
this?
Yeah, that's theta log n.

00:22:24.000 --> 00:22:29.000
That's just,
once again, master theorem,

00:22:29.000 --> 00:22:36.000
case two, because the solution
to this is n to the log base two

00:22:36.000 --> 00:22:44.000
of one, which is n to the zero.
So we have, on this side,

00:22:44.000 --> 00:22:47.000
we have one,
and here, we're comparing it

00:22:47.000 --> 00:22:50.000
with one.
They're the same,

00:22:50.000 --> 00:22:53.000
so therefore we tack on that
extra log n.

00:22:53.000 --> 00:22:58.000
OK, so tack on one log n.
OK, so case two of the master

00:22:58.000 --> 00:23:01.000
method.
Pretty good.

00:23:01.000 --> 00:23:06.000
OK, so that's pretty good,
because the critical path is

00:23:06.000 --> 00:23:11.000
pretty short,
log n compared to the work,

00:23:11.000 --> 00:23:12.000
n^2.
So, let's do,

00:23:12.000 --> 00:23:18.000
then, this one which is a
little bit more interesting,

00:23:18.000 --> 00:23:22.000
but not much harder.
How about this one?

00:23:22.000 --> 00:23:25.000
What's the recurrence going to
be?

00:23:25.000 --> 00:23:31.000
Critical path of the
multiplication?

00:23:38.000 --> 00:23:42.000
So once again,
it's going to be the maximum of

00:23:42.000 --> 00:23:46.000
everything we spawned off in
parallel, which is,

00:23:46.000 --> 00:23:50.000
by symmetry,
the same as one of them.

00:23:50.000 --> 00:23:54.000
So what do I get here?
M infinity, n over 2,

00:23:54.000 --> 00:23:58.000
plus theta log n.
Where'd the theta log n come

00:23:58.000 --> 00:24:02.000
from?
Yeah, from the addition.

00:24:02.000 --> 00:24:05.000
That's the critical path of the
addition.

00:24:05.000 --> 00:24:11.000
Now, why is it that the maximum
of that with all the spawns?

00:24:11.000 --> 00:24:16.000
You said that when you spawn
things off, you're going to do

00:24:16.000 --> 00:24:22.000
them, yeah, you sync first.
And, sync says you wait for all

00:24:22.000 --> 00:24:26.000
those to be done.
So, you're only taking the

00:24:26.000 --> 00:24:31.000
maximum, and across syncs you're
adding.

00:24:31.000 --> 00:24:34.000
So, you add across syncs,
and across things that you

00:24:34.000 --> 00:24:37.000
spawned off in parallel.
That's where you are doing the

00:24:37.000 --> 00:24:39.000
max.
OK, but if you have a sync,

00:24:39.000 --> 00:24:41.000
it says that that's the end.
You've got to wait for

00:24:41.000 --> 00:24:45.000
everything there to end.
This isn't going on in parallel

00:24:45.000 --> 00:24:47.000
with it.
This is going on after it.

00:24:47.000 --> 00:24:50.000
So, whatever the critical path
is here, OK, if I have an

00:24:50.000 --> 00:24:53.000
infinite number of processors,
I'd still have to wait up at

00:24:53.000 --> 00:24:57.000
this point, and that would
therefore make it so that the

00:24:57.000 --> 00:25:00.000
remaining execution gear was
whatever the critical,

00:25:00.000 --> 00:25:03.000
I would have to add whatever
the critical path was to this

00:25:03.000 --> 00:25:08.000
one.
Is that clear to everybody?

00:25:08.000 --> 00:25:16.000
OK, so we get this recurrence.
And, that has solution what?

00:25:16.000 --> 00:25:21.000
Yeah, theta log squared n.
OK, once again,

00:25:21.000 --> 00:25:27.000
by master method,
case two, where this ends up

00:25:27.000 --> 00:25:34.000
being a constant versus log n,
those don't differ by a

00:25:34.000 --> 00:25:41.000
polynomial amount,
or equal to a log factor.

00:25:41.000 --> 00:25:48.000
What we do in that circumstance
is tack on an extra log factor.

00:25:48.000 --> 00:25:53.000
OK, so as I say,
good idea to review the master

00:25:53.000 --> 00:25:56.000
method.
OK, that's great.

00:25:56.000 --> 00:26:04.000
So now, let's take a look at
the parallelism that we get.

00:26:04.000 --> 00:26:12.000
We'll just do it right here for
the multiplication.

00:26:12.000 --> 00:26:21.000
OK, so parallelism is what for
the multiplication?

00:26:21.000 --> 00:26:26.000
What's the formula for
parallelism?

00:26:26.000 --> 00:26:35.000
So, we have p bar is the
notation we use for this

00:26:35.000 --> 00:26:43.000
problem.
What's the parallelism going to

00:26:43.000 --> 00:26:47.000
be?
What's the ratio I take?

00:26:47.000 --> 00:26:54.000
Yeah, it's M_1 of n divided by
M infinity of n.

00:26:54.000 --> 00:27:00.000
OK, and that's equal to,
that's n^3.

00:27:00.000 --> 00:27:09.000
That's n^2, or log n^2,
sorry, log squared n.

00:27:09.000 --> 00:27:12.000
OK, so this is the parallelism.
That says you could run up to

00:27:12.000 --> 00:27:16.000
this many processors and expect
to be getting linear speed up.

00:27:16.000 --> 00:27:20.000
If I ran with more processors
than the parallelism,

00:27:20.000 --> 00:27:23.000
I don't expect to be getting
linear speed up anymore,

00:27:23.000 --> 00:27:26.000
OK, because what I expect to
run in, is just time

00:27:26.000 --> 00:27:30.000
proportional to critical path
length, and throwing more

00:27:30.000 --> 00:27:33.000
processors at the problem is not
going to help me very much,

00:27:33.000 --> 00:27:39.000
OK?
So let's just look at this just

00:27:39.000 --> 00:27:44.000
to get a sense of what's going
on here.

00:27:44.000 --> 00:27:50.000
Let's imagine that the
constants are irrelevant,

00:27:50.000 --> 00:27:55.000
and we have,
say, thousand by thousand

00:27:55.000 --> 00:27:59.000
matrices, OK,
so in that case,

00:27:59.000 --> 00:28:08.000
our parallelism is 1,000^3
divided by log of 1,000^2.

00:28:08.000 --> 00:28:12.000
What's log of 1,000?
Ten, approximately,

00:28:12.000 --> 00:28:16.000
right?
Log base two of 1,000 is about

00:28:16.000 --> 00:28:21.000
ten, so that's 10^2.
So, you have about 10^7

00:28:21.000 --> 00:28:24.000
parallelism.
How big is 10^7?

00:28:24.000 --> 00:28:30.000
Ten million processors.
OK, so who knows of a machine

00:28:30.000 --> 00:28:38.000
with ten million processors?
What's the most number of

00:28:38.000 --> 00:28:44.000
processors anybody knows about?
Yeah, not quite,

00:28:44.000 --> 00:28:52.000
the IBM Blue Jean has a
humungous number of processors,

00:28:52.000 --> 00:28:55.000
exceeding 10,000.
Yeah.

00:28:55.000 --> 00:29:03.000
Those were one bit processors.
OK, so this is actually a

00:29:03.000 --> 00:29:09.000
pretty big number,
and so, our parallelism is much

00:29:09.000 --> 00:29:15.000
bigger than a typical,
actual number of processors.

00:29:15.000 --> 00:29:22.000
So, we would expect to be able
to run this and get very good

00:29:22.000 --> 00:29:28.000
performance, OK,
because we're never going to be

00:29:28.000 --> 00:29:35.000
limited in this algorithm by
performance.

00:29:35.000 --> 00:29:38.000
However, there are some tricks
we can do.

00:29:38.000 --> 00:29:42.000
One of the things in this code
is that we actually have some

00:29:42.000 --> 00:29:47.000
overhead that's not apparent
because I haven't run this code

00:29:47.000 --> 00:29:51.000
with you, although I could,
which is that we have this

00:29:51.000 --> 00:29:54.000
temporary matrix,
T, and if you look at the

00:29:54.000 --> 00:29:58.000
execution stack,
we're always allocating T and

00:29:58.000 --> 00:30:01.000
getting rid of it,
etc.

00:30:01.000 --> 00:30:04.000
And, one of the things when you
actually look at the performance

00:30:04.000 --> 00:30:07.000
of real code,
which is now that you have your

00:30:07.000 --> 00:30:10.000
algorithmic background,
you're ready to go and do that

00:30:10.000 --> 00:30:13.000
with some insight.
Of course, you're interested in

00:30:13.000 --> 00:30:16.000
getting more than just
asymptotic behavior.

00:30:16.000 --> 00:30:19.000
You're interested in getting
real performance behavior on

00:30:19.000 --> 00:30:21.000
real things.
So, you do care about constants

00:30:21.000 --> 00:30:24.000
in that nature.
OK, and one of the things is

00:30:24.000 --> 00:30:26.000
having a large,
temporary variable.

00:30:26.000 --> 00:30:30.000
That turns out to be a lot of
overhead.

00:30:30.000 --> 00:30:33.000
And, in fact,
it's often the case when you're

00:30:33.000 --> 00:30:37.000
looking at real code that if you
can optimize for space,

00:30:37.000 --> 00:30:42.000
you also optimized for time.
If you can run your code with

00:30:42.000 --> 00:30:45.000
smaller space,
you can actually run it with

00:30:45.000 --> 00:30:49.000
smaller time,
tends to be a constant factor

00:30:49.000 --> 00:30:52.000
advantage.
But, those constants can add

00:30:52.000 --> 00:30:57.000
up, and can make a difference in
whether somebody else's code is

00:30:57.000 --> 00:31:01.000
faster or your code is faster,
once you have your basic

00:31:01.000 --> 00:31:03.000
algorithm.
So, the idea is to,

00:31:03.000 --> 00:31:07.000
in this case,
we're going to get rid of it by

00:31:07.000 --> 00:31:12.000
trading parallelism because
we've got oodles of parallelism

00:31:12.000 --> 00:31:22.000
here for space efficiency.
OK, and the idea is we're going

00:31:22.000 --> 00:31:33.000
to get rid of T.
OK, so let's throw this up.

00:31:33.000 --> 00:31:40.000
So, who can suggest how I might
get rid of T here,

00:31:40.000 --> 00:31:44.000
get rid of this temporary
matrix?

00:31:44.000 --> 00:31:46.000
Yeah?

00:31:58.000 --> 00:32:00.000
Yeah?

00:32:14.000 --> 00:32:15.000
So, if you just did adding it
into C?

00:32:15.000 --> 00:32:18.000
So, the issue that you get
there if they're both adding

00:32:18.000 --> 00:32:21.000
into C is you get interference
between the two subcomputations.

00:32:21.000 --> 00:32:24.000
Now, there are ways of doing
that that work out,

00:32:24.000 --> 00:32:27.000
but you now have to worry about
things we're not going to talk

00:32:27.000 --> 00:32:30.000
about such as mutual exclusion
to make sure that as you're

00:32:30.000 --> 00:32:33.000
updating it, somebody else isn't
updating it, and you don't have

00:32:33.000 --> 00:32:38.000
race conditions.
But you can actually do it in

00:32:38.000 --> 00:32:41.000
this context with no race
conditions.

00:32:41.000 --> 00:32:44.000
Yeah, exactly.
Exactly, OK,

00:32:44.000 --> 00:32:48.000
exactly.
So, the idea is spawn off four

00:32:48.000 --> 00:32:52.000
of them.
OK, they all update their copy

00:32:52.000 --> 00:32:58.000
of C, and then spawn off the
other four that add their values

00:32:58.000 --> 00:33:03.000
in.
So, that is a piece of code

00:33:03.000 --> 00:33:08.000
we'll call mult add.
And, it's actually going to do

00:33:08.000 --> 00:33:15.000
C gets C plus A times B.
OK, so it's actually going to

00:33:15.000 --> 00:33:19.000
add it in.
So, initially you'd have to

00:33:19.000 --> 00:33:26.000
zero out C, but we can do that
with code very similar to the

00:33:26.000 --> 00:33:33.000
addition code with order n^2
work, and order log n critical

00:33:33.000 --> 00:33:38.000
path.
So that's not going to be a big

00:33:38.000 --> 00:33:41.000
part of what we have to deal
with.

00:33:41.000 --> 00:33:45.000
OK, so here's the code.
We basically,

00:33:45.000 --> 00:33:51.000
once again, do the base and
partition which I'm not going to

00:33:51.000 --> 00:34:00.000
write out the code for.
We spawn a mult add of C1-1,

00:34:00.000 --> 00:34:09.000
A1-1, B1-1, n over 2,
and we do a few more of those

00:34:09.000 --> 00:34:14.000
down to the fourth one.

00:34:25.000 --> 00:34:32.000
And then we put in a sync.
And then we do the other four --

00:35:01.000 --> 00:35:03.000
-- and then sync when we're
done with that.

00:35:12.000 --> 00:35:14.000
OK, does everybody understand
that code?

00:35:14.000 --> 00:35:18.000
See that it basically does the
same calculation.

00:35:18.000 --> 00:35:22.000
We actually don't need to call
add anymore, because we are

00:35:22.000 --> 00:35:26.000
doing that as part of the
multiply because we're adding it

00:35:26.000 --> 00:35:28.000
in.
But we do have to initialize.

00:35:28.000 --> 00:35:33.000
OK, we do have to initialize
the matrix in this case.

00:35:33.000 --> 00:35:42.000
OK, so there is another phase.
So, people understand the

00:35:42.000 --> 00:35:50.000
semantics of this code.
So let's analyze it.

00:35:50.000 --> 00:35:58.000
OK, so what's the work of
multiply, add of n?

00:35:58.000 --> 00:36:06.000
It's basically the same thing,
right?

00:36:06.000 --> 00:36:10.000
It's order n^3 because the
serial code is almost the same

00:36:10.000 --> 00:36:14.000
as the serial code up there,
OK, not quite,

00:36:14.000 --> 00:36:19.000
OK, but you get essentially the
same recurrence except you don't

00:36:19.000 --> 00:36:23.000
even have the add.
You just get the same

00:36:23.000 --> 00:36:28.000
recurrence but with order one
here, oops, order one up here.

00:36:28.000 --> 00:36:33.000
So, it's still got the order
n^3 solution.

00:36:33.000 --> 00:36:37.000
OK, so that,
I think, is not too hard.

00:36:37.000 --> 00:36:42.000
OK, so the critical path
length, so there,

00:36:42.000 --> 00:36:45.000
let's write out,
so multiply,

00:36:45.000 --> 00:36:50.000
add, of n, OK,
what's my recurrence for this

00:36:50.000 --> 00:36:52.000
code?

00:37:02.000 --> 00:37:06.000
Yeah, 2M infinity,
M over 2 [ost that,

00:37:06.000 --> 00:37:09.000
so order one.
Plus order one,

00:37:09.000 --> 00:37:13.000
yeah.
OK, so the point is that we're

00:37:13.000 --> 00:37:17.000
going to have,
for the critical path,

00:37:17.000 --> 00:37:23.000
we're going to spawn these four
off, and so I take the maximum

00:37:23.000 --> 00:37:29.000
of whatever those is,
which since they're symmetric

00:37:29.000 --> 00:37:36.000
is any one of them,
OK, and then I have to wait.

00:37:36.000 --> 00:37:39.000
And then I do it again.
So, that sync,

00:37:39.000 --> 00:37:42.000
once again, translates into,
in the analysis,

00:37:42.000 --> 00:37:46.000
it translates into a plus of
the critical path,

00:37:46.000 --> 00:37:50.000
which are the things I spawn
off in parallel,

00:37:50.000 --> 00:37:53.000
I do the max.
OK, so people see that?

00:37:53.000 --> 00:37:58.000
So, I get this recurrence,
2MA of n over 2 plus order one,

00:37:58.000 --> 00:38:02.000
and what's the solution to
that?

00:38:02.000 --> 00:38:10.000
OK, that's order n,
OK, because n to the log base

00:38:10.000 --> 00:38:18.000
two of two is n,
and that's bigger than one so

00:38:18.000 --> 00:38:25.000
we get order n.
OK, so the parallelism,

00:38:25.000 --> 00:38:36.000
we have p bar is equal to MA
one of n over MA infinity of n

00:38:36.000 --> 00:38:47.000
is equal to, in this case,
n^3 over n, or order n^2.

00:38:47.000 --> 00:38:51.000
OK, so for 1,000 by 1,000
matrices, for example,

00:38:51.000 --> 00:38:56.000
by the way, 1,000 by 1,000 is
considered a small matrix,

00:38:56.000 --> 00:39:02.000
these days, because that's only
one million entries.

00:39:02.000 --> 00:39:07.000
You can put that on your laptop
no sweat.

00:39:07.000 --> 00:39:14.000
OK, so, but for 1,000 by 1,000
matrices, our parallelism is

00:39:14.000 --> 00:39:18.000
about 10^6.
OK, so once again,

00:39:18.000 --> 00:39:27.000
ample parallelism for anything
we would run it on today.

00:39:27.000 --> 00:39:28.000
And as it turns out,
it's faster in practice --

00:39:38.000 --> 00:39:43.000
-- because we have less space.
OK, so here's a game where,

00:39:43.000 --> 00:39:49.000
so, often the game you'll see
in theory papers if you look at

00:39:49.000 --> 00:39:53.000
research papers,
people are often striving to

00:39:53.000 --> 00:39:59.000
get the most parallelism,
and that's a good game to play,

00:39:59.000 --> 00:40:04.000
OK, but it's not necessarily
the only game.

00:40:04.000 --> 00:40:06.000
Particularly,
if you have a lot of

00:40:06.000 --> 00:40:09.000
parallelism, one of the things
that's very easy to do is to

00:40:09.000 --> 00:40:14.000
retreat on the parallelism and
gain other aspects that you may

00:40:14.000 --> 00:40:16.000
want in your code.
OK, and so this is a good

00:40:16.000 --> 00:40:19.000
example of that.
In fact, and this is an

00:40:19.000 --> 00:40:22.000
exercise, you can actually
achieve work n^3,

00:40:22.000 --> 00:40:25.000
order n^3 work,
and a critical path of log n,

00:40:25.000 --> 00:40:29.000
so even better than either of
these two algorithms in terms of

00:40:29.000 --> 00:40:33.000
parallelism.
OK, so that gives you n^3 over

00:40:33.000 --> 00:40:36.000
log n parallelism.
So, that's an exercise.

00:40:36.000 --> 00:40:40.000
And then, the other exercise
that I mention that that's good

00:40:40.000 --> 00:40:44.000
to do is parallel Strassen,
OK, doing the same thing with

00:40:44.000 --> 00:40:48.000
Strassen, and analyze,
what's the working critical

00:40:48.000 --> 00:40:51.000
path and parallelism of the
Strassen code?

00:40:51.000 --> 00:40:54.000
OK, any questions about matrix
multiplication?

00:40:54.000 --> 00:40:56.000
Yeah?

00:41:03.000 --> 00:41:07.000
Yeah, so that would take,
that would add a log n to the

00:41:07.000 --> 00:41:10.000
critical path,
which is nothing compared to

00:41:10.000 --> 00:41:12.000
the n.
Excuse me?

00:41:12.000 --> 00:41:16.000
Well, you got to make sure C is
zero to begin with.

00:41:16.000 --> 00:41:20.000
OK, so you have to set all the
entries to zero,

00:41:20.000 --> 00:41:25.000
and so that will take you n^2
work, which is nothing compared

00:41:25.000 --> 00:41:30.000
to the n^3 work you're doing
here, and it will cost you log n

00:41:30.000 --> 00:41:34.000
additional to the critical path,
which is nothing compared to

00:41:34.000 --> 00:41:39.000
the order n that you're
spending.

00:41:39.000 --> 00:41:45.000
Any other questions about
matrix multiplication?

00:41:45.000 --> 00:41:51.000
OK, as they say,
this all goes back to week two,

00:41:51.000 --> 00:41:55.000
or something,
in the class.

00:41:55.000 --> 00:42:01.000
Did you have a comment?
Yes, you can.

00:42:01.000 --> 00:42:05.000
OK, yes you can.
It's actually kind of

00:42:05.000 --> 00:42:11.000
interesting to look at that.
Actually, we'll talk later.

00:42:11.000 --> 00:42:17.000
We'll write a research paper
after the class is over,

00:42:17.000 --> 00:42:24.000
OK, because there's actually
some interesting open questions

00:42:24.000 --> 00:42:28.000
there.
OK, let's move on to something

00:42:28.000 --> 00:42:34.000
that you thought you'd gotten
rid of weeks ago,

00:42:34.000 --> 00:42:40.000
and that would be the topic of
sorting.

00:42:40.000 --> 00:42:44.000
Back to sorting.
OK, so we want to parallel sort

00:42:44.000 --> 00:42:47.000
now, OK?
Hugely important problem.

00:42:47.000 --> 00:42:52.000
So, let's take a look at,
so if I think about algorithms

00:42:52.000 --> 00:42:57.000
for sorting that sound easy to
parallelize, which ones sound

00:42:57.000 --> 00:43:01.000
kind of easy to parallelize?
Quick sort, yeah,

00:43:01.000 --> 00:43:05.000
that's a good one.
Yeah, quick sort is a pretty

00:43:05.000 --> 00:43:08.000
good one to parallelize and
analyze.

00:43:08.000 --> 00:43:10.000
But remember,
quick sort has a little bit

00:43:10.000 --> 00:43:13.000
more complicated analysis than
some other sorts.

00:43:13.000 --> 00:43:17.000
What's another one that looks
like it should be pretty easy to

00:43:17.000 --> 00:43:19.000
parallelize?
Merge sort.

00:43:19.000 --> 00:43:21.000
When did we teach merge sort?
Day one.

00:43:21.000 --> 00:43:25.000
OK, so do merge sort because
it's just a little bit easier to

00:43:25.000 --> 00:43:27.000
analyze.
OK, we could do the same thing

00:43:27.000 --> 00:43:32.000
for quick sort.
Here's merge sort,

00:43:32.000 --> 00:43:38.000
OK, and it's going to sort A of
p to r.

00:43:38.000 --> 00:43:47.000
So, if p is less than r,
then we get the middle element,

00:43:47.000 --> 00:43:55.000
and then we'll spawn off since
we have to, as you recall,

00:43:55.000 --> 00:44:03.000
when you merge sort you first
recursively sort the two

00:44:03.000 --> 00:44:11.000
sub-arrays.
There's no reason not to do

00:44:11.000 --> 00:44:18.000
those parallel.
Let's just do them in parallel.

00:44:18.000 --> 00:44:23.000
Let's spawn off,
merge sort of A,

00:44:23.000 --> 00:44:30.000
p, q, and spawn off,
then, merge sort of A,

00:44:30.000 --> 00:44:38.000
q plus one r.
And then, we wait for them to

00:44:38.000 --> 00:44:42.000
be done.
Don't forget your syncs.

00:44:42.000 --> 00:44:48.000
Sync or swim.
OK, and then what to do what we

00:44:48.000 --> 00:44:54.000
are done with this?
OK, we merge.

00:44:54.000 --> 00:45:03.000
OK, so we merge of A,
p, q, r, which is merge A of p

00:45:03.000 --> 00:45:10.000
up to q with A of q plus one up
to r.

00:45:10.000 --> 00:45:16.000
And, once we've merged,
we're done.

00:45:16.000 --> 00:45:27.000
OK, so this is the same code as
we saw before in day one except

00:45:27.000 --> 00:45:37.000
we've got a couple of spawns in
the sync.

00:45:37.000 --> 00:45:52.000
So let's analyze this.
So, the work is called T_1 of

00:45:52.000 --> 00:46:03.000
n, what's the recurrence for
this?

00:46:03.000 --> 00:46:07.000
This really is going back to
day one, right?

00:46:07.000 --> 00:46:11.000
We actually did this on day
one.

00:46:11.000 --> 00:46:17.000
OK, so what's the recurrence?
2T1 of n over 2 plus order n

00:46:17.000 --> 00:46:21.000
merges order n time operation,
OK?

00:46:21.000 --> 00:46:25.000
And so, that gives us a
solution of n log n,

00:46:25.000 --> 00:46:32.000
OK, even if you didn't know the
solution, you should know the

00:46:32.000 --> 00:46:37.000
answer, OK, which is the same as
the serial code,

00:46:37.000 --> 00:46:45.000
not surprisingly.
That's what we want.

00:46:45.000 --> 00:46:57.000
OK, critical path length,
AT infinity of n is equal to,

00:46:57.000 --> 00:47:08.000
OK, T infinity of n over 2 plus
order n again.

00:47:08.000 --> 00:47:15.000
And that's equal to order n,
OK?

00:47:15.000 --> 00:47:29.000
So, the parallelism is then p
bar equals T_1 of n over T

00:47:29.000 --> 00:47:40.000
infinity of n is equal to theta
of log n.

00:47:40.000 --> 00:47:45.000
Is that a lot of parallelism?
No, we have a technical name

00:47:45.000 --> 00:47:47.000
for that.
We call it puny.

00:47:47.000 --> 00:47:50.000
OK, that's puny parallelism.
Log n?

00:47:50.000 --> 00:47:55.000
Now, so this is actually
probably a decent algorithm for

00:47:55.000 --> 00:48:00.000
some of the small scale
processors, especially the

00:48:00.000 --> 00:48:04.000
multicore processors that are
coming on the market,

00:48:04.000 --> 00:48:09.000
and some of the smaller SMP,
symmetric multiprocessors,

00:48:09.000 --> 00:48:15.000
that are available.
You know, they have four or

00:48:15.000 --> 00:48:19.000
eight processors or something.
It might be OK.

00:48:19.000 --> 00:48:22.000
There's not a lot of
parallelism.

00:48:22.000 --> 00:48:25.000
For a million elements,
log n is about 20.

00:48:25.000 --> 00:48:29.000
OK, and so and then there's
constant overheads,

00:48:29.000 --> 00:48:34.000
etc.
This is not very much

00:48:34.000 --> 00:48:39.000
parallelism at all.
Question?

00:48:39.000 --> 00:48:46.000
Yeah, so how can we do better?
I mean, it's like,

00:48:46.000 --> 00:48:53.000
man, at merge,
right, it takes order n.

00:48:53.000 --> 00:48:59.000
if I want to do better,
what should I do?

00:48:59.000 --> 00:49:03.000
Yeah?
Sort in-place,

00:49:03.000 --> 00:49:08.000
but for example if you do quick
sort and partition,

00:49:08.000 --> 00:49:12.000
you still have a linear time
partition.

00:49:12.000 --> 00:49:17.000
So you're going to be very much
in the same situation.

00:49:17.000 --> 00:49:21.000
But what can I do here?
Parallel merge.

00:49:21.000 --> 00:49:24.000
OK, let's make merge go in
parallel.

00:49:24.000 --> 00:49:28.000
That's where all the critical
path is.

00:49:28.000 --> 00:49:33.000
Let's figure out a way of
building a merge program that

00:49:33.000 --> 00:49:41.000
has a very short critical path.
You have to parallelize the

00:49:41.000 --> 00:49:43.000
merge.
This is great.

00:49:43.000 --> 00:49:50.000
It's so nice to see at the end
of a course like this that

00:49:50.000 --> 00:49:57.000
people have the intuition that,
oh, you can look at it and sort

00:49:57.000 --> 00:50:03.000
of see, where should you put in
your work?

00:50:03.000 --> 00:50:05.000
OK, the one thing about
algorithms is it doesn't stop

00:50:05.000 --> 00:50:09.000
you from having to engineer a
program when you code it.

00:50:09.000 --> 00:50:12.000
There's a lot more to coding a
program well than just having

00:50:12.000 --> 00:50:15.000
the algorithm as we talked
about, also, in day one.

00:50:15.000 --> 00:50:18.000
There's things like making it
modular, and making it

00:50:18.000 --> 00:50:20.000
maintainable,
and a whole bunch of things

00:50:20.000 --> 00:50:22.000
like that.
But one of the things that

00:50:22.000 --> 00:50:25.000
algorithms does is it tells you,
where should you focus your

00:50:25.000 --> 00:50:28.000
work?
OK, there's no point in,

00:50:28.000 --> 00:50:30.000
for example,
sort of saying,

00:50:30.000 --> 00:50:34.000
OK, let me spawn off four of
these things of size n over 4 in

00:50:34.000 --> 00:50:37.000
hopes of getting,
I mean, it's like,

00:50:37.000 --> 00:50:39.000
that's not where you put the
work.

00:50:39.000 --> 00:50:42.000
You put the work in merge
because that's the one that's

00:50:42.000 --> 00:50:44.000
the bottleneck,
OK?

00:50:44.000 --> 00:50:47.000
And, that's the nice thing
about algorithms is it very

00:50:47.000 --> 00:50:51.000
quickly lets you hone in on
where you should put your

00:50:51.000 --> 00:50:54.000
effort, OK, when you're doing
algorithmic design in

00:50:54.000 --> 00:50:58.000
engineering practice.
So you must parallelize the

00:50:58.000 --> 00:51:00.000
merge.

00:51:09.000 --> 00:51:12.000
The merge we're taking,
so here's the basic idea we're

00:51:12.000 --> 00:51:14.000
going to use.
So, in general,

00:51:14.000 --> 00:51:17.000
when we merge,
when we do our recursive merge,

00:51:17.000 --> 00:51:21.000
we're going to have two arrays.
Let's call them A and B.

00:51:21.000 --> 00:51:25.000
I called them A there.
I probably shouldn't have used

00:51:25.000 --> 00:51:27.000
A.
I probably should have called

00:51:27.000 --> 00:51:30.000
them something else,
but that's what my notes have,

00:51:30.000 --> 00:51:36.000
so we're going to stick to it.
But we get a little bit more

00:51:36.000 --> 00:51:39.000
space there and see what's going
on.

00:51:39.000 --> 00:51:43.000
We have two arrays.
I'll call them A and B,

00:51:43.000 --> 00:51:46.000
OK?
And, what we're going to do,

00:51:46.000 --> 00:51:49.000
these are going to be already
sorted.

00:51:49.000 --> 00:51:54.000
And our job is going to be to
merge them together.

00:51:54.000 --> 00:52:00.000
So, what I'll do is I'll take
the middle element of A.

00:52:00.000 --> 00:52:05.000
So this, let's say,
goes from one to l,

00:52:05.000 --> 00:52:11.000
and this goes from one to m.
OK, I'll take the middle

00:52:11.000 --> 00:52:20.000
element, the element at l over
2, say, and what I'll do is use

00:52:20.000 --> 00:52:27.000
binary search to figure out,
where does it go in the array

00:52:27.000 --> 00:52:31.000
B?
Where does this element go?

00:52:31.000 --> 00:52:36.000
It goes to some point here
where we have j here and j plus

00:52:36.000 --> 00:52:38.000
one here.
So, we know,

00:52:38.000 --> 00:52:43.000
since this is sorted,
that all these things are less

00:52:43.000 --> 00:52:48.000
than or equal to A of l over 2,
and all these things are

00:52:48.000 --> 00:52:52.000
greater than or equal to A of l
over 2.

00:52:52.000 --> 00:52:56.000
And similarly,
since that element falls here,

00:52:56.000 --> 00:53:02.000
all these are less than or
equal to A of l over 2.

00:53:02.000 --> 00:53:09.000
And all these are going to be
less greater than or equal to

00:53:09.000 --> 00:53:13.000
two.
OK, and so now what I can do is

00:53:13.000 --> 00:53:20.000
once I figured out where this
goes, I can recursively merge

00:53:20.000 --> 00:53:27.000
this array with this one,
and this one with this one,

00:53:27.000 --> 00:53:33.000
and then if I can just
concatenate them altogether,

00:53:33.000 --> 00:53:41.000
I've got my merged array.
OK, so let's write that code.

00:53:41.000 --> 00:53:47.000
Everybody get the gist of
what's going on there,

00:53:47.000 --> 00:53:52.000
how we're going to parallelize
the merge?

00:53:52.000 --> 00:53:58.000
Of course, you can see,
it's going to get a little

00:53:58.000 --> 00:54:02.000
messy because j could be
anywhere.

00:54:02.000 --> 00:54:06.000
Secures my code,
parallel merge of,

00:54:06.000 --> 00:54:13.000
and we're going to put it in C
of one to n, so I'm going to

00:54:13.000 --> 00:54:21.000
have n elements.
So, this is doing merge A and B

00:54:21.000 --> 00:54:27.000
into C, and n is equal to l plus
n.

00:54:27.000 --> 00:54:36.000
OK, so we're going to take two
arrays and merge it into the

00:54:36.000 --> 00:54:42.000
third array, OK?
So, without loss of generality,

00:54:42.000 --> 00:54:45.000
I'm going to say,
let's see, without loss of

00:54:45.000 --> 00:54:49.000
generality, I'm going to say l
is bigger than m as I show here

00:54:49.000 --> 00:54:52.000
because if it's not,
what do I do?

00:54:52.000 --> 00:54:55.000
Just do it the other way
around, right?

00:54:55.000 --> 00:54:57.000
So, I figure out which one was
bigger.

00:54:57.000 --> 00:55:01.000
So that only cost me order one
to test that,

00:55:01.000 --> 00:55:04.000
or whatever.
And then, I basically do a base

00:55:04.000 --> 00:55:07.000
case, you know,
if the two arrays are empty or

00:55:07.000 --> 00:55:10.000
whatever, what you do in
practice, of course,

00:55:10.000 --> 00:55:14.000
is if they're small enough,
you just do a serial merge,

00:55:14.000 --> 00:55:17.000
OK, if they're small enough,
and I don't really expect to

00:55:17.000 --> 00:55:20.000
get much parallelism.
There isn't much work there.

00:55:20.000 --> 00:55:23.000
You might as well just do
serial merge,

00:55:23.000 --> 00:55:25.000
and be a little bit more
efficient, OK?

00:55:25.000 --> 00:55:32.000
So, do the base case.
So then, what I do is I find

00:55:32.000 --> 00:55:41.000
the j such that B of j is less
than or equal to A of l over 2,

00:55:41.000 --> 00:55:50.000
less than or equal to B of j
plus one, using binary search.

00:55:50.000 --> 00:55:59.000
What did recover binary search?
Oh yeah, that was week one,

00:55:59.000 --> 00:56:07.000
right?
That was first recitation or

00:56:07.000 --> 00:56:13.000
something.
Yeah, it's amazing.

00:56:13.000 --> 00:56:20.000
OK, and now,
what we do is we spawn off p

00:56:20.000 --> 00:56:29.000
merge of A of one,
l over 2, B of one to j,

00:56:29.000 --> 00:56:40.000
and stick it into C of one,
two, l over 2 plus j.

00:56:40.000 --> 00:56:53.000
OK, and similarly now,
we can spawn off a merge of A

00:56:53.000 --> 00:57:07.000
of l over 2 plus one up to l.
B of j plus one up to M,

00:57:07.000 --> 00:57:19.000
and a C of l over two plus j
plus one up to n.

00:57:19.000 --> 00:57:24.000
And then, I sync.

00:57:32.000 --> 00:57:35.000
So, code is pretty
straightforward,

00:57:35.000 --> 00:57:42.000
doing exactly what I said we
were going to do over here,

00:57:42.000 --> 00:57:47.000
analysis, a little messier,
a little messier.

00:57:47.000 --> 00:57:53.000
So, let's just try to
understand us before we do the

00:57:53.000 --> 00:57:57.000
analysis.
Why is it that I want to pick

00:57:57.000 --> 00:58:05.000
the middle of the big array
rather than the small array?

00:58:05.000 --> 00:58:13.000
What sort of my rationale
there?

00:58:13.000 --> 00:58:27.000
That's actually a key part,
going to be a key part of the

00:58:27.000 --> 00:58:31.000
analysis.
Yeah?

00:58:31.000 --> 00:58:36.000
OK.
Yeah, imagine that B,

00:58:36.000 --> 00:58:40.000
for example,
had only one element in it,

00:58:40.000 --> 00:58:46.000
OK, or just a few elements,
then finding it in A might mean

00:58:46.000 --> 00:58:50.000
finding it right near the
beginning of A.

00:58:50.000 --> 00:58:56.000
And now, I'd be left with
subproblems that were very big,

00:58:56.000 --> 00:59:00.000
whereas here,
as you're pointing out,

00:59:00.000 --> 00:59:04.000
if I start here,
if my total number of elements

00:59:04.000 --> 00:59:11.000
is n, what's the smallest that
one of these recursions could

00:59:11.000 --> 00:59:15.000
be?
n over 4 is the smallest it

00:59:15.000 --> 00:59:18.000
could be, OK,
because I would have at least a

00:59:18.000 --> 00:59:23.000
quarter of the total number of
elements to the left here or to

00:59:23.000 --> 00:59:26.000
the right here.
If I do it the other way

00:59:26.000 --> 00:59:30.000
around, my recursion,
I might get a recursion that

00:59:30.000 --> 00:59:33.000
was nearly as big as n,
and that's sort of,

00:59:33.000 --> 00:59:37.000
once again, sort of like the
difference when we were

00:59:37.000 --> 00:59:41.000
analyzing quick sort with
whether we got a good

00:59:41.000 --> 00:59:46.000
partitioning element or not.
The partitioning element is

00:59:46.000 --> 00:59:49.000
somewhere in the middle,
we're really good,

00:59:49.000 --> 00:59:52.000
but it's always at one end,
it's no better than insertion

00:59:52.000 --> 00:59:54.000
sort.
You want to cut off at least a

00:59:54.000 --> 00:59:57.000
constant fraction in your
divided and conquered in order

00:59:57.000 --> 01:00:02.326
to get the logarithmic behavior.
OK, so we'll see that in the

01:00:02.326 --> 01:00:05.566
analysis.
But the key thing here is that

01:00:05.566 --> 01:00:10.302
what we are going to do the
recursion, we're going to have

01:00:10.302 --> 01:00:15.204
at least n over 4 elements in
whatever the smaller thing is.

01:00:15.204 --> 01:00:19.192
OK, but let's start.
It turns out the work is the

01:00:19.192 --> 01:00:23.181
hard part of this.
Let's start with critical path

01:00:23.181 --> 01:00:25.175
length.
OK, look at that,

01:00:25.175 --> 01:00:32.045
critical path length.
OK, so parallel merge,

01:00:32.045 --> 01:00:42.712
so infinity of n is going to
be, at most, so if the smaller

01:00:42.712 --> 01:00:53.379
piece has at least a quarter,
what's the larger piece going

01:00:53.379 --> 01:01:03.588
to be of these two things here?
So, we have two problems

01:01:03.588 --> 01:01:10.166
responding off.
Now, we really have to do max

01:01:10.166 --> 01:01:19.136
because they're not symmetric.
Which one's going to be worse?

01:01:19.136 --> 01:01:24.966
One could have,
at most, three quarters,

01:01:24.966 --> 01:01:30.647
OK, of n.
Woops, 3n, of 3n over 4 plus,

01:01:30.647 --> 01:01:39.767
OK, so the worst of those two
is going to be three quarters of

01:01:39.767 --> 01:01:45.000
the elements plus,
what?

01:01:45.000 --> 01:01:52.000
Plus log n.
What's the log n?

01:01:52.000 --> 01:02:02.250
The binary search.
OK, and that gives me a

01:02:02.250 --> 01:02:15.000
solution of, this ends up being
n to the, what?

01:02:15.000 --> 01:02:16.845
n to the zero,
right.

01:02:16.845 --> 01:02:20.996
OK, it's n to the log base four
thirds of one.

01:02:20.996 --> 01:02:25.147
OK, it was the log of anything
of one is zero.

01:02:25.147 --> 01:02:29.760
So, it's n to the zero.
So that's just one compared

01:02:29.760 --> 01:02:33.265
with log n, tack on this log
squared n.

01:02:33.265 --> 01:02:37.324
So, we have a critical path of
log squared n.

01:02:37.324 --> 01:02:44.090
That's good news.
Now, let's hope that we didn't

01:02:44.090 --> 01:02:49.545
blow up the work by a
substantial amount.

01:02:49.545 --> 01:02:55.545
OK, so the work is PM_1 of n is
equal to, OK,

01:02:55.545 --> 01:03:01.000
so we don't know what the split
is.

01:03:01.000 --> 01:03:07.529
So let's call it alpha.
OK, so alpha n in one side,

01:03:07.529 --> 01:03:15.235
and then the work on the other
side will be PM of one of one

01:03:15.235 --> 01:03:21.503
minus alpha n plus,
and then still order of log n

01:03:21.503 --> 01:03:26.858
to the binary search where,
as we've said,

01:03:26.858 --> 01:03:36.000
alpha is going to fall between
one quarter and three quarters.

01:03:46.000 --> 01:03:51.090
OK, how do we solve a
recurrence like this?

01:03:51.090 --> 01:03:57.515
What's the technical name for
this kind of recurrence?

01:03:57.515 --> 01:04:01.151
Hairy.
It's a hairy recurrence.

01:04:01.151 --> 01:04:06.000
How do we solve hairy
recurrences?

01:04:06.000 --> 01:04:09.318
Substitution.
OK, good.

01:04:09.318 --> 01:04:15.502
Substitution.
OK, so we're going to say PM

01:04:15.502 --> 01:04:24.402
one of k is less than or equal
to, OK, I want to make a good

01:04:24.402 --> 01:04:31.340
guess here, OK,
because I've fooled around with

01:04:31.340 --> 01:04:34.493
it.
I want it to be linear,

01:04:34.493 --> 01:04:37.870
so it's going to have a linear
term, a times k minus,

01:04:37.870 --> 01:04:39.948
and then I'm going to do b log
k.

01:04:39.948 --> 01:04:43.454
So, this is this trick of
subtracting a low order term.

01:04:43.454 --> 01:04:47.220
Remember that in substitution
in order to make it stronger?

01:04:47.220 --> 01:04:51.311
If I just did ak it's not going
to work because here I would get

01:04:51.311 --> 01:04:55.077
n, and then when I did this
substitution I'm going to get a

01:04:55.077 --> 01:04:58.974
alpha n, and then a one minus
alpha n, and those two together

01:04:58.974 --> 01:05:03.000
are already going to add up to
everything here.

01:05:03.000 --> 01:05:08.411
So, there's no way I'm going to
get it down when I add this term

01:05:08.411 --> 01:05:10.558
in.
So, I need to subtract

01:05:10.558 --> 01:05:15.196
something from both of these so
as to absorb this term,

01:05:15.196 --> 01:05:17.773
OK?
So, I'm skipping over those

01:05:17.773 --> 01:05:22.411
steps, OK, because we did those
steps in lecture two or

01:05:22.411 --> 01:05:25.588
something.
OK, so that's the thing I'm

01:05:25.588 --> 01:05:31.000
going to guess where a and b are
greater than zero.

01:05:31.000 --> 01:05:34.000
OK, so let's do the
substitution.

01:05:46.000 --> 01:05:52.000
OK, so we have PM one of n is
less than or equal to,

01:05:52.000 --> 01:05:57.764
OK, we substitute this
inductive hypothesis in for

01:05:57.764 --> 01:06:02.234
these two guys.
So, we get a alpha n minus b

01:06:02.234 --> 01:06:07.023
log of alpha n plus a of one
minus alpha n minus b log of one

01:06:07.023 --> 01:06:10.535
minus alpha, maybe another
parentheses there,

01:06:10.535 --> 01:06:14.206
one minus alpha n,
and even leave myself enough

01:06:14.206 --> 01:06:19.154
space here plus a of one minus
alpha n minus b log of one minus

01:06:19.154 --> 01:06:23.704
alpha, maybe another parenthesis
there, one minus alpha n.

01:06:23.704 --> 01:06:27.215
I didn't even leave myself
enough space here.

01:06:27.215 --> 01:06:31.924
Plus, let me just move this
over so I don't end up using too

01:06:31.924 --> 01:06:39.704
much space.
So, b log of one minus alpha n

01:06:39.704 --> 01:06:45.598
plus theta of log n.
How's that?

01:06:45.598 --> 01:06:52.443
Are we OK on that?
OK, so that's just

01:06:52.443 --> 01:07:01.000
substitution.
Let's do a little algebra.

01:07:01.000 --> 01:07:07.977
That's equal to a of times
alpha na times one minus alpha

01:07:07.977 --> 01:07:10.095
n.
That's just an,

01:07:10.095 --> 01:07:15.578
OK, minus, well,
the b isn't quite so simple.

01:07:15.578 --> 01:07:22.057
OK, so I have a b term.
Now I've got a whole bunch of

01:07:22.057 --> 01:07:26.543
stuff there.
I've got log of alpha n.

01:07:26.543 --> 01:07:31.900
I have, then,
this log of one minus alpha n,

01:07:31.900 --> 01:07:40.000
OK, I'll start with the n,
and then plus theta log n.

01:07:40.000 --> 01:07:45.947
Did I do that right?
Does that look OK?

01:07:45.947 --> 01:07:53.773
OK, so look at that.
OK, so now let's just multiply

01:07:53.773 --> 01:08:01.943
some of this stuff out.
So, I have an minus b times,

01:08:01.943 --> 01:08:08.845
well, log of alpha n is just
log alpha plus log n.

01:08:08.845 --> 01:08:16.450
And then I have plus log of one
minus alpha plus log n,

01:08:16.450 --> 01:08:22.929
OK, plus theta log n.
That's just more algebra,

01:08:22.929 --> 01:08:32.035
OK, using our rules for logs.
Now let me express this as my

01:08:32.035 --> 01:08:39.131
solution minus my desired
solution minus a residual,

01:08:39.131 --> 01:08:43.446
an minus b log n,
OK, minus, OK,

01:08:43.446 --> 01:08:49.707
and so that was one of these b
log n's, right,

01:08:49.707 --> 01:08:54.718
is here.
And the other one's going to

01:08:54.718 --> 01:09:00.841
end up in here.
I have B times log n plus log

01:09:00.841 --> 01:09:11.000
of alpha times one minus alpha
minus, oops, I've got too many.

01:09:11.000 --> 01:09:17.716
Do I have the right number of
closes.

01:09:17.716 --> 01:09:24.246
Close that, close that,
that's good,

01:09:24.246 --> 01:09:29.470
minus theta log n.
Two there.

01:09:29.470 --> 01:09:38.895
Boy, my writing is degrading.
OK, did I do that right?

01:09:38.895 --> 01:09:42.636
Do I have the parentheses
right?

01:09:42.636 --> 01:09:45.774
That matches,
that matches,

01:09:45.774 --> 01:09:47.948
that matches,
good.

01:09:47.948 --> 01:09:51.931
And then B goes to that,
OK, good.

01:09:51.931 --> 01:09:59.051
OK, and I claim that is less
than or equal to AN minus B log

01:09:59.051 --> 01:10:09.860
n if we choose B large enough.
OK, this mess dominates this

01:10:09.860 --> 01:10:22.837
because this is basically a log
n here, and this is essentially

01:10:22.837 --> 01:10:29.952
a constant.
OK, so if I increase B,

01:10:29.952 --> 01:10:40.000
OK, times log n,
I can overcome that log n,

01:10:40.000 --> 01:10:47.587
whatever the constant is,
hidden by the asymptotic

01:10:47.587 --> 01:10:53.934
notation, OK,
such that B times log n plus

01:10:53.934 --> 01:11:04.000
log of alpha times one minus
alpha dominates the theta log n.

01:11:04.000 --> 01:11:13.961
OK, and I can also choose my
base condition to be big enough

01:11:13.961 --> 01:11:22.740
to handle the initial
conditions, whatever they might

01:11:22.740 --> 01:11:27.467
be.
OK, so we'll choose A big

01:11:27.467 --> 01:11:30.000
enough --

01:11:48.000 --> 01:11:55.172
-- to satisfy the base of the
induction.

01:11:55.172 --> 01:12:04.000
OK, so thus PM_1 of n is equal
to theta n, OK?

01:12:04.000 --> 01:12:07.384
So I actually showed O,
and it turns out,

01:12:07.384 --> 01:12:12.207
the lower bound that it is
omega n is more straightforward

01:12:12.207 --> 01:12:17.030
because the recurrence is easier
because I can do the same

01:12:17.030 --> 01:12:20.584
substitution.
I just don't have to subtract

01:12:20.584 --> 01:12:24.561
off low order terms.
OK, so it's actually theta,

01:12:24.561 --> 01:12:27.776
not just O.
OK, so that gives us a log,

01:12:27.776 --> 01:12:30.907
what did we say the critical
path was?

01:12:30.907 --> 01:12:35.138
The critical path is log
squared n for the parallel

01:12:35.138 --> 01:12:40.787
merge.
So, let's do the analysis of

01:12:40.787 --> 01:12:45.927
merge sort using this.
So, the work is,

01:12:45.927 --> 01:12:52.285
as we know already,
T_1 of n is theta of n log n

01:12:52.285 --> 01:12:59.048
because our work that we just
analyzed was order n,

01:12:59.048 --> 01:13:05.000
same as for the serial
algorithm, OK?

01:13:05.000 --> 01:13:10.481
The critical path length,
now, is T infinity of n is

01:13:10.481 --> 01:13:14.457
equal to, OK,
so in normal merge sort,

01:13:14.457 --> 01:13:20.261
we have a problem of half the
size, T of n over 2 plus,

01:13:20.261 --> 01:13:26.387
now, my critical path for
merging is not order n as it was

01:13:26.387 --> 01:13:37.428
before.
Instead, it's just over there.

01:13:37.428 --> 01:13:45.600
Log squared n,
there we go.

01:13:45.600 --> 01:14:01.000
OK, and so that gives us theta
of log cubed n.

01:14:01.000 --> 01:14:10.312
So, our parallelism is then
theta of n over log cubed n.

01:14:10.312 --> 01:14:17.423
And, in fact,
the best that's been done is,

01:14:17.423 --> 01:14:23.179
sorry, log squared n,
you're right.

01:14:23.179 --> 01:14:33.000
Log squared n because it's n
log n over log cubed n.

01:14:33.000 --> 01:14:37.247
It's n over log squared n,
OK?

01:14:37.247 --> 01:14:43.106
And the best,
so now I wonder if I have a

01:14:43.106 --> 01:14:48.085
typo here.
I have that the best is,

01:14:48.085 --> 01:14:54.676
p bar is theta of n over log n.
Is that right?

01:14:54.676 --> 01:15:02.000
I think so.
Yeah, that's the best to date.

01:15:02.000 --> 01:15:05.576
That's the best to date.
By Occoli, I believe,

01:15:05.576 --> 01:15:09.153
is who did this.
OK, so you can actually get a

01:15:09.153 --> 01:15:13.446
fairly good, but it turns out
sorting is a really tough

01:15:13.446 --> 01:15:18.215
problem to parallelize to get
really good constants where you

01:15:18.215 --> 01:15:22.030
want to make it so it's running
exactly the same.

01:15:22.030 --> 01:15:26.243
Matrix multiplication,
you can make it run in parallel

01:15:26.243 --> 01:15:30.058
and get straight,
hard rail, linear speed up with

01:15:30.058 --> 01:15:35.356
a number of processors.
There is plenty of parallelism,

01:15:35.356 --> 01:15:39.994
and running on more processors,
every processor carries a full

01:15:39.994 --> 01:15:41.514
weight.
With sorting,

01:15:41.514 --> 01:15:43.947
typically you lose,
I don't know,

01:15:43.947 --> 01:15:47.596
20% in my experience,
OK, in terms of other stuff

01:15:47.596 --> 01:15:51.777
going on because you have to
work really hard to get the

01:15:51.777 --> 01:15:55.883
constants of this merge
algorithm down to the constants

01:15:55.883 --> 01:15:59.000
of that normal merge,
right?

01:15:59.000 --> 01:16:02.337
I mean that's a pretty good
algorithm, right,

01:16:02.337 --> 01:16:05.143
the one that just goes,
BUZZING SOUND,

01:16:05.143 --> 01:16:08.934
and just takes two lists and
merges them like that.

01:16:08.934 --> 01:16:13.410
So, it's an interesting issue.
And a lot of people work very

01:16:13.410 --> 01:16:16.975
hard on sorting,
because it's a hugely important

01:16:16.975 --> 01:16:21.601
problem, and how it is that you
can actually get the constants

01:16:21.601 --> 01:16:25.924
down while still guaranteeing
that it will scale up with a

01:16:25.924 --> 01:16:29.716
number of processors.
OK, that's our little sojourn

01:16:29.716 --> 01:16:33.281
into parallel land,
and next week we're going to

01:16:33.281 --> 01:16:37.073
talk about caching,
which is another very important

01:16:37.073 --> 01:16:40.000
area of algorithms,
and of programming in general.