WEBVTT

00:00:17.240 --> 00:00:19.490
PROFESSOR: We've been spending
quite a few lectures so

00:00:19.490 --> 00:00:22.460
far discussing Szemeredi's
regularity lemma,

00:00:22.460 --> 00:00:26.570
it's applications in variants
of the regularity Lemma.

00:00:26.570 --> 00:00:28.920
So I want to spend one more
lecture, before moving on

00:00:28.920 --> 00:00:31.640
to a different
topic, to tell you

00:00:31.640 --> 00:00:34.700
about other extensions
and other perspectives

00:00:34.700 --> 00:00:37.170
on Szemeredi's regularity lemma.

00:00:37.170 --> 00:00:41.000
So hopefully, you
all will become

00:00:41.000 --> 00:00:46.340
experts of the regularity
lemma, especially

00:00:46.340 --> 00:00:48.260
the next homework
problem set where

00:00:48.260 --> 00:00:50.480
there will be plenty of
problems for you to practice

00:00:50.480 --> 00:00:51.980
using the regularity lemma.

00:00:56.898 --> 00:00:59.190
So one of the things that I
would like to discuss today

00:00:59.190 --> 00:01:03.270
is a hypergraph extension of
the triangle removal lemma.

00:01:17.040 --> 00:01:19.150
So as we saw, the
triangle removal lemma

00:01:19.150 --> 00:01:23.500
was one of the important
application of Szemeredi's

00:01:23.500 --> 00:01:25.730
graph regularity lemma.

00:01:25.730 --> 00:01:26.230
OK.

00:01:26.230 --> 00:01:30.640
So that works inside graphs,
but now let's go to hypergraphs.

00:01:30.640 --> 00:01:34.270
In particular, even in
the case of three uniform

00:01:34.270 --> 00:01:37.120
hypergraphs where--

00:01:37.120 --> 00:01:39.700
so let's set some terminology.

00:01:39.700 --> 00:01:54.580
So when r uniform hypergraph, or
simply abbreviated as r graph,

00:01:54.580 --> 00:01:58.650
consists of a vertex
set and an edge

00:01:58.650 --> 00:02:09.082
set where the edge set
consists of r couples.

00:02:09.082 --> 00:02:22.930
So the edges are r element
subsets of the vertex set.

00:02:22.930 --> 00:02:26.656
So r equals the 2 corresponds
to the graph case.

00:02:26.656 --> 00:02:29.510
And you can talk about
sub-graphs or density,

00:02:29.510 --> 00:02:34.740
various counts all analogously
to how we did it for graphs.

00:02:34.740 --> 00:02:37.080
And you can imagine what
the hypergraph removal

00:02:37.080 --> 00:02:38.160
lemma might look like.

00:02:38.160 --> 00:02:40.220
So let me write
down a statement.

00:02:40.220 --> 00:02:50.560
That for all r graph h and
epsilon bigger than zero,

00:02:50.560 --> 00:02:55.010
there exists a delta such that--

00:02:57.770 --> 00:03:00.578
so the last time, there was
some complaints about sentences

00:03:00.578 --> 00:03:02.620
going on too long, so let
me try to cut sentences

00:03:02.620 --> 00:03:04.520
into smaller parts.

00:03:04.520 --> 00:03:21.440
So if g is an end vertex r graph
with small number of copies

00:03:21.440 --> 00:03:38.470
of each, so h is sub-graphs,
then g can be made h free.

00:03:38.470 --> 00:03:41.170
So far, everything's still
the same as in the graph case,

00:03:41.170 --> 00:03:44.020
but now in the graph
case, the number of edges

00:03:44.020 --> 00:03:46.130
is quadratic, most quadratic.

00:03:46.130 --> 00:03:48.820
Here, it's a most
n to the r, so I

00:03:48.820 --> 00:03:51.730
want to make this
graph, this r graph,

00:03:51.730 --> 00:04:00.350
h free by removing
less than epsilon

00:04:00.350 --> 00:04:05.570
n to the r edges from g.

00:04:14.030 --> 00:04:16.880
So that's the statement of
the hypergraph removal lemma.

00:04:16.880 --> 00:04:20.529
So it's an extension of
the graph removal lemma.

00:04:20.529 --> 00:04:22.343
Any questions about
the statement?

00:04:25.520 --> 00:04:27.630
So before discussing
the proof, let

00:04:27.630 --> 00:04:31.300
me show you why you might
care about this statement.

00:04:31.300 --> 00:04:36.700
And we used the triangle removal
lemma to deduce Roth's theorem.

00:04:36.700 --> 00:04:39.820
Remember, there was a graph
theoretic set up where

00:04:39.820 --> 00:04:41.950
start with a 3-AP-free subset.

00:04:41.950 --> 00:04:44.770
We set up a graph,
and then that graph

00:04:44.770 --> 00:04:46.390
has some nice
properties that allows

00:04:46.390 --> 00:04:50.470
us to use a corollary of
the triangle removal lemma,

00:04:50.470 --> 00:04:53.260
namely the corollary that says
that if you have a graph, where

00:04:53.260 --> 00:04:57.290
every edge sits on
exactly one triangle,

00:04:57.290 --> 00:05:00.710
then it has a sub
quadratic number of edges.

00:05:00.710 --> 00:05:03.620
So we can do a similar
type of deduction

00:05:03.620 --> 00:05:06.980
showing that the hypergraph
removal lemma implies

00:05:06.980 --> 00:05:09.060
Szemeredi's theorem.

00:05:09.060 --> 00:05:10.900
So that's what we'll do.

00:05:10.900 --> 00:05:21.470
So let's deduce
Szemeredi's theorem

00:05:21.470 --> 00:05:23.730
from the hypergraph
removal lemma.

00:05:23.730 --> 00:05:25.970
So recall Szemeredi's
theorem says

00:05:25.970 --> 00:05:31.070
that for every fixed
positive integer k,

00:05:31.070 --> 00:05:37.810
if a is a subset of 1
through n, that is kp free,

00:05:37.810 --> 00:05:40.640
has no k term
arithmetic progressions,

00:05:40.640 --> 00:05:45.305
then the size of
a is sub-lineal.

00:05:49.190 --> 00:05:52.380
Instead of illustrating how to
do this proof for general k,

00:05:52.380 --> 00:05:55.520
I'm just going to do it for
the case of k close to 4.

00:05:55.520 --> 00:05:58.370
And you can look at the
proof and it will be clear

00:05:58.370 --> 00:05:59.540
how to generalize.

00:05:59.540 --> 00:06:06.620
So we'll just illustrate
the proof for 4-APs.

00:06:10.070 --> 00:06:13.130
Now, before even showing
you what this proof looks

00:06:13.130 --> 00:06:16.130
like, you might wonder,
do we really need

00:06:16.130 --> 00:06:18.890
the hypergraph removal lemma?

00:06:18.890 --> 00:06:21.560
Could it be that with
the graph removal

00:06:21.560 --> 00:06:25.410
lemma and a more clever
choice of a graph,

00:06:25.410 --> 00:06:32.130
you could prove Szemeredi's
theorem using just that.

00:06:32.130 --> 00:06:33.840
So we set up some
graph previously

00:06:33.840 --> 00:06:35.940
where triangles
correspond to 3-APs.

00:06:35.940 --> 00:06:37.620
Maybe you can set
up some other graph

00:06:37.620 --> 00:06:41.790
where some other kinds of
structure correspond to 4-APs.

00:06:41.790 --> 00:06:44.040
And it turns out the
answer is emphatically no.

00:06:47.370 --> 00:06:52.280
So there's a very good
reason for this that for--

00:06:52.280 --> 00:06:54.870
these are things which we might
go into more when we discuss

00:06:54.870 --> 00:06:58.350
additive combinatorics,
but 4-APs

00:06:58.350 --> 00:07:01.590
is a pattern that's sometimes
called complexity two.

00:07:04.490 --> 00:07:09.110
Where as 3-APs is a pattern
which is called complexity one.

00:07:13.820 --> 00:07:16.760
I won't go into the precise
definitions of what this means,

00:07:16.760 --> 00:07:23.790
but the message is that you
cannot prove the 4-AP theorem

00:07:23.790 --> 00:07:26.610
with just graph machinery.

00:07:26.610 --> 00:07:28.880
You really have to use
something stronger.

00:07:28.880 --> 00:07:32.250
That there is a very real sense
in which just a graph removal

00:07:32.250 --> 00:07:34.690
lemma is at least-- or
Szemeredi's theorem,

00:07:34.690 --> 00:07:39.090
Szemeredi's regularity
lemma is not enough.

00:07:39.090 --> 00:07:41.210
And so we really do have
to go to hypergraph.

00:07:41.210 --> 00:07:43.730
And this extra
layer of complexity,

00:07:43.730 --> 00:07:45.830
like in this word sense
of the complexity,

00:07:45.830 --> 00:07:49.220
introduces also additional
difficulties in there.

00:07:49.220 --> 00:07:51.650
So that there makes
significantly harder

00:07:51.650 --> 00:07:53.930
than graph removal lemma.

00:07:53.930 --> 00:07:57.230
So I won't even
show you anything

00:07:57.230 --> 00:07:59.060
that's close to
a complete proof,

00:07:59.060 --> 00:08:01.580
but I will illustrate
some of the ideas

00:08:01.580 --> 00:08:04.630
and highlight some of
the difficulties today.

00:08:04.630 --> 00:08:08.170
But deducing Szemeredi's theorem
from the hypergraph removal

00:08:08.170 --> 00:08:09.970
lemma is actually not so bad.

00:08:09.970 --> 00:08:13.250
So I will show you how
to do that right now.

00:08:13.250 --> 00:08:16.330
So from the hypergraph
removal lemma,

00:08:16.330 --> 00:08:19.240
even just for three
uniform hypergraph, even

00:08:19.240 --> 00:08:24.250
for tetrahedra-- so into the
triangles graph, tetrahedron,

00:08:24.250 --> 00:08:26.770
we can have the following
corollary analogous to all

00:08:26.770 --> 00:08:28.640
we have for triangles.

00:08:28.640 --> 00:08:45.030
So if g is a three graph such
that every edge is contained

00:08:45.030 --> 00:08:59.930
in a unique tetrahedron, then g
has sub-cubic number of edges.

00:09:03.430 --> 00:09:05.100
So completely
analogous to the one

00:09:05.100 --> 00:09:07.305
we have for triangles and
the proof is identical.

00:09:07.305 --> 00:09:08.680
You read the proof
and everything

00:09:08.680 --> 00:09:12.300
works exactly the same way,
once you have the removal lemma.

00:09:12.300 --> 00:09:16.210
By tetrahedron, I mean
the complete graph

00:09:16.210 --> 00:09:19.380
on four vertices.

00:09:19.380 --> 00:09:21.730
Complete three graph
of four vertices.

00:09:21.730 --> 00:09:22.230
OK?

00:09:22.230 --> 00:09:30.120
So you four vertices and you
look at all possible triples

00:09:30.120 --> 00:09:31.105
of vertices.

00:09:34.160 --> 00:09:34.660
All right.

00:09:34.660 --> 00:09:37.180
So now let's prove
Szemeredi's theorem, at least

00:09:37.180 --> 00:09:38.350
the 4-AP case.

00:09:38.350 --> 00:09:41.200
The general case is
completely analogous.

00:09:41.200 --> 00:09:43.000
Of course, you have
to go to higher order.

00:09:43.000 --> 00:09:47.410
Instead of three graphs, you
have to look at r graphs.

00:09:47.410 --> 00:09:50.370
So just as in the proof
of Roth's theorem,

00:09:50.370 --> 00:09:53.160
we're going to set up
some particular three

00:09:53.160 --> 00:10:00.210
graph where let's look
at a certain modulus

00:10:00.210 --> 00:10:02.800
m being 6n plus 1.

00:10:02.800 --> 00:10:05.030
The exact number is
not so important here.

00:10:05.030 --> 00:10:06.780
I really just wanted
this number is bigger

00:10:06.780 --> 00:10:13.580
than 3n and it's co-prime to 6,
as well as t sum divisibility's

00:10:13.580 --> 00:10:16.930
by 2 and 3, that will be useful.

00:10:16.930 --> 00:10:29.610
So let's build a
4-partite three graph

00:10:29.610 --> 00:10:44.620
g where the four vertex
parts x, y, z, w, they all

00:10:44.620 --> 00:10:50.450
have m vertices each.

00:10:54.200 --> 00:10:56.850
And I'll show you
what the edges are.

00:11:00.380 --> 00:11:04.500
So four vertices, little
x and big X and so on.

00:11:08.320 --> 00:11:10.170
So here are the rules
for putting the edges.

00:11:10.170 --> 00:11:11.878
I'll just tell you
exactly what they are.

00:11:11.878 --> 00:11:17.780
So I put in an edge
xyz if and only

00:11:17.780 --> 00:11:34.512
if the following expression
3x plus 2y plus z lies in a.

00:11:34.512 --> 00:11:47.160
I put in the edge xyw if and
only if 2x plus y minus w

00:11:47.160 --> 00:11:47.871
lies in a.

00:11:51.390 --> 00:11:51.890
OK?

00:11:51.890 --> 00:12:00.957
So xzw if and only if x
minus the minus 2w lies in a.

00:12:03.699 --> 00:12:13.680
And finally, yzw if and only
if minus y minus 2z minus 3w

00:12:13.680 --> 00:12:16.760
lies in a.

00:12:16.760 --> 00:12:18.710
So these are the
rules for putting

00:12:18.710 --> 00:12:21.808
in edges of this hypergraph.

00:12:27.180 --> 00:12:28.870
Of course, you have
this hypergraph.

00:12:28.870 --> 00:12:31.495
And you might be wondering, why
did I choose these expressions?

00:12:31.495 --> 00:12:33.540
So if it's not clear
yet, it will soon

00:12:33.540 --> 00:12:34.900
be very clear why we do this.

00:12:37.440 --> 00:12:39.180
Just as in the proof
of Roth's theorem

00:12:39.180 --> 00:12:43.260
using triangle removal lemma,
let's examine the tetrahedra

00:12:43.260 --> 00:12:45.830
in this three graph.

00:12:45.830 --> 00:12:48.950
So what are the tetrahedra?

00:12:48.950 --> 00:12:55.261
Notice that xyzw
is a tetrahedron.

00:12:58.700 --> 00:13:03.330
So if all three
triples are present, if

00:13:03.330 --> 00:13:11.750
and only if all four of
these expressions lie in a.

00:13:30.885 --> 00:13:32.760
Well, just like in a
proof of Roth's theorem,

00:13:32.760 --> 00:13:35.340
these four expressions
form a 4-AP.

00:13:44.260 --> 00:13:52.040
And the common
difference is minus

00:13:52.040 --> 00:13:54.690
x minus y minus z minus w.

00:13:58.200 --> 00:13:58.700
OK?

00:13:58.700 --> 00:14:01.050
So they were chosen to
satisfy this property.

00:14:01.050 --> 00:14:03.540
But furthermore, notice
that I can't just

00:14:03.540 --> 00:14:04.970
put any expressions in.

00:14:04.970 --> 00:14:08.220
I put these expressions in
with the very nice property

00:14:08.220 --> 00:14:19.820
that the i-th linear form does
not use the i-th variable.

00:14:23.980 --> 00:14:26.890
So each expression
really corresponds

00:14:26.890 --> 00:14:29.500
to an edge in this three graph.

00:14:34.320 --> 00:14:36.740
All right.

00:14:36.740 --> 00:14:40.580
But we started with a
set a that is 4-AP free.

00:14:47.630 --> 00:14:52.050
It follows that you don't have
this kind of configurations

00:14:52.050 --> 00:14:55.350
unless the common
difference is zero.

00:14:59.360 --> 00:15:05.000
So the only
tetrahedra correspond

00:15:05.000 --> 00:15:08.290
to these trivial 4-APs.

00:15:13.690 --> 00:15:16.270
And just as in the
proof of Roth's theorem

00:15:16.270 --> 00:15:18.790
that we saw from
triangle removal lemma,

00:15:18.790 --> 00:15:26.890
the conclusion is
that every edge lies

00:15:26.890 --> 00:15:33.290
in exactly one tetrahedron.

00:15:37.700 --> 00:15:47.420
And therefore, by the
corollary, the number of edges

00:15:47.420 --> 00:15:52.460
is equal to little o of m cubed.

00:15:55.050 --> 00:15:57.900
But on the other hand,
how many edges are there?

00:15:57.900 --> 00:16:01.750
So for each of the
four conditions here,

00:16:01.750 --> 00:16:05.980
so for each of these four
parts, the number of edges is,

00:16:05.980 --> 00:16:10.050
well, you get to choose x
and y whatever you want.

00:16:10.050 --> 00:16:12.650
And the last variable
has a choices.

00:16:16.310 --> 00:16:20.310
So this implies that the
size of a is little o of m,

00:16:20.310 --> 00:16:23.700
and m is on the same order as
n, and that proves the theorem.

00:16:36.284 --> 00:16:36.790
OK.

00:16:36.790 --> 00:16:38.690
Any questions so far?

00:16:38.690 --> 00:16:39.292
Yeah.

00:16:39.292 --> 00:16:41.582
AUDIENCE: Where do we use
the m is co-prime to 6?

00:16:41.582 --> 00:16:42.290
PROFESSOR: Great.

00:16:42.290 --> 00:16:44.990
Question's where do
we use the condition

00:16:44.990 --> 00:16:48.440
that m is co-prime to 6?

00:16:48.440 --> 00:16:49.725
Anyone know the answer?

00:16:49.725 --> 00:16:54.234
AUDIENCE: To solve for that
last variable, divide by 2 or 3.

00:16:54.234 --> 00:16:54.942
PROFESSOR: Great.

00:16:54.942 --> 00:16:56.710
So to solve for
the last variable,

00:16:56.710 --> 00:16:58.680
we ought to maybe
divide by 2 or 3.

00:16:58.680 --> 00:17:02.670
And to do that, you need to
have co-primality with 6.

00:17:02.670 --> 00:17:04.420
Yeah, so I'm hiding a
bit of details here.

00:17:08.857 --> 00:17:09.359
OK?

00:17:09.359 --> 00:17:12.130
But it's a great question.

00:17:12.130 --> 00:17:14.450
So if you work out the details
of this statement here,

00:17:14.450 --> 00:17:17.690
every edge slicing exactly
one tetra tetrahedron

00:17:17.690 --> 00:17:19.430
and also counting
the number of edges,

00:17:19.430 --> 00:17:23.210
but more especially,
the first sentence.

00:17:23.210 --> 00:17:25.730
You need to actually do
just a tiny bit of work.

00:17:29.990 --> 00:17:31.195
Any more questions?

00:17:33.930 --> 00:17:36.780
So this deduction is the
same deduction as the one

00:17:36.780 --> 00:17:38.550
that we did four
triangles, but you

00:17:38.550 --> 00:17:43.250
have to come up with a slightly
different set of linear forms.

00:17:43.250 --> 00:17:47.400
And usually, if you're
given a specific pattern,

00:17:47.400 --> 00:17:50.195
you know you can play by
hand and try to come up

00:17:50.195 --> 00:17:51.320
with a set of linear forms.

00:17:53.840 --> 00:17:57.710
You can think about, also,
how to do this generally.

00:17:57.710 --> 00:18:01.760
And more generally, this
hypergraph removal lemma

00:18:01.760 --> 00:18:05.450
using this type of
ideas, it allows

00:18:05.450 --> 00:18:10.810
you to deduce multi-dimensional
Szemeredi theorem.

00:18:19.470 --> 00:18:23.810
So if you give me
some pattern, then

00:18:23.810 --> 00:18:26.480
a subset of the integer
lattice in the fixed dimension

00:18:26.480 --> 00:18:32.570
avoiding that pattern must
have density going to zero.

00:18:32.570 --> 00:18:35.280
So we stated this in
the very first lecture.

00:18:35.280 --> 00:18:39.200
And I won't spell
all the details,

00:18:39.200 --> 00:18:41.780
but you can follow
this kind of framework

00:18:41.780 --> 00:18:43.650
and gave you that theorem.

00:18:43.650 --> 00:18:46.070
And I will post
the problem where

00:18:46.070 --> 00:18:48.350
you asked to do this
for a specific pattern,

00:18:48.350 --> 00:18:52.010
namely that of a geometric
square, axis-aligned square.

00:18:52.010 --> 00:18:58.360
So if you have
that pattern in z2,

00:18:58.360 --> 00:19:01.530
so it's worth thinking about
how would you run this argument

00:19:01.530 --> 00:19:03.900
for that pattern in z2.

00:19:03.900 --> 00:19:04.400
Yes.

00:19:04.400 --> 00:19:09.007
AUDIENCE: How close
can the small o m be?

00:19:09.007 --> 00:19:09.590
PROFESSOR: OK.

00:19:09.590 --> 00:19:12.400
The question's how
close can the--

00:19:12.400 --> 00:19:15.310
so you're asking what is the
rate, this little o m gives?

00:19:18.410 --> 00:19:20.410
Let me address-- OK, so
hold onto this question.

00:19:20.410 --> 00:19:24.070
I will address it once
I discuss what is known

00:19:24.070 --> 00:19:25.620
about hypergraph removal lemma.

00:19:25.620 --> 00:19:28.360
And that's a great
question and there's

00:19:28.360 --> 00:19:32.750
a lot of mystery surrounding
what happens there.

00:19:32.750 --> 00:19:33.250
OK.

00:19:33.250 --> 00:19:33.750
Questions?

00:19:33.750 --> 00:19:34.830
Any others?

00:19:37.320 --> 00:19:37.820
Great.

00:19:37.820 --> 00:19:40.950
So let's discuss this
hypergraph removal lemma.

00:19:40.950 --> 00:19:44.420
And as I had warned you already
at the beginning of lecture,

00:19:44.420 --> 00:19:48.410
this one is very difficult. So
I mentioned in the very first

00:19:48.410 --> 00:19:53.420
lecture that the development
of Szemeredi's regularity lemma

00:19:53.420 --> 00:19:55.550
was a stroke of ingenuity.

00:19:55.550 --> 00:19:57.100
But this one here--

00:19:57.100 --> 00:19:58.710
but we saw that the
proof and we did

00:19:58.710 --> 00:20:01.010
the proof of Szemeredi's
graph regularity lemma

00:20:01.010 --> 00:20:02.170
in one lecture.

00:20:02.170 --> 00:20:04.740
And once you understand
it, it's not so bad.

00:20:04.740 --> 00:20:08.010
You do the energy increment,
conceptually it's not so bad.

00:20:08.010 --> 00:20:11.120
But that when there is
actually incredibly difficult.

00:20:11.120 --> 00:20:14.270
It's incredibly difficult both
conceptually and technically.

00:20:14.270 --> 00:20:16.540
But I want to at least
illustrate some of the ideas

00:20:16.540 --> 00:20:18.290
and give you some sense
of the difficulty,

00:20:18.290 --> 00:20:25.910
like why is this difficult.
So as we imagine,

00:20:25.910 --> 00:20:27.440
we have graph regularity.

00:20:27.440 --> 00:20:30.200
And to prove
hypergraph removal, one

00:20:30.200 --> 00:20:33.260
would develop some kind of a
hypergraph regularity method.

00:20:41.130 --> 00:20:44.930
And the basic idea in hypergraph
regularity or just regularity,

00:20:44.930 --> 00:20:47.420
in general, is that I give
you some arbitrary graph

00:20:47.420 --> 00:20:50.930
or hypergraph, and you want to
find some kind of partitioning,

00:20:50.930 --> 00:20:55.910
some kind of regularization into
some bounded number of pieces,

00:20:55.910 --> 00:20:57.860
some bound amount of
data so that that's

00:20:57.860 --> 00:21:01.170
a good approximation
for the actual graph.

00:21:01.170 --> 00:21:04.080
Just like in the
graph regularity case.

00:21:04.080 --> 00:21:06.020
So let's try to do this.

00:21:06.020 --> 00:21:09.060
So what does this
partition even look like?

00:21:09.060 --> 00:21:10.302
So here's an attempt.

00:21:10.302 --> 00:21:11.760
And of course, I
call it an attempt

00:21:11.760 --> 00:21:13.860
because eventually,
it will not work.

00:21:13.860 --> 00:21:17.250
But it's a very natural
first thing to try.

00:21:19.790 --> 00:21:24.630
And maybe, I shouldn't call
it naive because it's actually

00:21:24.630 --> 00:21:27.570
not a bad idea to begin with.

00:21:27.570 --> 00:21:29.430
OK, so let's see.

00:21:29.430 --> 00:21:37.470
So suppose you were
given a three graph g.

00:21:37.470 --> 00:21:42.120
I'm going to just to
help you remember what's

00:21:42.120 --> 00:21:44.820
the uniformity of each graph.

00:21:44.820 --> 00:21:48.580
I will denote in parentheses,
in the superscript,

00:21:48.580 --> 00:21:51.550
so just help you remember
that this is the three graph.

00:21:51.550 --> 00:21:54.120
So in geometry, they also
do this with manifolds.

00:21:54.120 --> 00:21:56.270
So if you put an n on
top, it's an n manifold.

00:21:56.270 --> 00:21:59.860
But this 3 is for three graph.

00:21:59.860 --> 00:22:09.650
Suppose we partition the
vertex set of this three graph

00:22:09.650 --> 00:22:16.280
similar to proof of
Szemeredi's regularity lemma.

00:22:22.360 --> 00:22:25.060
Think about how the proof of
Szemeredi's regularity lemma

00:22:25.060 --> 00:22:25.850
goes.

00:22:25.850 --> 00:22:27.880
So you have this partition,
but in the proof,

00:22:27.880 --> 00:22:30.130
there is this
iterative refinement.

00:22:30.130 --> 00:22:32.980
And each step you say, well, I
have this notion of regularity.

00:22:32.980 --> 00:22:34.450
If it doesn't
satisfy regularity,

00:22:34.450 --> 00:22:36.610
I can keep cutting
things up further.

00:22:36.610 --> 00:22:38.050
So what's the
notion of regularity

00:22:38.050 --> 00:22:43.080
that might get you some
kind of vertex partition?

00:22:43.080 --> 00:22:46.920
Can anyone think of a
notion of a regularity

00:22:46.920 --> 00:22:49.200
for three uniform hypergraphs?

00:22:49.200 --> 00:22:50.166
Yep.

00:22:50.166 --> 00:22:52.581
AUDIENCE: So the same
sort of thing, [INAUDIBLE]

00:22:52.581 --> 00:22:54.996
variations like [INAUDIBLE].

00:22:59.360 --> 00:23:02.650
PROFESSOR: So let me try to
rephrase what you're saying.

00:23:02.650 --> 00:23:13.320
So if we have a
notion of regularity,

00:23:13.320 --> 00:23:19.200
let's say I have three
vertex sets, V1, V2, V3.

00:23:19.200 --> 00:23:29.970
And I want that the density
between these three,

00:23:29.970 --> 00:23:31.605
they do not differ from--

00:23:37.450 --> 00:23:45.250
if I restrict these
vertex sets to subsets

00:23:45.250 --> 00:23:46.530
that are not too small.

00:23:52.400 --> 00:23:54.210
Does this make sense?

00:23:54.210 --> 00:24:08.700
So here, this d is the
fraction of triples that

00:24:08.700 --> 00:24:11.980
are edges of the hypergraph.

00:24:15.050 --> 00:24:17.780
So this is the
natural extension,

00:24:17.780 --> 00:24:20.510
natural generalization of
the notion of regularity

00:24:20.510 --> 00:24:22.550
that we saw earlier for graphs.

00:24:22.550 --> 00:24:25.330
And indeed, it's a
very natural notion.

00:24:25.330 --> 00:24:26.570
it is a nice notion.

00:24:26.570 --> 00:24:28.130
And actually, if
you use this notion,

00:24:28.130 --> 00:24:30.650
if you use more or less
precisely what I've written,

00:24:30.650 --> 00:24:34.550
you can run through the entire
proof of Szemeredi's graph

00:24:34.550 --> 00:24:38.990
regularity lemma and produce
a regularity theorem that

00:24:38.990 --> 00:24:42.350
tells you given an arbitrary
three uniform hypergraph,

00:24:42.350 --> 00:24:46.070
you can decompose the
vertex set in such a way

00:24:46.070 --> 00:24:53.280
that most triples of vertex
sets have this property.

00:24:53.280 --> 00:25:02.400
So the same proof as Szemeredi's
regularity lemma implies--

00:25:02.400 --> 00:25:04.860
so I won't write down
the entire statement,

00:25:04.860 --> 00:25:05.880
but you get the idea.

00:25:05.880 --> 00:25:08.070
So for every
epsilon, there exists

00:25:08.070 --> 00:25:15.600
m such that we can partition
into the vertex set

00:25:15.600 --> 00:25:23.470
into at most m parts, even
equitable, if you like,

00:25:23.470 --> 00:25:39.220
so that at most an epsilon
fraction of triples or parts

00:25:39.220 --> 00:25:45.360
are not epsilon regular in
the sense that I just said.

00:25:45.360 --> 00:25:46.920
So it's literally
is the same proof.

00:25:46.920 --> 00:25:49.340
So you really look at the
proof and then you get that.

00:25:49.340 --> 00:25:49.840
OK?

00:25:49.840 --> 00:25:55.340
So far everything seems
pretty good, pretty easy.

00:25:55.340 --> 00:25:55.840
OK.

00:25:55.840 --> 00:26:00.530
So why did I say,
initially, that actually,

00:26:00.530 --> 00:26:02.890
hypergraph regularity
is incredibly difficult?

00:26:02.890 --> 00:26:06.260
So what not good about this one?

00:26:09.660 --> 00:26:11.940
Also remember, in
our application

00:26:11.940 --> 00:26:16.394
of the regularity method,
there were three steps.

00:26:16.394 --> 00:26:17.190
What are they?

00:26:17.190 --> 00:26:20.901
Partition, clean, and count.

00:26:20.901 --> 00:26:21.401
OK.

00:26:21.401 --> 00:26:22.950
So partition, OK,
you do partition.

00:26:22.950 --> 00:26:26.980
Clean, well, you do
some kind of cleaning.

00:26:26.980 --> 00:26:30.800
But counting,
that's a big thing.

00:26:30.800 --> 00:26:32.630
And that's something
that wasn't so hard.

00:26:32.630 --> 00:26:35.420
We had to do a little bit of
work, but it wasn't so hard.

00:26:35.420 --> 00:26:38.690
And you can ask, is there
a counting lemma associated

00:26:38.690 --> 00:26:41.670
to this regularity lemma?

00:26:41.670 --> 00:26:44.550
And the answer is
emphatically no.

00:26:44.550 --> 00:26:47.910
And I want to convince you that
for this notion of regularity,

00:26:47.910 --> 00:26:50.104
there's no counting lemma.

00:26:50.104 --> 00:26:50.604
OK.

00:27:05.020 --> 00:27:06.246
Yes.

00:27:06.246 --> 00:27:07.871
AUDIENCE: Is this
version true, though?

00:27:07.871 --> 00:27:08.788
PROFESSOR: It is true.

00:27:08.788 --> 00:27:10.240
So you ask, is
this version true?

00:27:10.240 --> 00:27:14.585
So this statement is true
with this definition.

00:27:14.585 --> 00:27:15.960
And you can prove
it by literally

00:27:15.960 --> 00:27:19.110
rerunning the entire proof of
Szemeredi's graph regularity

00:27:19.110 --> 00:27:20.950
lemma.

00:27:20.950 --> 00:27:23.640
So the regularity statement
I've written down is true,

00:27:23.640 --> 00:27:25.410
but it is not useful.

00:27:25.410 --> 00:27:28.290
For example, it cannot be
used to prove the tetrahedron

00:27:28.290 --> 00:27:32.280
removal lemma because if you
try to run the same regularity

00:27:32.280 --> 00:27:35.340
proof of the removal lemma, you
run to the issue that you do

00:27:35.340 --> 00:27:37.650
not have a counting lemma.

00:27:37.650 --> 00:27:38.150
OK.

00:27:38.150 --> 00:27:40.785
So why is it that you do
not have a counting lemma?

00:27:40.785 --> 00:27:42.035
So let me show you an example.

00:27:45.680 --> 00:27:49.370
And keep in mind that the
notions of regularity,

00:27:49.370 --> 00:27:58.770
they are supposed to model
the idea of pseudo randomness,

00:27:58.770 --> 00:28:01.340
which is the topic we'll
explore in further length

00:28:01.340 --> 00:28:02.580
in the next chapter.

00:28:02.580 --> 00:28:04.130
But the idea of
pseudo randomness

00:28:04.130 --> 00:28:06.800
is that I want some graph
which is not random,

00:28:06.800 --> 00:28:09.810
but in some aspects look random.

00:28:09.810 --> 00:28:13.310
So this is an important concept
in mathematics and computer

00:28:13.310 --> 00:28:16.280
science, and it's a
very important idea.

00:28:16.280 --> 00:28:18.530
But of course, you can
generate a pseudo random object

00:28:18.530 --> 00:28:22.220
by just taking a random
object, and it should hopefully

00:28:22.220 --> 00:28:24.560
satisfy some properties
of pseudo randomness.

00:28:24.560 --> 00:28:30.900
So let's see what this
notion of regularity,

00:28:30.900 --> 00:28:36.030
how it works even for
random hypergraph.

00:28:36.030 --> 00:28:38.280
What's a random hypergraph?

00:28:38.280 --> 00:28:42.830
So there are different ways to
generate a random hypergraph.

00:28:42.830 --> 00:28:47.640
One way is to have
a bunch of triples

00:28:47.640 --> 00:28:50.130
all appearing
uniformly at random,

00:28:50.130 --> 00:28:53.630
independently at random.

00:28:53.630 --> 00:28:55.810
So I have a bunch
of possible triples

00:28:55.810 --> 00:28:58.940
that I can make as edges,
each one I flip a coin.

00:28:58.940 --> 00:29:00.440
But there's a
different way, and let

00:29:00.440 --> 00:29:03.770
me show you a different way to
generate a random three graph.

00:29:11.080 --> 00:29:15.480
Let me give you two
parameters, p and q.

00:29:15.480 --> 00:29:17.650
They are constants
between 0 and 1.

00:29:23.630 --> 00:29:34.370
So let's build first a
graph, so a random two graph

00:29:34.370 --> 00:29:37.630
which I'll call g2.

00:29:37.630 --> 00:29:41.560
So this is just the
Erdos-Renyi random graph, gnp.

00:29:41.560 --> 00:29:44.900
The usual one where you
flip a coin for each edge

00:29:44.900 --> 00:29:50.000
so that each edge appears with
probability p independently.

00:29:50.000 --> 00:29:56.900
And now, I make the actual
three graph that want, g3,

00:29:56.900 --> 00:30:17.550
by including every triple,
so every triangle of g

00:30:17.550 --> 00:30:19.260
has an edge.

00:30:19.260 --> 00:30:23.030
So here, an edge
means a triple--

00:30:23.030 --> 00:30:25.690
edge is three vertices
in the hypergraph--

00:30:25.690 --> 00:30:26.890
with probability q.

00:30:30.880 --> 00:30:32.100
So it's a two-step process.

00:30:32.100 --> 00:30:35.430
I first generate a
random graph, and then I

00:30:35.430 --> 00:30:37.450
look at the triangles
on top of that graph.

00:30:37.450 --> 00:30:42.050
And each triangle I include as
a triple with a probability q.

00:30:42.050 --> 00:30:43.790
If you like, q be even one.

00:30:43.790 --> 00:30:46.700
So I do a random graph
and my hypergraph

00:30:46.700 --> 00:30:47.930
is a set of triangles.

00:30:52.460 --> 00:30:55.470
And let's compare
this construction

00:30:55.470 --> 00:31:01.410
to the more naive version
of a random hypergraph where

00:31:01.410 --> 00:31:06.520
we look at this
hypergraph of graph

00:31:06.520 --> 00:31:17.780
where I put in each triple
appearing independently

00:31:17.780 --> 00:31:22.394
with probability p cubed q.

00:31:25.260 --> 00:31:27.540
So these are two
different constructions

00:31:27.540 --> 00:31:30.790
of random hypergraph.

00:31:30.790 --> 00:31:33.790
And you can check that they
have basically the same edge

00:31:33.790 --> 00:31:35.770
density.

00:31:35.770 --> 00:31:38.820
So how many edges
appear in the first one?

00:31:44.480 --> 00:31:51.450
While the density of
triangles in g2 is p cubed,

00:31:51.450 --> 00:31:54.260
and each of those triangles
appears an edge further

00:31:54.260 --> 00:31:55.310
with probability q.

00:31:58.530 --> 00:32:02.070
So they have similar
edge densities.

00:32:02.070 --> 00:32:07.830
And furthermore, you can
check that this condition here

00:32:07.830 --> 00:32:11.300
is true for both graphs.

00:32:11.300 --> 00:32:21.540
So both graphs satisfy this
notion of epsilon regularity

00:32:21.540 --> 00:32:27.777
as justifying with
high probability.

00:32:31.360 --> 00:32:32.110
OK.

00:32:32.110 --> 00:32:33.470
Great.

00:32:33.470 --> 00:32:37.550
So if you have the counting
lemma, it should give you

00:32:37.550 --> 00:32:40.340
some prediction as to
the number of tetrahedra

00:32:40.340 --> 00:32:43.460
that come directly from the
densities, in particular,

00:32:43.460 --> 00:32:47.363
they should be the same for
these two constructions.

00:32:47.363 --> 00:32:48.280
But are they the same?

00:32:53.070 --> 00:33:08.640
So the density of tetrahedra
in the first case,

00:33:08.640 --> 00:33:10.980
actually, let's do
the second case first.

00:33:10.980 --> 00:33:15.270
In b, so what's the
density of tetrahedra?

00:33:15.270 --> 00:33:19.260
So if I have four vertices,
so each of those three edges

00:33:19.260 --> 00:33:22.320
appear uniformly at
random, independently,

00:33:22.320 --> 00:33:27.360
so the density of tetrahedra
is just the edge density

00:33:27.360 --> 00:33:28.826
raised to the power of 4.

00:33:32.038 --> 00:33:33.080
What about the first one?

00:33:35.930 --> 00:33:38.310
AUDIENCE: 6.

00:33:38.310 --> 00:33:41.750
PROFESSOR: So in the first
one, to get a tetrahedra,

00:33:41.750 --> 00:33:45.230
the underlying graph
needs to have a k4.

00:33:45.230 --> 00:33:51.720
So p raised to 6, and then
on top of that, I want 4q's.

00:33:51.720 --> 00:33:54.170
So p raised to 6, q raise to 4.

00:33:54.170 --> 00:33:56.731
And when p is different,
these numbers are different.

00:33:59.500 --> 00:34:01.620
So this is an
example showing why

00:34:01.620 --> 00:34:04.962
there is no counting
lemma because you

00:34:04.962 --> 00:34:06.420
have two different
graphs that have

00:34:06.420 --> 00:34:08.699
the same type of
regularity and densities,

00:34:08.699 --> 00:34:11.973
but have vastly different
densities of tetrahedra.

00:34:15.600 --> 00:34:16.980
Any questions
about this example?

00:34:20.429 --> 00:34:24.870
It shows you, at least, why
this naive attempt does not

00:34:24.870 --> 00:34:28.719
work, at least if you follow
our regularity recipe.

00:34:34.730 --> 00:34:37.500
But in any case, it's
good for something.

00:34:37.500 --> 00:34:41.010
So you do not have a counting
lemma for tetrahedra,

00:34:41.010 --> 00:34:44.690
but you can still
salvage something.

00:34:44.690 --> 00:34:48.610
So it turns out
there is a counting

00:34:48.610 --> 00:34:52.650
lemma if your graph h--

00:35:00.820 --> 00:35:04.640
if this r graph h is linear.

00:35:04.640 --> 00:35:08.880
So linear means every pair of
edges intersecting at most one

00:35:08.880 --> 00:35:09.380
vertex.

00:35:22.700 --> 00:35:27.270
So for example, if you look at--

00:35:32.640 --> 00:35:36.930
so hypergraph or each line
is an edge of triples.

00:35:36.930 --> 00:35:40.050
So that's a linear hypergraph
because each pair intersecting

00:35:40.050 --> 00:35:41.700
at most one vertex.

00:35:41.700 --> 00:35:43.440
Tetrahedron is not linear.

00:35:43.440 --> 00:35:47.500
Two faces of a tetrahedron
can intersect in two vertices.

00:35:47.500 --> 00:35:48.000
OK.

00:35:48.000 --> 00:35:49.800
So we can try to prove
that this is true.

00:35:49.800 --> 00:35:51.675
And actually, the proof
is basically the same

00:35:51.675 --> 00:35:54.000
as the counting lemma
that we saw for graphs.

00:35:56.730 --> 00:35:57.325
Yes.

00:35:57.325 --> 00:35:59.875
AUDIENCE: How many edges
can a linear graph have?

00:35:59.875 --> 00:36:01.500
PROFESSOR: The
question, how many edges

00:36:01.500 --> 00:36:04.140
can a linear hypergraph have?

00:36:04.140 --> 00:36:06.720
You mean, given the
bounded number of vertices.

00:36:06.720 --> 00:36:08.250
OK.

00:36:08.250 --> 00:36:12.060
I'll leave you to
think about it.

00:36:12.060 --> 00:36:15.050
Any more questions?

00:36:15.050 --> 00:36:16.980
But for the graph that
we really care about,

00:36:16.980 --> 00:36:20.180
namely tetrahedra which
relates to Szemeredi's theorem,

00:36:20.180 --> 00:36:21.280
this method does not work.

00:36:26.180 --> 00:36:29.780
So what should we do instead?

00:36:29.780 --> 00:36:33.440
Let's come up with a different
notion of regularity.

00:36:33.440 --> 00:36:36.200
And that's somewhat
inspired by that example

00:36:36.200 --> 00:36:42.280
up there where we need to look
at not just triple densities

00:36:42.280 --> 00:36:45.670
between three vertex
sets, but also

00:36:45.670 --> 00:36:50.380
what happens to triples
that sit on top of a graph.

00:36:53.960 --> 00:37:03.430
So we should come up with
some notion of an edge density

00:37:03.430 --> 00:37:05.990
on top of two graphs.

00:37:10.650 --> 00:37:19.310
So given a, b, and c being
edge sets of a complete graphs,

00:37:19.310 --> 00:37:21.140
so these are graphs
a, b, and c--

00:37:21.140 --> 00:37:23.240
you should think
of them as graphs--

00:37:23.240 --> 00:37:36.580
and a three graph g, we can
define this quantity b of abc--

00:37:36.580 --> 00:37:39.220
so there's always a hidden
g which I'll usually omit--

00:37:41.830 --> 00:37:56.870
to be the fraction
of triples xyz

00:37:56.870 --> 00:38:03.790
where xyz are such that
they sit on top of abc.

00:38:03.790 --> 00:38:06.670
So yz lie in a.

00:38:06.670 --> 00:38:10.285
xz lie in b.

00:38:10.285 --> 00:38:11.800
xy lie in c.

00:38:11.800 --> 00:38:24.200
So diffraction off such triples
that are actually triples of g.

00:38:30.290 --> 00:38:38.300
In the case when
abc are the same,

00:38:38.300 --> 00:38:53.510
it's asking what fraction
of triangles are edges of g.

00:38:53.510 --> 00:38:57.150
But you are allowed to
use three different sets.

00:38:57.150 --> 00:39:00.360
So think of abc as red,
green, blue masking

00:39:00.360 --> 00:39:03.690
for fractions of red,
green, blue triangles

00:39:03.690 --> 00:39:06.887
that are actually triples
of the hypergraph.

00:39:11.000 --> 00:39:12.030
All right.

00:39:12.030 --> 00:39:20.157
So now, we can try to come up
with some notion of regularity.

00:39:36.080 --> 00:39:39.630
And as you might
expect, at this point,

00:39:39.630 --> 00:39:43.310
it's not sufficient to
partition the vertex set.

00:39:43.310 --> 00:39:45.150
Instead, we'll go further.

00:39:45.150 --> 00:39:50.860
We'll partition the edge
set of the complete graph.

00:39:50.860 --> 00:39:54.545
So we'll partition the
set of pairs of vertices.

00:39:57.700 --> 00:40:02.650
Let's partition each set
of the complete graph

00:40:02.650 --> 00:40:16.850
as a union of
graphs such that we

00:40:16.850 --> 00:40:21.970
would like a similar type
of regularity condition

00:40:21.970 --> 00:40:25.050
but for those
types of densities.

00:40:25.050 --> 00:40:41.430
Such that for most ijk such
that we have lots of triangles

00:40:41.430 --> 00:40:49.663
on top of these three
graphs in the partition.

00:40:53.450 --> 00:41:00.900
So for most ijk, such that this
is the case, this partition,

00:41:00.900 --> 00:41:27.482
so this triple is regular in
the sense that for all subgroups

00:41:27.482 --> 00:41:40.700
with not too few copies,
so not too few triangles,

00:41:40.700 --> 00:41:42.080
on top of these a's.

00:41:50.440 --> 00:41:53.950
One has that the density,
the triple density,

00:41:53.950 --> 00:42:04.550
among the g's is similar to
the triple density on top

00:42:04.550 --> 00:42:05.150
of the a's.

00:42:20.970 --> 00:42:22.980
So I'm doing some
kind of partition

00:42:22.980 --> 00:42:26.280
where g1 is like that.

00:42:26.280 --> 00:42:35.080
g2 like that, and g3 that.

00:42:35.080 --> 00:42:40.580
And what I'm saying is that if
you take subset of g1, g2, g3

00:42:40.580 --> 00:42:42.910
so that there are still
lots of triangles,

00:42:42.910 --> 00:42:45.290
and that's analogous to
this condition of a i's

00:42:45.290 --> 00:42:52.820
now being too small, then
counting the number of triples

00:42:52.820 --> 00:42:57.360
to the fraction of those
triangles that are edges of g.

00:42:57.360 --> 00:43:00.320
That fraction is
roughly the same when

00:43:00.320 --> 00:43:02.080
you pass down to sub-graphs.

00:43:08.435 --> 00:43:10.060
Don't worry about
the specific details,

00:43:10.060 --> 00:43:12.477
and I'm not going to try to
give you the specific details.

00:43:12.477 --> 00:43:14.850
But think about the analogy
to instead of partitioning

00:43:14.850 --> 00:43:17.730
the vertex set, we are
partitioning the edge

00:43:17.730 --> 00:43:21.510
set of a complete graph.

00:43:21.510 --> 00:43:24.360
But actually,
hypergraph regularity

00:43:24.360 --> 00:43:27.090
involves one more
step, namely that we

00:43:27.090 --> 00:43:48.530
need to further regularize these
g's via partitioning the vertex

00:43:48.530 --> 00:43:50.510
set.

00:43:50.510 --> 00:43:53.900
Similar to what happens in
Szemeredi's graph regularity

00:43:53.900 --> 00:43:57.020
lemma, but actually more similar
to the strong regularity lemma

00:43:57.020 --> 00:43:59.580
that we discussed last time.

00:43:59.580 --> 00:44:08.010
So the data of
hypergraph regularity

00:44:08.010 --> 00:44:14.350
is not simply a partition of the
vertex set, but it's twofold.

00:44:14.350 --> 00:44:19.470
One is a partition of the edge
set of the complete graph,

00:44:19.470 --> 00:44:21.720
so partition of
the vertex pairs,

00:44:21.720 --> 00:44:30.080
into pseudo random
graphs, so in into graphs,

00:44:30.080 --> 00:44:38.170
so that the hypergraph g
sits pseudo randomly on top.

00:44:46.330 --> 00:44:51.820
And furthermore,
there's also a partition

00:44:51.820 --> 00:45:04.580
of the vertex set of g so
that the graphs in part one

00:45:04.580 --> 00:45:17.720
are extremely pseudo random
with respect to this partition.

00:45:17.720 --> 00:45:19.260
And this idea of
extremely random

00:45:19.260 --> 00:45:22.500
we saw in the last lecture,
you have some the sequence

00:45:22.500 --> 00:45:25.890
of epsilons that depend
on how many parts

00:45:25.890 --> 00:45:28.833
you have in the first
step of the regularity.

00:45:31.731 --> 00:45:32.700
OK.

00:45:32.700 --> 00:45:33.938
Any questions?

00:45:37.110 --> 00:45:38.500
Yes.

00:45:38.500 --> 00:45:40.872
AUDIENCE: What happens
with triples g's that don't

00:45:40.872 --> 00:45:42.405
have a lot of triangles?

00:45:42.405 --> 00:45:45.030
PROFESSOR: The question is, what
happens to triples of g's that

00:45:45.030 --> 00:45:47.080
do not have lots of triangles?

00:45:47.080 --> 00:45:50.460
So they are similar
to in graphs,

00:45:50.460 --> 00:45:54.380
you have these small
sets of vertices.

00:45:54.380 --> 00:45:56.990
So you have to deal
with them somehow,

00:45:56.990 --> 00:45:59.780
but I'm, again, leaving out
all these technical details.

00:45:59.780 --> 00:46:04.560
And in fact, I am writing
down a very sketchy version

00:46:04.560 --> 00:46:05.787
of hypergraph regularity.

00:46:05.787 --> 00:46:07.620
You could write down a
more precise version.

00:46:07.620 --> 00:46:09.150
You can find it in literature.

00:46:09.150 --> 00:46:11.670
In fact, you can find
more than one version

00:46:11.670 --> 00:46:15.180
of the statement of hypergraph
regularity in literature.

00:46:15.180 --> 00:46:16.980
And they're not all
obviously equivalent.

00:46:16.980 --> 00:46:18.480
It actually takes
a lot of work even

00:46:18.480 --> 00:46:20.480
to show that different
versions of the statement

00:46:20.480 --> 00:46:21.950
are equivalent to each other.

00:46:21.950 --> 00:46:26.310
And it's still somewhat
mysterious as to what

00:46:26.310 --> 00:46:30.030
is the right, the most
natural formulation

00:46:30.030 --> 00:46:31.318
of hypergraph regularity.

00:46:31.318 --> 00:46:33.360
That's something that I
think we still do not yet

00:46:33.360 --> 00:46:36.540
have a satisfactory answer.

00:46:36.540 --> 00:46:40.320
There was a question
earlier about bounds.

00:46:40.320 --> 00:46:40.820
OK.

00:46:40.820 --> 00:46:44.925
So what kind of bounds do you
get for hypergraph regularity?

00:46:44.925 --> 00:46:46.300
So let me address
that issue now.

00:47:02.130 --> 00:47:03.630
So what kind of
bounds do you get?

00:47:06.530 --> 00:47:09.020
Well for Szemeredi's
graph regularity lemma,

00:47:09.020 --> 00:47:11.480
the bound is a power
function because we

00:47:11.480 --> 00:47:13.850
have to iterate the
exponential which

00:47:13.850 --> 00:47:15.920
comes out of the partitioning.

00:47:15.920 --> 00:47:20.900
And in hypergraph regularity,
because of this extremely

00:47:20.900 --> 00:47:21.622
pseudo random--

00:47:21.622 --> 00:47:24.080
so you are doing some kind of
partitioning the first stage,

00:47:24.080 --> 00:47:27.740
and then you are iterating that
on top for the second stage.

00:47:27.740 --> 00:47:32.970
Similar to how we did strong
regularity in the last lecture.

00:47:32.970 --> 00:47:38.060
So the bounds for
hypergraph regularity

00:47:38.060 --> 00:47:43.690
is also iterated power
which we saw last time,

00:47:43.690 --> 00:47:44.940
and this is known as a Wowzer.

00:47:49.730 --> 00:47:53.130
So it's even worse
than graph regularity.

00:47:53.130 --> 00:47:56.730
And just like in the
case of graph regularity,

00:47:56.730 --> 00:48:00.290
this was Wowzer type
bound is necessary,

00:48:00.290 --> 00:48:04.340
at least for most statements,
any of these useful statements

00:48:04.340 --> 00:48:08.220
of hypergraph regularity.

00:48:08.220 --> 00:48:10.520
What about the applications?

00:48:10.520 --> 00:48:17.105
So applications to
multi-dimensional

00:48:17.105 --> 00:48:17.980
Szemeredi's theorem--

00:48:17.980 --> 00:48:27.950
OK, so first of all to
Szemeredi theorem, well,

00:48:27.950 --> 00:48:30.290
you can prove Szemeredi
theorem this way

00:48:30.290 --> 00:48:33.840
and you would get inverse
Wowzer type bounds,

00:48:33.840 --> 00:48:36.860
which is it's not so great.

00:48:36.860 --> 00:48:38.310
But there are better proofs.

00:48:38.310 --> 00:48:41.300
So there are more efficient
proofs quantitatively.

00:48:41.300 --> 00:48:45.790
So for Szemeredi's theorem,
the best result for general k

00:48:45.790 --> 00:48:52.480
is due to Gowers' which tells
you that a must be, at most,

00:48:52.480 --> 00:48:57.480
n over something that's
log log n raised to power

00:48:57.480 --> 00:49:01.530
some constant c depending on k.

00:49:01.530 --> 00:49:04.510
That's for k equals 3 and 4,
you can do somewhat better

00:49:04.510 --> 00:49:06.160
before general k.

00:49:06.160 --> 00:49:08.290
This is the best bounds so far.

00:49:14.590 --> 00:49:21.040
But for multi-dimensions, for
multi-dimensional patterns,

00:49:21.040 --> 00:49:22.880
it turns out that--

00:49:22.880 --> 00:49:24.640
well, historically,
the first proof

00:49:24.640 --> 00:49:26.620
of the multi-dimensional
Szemeredi's theorem

00:49:26.620 --> 00:49:29.230
was done using
Ergodic theory which

00:49:29.230 --> 00:49:31.760
has even worse bounds compared
to this approach in that

00:49:31.760 --> 00:49:35.610
the Ergodic theoretic proof
gives no bounds because it has

00:49:35.610 --> 00:49:37.800
to use compactness
arguments, so they actually

00:49:37.800 --> 00:49:39.600
give no quantitative bounds.

00:49:39.600 --> 00:49:42.510
And one of the motivations
for this hypergraph regularity

00:49:42.510 --> 00:49:45.420
method, the removal
lemma, is to produce

00:49:45.420 --> 00:49:48.540
quantitative proof of
multi-dimensional Szemeredi's

00:49:48.540 --> 00:49:49.120
theorem.

00:49:49.120 --> 00:49:56.330
So in general, still
the best bounds

00:49:56.330 --> 00:50:05.810
come from this removal lemma,
so hypergraph removal lemma.

00:50:05.810 --> 00:50:08.030
Although in special
cases, and really,

00:50:08.030 --> 00:50:10.070
not that many special
cases, but really just

00:50:10.070 --> 00:50:13.970
the case of a corner
as we saw earlier

00:50:13.970 --> 00:50:15.670
you have somewhat better bounds.

00:50:15.670 --> 00:50:18.790
So for corner, you
have bounce, which

00:50:18.790 --> 00:50:21.680
have density like polylog log.

00:50:21.680 --> 00:50:24.860
But even for a
geometric square, we

00:50:24.860 --> 00:50:27.650
do not know any Fourier
analytic methods,

00:50:27.650 --> 00:50:29.590
we do not know other methods.

00:50:29.590 --> 00:50:32.210
And this is basically
the best bound coming out

00:50:32.210 --> 00:50:35.203
of hypergraph regularity.

00:50:35.203 --> 00:50:36.620
And there are
serious obstructions

00:50:36.620 --> 00:50:41.150
for trying to use Fourier
methods to do other patterns

00:50:41.150 --> 00:50:45.110
such as geometric square.

00:50:45.110 --> 00:50:45.610
OK?

00:50:45.610 --> 00:50:47.560
Any questions?

00:50:47.560 --> 00:50:48.237
Yes.

00:50:48.237 --> 00:50:51.006
AUDIENCE: So what do the bounds
look like for higher degree

00:50:51.006 --> 00:50:51.506
uniformity.

00:50:51.506 --> 00:50:53.937
Are they still just Wowzers?

00:50:53.937 --> 00:50:54.520
PROFESSOR: OK.

00:50:54.520 --> 00:50:59.050
So what are the bounds like
for higher degree uniformity?

00:50:59.050 --> 00:51:04.480
So this is Wowzers for free
uniform, and for full uniform,

00:51:04.480 --> 00:51:07.040
you iterate Wowzer.

00:51:07.040 --> 00:51:10.920
So you go up in
Ackermann hierarchy.

00:51:10.920 --> 00:51:13.220
You iterate Wowzer,
you get a four uniform

00:51:13.220 --> 00:51:17.392
hypergraph regularity
lemma as so.

00:51:21.078 --> 00:51:22.120
Let's take a short break.

00:51:27.620 --> 00:51:29.900
So the second topic I
want to discuss today

00:51:29.900 --> 00:51:33.830
is a different approach to
proving Szemeredi's graph

00:51:33.830 --> 00:51:35.120
regularity lemma.

00:51:35.120 --> 00:51:38.810
And this is a good segue
into our next topic,

00:51:38.810 --> 00:51:41.480
the next lecture, which is
about pseudo random graphs,

00:51:41.480 --> 00:51:46.670
in particular, the idea of
the spectrum eigenvalues,

00:51:46.670 --> 00:51:50.110
in particular, play
a central role.

00:51:50.110 --> 00:51:59.070
So I want to consider a
spectral approach giving

00:51:59.070 --> 00:52:03.570
an alternative way to prove
the Szemeredi regularity lemma.

00:52:18.290 --> 00:52:20.220
And if you're already
sick of the regularity

00:52:20.220 --> 00:52:23.070
lemma at this point, this
will be the last topic

00:52:23.070 --> 00:52:25.470
on regularity lemma
for now, although it

00:52:25.470 --> 00:52:27.500
will come up again
later in this course

00:52:27.500 --> 00:52:29.240
when we discuss graph limits.

00:52:29.240 --> 00:52:31.590
But for now, this is the
last thing I want to say.

00:52:31.590 --> 00:52:34.443
And just like the discussion
about hypergraph regularity,

00:52:34.443 --> 00:52:35.610
it will be somewhat sketchy.

00:52:41.490 --> 00:52:45.630
So this idea has appeared
in literature in the past,

00:52:45.630 --> 00:52:48.750
but it was popularized by
many good things in life

00:52:48.750 --> 00:52:52.133
by Terry Tao's blog.

00:52:52.133 --> 00:52:54.300
So it's a good place to
look up a discussion of what

00:52:54.300 --> 00:52:56.710
I'm about to say.

00:52:56.710 --> 00:52:57.210
OK.

00:52:57.210 --> 00:52:59.550
So we saw the proof
of regularity lemma

00:52:59.550 --> 00:53:02.370
via this iterated
partitioning and keeping

00:53:02.370 --> 00:53:06.990
track of our progress
through the use of an energy.

00:53:06.990 --> 00:53:09.030
But here's a
different perspective,

00:53:09.030 --> 00:53:13.560
namely if we start
with a graph g,

00:53:13.560 --> 00:53:17.280
I can look at the
adjacency matrix a sub g.

00:53:17.280 --> 00:53:20.050
So this is the n
by n matrix where

00:53:20.050 --> 00:53:34.630
n is a number of vertices
whose i j-th is zero

00:53:34.630 --> 00:53:41.430
if i is not adjacent to j,
and 1 if i is adjacent to j.

00:53:41.430 --> 00:53:43.230
So this is a pretty
standard thing

00:53:43.230 --> 00:53:46.640
to look at to associate
a graph to this matrix.

00:53:46.640 --> 00:53:54.140
So this graph here would
be like that and so on.

00:53:56.670 --> 00:54:04.360
It's a real symmetric matrix
and that's always pretty nice.

00:54:04.360 --> 00:54:07.870
Symmetric matrices have
lots of great properties

00:54:07.870 --> 00:54:09.200
that will be convenient to use.

00:54:09.200 --> 00:54:11.290
In fact, if you're
like myself, if you're

00:54:11.290 --> 00:54:13.270
too used to working
with symmetric matrices,

00:54:13.270 --> 00:54:14.937
you forget that some
of these properties

00:54:14.937 --> 00:54:18.460
actually do not apply in general
to non-symmetric matrices.

00:54:18.460 --> 00:54:21.440
But it is symmetric,
so we're happy.

00:54:21.440 --> 00:54:25.630
So for symmetric matrices, we
have a set of real eigenvalues.

00:54:29.380 --> 00:54:31.820
We have real eigenvalues
and eigenvectors.

00:54:31.820 --> 00:54:42.070
And for now, let me enumerate
the eigenvalues by lambda 1

00:54:42.070 --> 00:54:46.270
through lambda n, so
multiplicity included,

00:54:46.270 --> 00:54:51.730
and I sort them according to the
size of their absolute value.

00:54:55.780 --> 00:55:00.650
So the spectral theorem
tells us a decomposition.

00:55:04.500 --> 00:55:07.130
So here again, we're using
the a as real symmetric.

00:55:07.130 --> 00:55:09.540
So it tells us
that this matrix a

00:55:09.540 --> 00:55:16.710
can be written as the sum
coming from the eigenvalues

00:55:16.710 --> 00:55:30.930
and eigenvectors where the
u i's are the eigenvectors,

00:55:30.930 --> 00:55:36.380
but I can choose them so that
they form an orthogonal basis

00:55:36.380 --> 00:55:38.930
orthonormal basis, so
they're all unit vectors.

00:55:46.070 --> 00:55:50.610
So when I say the spectrum,
I mean this data also,

00:55:50.610 --> 00:55:52.590
specifically, this
set of eigenvalues.

00:55:54.756 --> 00:55:55.256
All right.

00:55:55.256 --> 00:55:58.600
So let's go through some basic
properties of the spectrum.

00:55:58.600 --> 00:56:02.650
So first, how big
can the lambdas be?

00:56:02.650 --> 00:56:05.140
So I claim that--

00:56:05.140 --> 00:56:16.623
so first of all, the sum of the
squares of these lambdas is--

00:56:16.623 --> 00:56:18.040
let me not even
call this a lemma,

00:56:18.040 --> 00:56:19.460
so it's just an observation.

00:56:19.460 --> 00:56:22.230
So the sum of the
squares is this one.

00:56:22.230 --> 00:56:29.240
So this is the
trace of a squared,

00:56:29.240 --> 00:56:34.050
which is also the sum of
the squares of the entries.

00:56:34.050 --> 00:56:39.520
So here, I'm always using
that a is real symmetric--

00:56:39.520 --> 00:56:42.960
sum of squares of entries of a.

00:56:46.730 --> 00:56:50.420
And the case when you have
a being an adjacency matrix,

00:56:50.420 --> 00:56:54.920
this is simply twice the number
of edges, which is at most n

00:56:54.920 --> 00:56:56.270
squared.

00:56:56.270 --> 00:56:58.020
So that's always a
good thing to remember.

00:57:00.840 --> 00:57:01.780
OK?

00:57:01.780 --> 00:57:10.650
So as a result, the i-th
eigenvalue cannot be bigger

00:57:10.650 --> 00:57:12.620
than what?

00:57:12.620 --> 00:57:15.040
So you have i eigenvalues.

00:57:15.040 --> 00:57:18.390
So they're sorted
in decreasing order.

00:57:18.390 --> 00:57:20.790
So the i-th eigenvalue
cannot be too large,

00:57:20.790 --> 00:57:27.300
particular it cannot be
larger than n over root i.

00:57:27.300 --> 00:57:32.280
Because otherwise, the sum of
the first i eigenvalue squared

00:57:32.280 --> 00:57:36.310
would exceed n squared.

00:57:36.310 --> 00:57:39.860
So these things, they do decay.

00:57:39.860 --> 00:57:47.640
So second observation
is that if you

00:57:47.640 --> 00:57:55.120
have some epsilon and
an arbitrary function,

00:57:55.120 --> 00:57:58.930
so this is known as
a growth function.

00:57:58.930 --> 00:58:00.750
That's just a name,
don't worry about it.

00:58:00.750 --> 00:58:03.000
So which we'll call f.

00:58:03.000 --> 00:58:05.700
So its function from
the positive integers,

00:58:05.700 --> 00:58:08.140
the positive integers,
and for convenience,

00:58:08.140 --> 00:58:11.580
I'm going to assume that f
of j is always at least j.

00:58:11.580 --> 00:58:19.610
For every j there
exists some c which

00:58:19.610 --> 00:58:23.120
depends only on your epsilon
and this growth function.

00:58:23.120 --> 00:58:25.160
So this growth function
plays the same role

00:58:25.160 --> 00:58:27.740
as the sequence of
decaying epsilons

00:58:27.740 --> 00:58:29.410
in these strong
regularity lemma.

00:58:32.090 --> 00:58:34.580
So there exists
some constant bound

00:58:34.580 --> 00:58:45.670
such that for every graph g
and ag as above, so associated

00:58:45.670 --> 00:58:54.390
with the lambdas and u's, there
exists a j less than c such

00:58:54.390 --> 00:58:59.550
that if I sum up the
eigenvalues squared

00:58:59.550 --> 00:59:07.050
for eigenvalues i index
between j and c of j,

00:59:07.050 --> 00:59:09.530
the sum is fairly small.

00:59:09.530 --> 00:59:11.595
It's at most epsilon n squared.

00:59:16.730 --> 00:59:19.420
I'll let you ponder
that for a second.

00:59:19.420 --> 00:59:21.880
So choose your favorite
growth function.

00:59:21.880 --> 00:59:23.830
It can be as quickly
growing as you can.

00:59:23.830 --> 00:59:26.280
It can be exponential,
power, or whatever.

00:59:26.280 --> 00:59:31.730
And it's saying that I can
look up to a bounded point

00:59:31.730 --> 00:59:35.990
so that this stretch
of spectrum squared

00:59:35.990 --> 00:59:38.490
is, at most, epsilon n squared.

00:59:38.490 --> 00:59:39.275
Question.

00:59:39.275 --> 00:59:40.640
AUDIENCE: What is c of j?

00:59:43.180 --> 00:59:43.930
PROFESSOR: F of j.

00:59:43.930 --> 00:59:44.430
Thank you.

00:59:58.568 --> 01:00:00.610
Well, the statement
hopefully will become clearer

01:00:00.610 --> 01:00:01.693
once I show you the proof.

01:00:07.780 --> 01:00:08.280
OK.

01:00:08.280 --> 01:00:10.110
So here's how you
would prove it.

01:00:13.310 --> 01:00:20.340
So you first let j1
equal to 1, and I

01:00:20.340 --> 01:00:26.130
obtained the subsequent
j's by applying f to j.

01:00:35.100 --> 01:00:41.890
So I claim that one cannot
have this inequality violated

01:00:41.890 --> 01:00:44.100
for too many of these j i's.

01:00:44.100 --> 01:00:59.550
So one cannot have the sum
going between jk and jk plus 1

01:00:59.550 --> 01:01:09.400
for all k from 1
to 1 over epsilon.

01:01:15.550 --> 01:01:17.305
Let's change this to zero.

01:01:23.720 --> 01:01:27.650
But you cannot have this because
if you had this then you sum up

01:01:27.650 --> 01:01:31.100
all of these inequalities you
would get that the total sum

01:01:31.100 --> 01:01:35.960
would exceed and squared, which
would violate the inequality

01:01:35.960 --> 01:01:38.780
about sum of the
squares of the spectrum.

01:01:43.030 --> 01:01:49.560
And so therefore, so thus
to the claimed inequality,

01:01:49.560 --> 01:02:02.600
so this is true
star holds for sum j

01:02:02.600 --> 01:02:10.610
equal to ji so jk where k
is less than 1 over epsilon.

01:02:10.610 --> 01:02:14.880
And this j, in
particular, is less than--

01:02:14.880 --> 01:02:17.670
well, whatever it
is, it's bounded.

01:02:17.670 --> 01:02:27.250
So it's less than f
applied to itself at most 1

01:02:27.250 --> 01:02:28.510
over epsilon times--

01:02:33.210 --> 01:02:34.160
OK.

01:02:34.160 --> 01:02:36.360
So this should look
somewhat familiar.

01:02:36.360 --> 01:02:39.930
And I'll ask you to
think about, later on,

01:02:39.930 --> 01:02:44.310
how this proof of spectral proof
of Szemeredi's graph regularity

01:02:44.310 --> 01:02:47.310
lemma compare to the
proof that we saw earlier.

01:02:47.310 --> 01:02:50.550
And you should see where
the analogous step is here.

01:02:50.550 --> 01:02:51.593
This is that density.

01:02:51.593 --> 01:02:53.010
This is the energy
increment step.

01:02:57.456 --> 01:02:58.600
All right.

01:02:58.600 --> 01:02:59.100
OK.

01:02:59.100 --> 01:03:00.850
So what's the regularity
decomposition?

01:03:00.850 --> 01:03:01.950
So I give you this graph.

01:03:01.950 --> 01:03:05.010
I give you this
adjacency matrix.

01:03:05.010 --> 01:03:07.860
And I want be able
to find a partition,

01:03:07.860 --> 01:03:10.740
but there's a different
way to view a partition.

01:03:10.740 --> 01:03:15.060
So this is, I think,
a important idea

01:03:15.060 --> 01:03:18.320
which, again, is
popularized by Terry Tao,

01:03:18.320 --> 01:03:20.790
that instead of looking
at things as a regularity

01:03:20.790 --> 01:03:24.950
partition, we can view
these ideas as regularity

01:03:24.950 --> 01:03:25.920
de-compositions.

01:03:34.330 --> 01:03:45.330
Namely, pick j as in the lemma
and I now write my adjacency

01:03:45.330 --> 01:03:50.620
matrix a sub g as a sum
of three matrices, which

01:03:50.620 --> 01:03:57.910
we'll call a structured plus
a small plus a pseudo random.

01:04:01.600 --> 01:04:15.310
Where a structured equals to
the sum for basically that sum,

01:04:15.310 --> 01:04:24.180
this sum here, so this
spectral de-competition

01:04:24.180 --> 01:04:27.204
but only for the first
j minus 1 eigenvalues.

01:04:35.060 --> 01:04:36.790
So those of you
coming from or who

01:04:36.790 --> 01:04:39.050
have taken classes in
something like Statistics

01:04:39.050 --> 01:04:41.820
might recognize this as a
principal component analysis.

01:04:41.820 --> 01:04:42.900
So this has many names.

01:04:42.900 --> 01:04:44.390
It's a very powerful idea.

01:04:44.390 --> 01:04:46.880
You look at the
top spectral data,

01:04:46.880 --> 01:04:49.610
and that should describe
you most of the information

01:04:49.610 --> 01:04:54.740
that you care about about a
graph or a matrix, in general.

01:04:54.740 --> 01:05:03.920
The small piece is the sum
but only for i between j

01:05:03.920 --> 01:05:04.880
and f of j.

01:05:07.510 --> 01:05:15.810
And the pseudo random piece
is for i at least f of j.

01:05:23.840 --> 01:05:24.340
OK.

01:05:24.340 --> 01:05:30.610
So we decompose this adjacency
matrix into these three pieces.

01:05:30.610 --> 01:05:32.590
And the question now
is, what does this

01:05:32.590 --> 01:05:35.830
have to do with Szemeredi's
graph regularity lemma?

01:05:35.830 --> 01:05:37.330
So what do the
individual components

01:05:37.330 --> 01:05:40.030
correspond to in the version
of the regularity lemma

01:05:40.030 --> 01:05:44.200
that you've seen and
are now familiar with?

01:05:44.200 --> 01:05:46.670
So here is what's going on.

01:05:46.670 --> 01:05:53.620
So I want to show you that
this structured piece roughly

01:05:53.620 --> 01:05:57.510
corresponds to the partition.

01:05:57.510 --> 01:06:02.648
So this is the
bounded partition.

01:06:06.000 --> 01:06:08.670
And the small piece
roughly corresponds

01:06:08.670 --> 01:06:12.495
to the small fraction
of irregular pairs.

01:06:15.370 --> 01:06:17.300
And the pseudo
random piece roughly

01:06:17.300 --> 01:06:20.510
corresponds to the idea
of pseudo randomness

01:06:20.510 --> 01:06:21.380
between pairs.

01:06:37.850 --> 01:06:41.810
First, to understand what the
spectral data have anything

01:06:41.810 --> 01:06:44.330
to do with partitions,
let me remind

01:06:44.330 --> 01:06:50.030
you a basic fact about
how the spectrum, how

01:06:50.030 --> 01:06:53.600
the eigenvalues of a
real symmetric matrix

01:06:53.600 --> 01:06:57.090
relate to other
properties of this matrix.

01:06:57.090 --> 01:07:00.800
And namely, this
notion of a spectral

01:07:00.800 --> 01:07:06.370
radius or sometimes
called spectral norm.

01:07:09.200 --> 01:07:13.160
So far I'm only going to
discuss real symmetric matrices.

01:07:13.160 --> 01:07:15.200
So many of the things
I will say are not

01:07:15.200 --> 01:07:18.110
true for if you're not
in a real symmetric case.

01:07:18.110 --> 01:07:25.200
So the spectral radius
spectral norm of a

01:07:25.200 --> 01:07:32.670
is the largest eigenvalue
of a in absolute value.

01:07:38.540 --> 01:07:42.030
And this quantity turns out
to be equal to the operator

01:07:42.030 --> 01:07:54.750
norm which is the norm of
this a as a linear operator,

01:07:54.750 --> 01:07:59.100
namely it is the max
or super, it turns out

01:07:59.100 --> 01:08:06.620
to be a max, of av over--

01:08:06.620 --> 01:08:08.450
length of av divided
by length of v.

01:08:08.450 --> 01:08:11.240
So if you hit it with a unit
vector, how far can you go?

01:08:14.110 --> 01:08:23.130
So it's also equal to
this bi-linear form.

01:08:23.130 --> 01:08:25.220
If you hit it from left
and right by unit vectors,

01:08:25.220 --> 01:08:28.750
how big can you get?

01:08:28.750 --> 01:08:31.350
So for real symmetric
matrices, these quantities

01:08:31.350 --> 01:08:32.350
are equal to each other.

01:08:32.350 --> 01:08:33.910
And that will be
an essential fact

01:08:33.910 --> 01:08:38.915
for relating the spectral data
with combinatorial quantities.

01:08:41.590 --> 01:08:42.850
All right.

01:08:42.850 --> 01:08:50.960
So if you give me
this de-composition,

01:08:50.960 --> 01:08:53.653
how can I produce
for you a partition?

01:08:56.970 --> 01:09:04.029
Basically, you can
look at a structure

01:09:04.029 --> 01:09:10.450
which has its state in
its data a bounded number

01:09:10.450 --> 01:09:13.130
of eigenvectors.

01:09:13.130 --> 01:09:24.160
And by rounding, we can
basically round these guys so

01:09:24.160 --> 01:09:27.115
when you round the
individual values by rounding

01:09:27.115 --> 01:09:28.120
the coordinate values--

01:09:31.120 --> 01:09:42.510
So let's pretend that they
take only a bounded, let's see,

01:09:42.510 --> 01:09:43.589
a small number of values.

01:09:47.609 --> 01:09:50.310
So just to simplify
things in your mind,

01:09:50.310 --> 01:09:53.715
pretend for a second--

01:09:53.715 --> 01:09:55.590
well, of course, this
is far from the truth--

01:09:55.590 --> 01:10:01.860
pretend for a second that these
guys are 0 comma 1 valued.

01:10:01.860 --> 01:10:04.140
Of course, that's not going
to be the case but 0 comma

01:10:04.140 --> 01:10:07.270
1 or plus/minus 1, if you like.

01:10:07.270 --> 01:10:07.770
OK.

01:10:07.770 --> 01:10:09.840
So this is definitely not true.

01:10:09.840 --> 01:10:13.350
But for the purpose
of exposition,

01:10:13.350 --> 01:10:14.820
let's pretend this is the case.

01:10:14.820 --> 01:10:16.770
And you can more
or less achieve it

01:10:16.770 --> 01:10:22.160
by rounding the individual
values to their nearby closest

01:10:22.160 --> 01:10:23.414
multiple of something.

01:10:26.020 --> 01:10:36.950
Then the level sets of
these top eigenvectors,

01:10:36.950 --> 01:10:44.940
they partition the vertex set
into a bounded number of parts.

01:10:54.800 --> 01:10:57.430
So if you, for example, in the
simplified version where you're

01:10:57.430 --> 01:11:00.940
only have plus minus 1
values for this eigenvectors,

01:11:00.940 --> 01:11:03.970
then you have, at
most, 2 to the j parts.

01:11:03.970 --> 01:11:06.640
But you may get some more
because some epsilons,

01:11:06.640 --> 01:11:09.190
but for the purpose
of illustration,

01:11:09.190 --> 01:11:10.480
let's not worry about that.

01:11:10.480 --> 01:11:13.330
And this is basically
the regularity partition.

01:11:13.330 --> 01:11:18.160
I want to show that this set
here has very nice properties

01:11:18.160 --> 01:11:20.890
that they basically behave
like the regularity partition

01:11:20.890 --> 01:11:24.210
we've gotten previously.

01:11:24.210 --> 01:11:29.620
So what I would like to show
is that the other two parts,

01:11:29.620 --> 01:11:32.920
they do not contribute very much
in the sense of our regularity

01:11:32.920 --> 01:11:34.420
partition.

01:11:34.420 --> 01:11:42.210
So for example, if you look
at the pseudo random piece,

01:11:42.210 --> 01:11:53.230
if I hit it left and right with
indicator vectors of vertex

01:11:53.230 --> 01:11:56.770
sets, how big can
this number get?

01:12:00.360 --> 01:12:05.420
So this number here is, at
most, while the norm of u times

01:12:05.420 --> 01:12:09.630
the norm of this
w which is just--

01:12:09.630 --> 01:12:14.930
so let me write down--

01:12:14.930 --> 01:12:18.690
so the norm of indicator
of u, norm indicator

01:12:18.690 --> 01:12:22.500
of w multiplied by
the operator norm

01:12:22.500 --> 01:12:26.990
of the pseudo random part of a.

01:12:26.990 --> 01:12:32.520
But these two guys here, so
they're, at most, root n each.

01:12:32.520 --> 01:12:38.910
So this number
here is, at most, n

01:12:38.910 --> 01:12:46.570
but we know from our hypothesis
on the pseudo random part

01:12:46.570 --> 01:12:50.970
of a that the spectral
norm is no more

01:12:50.970 --> 01:12:57.360
than this quantity over here.

01:12:57.360 --> 01:12:59.690
And by choosing f
appropriately large,

01:12:59.690 --> 01:13:02.160
I can make sure that this
number is extremely small.

01:13:10.900 --> 01:13:22.020
f to be large compared to
the number of parts in b

01:13:22.020 --> 01:13:29.220
partition so this
quantity is small.

01:13:36.670 --> 01:13:40.010
And this is basically the
notion of epsilon regularity

01:13:40.010 --> 01:13:42.710
that you saw in
the usual version

01:13:42.710 --> 01:13:44.880
or version of Szemeredi's
regularity lemma

01:13:44.880 --> 01:13:48.170
I presented in
the first version,

01:13:48.170 --> 01:13:53.170
in the very first lecture
that we discussed regularity.

01:13:53.170 --> 01:13:55.510
This quantity here
is something which

01:13:55.510 --> 01:13:58.090
measures the difference
between-- so for now,

01:13:58.090 --> 01:14:01.480
if for a second ignore
the middle piece.

01:14:01.480 --> 01:14:03.580
If you ignore the
small piece, then this

01:14:03.580 --> 01:14:07.690
is precisely the difference
between the actual densities

01:14:07.690 --> 01:14:10.570
between u and w and
the predicted density

01:14:10.570 --> 01:14:11.853
between u and w.

01:14:11.853 --> 01:14:14.070
AUDIENCE: Why is there
a square root here?

01:14:14.070 --> 01:14:16.750
PROFESSOR: The question is, why
is there a square root here?

01:14:16.750 --> 01:14:18.850
There should not be
a square root here.

01:14:18.850 --> 01:14:20.610
Good.

01:14:20.610 --> 01:14:25.120
It then becomes a
square root of the--

01:14:25.120 --> 01:14:26.650
yeah, so there is
no square root,

01:14:26.650 --> 01:14:30.860
but the length of this vector
is the square root of the size

01:14:30.860 --> 01:14:32.250
of u which is, at most, n.

01:14:35.512 --> 01:14:36.444
Yeah.

01:14:36.444 --> 01:14:39.246
AUDIENCE: Did you say to be
small in general or just small

01:14:39.246 --> 01:14:40.995
like you compare
it to f squared?

01:14:40.995 --> 01:14:42.495
Because I guess,
isn't it like going

01:14:42.495 --> 01:14:44.795
to be this constant function
that you choose before

01:14:44.795 --> 01:14:46.220
[INAUDIBLE]?

01:14:46.220 --> 01:14:47.020
PROFESSOR: OK.

01:14:47.020 --> 01:14:53.450
Question is do, how small do
we want this f of j to be?

01:14:53.450 --> 01:14:59.210
So I want this quantity to
be quite a bit smaller than,

01:14:59.210 --> 01:14:59.780
let's say--

01:14:59.780 --> 01:15:02.780
so basically, I want it to
be less than f of n squared,

01:15:02.780 --> 01:15:09.220
but f of n over the
number of parts square.

01:15:09.220 --> 01:15:15.250
Because this quantity is, let's
say, the sizes of each part.

01:15:15.250 --> 01:15:18.730
So let me just be not precise
and say much less than.

01:15:18.730 --> 01:15:21.580
So this quantity here is
the size of each part.

01:15:21.580 --> 01:15:25.030
And I want to think about
the case when u and w

01:15:25.030 --> 01:15:27.730
they lie inside each part.

01:15:27.730 --> 01:15:29.350
In which case, I
want the difference

01:15:29.350 --> 01:15:32.650
to be much less than epsilon
times the size of the part

01:15:32.650 --> 01:15:33.260
squared.

01:15:33.260 --> 01:15:33.760
Yeah.

01:15:33.760 --> 01:15:37.957
AUDIENCE: [INAUDIBLE] j is
different based on the graph?

01:15:37.957 --> 01:15:40.540
PROFESSOR: Question is, is the
j different based on the graph?

01:15:40.540 --> 01:15:41.040
Yes.

01:15:41.040 --> 01:15:43.950
And that's also the case for
Szemeredi's regularity lemma.

01:15:43.950 --> 01:15:48.657
In Szemeredi's regularity lemma,
you don't know when you stop.

01:15:48.657 --> 01:15:50.740
But you know that you stop
before a certain point.

01:15:56.692 --> 01:15:59.670
OK.

01:15:59.670 --> 01:16:04.390
And finally, what's happening
with a small part of a.

01:16:04.390 --> 01:16:14.930
So in a small, the sum of
the squares of entries--

01:16:17.940 --> 01:16:19.448
so this also has
a convenient name.

01:16:19.448 --> 01:16:20.990
It's called the
Hilbert-Schmidt norm.

01:16:24.190 --> 01:16:26.065
So the sum of the
squares of the entries.

01:16:29.010 --> 01:16:31.200
We basically saw this
calculation earlier,

01:16:31.200 --> 01:16:36.190
it's the sum of the squares of
the eigenvalues in which case

01:16:36.190 --> 01:16:38.210
we've truncated all
the other eigenvalues.

01:16:38.210 --> 01:16:44.490
So the only eigenvalues left are
between j and index between j

01:16:44.490 --> 01:16:45.700
and f of j.

01:16:45.700 --> 01:16:48.355
And we chose j so that
this number is small.

01:16:52.600 --> 01:16:57.900
So a small as in
a bunch of noise,

01:16:57.900 --> 01:17:00.570
but no adversarial
noise, if you will,

01:17:00.570 --> 01:17:03.300
into your graph, but
only a very small amount,

01:17:03.300 --> 01:17:05.610
at most, epsilon amount.

01:17:05.610 --> 01:17:18.830
So it might destroy the epsilon
regularity for, let's say,

01:17:18.830 --> 01:17:23.720
around epsilon
fraction of pairs.

01:17:26.420 --> 01:17:29.940
But that's all it could do.

01:17:29.940 --> 01:17:32.430
So all but epsilon
fraction of your pairs

01:17:32.430 --> 01:17:34.620
will still be epsilon regular.

01:17:34.620 --> 01:17:37.530
And that is the consequence of
Szemeredi's graph regularity

01:17:37.530 --> 01:17:39.650
lemma that we saw earlier.

01:17:39.650 --> 01:17:40.330
Yeah.

01:17:40.330 --> 01:17:43.157
AUDIENCE: Doesn't large
F have to be special?

01:17:43.157 --> 01:17:43.740
PROFESSOR: OK.

01:17:43.740 --> 01:17:45.698
So question, does a large
F have to be special?

01:17:45.698 --> 01:17:47.250
The F should be chosen--

01:17:47.250 --> 01:17:49.740
if you want to achieve
Szemeredi's graph regularity

01:17:49.740 --> 01:17:55.260
lemma, you should find this F so
that basically this inequality

01:17:55.260 --> 01:17:56.600
is true.

01:17:56.600 --> 01:17:58.860
So f should be
quite a bit larger

01:17:58.860 --> 01:18:01.850
than the number of parts.

01:18:01.850 --> 01:18:04.940
But if you choose even
bigger values of f,

01:18:04.940 --> 01:18:07.650
you can achieve more regularity.

01:18:07.650 --> 01:18:09.260
And this is akin
to what happened

01:18:09.260 --> 01:18:10.950
with strong regularity.

01:18:10.950 --> 01:18:14.600
So there's this idea if you
iterate one version regularity,

01:18:14.600 --> 01:18:16.400
you can get a strong
version of regularity.

01:18:16.400 --> 01:18:18.450
And there's some iteration
happening over there.

01:18:18.450 --> 01:18:22.520
So if you choose your f to be
a much bigger function of j,

01:18:22.520 --> 01:18:25.610
you can achieve a much
stronger notion irregularity

01:18:25.610 --> 01:18:29.180
which is similar
and, perhaps, even

01:18:29.180 --> 01:18:33.470
equivalent to strong regularity
that we discussed last time.

01:18:33.470 --> 01:18:36.230
So you get to choose what
f you want to put in here.

01:18:36.230 --> 01:18:37.140
Yeah.

01:18:37.140 --> 01:18:41.240
AUDIENCE: How do you
make it equitable?

01:18:41.240 --> 01:18:45.330
PROFESSOR: The question is,
how do you make it equitable?

01:18:45.330 --> 01:18:45.830
OK.

01:18:45.830 --> 01:18:48.570
So let me now discuss that.

01:18:48.570 --> 01:18:52.910
So in this case, you can
also do very similar things

01:18:52.910 --> 01:18:56.960
to what we've done before,
but to massage the partitions.

01:18:56.960 --> 01:19:01.160
It's not entirely clear
from this formulation.

01:19:01.160 --> 01:19:02.680
But the message
here is that there's

01:19:02.680 --> 01:19:06.440
this equivalence between
operator norm on one hand

01:19:06.440 --> 01:19:08.930
and combinatorial discrepancy
on the other hand.

01:19:08.930 --> 01:19:10.700
And we'll explore
this notion further

01:19:10.700 --> 01:19:13.120
in the next several lectures.