WEBVTT

00:00:00.090 --> 00:00:02.490
The following content is
provided under a Creative

00:00:02.490 --> 00:00:04.030
Commons license.

00:00:04.030 --> 00:00:06.360
Your support will help
MIT OpenCourseWare

00:00:06.360 --> 00:00:10.720
continue to offer high quality
educational resources for free.

00:00:10.720 --> 00:00:13.320
To make a donation or
view additional materials

00:00:13.320 --> 00:00:17.280
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:17.280 --> 00:00:18.450
at ocw.mit.edu.

00:00:20.877 --> 00:00:22.960
ERIK DEMAINE: All right,
today is our last lecture

00:00:22.960 --> 00:00:24.820
on the predecessor problem.

00:00:24.820 --> 00:00:27.400
And we did two lectures
on upper bound,

00:00:27.400 --> 00:00:31.000
started with Van Emde
Boas x-Fast, y-Fast tries.

00:00:31.000 --> 00:00:34.264
And then fusion
tries was last class.

00:00:34.264 --> 00:00:36.430
This time, we're going to
show that those bounds are

00:00:36.430 --> 00:00:38.170
essentially optimal.

00:00:38.170 --> 00:00:41.020
And we're going to
prove this bound.

00:00:41.020 --> 00:00:44.160
So we currently
have min of log w,

00:00:44.160 --> 00:00:46.600
and log base w of n
as an upper bound.

00:00:46.600 --> 00:00:48.625
That was Van Emde
Boas and fusion tires.

00:00:48.625 --> 00:00:50.750
Were going to prove an
almost matching lower bound.

00:00:50.750 --> 00:00:53.230
There's a log log w factor here.

00:00:53.230 --> 00:00:57.010
And this bound holds for even
a static predecessor data

00:00:57.010 --> 00:01:00.100
structure no updates, as
long as the data structure

00:01:00.100 --> 00:01:02.620
has polynomial
space, which if you

00:01:02.620 --> 00:01:04.510
have any hope of making
something dynamic,

00:01:04.510 --> 00:01:06.400
you definitely want
polynomial space.

00:01:06.400 --> 00:01:09.280
Now, it's known that this
log log factor is not real

00:01:09.280 --> 00:01:12.100
if you have, essentially,
linear space or n log n space,

00:01:12.100 --> 00:01:13.497
or something like that.

00:01:13.497 --> 00:01:14.830
But that's much harder to prove.

00:01:14.830 --> 00:01:18.490
So this is where we will end
our coverage of predecessor.

00:01:18.490 --> 00:01:21.160
But I'm going to start out with
an overview of what's known.

00:01:21.160 --> 00:01:24.641
And proving this lower bound
is actually pretty cool,

00:01:24.641 --> 00:01:26.890
because at this point,
especially seeing fusion tries,

00:01:26.890 --> 00:01:31.810
you might think bit tricks
are kind of all about magic,

00:01:31.810 --> 00:01:33.580
and doing crazy
things, and using

00:01:33.580 --> 00:01:36.670
these operations in bizarre
ways they weren't intended.

00:01:36.670 --> 00:01:42.070
Bit tricks are necessary
as an upper bound tool,

00:01:42.070 --> 00:01:45.520
given our weird
historical precedent,

00:01:45.520 --> 00:01:47.860
which is computers are
built to do arithmetic.

00:01:47.860 --> 00:01:50.380
And why not do bit
operations too?

00:01:50.380 --> 00:01:53.844
And so it's sort of an
artifact of computers

00:01:53.844 --> 00:01:54.760
as we know them today.

00:01:54.760 --> 00:01:56.635
That's why we have the
word RAM, because it's

00:01:56.635 --> 00:02:01.360
based on C, because it's based
on computers that exist now.

00:02:01.360 --> 00:02:04.030
And that's legitimate in that
that's the computers we have.

00:02:04.030 --> 00:02:05.560
So we might as well
try to use them.

00:02:05.560 --> 00:02:09.220
The lower bounds, on the
other hand, are, in some ways,

00:02:09.220 --> 00:02:12.670
more beautiful, because they're
just about how much information

00:02:12.670 --> 00:02:17.930
has to go between Alice
and Bob, actually.

00:02:17.930 --> 00:02:19.930
This is the communication
complexity perspective

00:02:19.930 --> 00:02:21.280
of data structures.

00:02:21.280 --> 00:02:22.120
And it's nice.

00:02:22.120 --> 00:02:25.930
We will get, in some ways,
cleaner, simpler arguments

00:02:25.930 --> 00:02:28.569
because in lower bound land, you
don't have to worry about, oh,

00:02:28.569 --> 00:02:30.610
can I use this operation
or this operation to do?

00:02:30.610 --> 00:02:33.179
It's just, is there any
operation at all to do it?

00:02:33.179 --> 00:02:34.720
And then it's just
about information.

00:02:34.720 --> 00:02:36.430
It's just about
information theory.

00:02:36.430 --> 00:02:38.470
And it's, in some ways, cleaner.

00:02:38.470 --> 00:02:41.140
And this proof, in
particular using a technique

00:02:41.140 --> 00:02:44.140
called round elimination
is actually simple.

00:02:44.140 --> 00:02:47.230
And we will see in it the
concepts of Van Emde Boas

00:02:47.230 --> 00:02:50.830
and fusion tries again,
but without having

00:02:50.830 --> 00:02:55.030
to do any bit tricks to
actually make them happen.

00:02:55.030 --> 00:02:57.880
So if you felt like all of these
bit tricks are kind of weird,

00:02:57.880 --> 00:02:59.730
what is the real
essence of the problem?

00:02:59.730 --> 00:03:01.409
This, in some sense,
provides an answer.

00:03:01.409 --> 00:03:03.700
And it says, well, really
those were the right answers.

00:03:03.700 --> 00:03:06.490
That's what you should
have been getting.

00:03:06.490 --> 00:03:10.495
So let me start with the survey.

00:03:10.495 --> 00:03:11.995
This is more of a
historical survey.

00:03:16.900 --> 00:03:20.510
So the first bound for
predecessor problem,

00:03:20.510 --> 00:03:25.630
first lower bound,
Asvab Atjai in 1988.

00:03:25.630 --> 00:03:30.590
And he proved that
for every word size,

00:03:30.590 --> 00:03:37.560
there's a problem size such that
there is an omega square root

00:03:37.560 --> 00:03:40.420
log w lower bound.

00:03:40.420 --> 00:03:41.890
You can compare
that with this one.

00:03:41.890 --> 00:03:45.440
This is log w over log log w.

00:03:45.440 --> 00:03:48.580
Now, this is a bound that works
for all values of n and w,

00:03:48.580 --> 00:03:50.320
which is more interesting.

00:03:50.320 --> 00:03:51.790
Here, we're saying
that if you just

00:03:51.790 --> 00:03:56.710
want a bound in terms of w, you
need at least square root log

00:03:56.710 --> 00:03:59.260
w.

00:03:59.260 --> 00:04:01.020
The original paper
claimed log w.

00:04:01.020 --> 00:04:03.530
But that's not true, in general.

00:04:03.530 --> 00:04:06.250
But the proof actually
establishes square root log w.

00:04:06.250 --> 00:04:07.630
And in this proof
is complicated.

00:04:07.630 --> 00:04:08.530
It was the first one.

00:04:08.530 --> 00:04:10.010
It got the field started.

00:04:10.010 --> 00:04:11.620
There's been many
papers since then.

00:04:11.620 --> 00:04:16.150
Next one was by Miltersen,
who we've cited a few times.

00:04:21.260 --> 00:04:22.900
And so this a few years later.

00:04:22.900 --> 00:04:24.940
Miltersen, essentially,
took the same proof,

00:04:24.940 --> 00:04:27.462
presented it in a
more coherent way,

00:04:27.462 --> 00:04:29.920
and sort of really got to the
essence of what the proof was

00:04:29.920 --> 00:04:39.250
showing, and could prove
the same bound, but also

00:04:39.250 --> 00:04:41.380
a complementary
bound, because there

00:04:41.380 --> 00:04:44.980
is a symmetry between word size
and problem size, which we will

00:04:44.980 --> 00:04:48.730
see when we get to the
communication complexity

00:04:48.730 --> 00:04:52.840
perspective, which will
be right after the survey.

00:04:52.840 --> 00:04:54.628
It's not completely symmetric.

00:04:58.120 --> 00:05:00.280
But Miltersen proved
this lower bound

00:05:00.280 --> 00:05:01.930
that for every
problem size, there

00:05:01.930 --> 00:05:03.790
is a machine, there
is a word size,

00:05:03.790 --> 00:05:06.430
where you need omega
cube root of log n.

00:05:06.430 --> 00:05:09.160
Now, we know how to get
order square root of log n

00:05:09.160 --> 00:05:11.470
by taking them in.

00:05:11.470 --> 00:05:16.070
So this is matching
up to the exponent.

00:05:16.070 --> 00:05:18.490
And we know how to do
log w by Van Emde Boas.

00:05:18.490 --> 00:05:20.540
So this is matching
up to the exponent.

00:05:20.540 --> 00:05:22.230
So it's progress.

00:05:22.230 --> 00:05:24.540
But it doesn't give a complete
picture of w versus n.

00:05:27.390 --> 00:05:29.790
And this is the paper that
introduced this communication

00:05:29.790 --> 00:05:32.490
complexity idea, that this
would be a useful concept

00:05:32.490 --> 00:05:35.000
for predecessor lower bounds.

00:05:40.690 --> 00:05:50.172
Then the following year,
there's a bit of a breakthrough.

00:05:56.310 --> 00:06:00.610
So here, we've got the idea of
using communication complexity.

00:06:00.610 --> 00:06:05.263
Then Miltersen and others had
the idea of round elimination.

00:06:13.480 --> 00:06:15.720
And so just treat that
as a black box for now.

00:06:15.720 --> 00:06:18.330
We'll explain what
is in a moment.

00:06:18.330 --> 00:06:21.551
But it proved exactly the same
result, but in a clean way,

00:06:21.551 --> 00:06:22.550
using round elimination.

00:06:22.550 --> 00:06:23.810
These proofs were messy.

00:06:23.810 --> 00:06:26.510
This proof is pretty simple.

00:06:26.510 --> 00:06:32.420
And it's the beginning of
the proof that we will cover.

00:06:32.420 --> 00:06:36.140
But we're going to prove
stronger bounds than this.

00:06:36.140 --> 00:06:38.010
Let me go to another board.

00:06:38.010 --> 00:06:42.230
Next, we have Beame and Fich.

00:06:42.230 --> 00:06:43.730
Now is about the
time when I entered

00:06:43.730 --> 00:06:44.938
the world of data structures.

00:06:44.938 --> 00:06:53.810
And so Beame and Fich came out
just when I was starting, 1999.

00:06:53.810 --> 00:07:00.100
And this proved for
all of w exists an n.

00:07:08.510 --> 00:07:11.210
So log w over log log w,
which is the same thing we're

00:07:11.210 --> 00:07:12.920
going to prove.

00:07:12.920 --> 00:07:16.220
Except this only proves
it for certain values

00:07:16.220 --> 00:07:20.180
of n, whereas we're going to
prove it for all values of n

00:07:20.180 --> 00:07:23.690
where this is smaller than that.

00:07:23.690 --> 00:07:28.920
So this is a bit more special,
not covering the whole w n

00:07:28.920 --> 00:07:31.010
trade-off.

00:07:31.010 --> 00:07:33.440
And then there's this
symmetric version.

00:07:40.720 --> 00:07:43.720
There's going to be a
lot of log log n's today.

00:07:43.720 --> 00:07:48.090
So get ready for
log logs in general.

00:07:48.090 --> 00:07:53.570
So for every n,
there exists a w such

00:07:53.570 --> 00:07:56.012
that there's an omega
root log n over log log n.

00:07:56.012 --> 00:07:59.150
So this is almost matching
the min of fusion tries

00:07:59.150 --> 00:08:03.680
and Van Emde Boas, which
is order square root log n.

00:08:03.680 --> 00:08:08.130
They also proved-- did a little
bit on the upper bound side.

00:08:08.130 --> 00:08:12.810
And they found a static
data structure achieving--

00:08:25.200 --> 00:08:28.260
I'm just going to cheat
and use some arrows.

00:08:28.260 --> 00:08:31.500
So they achieved the
min of these two.

00:08:31.500 --> 00:08:33.510
So in a certain
sense, this is tight.

00:08:33.510 --> 00:08:35.909
This is not exactly
what you'd want,

00:08:35.909 --> 00:08:37.500
because the data
structures achieving

00:08:37.500 --> 00:08:40.909
the min, what you'd like is
a lower bound of the min.

00:08:40.909 --> 00:08:41.909
And it's not quite that.

00:08:41.909 --> 00:08:45.660
It's saying, there's a
particular pair of w and n

00:08:45.660 --> 00:08:47.310
values where this is optimal.

00:08:47.310 --> 00:08:49.044
There's another pair
of n and w values

00:08:49.044 --> 00:08:50.460
where this is
optimal with respect

00:08:50.460 --> 00:08:53.080
to either one of
these parameters.

00:08:53.080 --> 00:08:55.380
But there are other pairs
of values of w and n

00:08:55.380 --> 00:08:57.540
where it may or
may not be optimal.

00:08:57.540 --> 00:09:01.560
So it's kind of
matching, but not quite.

00:09:01.560 --> 00:09:04.960
The way to say it is
these are the best--

00:09:04.960 --> 00:09:07.760
this is the best pure w
bound you could hope for.

00:09:07.760 --> 00:09:11.150
And this is the best pure n
bound that you could hope for.

00:09:11.150 --> 00:09:15.620
And it's the best mix
of two pure bounds.

00:09:15.620 --> 00:09:21.720
OK, then we go back in time
a little bit, which is--

00:09:21.720 --> 00:09:24.390
there's a funny
chain here, which

00:09:24.390 --> 00:09:31.620
is this paper, 1995, cites
this paper from 1999.

00:09:31.620 --> 00:09:35.940
This paper cites
this paper of 1992.

00:09:35.940 --> 00:09:37.880
That's normal.

00:09:37.880 --> 00:09:40.750
This one's a little bit out
of order, bit of time travel.

00:09:40.750 --> 00:09:43.110
So I think this one was
in draft for a long time,

00:09:43.110 --> 00:09:46.200
and took awhile to finish it.

00:09:46.200 --> 00:09:48.850
But this paper is before 1995.

00:09:48.850 --> 00:09:51.120
It's not cited by this paper.

00:09:51.120 --> 00:09:53.010
What's interesting
about it is it

00:09:53.010 --> 00:09:55.560
proves both of
these lower bounds.

00:09:58.980 --> 00:10:01.260
I don't think it proves
the upper bounds.

00:10:04.350 --> 00:10:07.650
But these were independently
discovered by Beame and Fich

00:10:07.650 --> 00:10:09.810
in the future of this paper.

00:10:09.810 --> 00:10:11.160
This is actually a PhD thesis.

00:10:11.160 --> 00:10:14.310
So I think it was lost, or not
known about, for a long time.

00:10:14.310 --> 00:10:17.130
But it ends up establishing
the same bounds.

00:10:17.130 --> 00:10:18.720
And then the real
contribution here

00:10:18.720 --> 00:10:20.970
was that those were
the best bounds that

00:10:20.970 --> 00:10:24.180
are purely w and n, but
still not a full story.

00:10:28.320 --> 00:10:40.440
Next up is Sen in 2003, which
gave a round elimination

00:10:40.440 --> 00:10:50.060
proof of the same result.
And whereas both the Beame

00:10:50.060 --> 00:10:53.560
and Fich and the
Xiau proof are messy,

00:10:53.560 --> 00:10:56.470
this round elimination proof
is very slick and clean.

00:10:56.470 --> 00:10:59.860
And this is the proof
we're going to cover today.

00:10:59.860 --> 00:11:01.450
And it's going to prove--

00:11:01.450 --> 00:11:05.800
it will imply these pure
w and n lower bounds.

00:11:05.800 --> 00:11:09.070
But it will, in some
sense, in a stronger way,

00:11:09.070 --> 00:11:12.160
prove this bound, which
is a real min thing.

00:11:12.160 --> 00:11:15.130
And so we're just off
by this log log factor.

00:11:15.130 --> 00:11:19.120
Otherwise, we were proving
exactly the min of fusion tries

00:11:19.120 --> 00:11:21.280
in Van Emde Boas is optimal.

00:11:21.280 --> 00:11:24.190
Getting rid of the log
log factor is messier.

00:11:24.190 --> 00:11:27.140
That's the next paper,
which is the last--

00:11:27.140 --> 00:11:29.582
it's two papers, actually.

00:11:29.582 --> 00:11:32.850
But the final
papers in this story

00:11:32.850 --> 00:11:41.620
are by Patrascu and Thorup, 2006
and 2007 are the two papers.

00:11:41.620 --> 00:11:51.150
And they give a tight n, versus
w, versus space trade-off.

00:11:57.040 --> 00:11:59.320
I think I can fit
it on this board.

00:11:59.320 --> 00:11:59.890
Let's try.

00:12:02.990 --> 00:12:06.770
So it's the min of four terms.

00:12:06.770 --> 00:12:10.470
The first term is
nice, log base w of n.

00:12:10.470 --> 00:12:14.260
That's our good
friend fusion tries.

00:12:14.260 --> 00:12:19.960
The next one is roughly log w.

00:12:24.660 --> 00:12:26.357
So it's roughly Van Emde Boas.

00:12:26.357 --> 00:12:28.190
But it turns out you
can do a little better.

00:12:28.190 --> 00:12:31.140
Now, I need to define a.

00:12:31.140 --> 00:12:38.010
I'm going to assume here the
space is n, times 2 to the a.

00:12:38.010 --> 00:12:39.390
That's the notation they use.

00:12:39.390 --> 00:12:41.130
It's a little bit weird.

00:12:41.130 --> 00:12:44.010
But it makes this bound a
little bit easier to state.

00:12:44.010 --> 00:12:46.590
So, of course, you need linear
space to store the data.

00:12:46.590 --> 00:12:48.720
This is measuring words.

00:12:48.720 --> 00:12:51.030
So the question is, how
much more than linear space

00:12:51.030 --> 00:12:52.740
do you have?

00:12:52.740 --> 00:12:58.800
If you want poly log update,
then this better be poly log n.

00:12:58.800 --> 00:13:02.380
This is about static
data structures.

00:13:02.380 --> 00:13:06.420
So if you want poly log n,
then a to be order log log n.

00:13:06.420 --> 00:13:08.299
So you can think of a
being order log log n.

00:13:08.299 --> 00:13:09.840
Or you can think of
a being constant,

00:13:09.840 --> 00:13:11.520
if you want linear space.

00:13:11.520 --> 00:13:12.660
Then this stuff--

00:13:12.660 --> 00:13:15.950
I mean, then this
is basically log w.

00:13:15.950 --> 00:13:20.550
OK, so that's roughly
Van Emde Boas.

00:13:20.550 --> 00:13:22.815
Next, we get two crazier terms.

00:14:10.030 --> 00:14:14.220
These ones, I can't give you
intuition for, I'm afraid.

00:14:14.220 --> 00:14:15.890
You see, there's the log w.

00:14:15.890 --> 00:14:20.340
They're, again, versions of
Van Emde Boas like things.

00:14:20.340 --> 00:14:24.480
But they're improving by some
factors that make you better

00:14:24.480 --> 00:14:26.220
when a is large.

00:14:26.220 --> 00:14:28.410
And, in particular--

00:14:28.410 --> 00:14:31.920
I mean, they should match
this thing at some points.

00:14:31.920 --> 00:14:35.760
So when a is order
log n, that's when

00:14:35.760 --> 00:14:37.470
you have polynomial
space, which would

00:14:37.470 --> 00:14:39.710
be very difficult to update.

00:14:39.710 --> 00:14:41.510
You need polynomial
time updates.

00:14:41.510 --> 00:14:44.920
But that's what this
static data structure

00:14:44.920 --> 00:14:47.560
uses polynomial space.

00:14:47.560 --> 00:14:51.600
And so with polynomial space,
if you plug in a equals log n,

00:14:51.600 --> 00:14:55.000
you end up improving
by a log log n factor.

00:14:55.000 --> 00:14:57.360
And so that's where we are.

00:14:57.360 --> 00:15:00.270
I guess there should be a
square root in there somewhere.

00:15:00.270 --> 00:15:02.670
This is a log w over log log w.

00:15:02.670 --> 00:15:05.720
This is the part that diverges
a little bit from Van Emde Boas.

00:15:05.720 --> 00:15:08.700
And this really is achievable
if you have polynomial space,

00:15:08.700 --> 00:15:12.330
instead of like n or n poly log.

00:15:12.330 --> 00:15:14.970
So for static, yes, you can get
these log log n improvements.

00:15:14.970 --> 00:15:17.040
We won't cover them here.

00:15:17.040 --> 00:15:19.380
I mean, they're
small improvements.

00:15:19.380 --> 00:15:22.650
And we're mostly interested
in dynamic data structures.

00:15:22.650 --> 00:15:26.730
And for dynamic data
structures, these things

00:15:26.730 --> 00:15:28.950
don't turn out to help.

00:15:28.950 --> 00:15:31.440
So let me state the
consequence, which

00:15:31.440 --> 00:15:34.080
is what I stated a
couple of classes ago.

00:15:34.080 --> 00:15:36.590
If you have n poly
log n space, so if you

00:15:36.590 --> 00:15:38.550
have polylogarithmic
update, which is usually

00:15:38.550 --> 00:15:42.270
the situation we
care about, then we

00:15:42.270 --> 00:15:49.200
get min of log base w of n.

00:15:49.200 --> 00:15:51.420
And I'd like to say log
w, but you can actually

00:15:51.420 --> 00:15:52.290
do slightly better.

00:16:03.380 --> 00:16:05.820
Oh God, indeed.

00:16:05.820 --> 00:16:08.820
I think I stated this
two lectures ago.

00:16:08.820 --> 00:16:10.980
So log w is what
we know how to do.

00:16:10.980 --> 00:16:13.630
This is a very
small improvement.

00:16:13.630 --> 00:16:16.330
Let me state another
consequence of this,

00:16:16.330 --> 00:16:31.150
which is Van Emde Boas is
optimal for w poly log n.

00:16:34.320 --> 00:16:37.180
OK, that's when it gets
log log n performance.

00:16:37.180 --> 00:16:39.390
And so you can
check in that case

00:16:39.390 --> 00:16:41.550
this term doesn't
buy you anything.

00:16:41.550 --> 00:16:45.330
That's log w would be order
log log n, if you're poly log.

00:16:45.330 --> 00:16:46.260
So this disappears.

00:16:46.260 --> 00:16:48.120
And you just get log w.

00:16:48.120 --> 00:16:50.430
So that's one thing.

00:16:50.430 --> 00:17:07.740
Fusion tries are optimal for
log w order square root log n,

00:17:07.740 --> 00:17:09.248
times log log n.

00:17:13.589 --> 00:17:17.530
So this is saying w is at
least 2 to the root log n,

00:17:17.530 --> 00:17:19.460
or actually log n
to the root log n,

00:17:19.460 --> 00:17:21.910
if you want to
include that term.

00:17:21.910 --> 00:17:23.220
So that's a fairly large w.

00:17:23.220 --> 00:17:25.290
So for large w, fusion
tires are optimal.

00:17:25.290 --> 00:17:27.130
For small w, Van
Emde Boas is optimal.

00:17:27.130 --> 00:17:31.660
In between, this
thing is optimal.

00:17:31.660 --> 00:17:33.270
That's the reality of it.

00:17:33.270 --> 00:17:34.420
It's a little messy.

00:17:34.420 --> 00:17:35.590
But this is tight.

00:17:35.590 --> 00:17:38.620
So this is the right answer.

00:17:38.620 --> 00:17:41.530
I think most of the time, the
situations you care about,

00:17:41.530 --> 00:17:42.980
w is probably going to be small.

00:17:42.980 --> 00:17:44.313
So you should use Van Emde Boas.

00:17:44.313 --> 00:17:47.380
If w really large, you
should use fusion tries.

00:17:47.380 --> 00:17:49.420
In between, you could
consider this trade-off.

00:17:49.420 --> 00:17:53.950
But it's not it's not much
better than Van Emde Boas.

00:17:53.950 --> 00:17:56.530
So take that for what it is.

00:17:56.530 --> 00:17:57.970
So that's what's known.

00:17:57.970 --> 00:18:00.360
As I said, we're going
to cover this sen

00:18:00.360 --> 00:18:03.610
proof, which will be tight
up to this log log factor.

00:18:06.220 --> 00:18:09.860
And this holds as long as
your space is polynomial.

00:18:09.860 --> 00:18:14.314
So it doesn't assume
very much about space.

00:18:14.314 --> 00:18:15.730
And it's going to
be a lower bound

00:18:15.730 --> 00:18:18.844
for a slightly easier problem.

00:18:18.844 --> 00:18:21.010
If you can prove a lower
bound on an easier problem,

00:18:21.010 --> 00:18:23.200
that implies a lower bound
on the harder problem.

00:18:27.529 --> 00:18:32.040
The easier problem is called
the colored predecessor problem.

00:18:32.040 --> 00:18:40.730
In colored predecessor,
each element is red or blue.

00:18:44.540 --> 00:18:50.930
I should say blue, whatever,
anyway, no Team Fortress fans,

00:18:50.930 --> 00:18:52.670
I guess.

00:18:52.670 --> 00:19:05.780
So and now a query just reports
the color of the predecessor.

00:19:08.690 --> 00:19:11.270
You don't need to
report the key value,

00:19:11.270 --> 00:19:13.310
just whether it is red or blue.

00:19:13.310 --> 00:19:16.400
So, again, you've
got your universe.

00:19:16.400 --> 00:19:19.880
Some of the items are present.

00:19:19.880 --> 00:19:23.000
And your query is an
arbitrary universe element.

00:19:23.000 --> 00:19:26.030
And you want a predecessor
kind of this way.

00:19:26.030 --> 00:19:28.130
And this guy is
either red or blue.

00:19:28.130 --> 00:19:31.070
And you just want to
report which one is it?

00:19:31.070 --> 00:19:33.620
So this is, of course,
easier, because you could just

00:19:33.620 --> 00:19:36.332
have a lookup table, a
hash table for example,

00:19:36.332 --> 00:19:38.540
that once you determine the
key, you could figure out

00:19:38.540 --> 00:19:39.220
whether it's red or blue.

00:19:39.220 --> 00:19:40.610
It's a static data structure.

00:19:40.610 --> 00:19:44.305
So you could use
perfect hashing.

00:19:44.305 --> 00:19:45.680
It could be totally
deterministic

00:19:45.680 --> 00:19:48.560
once you set it up.

00:19:48.560 --> 00:19:50.900
And so, OK, just reporting
the colors, of course

00:19:50.900 --> 00:19:52.954
easier than reporting the key.

00:19:52.954 --> 00:19:54.620
One interesting thing
about this problem

00:19:54.620 --> 00:19:57.260
is it's very easy to get a
correct answer with probability

00:19:57.260 --> 00:19:58.640
50%.

00:19:58.640 --> 00:20:00.110
You flip a coin.

00:20:00.110 --> 00:20:03.200
Heads is red, tails is blue.

00:20:03.200 --> 00:20:06.110
And this lower bound
we proved will actually

00:20:06.110 --> 00:20:07.850
apply to randomized
data structures that

00:20:07.850 --> 00:20:09.530
work with some probability.

00:20:09.530 --> 00:20:12.400
But it has to be probability
greater than half

00:20:12.400 --> 00:20:14.720
of succeeding.

00:20:14.720 --> 00:20:16.430
And this will be
useful, because we're

00:20:16.430 --> 00:20:17.513
going to take our problem.

00:20:17.513 --> 00:20:19.610
And we're going to
modify it a little bit,

00:20:19.610 --> 00:20:21.710
and sort of make it
simpler and simpler.

00:20:21.710 --> 00:20:23.810
And it's easier to preserve
this color property

00:20:23.810 --> 00:20:27.750
than to preserve
actual key values.

00:20:27.750 --> 00:20:30.170
So that's the
colored predecessor.

00:20:30.170 --> 00:20:35.850
Now, we get to communication
complexity perspective.

00:20:35.850 --> 00:20:37.860
So this is pretty cool.

00:20:41.740 --> 00:20:44.188
And it brings us to the
idea of round elimination.

00:20:47.680 --> 00:21:10.190
So communication--
OK, why don't I

00:21:10.190 --> 00:21:13.700
tell you the generic
communication complexity

00:21:13.700 --> 00:21:15.800
picture out of Alice and Bob?

00:21:15.800 --> 00:21:18.560
And then I'll tell you how it
relates to data structures.

00:21:18.560 --> 00:21:19.790
They're both simple.

00:21:19.790 --> 00:21:22.850
But I've got to do
something first.

00:21:22.850 --> 00:21:31.370
So let's say Alice is one
person who knows some value x.

00:21:31.370 --> 00:21:38.480
Over here, we have Bob
who knows some value y.

00:21:38.480 --> 00:21:44.994
The goal is to--

00:21:44.994 --> 00:21:46.660
I'll move this a
little bit to the left.

00:21:50.310 --> 00:21:53.400
Their collective goal--
they're trying to cooperate.

00:21:53.400 --> 00:21:57.570
Their goal is to compute
some function of x and y.

00:21:57.570 --> 00:22:01.604
The trouble is only Alice
knows x, and only Bob knows y.

00:22:01.604 --> 00:22:02.520
So how did they do it?

00:22:02.520 --> 00:22:03.990
They have to talk to each other.

00:22:03.990 --> 00:22:07.650
Ideally, Alice sends Bob
x, or Bob sends Alice y.

00:22:07.650 --> 00:22:10.110
One of them could compute
it, send back the answer.

00:22:10.110 --> 00:22:11.080
That's one possibility.

00:22:11.080 --> 00:22:12.496
But maybe x and y
are kind of big,

00:22:12.496 --> 00:22:15.030
and you can't just
send it in one message.

00:22:15.030 --> 00:22:18.750
So here's the restriction.

00:22:18.750 --> 00:22:24.960
Alice can send messages with
less than or equal to little

00:22:24.960 --> 00:22:28.120
a bits, a for Alice.

00:22:28.120 --> 00:22:34.860
Bob can send messages with
less than or equal to little b

00:22:34.860 --> 00:22:38.019
bits, b for Bob.

00:22:38.019 --> 00:22:39.060
So that's the constraint.

00:22:39.060 --> 00:22:41.930
And, potentially, x
is much larger than a,

00:22:41.930 --> 00:22:44.300
and/or y is much longer than b.

00:22:44.300 --> 00:22:47.510
And so you're going to have
to spend many messages.

00:22:47.510 --> 00:22:51.170
And let's restrict to protocols
of the form Alice talks

00:22:51.170 --> 00:22:54.350
to Bob, then Bob talks to Alice,
and back and forth, so rounds

00:22:54.350 --> 00:22:55.295
of communication.

00:22:55.295 --> 00:22:57.170
And you want to know,
how many rounds does it

00:22:57.170 --> 00:22:59.660
take to compute f of xy?

00:22:59.660 --> 00:23:02.300
And, of course, it depends on f.

00:23:02.300 --> 00:23:04.790
OK, that's the totally
generic picture.

00:23:04.790 --> 00:23:06.710
And there's
techniques which we're

00:23:06.710 --> 00:23:09.500
going to use for lower
bounds on how long

00:23:09.500 --> 00:23:11.030
these protocols have to be.

00:23:11.030 --> 00:23:15.170
How does this relate
to colored predecessor?

00:23:15.170 --> 00:23:17.810
And, remember, also there's
the model of computation,

00:23:17.810 --> 00:23:21.330
which I should mention,
cell probe model.

00:23:21.330 --> 00:23:27.025
Cell probe model, we just
count the number of--

00:23:27.025 --> 00:23:29.150
in this case, because it's
a static data structure,

00:23:29.150 --> 00:23:32.110
we don't really change it.

00:23:32.110 --> 00:23:37.590
Let's say memory word reads.

00:23:37.590 --> 00:23:40.370
We want to know how many
words of the data structure

00:23:40.370 --> 00:23:43.010
do we need to read in order to
answer a colored predecessor

00:23:43.010 --> 00:23:43.670
data structure?

00:23:43.670 --> 00:23:44.780
If we can prove a
lower bound on this,

00:23:44.780 --> 00:23:47.060
of course it applies lower
bound on the word RAM,

00:23:47.060 --> 00:23:50.710
or pick your favorite model
that works with words,

00:23:50.710 --> 00:23:54.410
transdichotomous RAM, whatever.

00:23:54.410 --> 00:23:57.170
OK, so I want to cast
that cell probe picture

00:23:57.170 --> 00:23:59.760
in terms of this picture.

00:23:59.760 --> 00:24:01.190
So here's the idea.

00:24:01.190 --> 00:24:02.390
Maybe I'll switch colors.

00:24:06.440 --> 00:24:11.810
Alice is the--

00:24:11.810 --> 00:24:14.000
I guess what do you
want to call it,

00:24:14.000 --> 00:24:21.370
the algorithm, the
query algorithm.

00:24:21.370 --> 00:24:24.920
OK, Alice is the poor
soul who has to compute

00:24:24.920 --> 00:24:27.890
the color of a predecessor.

00:24:27.890 --> 00:24:31.610
And so what's x? x is the query.

00:24:31.610 --> 00:24:32.990
We're used to that.

00:24:32.990 --> 00:24:35.300
That is the input
to the predecessor.

00:24:35.300 --> 00:24:40.010
So it's a single word, which
is I want another predecessor

00:24:40.010 --> 00:24:40.640
of some value.

00:24:43.640 --> 00:24:47.363
Bob, on the other hand,
is the data structure.

00:24:50.470 --> 00:24:52.514
That's the static thing.

00:24:52.514 --> 00:24:54.180
Where Bob represents
the data structure,

00:24:54.180 --> 00:24:58.660
y actually is the
data structure.

00:24:58.660 --> 00:25:05.080
I guess if I want to be a little
bit more prosaic or something,

00:25:05.080 --> 00:25:07.180
Bob you can think of as memory.

00:25:12.240 --> 00:25:16.590
We'll call it RAM, to be a
little more space saving.

00:25:16.590 --> 00:25:21.750
So Bob is the memory which you
can access in random access.

00:25:21.750 --> 00:25:23.676
And what it knows is
the data structure.

00:25:23.676 --> 00:25:25.050
I mean, that's
what it's storing.

00:25:25.050 --> 00:25:26.460
That's all it's storing.

00:25:26.460 --> 00:25:28.710
That's y.

00:25:28.710 --> 00:25:31.200
Now, what are these rounds?

00:25:31.200 --> 00:25:34.020
They are memory reads.

00:25:34.020 --> 00:25:37.690
So what's a?

00:25:37.690 --> 00:25:42.240
a is a log of the size
of the data structure,

00:25:42.240 --> 00:25:44.640
because that's how many bits
you need in order to specify

00:25:44.640 --> 00:25:46.910
which word you want to read.

00:25:46.910 --> 00:25:50.610
So if you have, let's
say, s words of space,

00:25:50.610 --> 00:25:53.770
a is going to be log s.

00:25:56.840 --> 00:25:58.080
OK, you could make it larger.

00:25:58.080 --> 00:25:59.000
But it doesn't help you.

00:25:59.000 --> 00:26:00.958
So we're going to make
it as small as possible,

00:26:00.958 --> 00:26:03.080
because that will
let us prove things.

00:26:03.080 --> 00:26:06.980
It's fine to let a be
log s, because Bob is not

00:26:06.980 --> 00:26:08.350
very intelligent.

00:26:08.350 --> 00:26:11.960
Bob, you just say, look, I
would like word five please.

00:26:11.960 --> 00:26:13.544
And it says, here's word five.

00:26:13.544 --> 00:26:14.960
So it's not doing
any computation.

00:26:14.960 --> 00:26:17.140
Alice can do lots of
computation and whatnot.

00:26:17.140 --> 00:26:20.259
In fact, free computation,
we don't count that.

00:26:20.259 --> 00:26:21.800
The question is
just, how many things

00:26:21.800 --> 00:26:23.008
do you have to read from Bob?

00:26:23.008 --> 00:26:25.670
Now, in this picture, Bob could
potentially compute stuff.

00:26:25.670 --> 00:26:28.799
But we know, in
reality, it won't.

00:26:28.799 --> 00:26:30.590
Lower bounds aren't
going to use that fact.

00:26:30.590 --> 00:26:33.245
But that's why we can
set a to just be log s,

00:26:33.245 --> 00:26:37.480
because Bob wouldn't do anything
with the extra information.

00:26:37.480 --> 00:26:42.950
How big is b? b is just w,
because the response to a word

00:26:42.950 --> 00:26:44.840
read is a word.

00:26:44.840 --> 00:26:46.560
So this is the picture.

00:26:46.560 --> 00:26:50.900
Query can probe
the data structure.

00:26:50.900 --> 00:26:55.700
It says, give me word something,
which is only log s bits.

00:26:55.700 --> 00:26:58.250
The response is w bits.

00:26:58.250 --> 00:27:01.310
And you repeat this
process over and over.

00:27:01.310 --> 00:27:05.000
And then, somehow, Alice
has to compute f of xy.

00:27:05.000 --> 00:27:07.154
In this model, Bob doesn't
need to know the answer.

00:27:07.154 --> 00:27:08.570
Of course, it's
just a single bit.

00:27:08.570 --> 00:27:09.950
What is f of xy?

00:27:09.950 --> 00:27:11.802
This is colored predecessor.

00:27:18.890 --> 00:27:20.210
x is the query.

00:27:20.210 --> 00:27:21.910
y is the data structure.

00:27:21.910 --> 00:27:24.590
And f of xy, is it red or blue?

00:27:24.590 --> 00:27:26.495
Is the predecessor
of x in this data

00:27:26.495 --> 00:27:28.370
structure in the set
represented by this data

00:27:28.370 --> 00:27:29.286
structure red or blue?

00:27:29.286 --> 00:27:31.590
So it's one bit of information.

00:27:31.590 --> 00:27:33.740
Alice could then
tell the bit to Bob.

00:27:33.740 --> 00:27:35.480
But actually, in
this model, we just

00:27:35.480 --> 00:27:38.786
want Alice to know the answer.

00:27:38.786 --> 00:27:40.160
So if you can
prove a lower bound

00:27:40.160 --> 00:27:43.790
on how many rounds of
communication you need,

00:27:43.790 --> 00:27:46.340
then you prove a lower bound
on the number of memory reads.

00:27:50.090 --> 00:27:55.760
Each round corresponds to
one word read from memory.

00:27:55.760 --> 00:27:56.980
Clear?

00:27:56.980 --> 00:28:03.080
So a very simple idea, but a
powerful one, as we will see,

00:28:03.080 --> 00:28:07.090
because it lets us talk about
an idea which makes sense when

00:28:07.090 --> 00:28:10.624
you're thinking about protocols
of rounds of communication,

00:28:10.624 --> 00:28:12.790
but does not really make
sense from a data structure

00:28:12.790 --> 00:28:14.110
perspective--

00:28:14.110 --> 00:28:16.930
well, I mean, not as much sense.

00:28:21.460 --> 00:28:23.440
Round elimination
is a concept that

00:28:23.440 --> 00:28:30.970
makes sense for any
communication style protocol--

00:28:30.970 --> 00:28:33.580
not just the red one, but
the generic white picture.

00:28:37.630 --> 00:28:41.460
I need to define a little bit
before I can get to the claim.

00:28:41.460 --> 00:28:45.400
This will seem weird and
arbitrary for a little while,

00:28:45.400 --> 00:28:48.090
until I draw the red picture,
which is what it corresponds to

00:28:48.090 --> 00:28:49.900
in the predecessor problem.

00:28:49.900 --> 00:28:51.770
But just bear with
me for a minute.

00:28:51.770 --> 00:28:55.086
Imagine this weird variation
on whatever problem

00:28:55.086 --> 00:28:55.960
you're starting with.

00:29:01.694 --> 00:29:03.360
So we have some problem
f, which happens

00:29:03.360 --> 00:29:04.300
to be colored predecessor.

00:29:04.300 --> 00:29:06.674
We're going to make a new
version of that problem, called

00:29:06.674 --> 00:29:08.610
f to the k.

00:29:08.610 --> 00:29:10.830
And here is the setup.

00:29:10.830 --> 00:29:13.471
It's going to be a little
different from this,

00:29:13.471 --> 00:29:14.970
and kind of fits
the same framework.

00:29:14.970 --> 00:29:23.400
But now, Alice has k
inputs, x1, x2, up to xk.

00:29:27.060 --> 00:29:34.320
Bob has y, as before.

00:29:34.320 --> 00:29:41.730
And it has an integer, i,
which is between 1 and k.

00:29:44.550 --> 00:29:48.970
Also, this is a technicality
because we'll need it

00:29:48.970 --> 00:29:51.320
for colored predecessor--

00:29:51.320 --> 00:29:53.280
we won't see that
for a few minutes.

00:29:53.280 --> 00:30:00.964
But Bob happens to know all
the xi's up to xi minus 1.

00:30:03.630 --> 00:30:13.650
And the goal is to
compute f of x i, y.

00:30:16.930 --> 00:30:20.890
Maybe I should draw
a picture of this.

00:30:40.780 --> 00:30:42.530
So we have Alice.

00:30:42.530 --> 00:30:53.150
Alice has x1 up to xk.

00:30:53.150 --> 00:31:01.400
Bob has y and i, same
communication setup.

00:31:01.400 --> 00:31:07.640
And the goal is to
compute f of xi, y.

00:31:07.640 --> 00:31:09.777
Before it was x, y.

00:31:09.777 --> 00:31:11.360
And now, we're saying,
well, actually,

00:31:11.360 --> 00:31:13.400
x consists of these k parts.

00:31:13.400 --> 00:31:15.380
We really just care
about the i-th part.

00:31:15.380 --> 00:31:17.900
So this function does
not depend on any other

00:31:17.900 --> 00:31:20.570
of the xj's, just xi.

00:31:20.570 --> 00:31:24.440
So naturally, Alice should just
communicate to Bob about xi.

00:31:24.440 --> 00:31:26.720
Trouble is, Alice
doesn't know what i is.

00:31:26.720 --> 00:31:29.420
Only Bob knows what i is.

00:31:29.420 --> 00:31:31.630
So if you think about a
communication protocol,

00:31:31.630 --> 00:31:35.060
where initially Alice sends
a message, then Bob responds,

00:31:35.060 --> 00:31:40.950
that first message that Alice
sends is probably useless.

00:31:40.950 --> 00:31:43.340
I mean, probably the first
question is, what's i?

00:31:43.340 --> 00:31:44.610
That has no information.

00:31:44.610 --> 00:31:47.180
It's just every time in the
beginning, you say, what's i?

00:31:47.180 --> 00:31:49.490
Then Bob says, here's i.

00:31:49.490 --> 00:31:53.570
And then after that one
round, Alice can you just

00:31:53.570 --> 00:31:55.710
think about xi, from then on.

00:31:58.310 --> 00:32:03.060
OK, one message may
seem like nothing.

00:32:03.060 --> 00:32:07.890
But it's like every time
you put a penny in the jar.

00:32:07.890 --> 00:32:11.740
After you do that enough
times, you have a lot of money.

00:32:11.740 --> 00:32:14.250
So one message may
seem like very few.

00:32:14.250 --> 00:32:17.150
But we just need to prove
a lower bound of log w

00:32:17.150 --> 00:32:18.610
over log log w messages.

00:32:18.610 --> 00:32:21.690
So you do this a few times.

00:32:21.690 --> 00:32:24.140
Eventually, you'll get
rid of all the messages.

00:32:24.140 --> 00:32:26.780
Now, if we can get rid
of all the messages,

00:32:26.780 --> 00:32:27.960
it may seem crazy.

00:32:27.960 --> 00:32:31.710
But it turns out you can iterate
this process of eliminating

00:32:31.710 --> 00:32:32.820
that first message.

00:32:32.820 --> 00:32:34.890
If we get rid of
all the messages,

00:32:34.890 --> 00:32:36.780
the best we could hope
for is an algorithm

00:32:36.780 --> 00:32:39.790
that is correct with
50% probability.

00:32:39.790 --> 00:32:43.460
If Alice can do nothing,
then the best Alice could do

00:32:43.460 --> 00:32:45.040
is flip a coin.

00:32:45.040 --> 00:32:46.950
So we will get a
contradiction if we

00:32:46.950 --> 00:32:49.530
get a zero message
protocol that wins

00:32:49.530 --> 00:32:52.290
with more than 50% probability.

00:32:52.290 --> 00:32:55.200
That's what we're
going to try to do.

00:32:55.200 --> 00:32:59.370
But what does it mean to
eliminate this first message?

00:32:59.370 --> 00:33:01.450
Let me formalize the
round elimination

00:33:01.450 --> 00:33:10.261
a little bit over here.

00:33:28.950 --> 00:33:50.930
So here's the round elimination.

00:33:50.930 --> 00:33:53.010
This, again, works
for any function f.

00:34:03.820 --> 00:34:07.840
So if there's a protocol
for this f to the k problem,

00:34:07.840 --> 00:34:11.110
and Alice speaks first,
then that first message

00:34:11.110 --> 00:34:12.940
is going to be roughly useless.

00:34:17.650 --> 00:34:25.219
So let's suppose it has
error probability delta.

00:34:25.219 --> 00:34:27.469
So there's some probability
it gives the wrong answer.

00:34:30.030 --> 00:34:34.714
And let's suppose that
it uses m messages.

00:34:38.909 --> 00:35:01.260
Then there exists a protocol
for f where Bob speaks first,

00:35:01.260 --> 00:35:02.995
the error probability
goes up slightly.

00:35:20.020 --> 00:35:21.685
And it uses one fewer message.

00:35:26.890 --> 00:35:28.810
OK, ultimately, we
care about rounds,

00:35:28.810 --> 00:35:30.220
which are pairs of messages.

00:35:30.220 --> 00:35:34.390
But we're going
to count messages.

00:35:34.390 --> 00:35:36.940
And then, of course, divide
by two, you get rounds.

00:35:36.940 --> 00:35:39.430
So the idea is you can
eliminate the first message

00:35:39.430 --> 00:35:41.440
that Alice sent.

00:35:41.440 --> 00:35:44.710
The difference is, before you
were solving this f to the k

00:35:44.710 --> 00:35:46.416
problem.

00:35:46.416 --> 00:35:48.790
If you start with Bob, then
of course you know what i is.

00:35:48.790 --> 00:35:50.331
And so then your
problem just reduces

00:35:50.331 --> 00:35:53.800
to computing this f on xi, y.

00:35:53.800 --> 00:35:56.170
So you don't get a protocol
for f to the k anymore.

00:35:56.170 --> 00:35:57.610
But you get a protocol for f.

00:36:00.474 --> 00:36:02.140
And we're going to
iterate this process,

00:36:02.140 --> 00:36:04.250
and eventually eliminate
all the messages.

00:36:04.250 --> 00:36:06.940
That's the plan.

00:36:06.940 --> 00:36:12.710
Let me give you some intuition
for why this dilemma is true.

00:36:12.710 --> 00:36:15.190
It's a little messy
for us to prove.

00:36:15.190 --> 00:36:17.789
I'm not going to
give a proof here.

00:36:17.789 --> 00:36:20.080
If there's time at the end,
I'll give a little bit more

00:36:20.080 --> 00:36:20.580
of a proof.

00:36:20.580 --> 00:36:24.220
But it will still use
some information theory

00:36:24.220 --> 00:36:29.547
that we will not prove, and some
communication complexity which

00:36:29.547 --> 00:36:31.630
we won't prove, because
it's a little bit involved

00:36:31.630 --> 00:36:32.213
to prove this.

00:36:32.213 --> 00:36:34.930
But once you have this
lemma, this lower bound

00:36:34.930 --> 00:36:38.410
is actually quite easy,
and intuitive, and nice.

00:36:38.410 --> 00:36:41.922
So that's where I
want to get to today.

00:36:41.922 --> 00:36:43.380
But let's start
with some intuition

00:36:43.380 --> 00:36:51.490
of why this should be true, why
there's this extra error term.

00:36:51.490 --> 00:36:52.120
Yeah, question?

00:36:54.100 --> 00:36:57.541
AUDIENCE: Does it matter
who reports the answer?

00:36:57.541 --> 00:37:01.386
Because there seems to
be some symmetry there.

00:37:01.386 --> 00:37:03.010
ERIK DEMAINE: Right,
does it matter who

00:37:03.010 --> 00:37:05.230
has the answer, Alice or Bob?

00:37:05.230 --> 00:37:07.870
Let's just say you're
done when anyone

00:37:07.870 --> 00:37:11.414
knows the answer,
either Alice or Bob.

00:37:11.414 --> 00:37:12.580
Yeah, that would be cleaner.

00:37:12.580 --> 00:37:14.663
Otherwise, you'd have to
send a message at the end

00:37:14.663 --> 00:37:16.210
to tell it to
Alice or something.

00:37:16.210 --> 00:37:18.520
So let's make it
symmetric, by saying

00:37:18.520 --> 00:37:20.150
either Alice or Bob knows it.

00:37:20.150 --> 00:37:21.817
Then the protocol can end.

00:37:21.817 --> 00:37:22.400
Good question.

00:37:25.530 --> 00:37:28.660
That way, we won't pay for
an additive one every time,

00:37:28.660 --> 00:37:30.830
only pay for it at the end.

00:37:30.830 --> 00:37:33.330
Good questions.

00:37:33.330 --> 00:37:36.290
All right, so there's
an issue here.

00:37:36.290 --> 00:37:38.890
It said, oh, Alice's
message is probably useless.

00:37:38.890 --> 00:37:41.650
But maybe Alice gets
lucky, and sends xi.

00:37:41.650 --> 00:37:43.720
Then, that message is useful.

00:37:43.720 --> 00:37:45.920
What's the chance
that Alice sends xi?

00:37:48.520 --> 00:37:50.980
Well, 1 out of k.

00:37:50.980 --> 00:37:53.560
So there's something
going on there.

00:37:53.560 --> 00:37:56.530
Alice doesn't just have to
send an entire xj, though.

00:37:59.390 --> 00:38:02.230
Alice could send some
mix of these guys.

00:38:02.230 --> 00:38:04.920
Maybe it sends the x
or of all the xj's.

00:38:04.920 --> 00:38:08.470
Or it could be anything.

00:38:08.470 --> 00:38:09.430
But there's a limit.

00:38:09.430 --> 00:38:14.170
There's only a bits being
sent from Alice to Bob.

00:38:14.170 --> 00:38:16.450
So the idea is if the
total number of bits

00:38:16.450 --> 00:38:20.470
here is much bigger than a,
then very few of the bits

00:38:20.470 --> 00:38:27.310
that you send are going to be
about a particular element xi,

00:38:27.310 --> 00:38:28.455
in expectation.

00:38:28.455 --> 00:38:29.830
So this is a
probabilistic thing.

00:38:40.690 --> 00:38:43.595
So just imagine
this for the moment.

00:38:48.550 --> 00:38:51.160
Basically, because we're
in lower bound world,

00:38:51.160 --> 00:38:53.764
we get to, essentially, set
up the input however we want.

00:38:53.764 --> 00:38:55.180
And so, in particular,
we're going

00:38:55.180 --> 00:38:59.079
to prove a lower bound
that, in expectation--

00:38:59.079 --> 00:39:01.120
so we're going to have a
probability distribution

00:39:01.120 --> 00:39:03.790
of data structure, or
not data structures,

00:39:03.790 --> 00:39:07.900
but of sets of values
that are in the set.

00:39:07.900 --> 00:39:11.020
And the claim is that, in
expectation, any data structure

00:39:11.020 --> 00:39:14.080
must do at least log
w over log log w,

00:39:14.080 --> 00:39:21.500
minned with log base w of
n queries, in expectation.

00:39:21.500 --> 00:39:24.030
So we get to assume that
the input is random.

00:39:24.030 --> 00:39:27.052
And we'll see why in a bit.

00:39:27.052 --> 00:39:28.635
So in this world,
we can assume that i

00:39:28.635 --> 00:39:30.080
is chosen uniformly at random.

00:39:32.620 --> 00:39:40.395
And given that assumption,
you would expect exactly a

00:39:40.395 --> 00:39:50.230
over k bits to be about xi.

00:39:50.230 --> 00:39:52.560
That would be the best
you could hope to do.

00:39:52.560 --> 00:39:54.610
Sort of, you have a bits.

00:39:54.610 --> 00:39:56.980
You spread them out
evenly talk about all

00:39:56.980 --> 00:39:58.282
the xi's that you can.

00:39:58.282 --> 00:40:00.490
So you get to communicate
a little bit of information

00:40:00.490 --> 00:40:03.766
about the particular xi
that Bob will care about.

00:40:03.766 --> 00:40:05.140
You don't know
which one that is.

00:40:05.140 --> 00:40:07.015
So you have to communicate
about all of them.

00:40:07.015 --> 00:40:10.880
So it's going to be a
over k in expectation.

00:40:10.880 --> 00:40:14.380
So here's the idea.

00:40:14.380 --> 00:40:17.020
We want to remove that message
so Alice can't communicate

00:40:17.020 --> 00:40:18.670
those a over k bits.

00:40:18.670 --> 00:40:20.170
So what's Bob going to do?

00:40:20.170 --> 00:40:31.140
Bob is going to guess
them by flipping coins

00:40:31.140 --> 00:40:37.250
so he gets them uniformly
randomly, a over k bits.

00:40:37.250 --> 00:40:40.580
What's the probability
that Bob is right?

00:40:44.970 --> 00:40:50.370
Well, it's going to be 1
over 2 to the a over k.

00:40:57.590 --> 00:40:59.780
That seems not so big.

00:40:59.780 --> 00:41:02.310
But if k-- how does it go?

00:41:02.310 --> 00:41:06.450
If k is much larger than a, then
this is actually a good thing.

00:41:06.450 --> 00:41:15.900
So let me-- so this was the
probability of being correct.

00:41:15.900 --> 00:41:21.320
So the probability
of being incorrect

00:41:21.320 --> 00:41:24.660
is going to be 1 minus that.

00:41:24.660 --> 00:41:26.480
And we're interested in--

00:41:26.480 --> 00:41:29.990
we had some probability
delta failing before.

00:41:29.990 --> 00:41:32.120
And now there's this new
way that we can fail.

00:41:32.120 --> 00:41:33.842
So I'm using union
bound here, saying,

00:41:33.842 --> 00:41:36.050
well, we could fail the old
way, or fail the new way.

00:41:36.050 --> 00:41:37.549
Maybe they're
correlated, maybe not.

00:41:37.549 --> 00:41:39.370
But the worst cases
is that it wants

00:41:39.370 --> 00:41:41.680
the sum of the two errors.

00:41:41.680 --> 00:41:47.360
So the increase in error is,
at most, 1 minus this thing.

00:41:47.360 --> 00:41:50.180
Now, this 1 minus 1 over
something to the power

00:41:50.180 --> 00:41:52.850
or something, if this
something is large,

00:41:52.850 --> 00:41:57.350
there's this fun fact 1
minus 1, over e to the x

00:41:57.350 --> 00:42:01.440
is approximately x.

00:42:01.440 --> 00:42:06.790
So this is going to be
approximately a over k,

00:42:06.790 --> 00:42:07.930
if k is large enough.

00:42:07.930 --> 00:42:09.790
So this is for large x--

00:42:15.040 --> 00:42:17.450
small x, sorry.

00:42:17.450 --> 00:42:19.000
k is large.

00:42:19.000 --> 00:42:22.120
And so a over k is
small, very close to 0.

00:42:26.590 --> 00:42:29.230
So if this were true,
then the error increase

00:42:29.230 --> 00:42:31.450
would be order a over k.

00:42:31.450 --> 00:42:32.450
There's e versus 2.

00:42:32.450 --> 00:42:35.860
So there's a constant
factor I'm losing there.

00:42:35.860 --> 00:42:38.200
It's not quite that good.

00:42:38.200 --> 00:42:39.665
So this is only intuition.

00:42:39.665 --> 00:42:41.290
The real bound has
a square route here.

00:42:41.290 --> 00:42:42.587
And it's necessary.

00:42:42.587 --> 00:42:44.920
And I don't have a great
intuition why it's square root.

00:42:44.920 --> 00:42:49.540
It's just a little bit
worse than this intuition.

00:42:49.540 --> 00:42:52.390
I mean, the main issue
is, what does it mean

00:42:52.390 --> 00:42:54.580
for bits to be about something?

00:42:54.580 --> 00:42:57.030
And can you really
just guess those bits?

00:42:57.030 --> 00:42:59.740
Actually, you have to guess
the message that Bob sent.

00:42:59.740 --> 00:43:02.500
So it's a little bit more
than just the bits that--

00:43:02.500 --> 00:43:04.820
sorry that Alice sent.

00:43:04.820 --> 00:43:06.760
So you lose a little bit more.

00:43:06.760 --> 00:43:10.208
But it won't make a huge
difference to us, square root.

00:43:12.920 --> 00:43:15.770
So that's some rough intuition
for round elimination.

00:43:15.770 --> 00:43:21.200
Let's see why it is so cool
to have round elimination,

00:43:21.200 --> 00:43:24.410
how it lets us prove
a pretty strong lower

00:43:24.410 --> 00:43:33.100
bound on colored predecessor
in the cell pro model.

00:43:49.300 --> 00:43:53.400
OK, I think I missed a
claimed somewhere here.

00:44:20.870 --> 00:44:23.150
This is the lower bound
we're going to prove.

00:44:23.150 --> 00:44:27.470
And it's nice and symmetric, log
base a of w, log base b of n.

00:44:27.470 --> 00:44:31.740
This is kind of perfectly
symmetric in Alice and Bob.

00:44:31.740 --> 00:44:36.080
Alice, well I don't
know which one

00:44:36.080 --> 00:44:40.930
represents-- and, apparently,
Alice represents w.

00:44:40.930 --> 00:44:44.600
Alice has got a bits to
communicate with to Bob.

00:44:47.740 --> 00:44:49.850
I mean, the input, I
guess, is of size w.

00:44:49.850 --> 00:44:51.179
That's the query.

00:44:51.179 --> 00:44:52.970
Bob, on the other hand,
knows all the data.

00:44:52.970 --> 00:44:55.610
So it's, in some sense,
represents the n side.

00:44:55.610 --> 00:44:58.550
It's able to communicate
with b bits of information.

00:44:58.550 --> 00:45:03.051
So log base b of n, somehow
enough to communicate n.

00:45:03.051 --> 00:45:04.550
It's not a great
intuition for this.

00:45:04.550 --> 00:45:06.720
But at least it's
nice and symmetric.

00:45:06.720 --> 00:45:13.760
Now, let's work out what it
actually corresponds to for--

00:45:13.760 --> 00:45:16.670
this is a lower bound
on colored predecessor.

00:45:16.670 --> 00:45:26.360
So for any colored, even
static, colored predecessor data

00:45:26.360 --> 00:45:33.680
structure, static
can be randomized.

00:45:33.680 --> 00:45:36.781
And this will be a lower bound
on the expected performance.

00:45:40.220 --> 00:45:44.360
OK, so what this implies
for polynomial space, which

00:45:44.360 --> 00:45:51.410
is kind of the
case we care about,

00:45:51.410 --> 00:45:54.260
for polynomial space
data structure,

00:45:54.260 --> 00:46:00.860
a is going to be order log n.

00:46:00.860 --> 00:46:04.820
In fact, we only need a
to be poly log n for what

00:46:04.820 --> 00:46:07.580
I'm about to say to be true.

00:46:07.580 --> 00:46:12.275
So then this becomes min log--

00:46:14.800 --> 00:46:15.300
what n?

00:46:20.350 --> 00:46:21.300
I guess.

00:46:23.927 --> 00:46:25.260
Is that what I wrote over there?

00:46:25.260 --> 00:46:26.700
I don't even remember.

00:46:26.700 --> 00:46:28.069
I wrote a log log w.

00:46:28.069 --> 00:46:29.610
I guess I'm going
to write log log n.

00:46:37.239 --> 00:46:38.280
This is the right answer.

00:46:46.730 --> 00:46:52.570
OK, fine, so we get a--

00:46:52.570 --> 00:46:53.590
b is just w.

00:46:53.590 --> 00:46:54.880
So that's a log base w of n.

00:46:54.880 --> 00:46:56.810
That's just fusion tries.

00:46:56.810 --> 00:46:58.660
This one, ideally,
would be log w.

00:46:58.660 --> 00:47:02.950
But we're off by this log
log factor if a is log n.

00:47:02.950 --> 00:47:07.220
That's the best we'll
be able to prove today.

00:47:07.220 --> 00:47:09.360
So this is slightly
less beautiful,

00:47:09.360 --> 00:47:11.300
because it's both w and n.

00:47:11.300 --> 00:47:13.580
But so was this one.

00:47:13.580 --> 00:47:15.000
And this is not the true answer.

00:47:15.000 --> 00:47:16.910
The true answer has no log log.

00:47:16.910 --> 00:47:20.530
But that's what we get
from nice symmetric bound

00:47:20.530 --> 00:47:23.120
for polynomial space.

00:47:23.120 --> 00:47:26.840
You can also use this
to prove the Beame Fich

00:47:26.840 --> 00:47:31.820
lower bounds, which I've by now
erased, which are the pure--

00:47:31.820 --> 00:47:37.910
the for all n, there exists a
w, for all w, there exists an n.

00:47:37.910 --> 00:47:39.710
Why don't I briefly cover that?

00:48:01.700 --> 00:48:04.585
Again, let's assume
that a is order log

00:48:04.585 --> 00:48:07.270
n with polynomial space.

00:48:07.270 --> 00:48:15.400
Then, the lower bound will
be largest when the two

00:48:15.400 --> 00:48:18.880
terms in the min are equal.

00:48:18.880 --> 00:48:27.970
So log base a of w
equals log base b of n.

00:48:32.130 --> 00:48:35.010
And that's these two guys.

00:48:35.010 --> 00:48:39.590
So this is log w over
a is order log n.

00:48:39.590 --> 00:48:41.480
So this is going to
be order log log n.

00:48:44.660 --> 00:48:51.200
And I want this to be
equal to log n over log w.

00:48:51.200 --> 00:48:59.390
So we can cross multiply, get
log square w equals log n,

00:48:59.390 --> 00:49:00.020
log log n.

00:49:03.070 --> 00:49:11.510
And so log w is equal to
square root log n, log log n.

00:49:16.130 --> 00:49:19.820
And let's see.

00:49:19.820 --> 00:49:24.860
You can also take logs, and
from this conclude that log log

00:49:24.860 --> 00:49:29.810
w equals log log n.

00:49:32.640 --> 00:49:34.410
I'm throwing away
constant factors.

00:49:34.410 --> 00:49:37.290
And when I say equal here, I
mean up to constant factors

00:49:37.290 --> 00:49:40.532
for throughout this half board.

00:49:40.532 --> 00:49:41.240
So you take logs.

00:49:41.240 --> 00:49:43.610
You get 2 log log w
on the left hand side.

00:49:43.610 --> 00:49:46.190
Over here, you get log
log n, plus log log log n.

00:49:46.190 --> 00:49:48.050
So it's dominated by log log n.

00:49:48.050 --> 00:49:51.530
So in this scenario, where these
two things are roughly equal,

00:49:51.530 --> 00:49:55.580
you get log log w roughly
equal to log log n.

00:49:55.580 --> 00:49:59.990
And so, from this bound,
or from this equality,

00:49:59.990 --> 00:50:02.760
we should get the old
Beame Fich bounds,

00:50:02.760 --> 00:50:05.270
which let me write them again.

00:50:18.830 --> 00:50:19.730
So they are--

00:50:24.630 --> 00:50:27.940
I mean, we want to compute
what was this bound.

00:50:27.940 --> 00:50:33.980
We have log w, divided by log a,
or log w divided by log log n--

00:50:33.980 --> 00:50:37.400
so log w, divided by log log n.

00:50:37.400 --> 00:50:39.500
But log log n now is log log w.

00:50:39.500 --> 00:50:44.570
So this is going to be log
w over log log w, which was

00:50:44.570 --> 00:50:47.000
one of the Beam Fich bounds.

00:50:47.000 --> 00:50:54.560
So for all w, there exists an
n, such that omega this thing

00:50:54.560 --> 00:50:56.480
holds.

00:50:56.480 --> 00:50:58.220
On the other hand,
we have log n,

00:50:58.220 --> 00:51:10.360
divided by log w is now square
root of log n, log log n.

00:51:10.360 --> 00:51:15.400
So this is log n, divided
by square root of log n,

00:51:15.400 --> 00:51:17.360
log log n.

00:51:17.360 --> 00:51:20.160
So the square root log
n cancels with this.

00:51:20.160 --> 00:51:22.760
We end up with square
root of log n on the top.

00:51:22.760 --> 00:51:27.710
And we keep the square root
of log log n on the bottom.

00:51:27.710 --> 00:51:32.780
And so we get the other Beam
Fich Xiau bound, for all n,

00:51:32.780 --> 00:51:37.795
there exists a w, such
that omega this holds.

00:51:37.795 --> 00:51:39.170
And when you just
need existence,

00:51:39.170 --> 00:51:41.419
then we get to choose the
relationship between n and w

00:51:41.419 --> 00:51:42.410
however we want.

00:51:42.410 --> 00:51:43.550
So you don't have to
believe me that it's

00:51:43.550 --> 00:51:44.674
largest when they're equal.

00:51:44.674 --> 00:51:46.220
It happens to be true.

00:51:46.220 --> 00:51:49.770
But the point is, there
is a choice of w and n--

00:51:49.770 --> 00:51:53.230
namely, this choice, log w
equals root log n, log log n,

00:51:53.230 --> 00:51:56.060
where you get a
lower bound of this.

00:51:56.060 --> 00:51:59.750
And there's another choice for
n versus w where this happens.

00:51:59.750 --> 00:52:04.660
So this implies the
Beame Fich Xiau bounds.

00:52:04.660 --> 00:52:07.040
But I kind of prefer this
form, because it's clear

00:52:07.040 --> 00:52:09.710
up to this log log n
factor, we understand

00:52:09.710 --> 00:52:12.710
the complete trade-off
between w and n,

00:52:12.710 --> 00:52:13.980
assuming polynomial space.

00:52:16.880 --> 00:52:19.010
So we're going to
prove this bound, which

00:52:19.010 --> 00:52:23.450
implies all this stuff, using
round elimination lemma.

00:52:57.460 --> 00:53:03.620
So we're proving
this claim here,

00:53:03.620 --> 00:53:07.640
omega min log base of a
of w, log base b of n,

00:53:07.640 --> 00:53:10.515
for any colored
predecessor data structure.

00:53:13.190 --> 00:53:16.070
So let's suppose you have
a colored predecessor data

00:53:16.070 --> 00:53:17.420
structure.

00:53:17.420 --> 00:53:20.795
And it can answer
queries in t cell probes.

00:53:25.130 --> 00:53:28.040
which in the communication
complexity perspective,

00:53:28.040 --> 00:53:38.810
that's rounds of communication,
or colored predecessor.

00:53:38.810 --> 00:53:44.440
Our goal is to do t
round eliminations.

00:53:48.530 --> 00:53:50.770
Slight discrepancy
in terminology here--

00:53:50.770 --> 00:53:52.580
round elimination
lemma is really

00:53:52.580 --> 00:53:55.760
about eliminating one message,
which is half a round.

00:53:55.760 --> 00:53:58.470
But you do it twice,
you eliminate a round.

00:53:58.470 --> 00:54:02.270
And so we need to do two
t calls to this lemma.

00:54:02.270 --> 00:54:04.010
We will eliminate all messages.

00:54:04.010 --> 00:54:06.320
Then there's zero communication.

00:54:06.320 --> 00:54:16.220
Then the best you could hope
to do is by flipping a coin.

00:54:23.630 --> 00:54:24.920
Maybe you're worse than that.

00:54:24.920 --> 00:54:29.630
But the probability of error
has to be at least 1/2.

00:54:29.630 --> 00:54:31.520
So we'll either get
a contradiction.

00:54:31.520 --> 00:54:35.060
Or we're going to set things up
so the error is, at most, 1/3.

00:54:35.060 --> 00:54:39.810
And, therefore, this will
prove that t has to be large.

00:54:39.810 --> 00:54:41.750
So you couldn't eliminate
all the messages.

00:54:41.750 --> 00:54:43.850
Anyway, we'll see
that in a moment.

00:54:46.890 --> 00:54:48.750
So we have a setup like this.

00:54:51.510 --> 00:54:55.490
And in our case,
with this picture,

00:54:55.490 --> 00:54:57.740
there is an asymmetry
between Alice and Bob.

00:54:57.740 --> 00:54:59.750
I mean, yeah, the picture
is nice and clean.

00:54:59.750 --> 00:55:03.320
But in reality, this has to
represent a colored predecessor

00:55:03.320 --> 00:55:04.380
problem.

00:55:04.380 --> 00:55:06.770
So in reality, Bob
is a data structure

00:55:06.770 --> 00:55:10.040
and is not very smart,
just does random access.

00:55:10.040 --> 00:55:14.420
Alice, we don't know,
could be very smart.

00:55:14.420 --> 00:55:18.090
And Alice just
has a single word.

00:55:18.090 --> 00:55:20.210
So there's an asymmetry
between Alice and Bob.

00:55:20.210 --> 00:55:22.589
So when we eliminate a
message from Alice to Bob,

00:55:22.589 --> 00:55:24.380
it's going to be
different from eliminating

00:55:24.380 --> 00:55:25.870
a message from Bob to Alice.

00:55:25.870 --> 00:55:29.360
This f to the k
thing here is going

00:55:29.360 --> 00:55:31.460
to have to be
different when we're

00:55:31.460 --> 00:55:34.160
doing an Alice to Bob message,
versus doing a Bob to Alice

00:55:34.160 --> 00:55:35.320
message.

00:55:35.320 --> 00:55:39.440
And that's where we're
going to get a min.

00:55:39.440 --> 00:55:41.595
When we go from Alice to
Bob, we're going to be,

00:55:41.595 --> 00:55:43.220
essentially, contributing
to this term.

00:55:43.220 --> 00:55:44.719
When we go Bob to
Alice, we're going

00:55:44.719 --> 00:55:47.825
to be contributing to this
term, I think, or the reverse.

00:55:47.825 --> 00:55:48.950
We'll find out in a second.

00:55:52.230 --> 00:56:09.170
So let's do first Alice
to Bob, because that's

00:56:09.170 --> 00:56:12.870
the first type of message
we need to eliminate.

00:56:12.870 --> 00:56:19.130
So let's suppose-- now, as
we eliminate things, w and n

00:56:19.130 --> 00:56:21.120
are going to decrease.

00:56:21.120 --> 00:56:25.110
So I'm going to
suppose at this point,

00:56:25.110 --> 00:56:31.460
Alice's input has
w prime bits left.

00:56:31.460 --> 00:56:33.000
Initially, w prime is w.

00:56:33.000 --> 00:56:34.820
But I want to do the
generic case so I

00:56:34.820 --> 00:56:36.440
don't have to repeat anything.

00:56:38.970 --> 00:56:40.650
Here's the concept.

00:56:40.650 --> 00:56:42.690
Remember, we're
proving a lower bound.

00:56:42.690 --> 00:56:46.020
We get to set up the set of
elements however we want.

00:56:46.020 --> 00:56:47.557
We're going to
define a distribution

00:56:47.557 --> 00:56:48.515
on the set of elements.

00:56:51.620 --> 00:56:57.290
And we're going to
do that by breaking

00:56:57.290 --> 00:57:03.590
this input, w prime bits,
into a lot of pieces.

00:57:15.340 --> 00:57:17.285
Alice's input,
you think of as x.

00:57:20.330 --> 00:57:21.890
Don't necessarily
want to call it x.

00:57:21.890 --> 00:57:23.630
Well, sure, we can call it x.

00:57:23.630 --> 00:57:26.470
There it Is

00:57:26.470 --> 00:57:30.620
What I'd like to do is break
that input into chunks, x1 up

00:57:30.620 --> 00:57:31.120
to xk.

00:57:31.120 --> 00:57:33.550
This is basically what
round elimination tells me

00:57:33.550 --> 00:57:34.120
I should do.

00:57:34.120 --> 00:57:37.390
If I want to view my problem
as an f to the k problem,

00:57:37.390 --> 00:57:39.950
somehow my input is
not just one thing.

00:57:39.950 --> 00:57:40.690
It's k things.

00:57:40.690 --> 00:57:44.090
Well, in reality, the
input is a single word.

00:57:44.090 --> 00:57:46.090
So I'm going to have to
split it into sub words,

00:57:46.090 --> 00:57:48.270
in the usual way.

00:57:48.270 --> 00:57:50.712
Van Emde Boas would
split it in half.

00:57:50.712 --> 00:57:52.420
We're going to split
it into more pieces,

00:57:52.420 --> 00:57:55.600
because we need to guarantee
that this error is small.

00:57:55.600 --> 00:57:59.740
We need k to be large for
this error to be small.

00:57:59.740 --> 00:58:03.250
I claim this is the
good choice of k.

00:58:03.250 --> 00:58:08.887
And so now, this is x1,
this is x 2x, this is xk.

00:58:08.887 --> 00:58:09.970
The low order bits are xk.

00:58:09.970 --> 00:58:11.299
The high order bits aer x1.

00:58:11.299 --> 00:58:13.090
Or the reverse, it
doesn't actually matter.

00:58:17.470 --> 00:58:19.750
So why is this a
good choice of k?

00:58:19.750 --> 00:58:24.250
Because if we then look
at the error increase

00:58:24.250 --> 00:58:27.820
we get from here,
error increase is

00:58:27.820 --> 00:58:32.630
square root of a divided by k.

00:58:32.630 --> 00:58:40.260
So error increase,
which is order a over k,

00:58:40.260 --> 00:58:42.490
is going to be k is now this.

00:58:42.490 --> 00:58:45.460
So the a's cancel--

00:58:45.460 --> 00:58:49.467
sorry, square root of a over k.

00:58:49.467 --> 00:58:50.800
A over k was the wrong analysis.

00:58:50.800 --> 00:58:53.470
The square root of a over
k is the correct bound.

00:58:53.470 --> 00:58:57.100
Still, the a's cancel,
because the k has an a factor.

00:58:57.100 --> 00:59:00.370
And so we get order
square root of 1

00:59:00.370 --> 00:59:03.550
over t squared, also
known as 1 over t.

00:59:06.560 --> 00:59:07.800
This is good.

00:59:07.800 --> 00:59:09.410
There's a constant here.

00:59:09.410 --> 00:59:11.980
And if I tune this
constant correct,

00:59:11.980 --> 00:59:15.342
I can make this
constant less than 1/3.

00:59:15.342 --> 00:59:17.050
So if I start with a
protocol, let's say,

00:59:17.050 --> 00:59:18.460
that's completely correct--

00:59:18.460 --> 00:59:19.210
you don't have to.

00:59:19.210 --> 00:59:21.418
You could start with one
that's correct with at least

00:59:21.418 --> 00:59:23.490
probability 3/4 or something.

00:59:23.490 --> 00:59:25.810
But let's just say, for
simplicity, the initial data

00:59:25.810 --> 00:59:27.630
structure is completely correct.

00:59:27.630 --> 00:59:32.090
If every time I do an
elimination of a message,

00:59:32.090 --> 00:59:36.190
I guess I should set it
to be 1/6 time's 1 over t,

00:59:36.190 --> 00:59:39.850
and I do this 2 times
t times, in the end

00:59:39.850 --> 00:59:42.260
the error will be only 1/3.

00:59:42.260 --> 00:59:45.310
So I'll be correct
with 2/3 probability.

00:59:45.310 --> 00:59:47.520
So never mind the constants.

00:59:47.520 --> 00:59:50.931
I can set this constant right
so this is some epsilon times 1

00:59:50.931 --> 00:59:51.430
over t.

00:59:51.430 --> 00:59:54.550
So after I do this 2 t
times, I end up with an error

00:59:54.550 --> 00:59:58.160
that's only epsilon, or
2 epsilon, or whatever.

00:59:58.160 --> 00:59:59.881
So that would be
a contradiction.

00:59:59.881 --> 01:00:01.380
So that's why this
is a good choice.

01:00:01.380 --> 01:00:03.880
I'm basically balancing the
error I get from every message

01:00:03.880 --> 01:00:04.450
elimination.

01:00:04.450 --> 01:00:06.730
I want them all to
be order 1 over t

01:00:06.730 --> 01:00:07.990
so it's nice and balanced.

01:00:07.990 --> 01:00:12.220
That will give me the best
lower bound, it turns out.

01:00:12.220 --> 01:00:13.570
But what does this mean?

01:00:13.570 --> 01:00:19.510
I mean, somehow I have to have
an f to the k problem, meaning,

01:00:19.510 --> 01:00:22.690
really I should
only care about 1xi

01:00:22.690 --> 01:00:26.530
here, that all the interesting
stuff is these bits.

01:00:26.530 --> 01:00:28.960
But Alice doesn't
know which is i.

01:00:28.960 --> 01:00:30.880
Only Bob knows which is i.

01:00:30.880 --> 01:00:31.840
What does that mean?

01:00:31.840 --> 01:00:36.050
Bob knows the set of elements
in the data structure.

01:00:36.050 --> 01:00:39.220
So, basically, the
set of elements--

01:00:39.220 --> 01:00:41.710
maybe they all
differ in these bits.

01:00:41.710 --> 01:00:43.960
Or maybe they all differ in
these bits, or these bits,

01:00:43.960 --> 01:00:46.520
or these bits, but
only one of these.

01:00:46.520 --> 01:00:49.430
So that's a
distribution of inputs.

01:00:49.430 --> 01:00:51.580
There's one class of
inputs where they all

01:00:51.580 --> 01:00:53.034
differ in the x1 part.

01:00:53.034 --> 01:00:54.700
But then they're
identical for the rest.

01:00:57.490 --> 01:01:00.130
For each xi, there's
a set of inputs

01:01:00.130 --> 01:01:02.860
where they all differ in
the xi's, and nowhere else,

01:01:02.860 --> 01:01:04.300
let's say.

01:01:04.300 --> 01:01:08.590
But Alice doesn't know
which setting she is in.

01:01:08.590 --> 01:01:10.570
Alice just knows x.

01:01:10.570 --> 01:01:12.520
And so Alice is
kind of in trouble,

01:01:12.520 --> 01:01:14.320
because you can only
send a few bits.

01:01:14.320 --> 01:01:16.670
You can only send a
bits out of this thing.

01:01:16.670 --> 01:01:20.230
So you can't
communicate very much.

01:01:20.230 --> 01:01:32.975
Let's go over here.

01:01:32.975 --> 01:01:34.690
I need the bound.

01:01:34.690 --> 01:01:35.967
I don't need the corollary.

01:01:39.450 --> 01:01:43.970
Let me draw a picture
for Alice, or for what

01:01:43.970 --> 01:01:45.110
the data looks like.

01:01:49.360 --> 01:01:51.970
So there's some
initial segment, which

01:01:51.970 --> 01:01:54.250
is shared by all elements.

01:01:54.250 --> 01:01:58.140
Then they all differ
in this middle chunk.

01:01:58.140 --> 01:01:59.730
And then they all have--

01:01:59.730 --> 01:02:03.180
I don't really care what
they have after that.

01:02:03.180 --> 01:02:07.720
So these are the elements
in the data structure.

01:02:07.720 --> 01:02:10.860
And this is our usual
picture of a binary tree.

01:02:10.860 --> 01:02:12.210
Except, I didn't draw it binary.

01:02:12.210 --> 01:02:14.360
I drew it with some
branching factor

01:02:14.360 --> 01:02:20.820
so that the height of this
tree is theta at squared.

01:02:20.820 --> 01:02:24.960
So I set the base
of my representation

01:02:24.960 --> 01:02:28.916
so that you branch whatever,
w divided by at squared here.

01:02:28.916 --> 01:02:30.040
That's my branching factor.

01:02:32.720 --> 01:02:35.810
So cool, that's my picture.

01:02:35.810 --> 01:02:39.961
And this is depth i.

01:02:43.400 --> 01:02:44.870
So Bob knows this picture.

01:02:44.870 --> 01:02:46.820
Bob knows what all the
elements are so it can

01:02:46.820 --> 01:02:49.220
build this picture or whatever.

01:02:49.220 --> 01:02:53.072
Bob knows which is the relevant
depth that they differ.

01:02:53.072 --> 01:02:54.530
But in the lower
bound, we're going

01:02:54.530 --> 01:02:57.180
to say a is chosen
uniformly at random.

01:02:57.180 --> 01:03:01.100
So Alice has no idea
which bits to send.

01:03:01.100 --> 01:03:05.360
And so probably, Alice is
going to say, oh, here,

01:03:05.360 --> 01:03:06.170
I know these bits.

01:03:06.170 --> 01:03:07.503
But Bob already knew those bits.

01:03:07.503 --> 01:03:08.720
So Bob learned nothing.

01:03:08.720 --> 01:03:11.060
And so that's why you
can eliminate the round.

01:03:11.060 --> 01:03:12.740
That's the intuition, anyway.

01:03:12.740 --> 01:03:16.150
The proof is, well, you just
apply round elimination.

01:03:16.150 --> 01:03:20.780
You see that the problem now
becomes to compute predecessor

01:03:20.780 --> 01:03:21.890
on this node.

01:03:21.890 --> 01:03:25.100
Just like in fusion tries,
at every step of the way,

01:03:25.100 --> 01:03:28.710
the hard part was to compute
predecessor at the node.

01:03:28.710 --> 01:03:33.140
Here, if Bob is allowed
to do computation,

01:03:33.140 --> 01:03:35.870
then, really, Alice
just needs to say,

01:03:35.870 --> 01:03:37.454
what are the things here?

01:03:37.454 --> 01:03:38.870
Or together, Alice
and Bob somehow

01:03:38.870 --> 01:03:43.460
have to figure out, what is the
predecessor at this node of xi?

01:03:43.460 --> 01:03:45.200
Which way the query
goes, which could

01:03:45.200 --> 01:03:48.290
be in between one of
these, that is xi.

01:03:48.290 --> 01:03:51.650
The rest of the
xj's don't matter.

01:03:51.650 --> 01:03:53.420
Only xi matters.

01:03:53.420 --> 01:03:56.450
And so the problem reduces to
computing predecessor here.

01:03:56.450 --> 01:03:59.280
And that matches the
definition of f to the k.

01:03:59.280 --> 01:04:02.195
So we have successfully set
up an f to the k problem.

01:04:02.195 --> 01:04:05.330
The only thing you need to do
to solve the overall predecessor

01:04:05.330 --> 01:04:08.060
problem is to find your
predecessor in the node,

01:04:08.060 --> 01:04:11.480
because there's only one element
within each subtree that's

01:04:11.480 --> 01:04:13.902
enough to figure out
your overall predecessor.

01:04:21.900 --> 01:04:31.180
OK, depth i, at squared, I guess
you can think of this as y.

01:04:31.180 --> 01:04:33.970
That's the part that Bob knows.

01:04:33.970 --> 01:04:39.610
These are, in some sense,
xi up to xk minus 1.

01:04:39.610 --> 01:04:42.880
And this is why we had to say
over here that Bob already

01:04:42.880 --> 01:04:48.670
knew xi up to xi minus
1, because Bob already

01:04:48.670 --> 01:04:52.410
knows that the shared
prefix among all the items--

01:04:52.410 --> 01:04:55.056
sorry, not minus 1, i minus i.

01:04:58.630 --> 01:05:02.990
And so all the content is
here in this node, which is y.

01:05:02.990 --> 01:05:06.280
And so it reduces to a regular
predecessor problem, which

01:05:06.280 --> 01:05:11.595
is what f will always denote,
colored predecessor of xi, y.

01:05:11.595 --> 01:05:12.095
Cool.

01:05:14.710 --> 01:05:18.600
OK, what happens here
in terms of n and w?

01:05:18.600 --> 01:05:20.110
We have a smaller problem here.

01:05:20.110 --> 01:05:23.490
We threw away all this stuff.

01:05:23.490 --> 01:05:28.180
What got reduced is
our word size for here.

01:05:28.180 --> 01:05:32.085
We started with something
of size w prime.

01:05:32.085 --> 01:05:33.460
And now, we end
up with something

01:05:33.460 --> 01:05:36.130
of this size, which
was W prime divided

01:05:36.130 --> 01:05:41.770
by at squared up to theta.

01:05:41.770 --> 01:05:47.260
So this reduction reduces--

01:05:47.260 --> 01:05:50.830
or this round elimination
reduces w prime

01:05:50.830 --> 01:05:55.030
to w prime, divided by
at squared, theta that.

01:06:01.180 --> 01:06:05.680
I think that's all I need
to say at this point.

01:06:05.680 --> 01:06:09.576
The claim is that this picture
is kind of like Van Emde

01:06:09.576 --> 01:06:11.870
Boas in the following sense.

01:06:11.870 --> 01:06:14.500
Van Emde Boas is,
essentially, binary

01:06:14.500 --> 01:06:17.350
searching on which
level matters.

01:06:17.350 --> 01:06:19.450
If it goes too low,
things are empty.

01:06:19.450 --> 01:06:21.429
If it goes too high,
there's too many items.

01:06:21.429 --> 01:06:23.470
So it's binary searching
for that critical point,

01:06:23.470 --> 01:06:28.360
where you basically have
a node and nothing else.

01:06:28.360 --> 01:06:30.550
And that's why it took log w.

01:06:30.550 --> 01:06:33.340
So here, one level matters.

01:06:33.340 --> 01:06:36.190
And the goal is to
figure out which one.

01:06:36.190 --> 01:06:38.230
And it's not exactly
binary search here.

01:06:38.230 --> 01:06:41.090
We're losing a larger factor,
at squared, at each step.

01:06:41.090 --> 01:06:43.300
So we're not reducing
w to w over 2,

01:06:43.300 --> 01:06:44.750
like we did with Van Emde Boas.

01:06:44.750 --> 01:06:46.500
But this is why we're
losing a little bit.

01:06:46.500 --> 01:06:49.430
We reduce by a
factor of at squared.

01:06:49.430 --> 01:06:51.680
So it's kind of like the
Van Emde Boas upper bound.

01:06:51.680 --> 01:06:55.240
But here, we're setting up a
lower bound picture so that--

01:06:55.240 --> 01:06:57.520
this is still an arbitrary
predecessor problem.

01:06:57.520 --> 01:07:00.352
But figuring out which level
matters, that's tricky.

01:07:00.352 --> 01:07:01.810
And you really
can't do much better

01:07:01.810 --> 01:07:05.080
than Van Emde Boas to
style a binary search.

01:07:05.080 --> 01:07:08.740
OK, maybe you can do at
squared wave binary search,

01:07:08.740 --> 01:07:12.040
instead of binary search.

01:07:12.040 --> 01:07:14.340
So that was eliminating a
message from Alice to Bob.

01:07:14.340 --> 01:07:16.090
Eliminating a message
from Bob to Alice

01:07:16.090 --> 01:07:18.370
is going to look
like fusion trees.

01:07:18.370 --> 01:07:21.520
That's the cool thing.

01:07:21.520 --> 01:07:22.520
So let's do that.

01:07:35.240 --> 01:07:35.900
Next page.

01:07:55.490 --> 01:08:00.465
OK, let's suppose--
so this is going

01:08:00.465 --> 01:08:02.590
to get slightly confusing
notationally, because you

01:08:02.590 --> 01:08:04.690
have to reverse a and b.

01:08:04.690 --> 01:08:08.830
The picture I set up over here,
and for the round elimination

01:08:08.830 --> 01:08:13.210
lamma f to the k, and
round elimination lamma,

01:08:13.210 --> 01:08:15.130
are all phrased in
terms of eliminating

01:08:15.130 --> 01:08:16.467
the Alice to Bob message.

01:08:16.467 --> 01:08:17.800
You've got to invert everything.

01:08:17.800 --> 01:08:20.950
So we're going to get square
root of b over k error

01:08:20.950 --> 01:08:23.020
if I don't relabel things.

01:08:23.020 --> 01:08:25.479
So you can restate this,
if Bob speaks first,

01:08:25.479 --> 01:08:27.340
and then Alice
speaks next, I get

01:08:27.340 --> 01:08:30.399
square root of b over k error.

01:08:30.399 --> 01:08:34.330
The F to the k problem is
now that Bob has k inputs.

01:08:34.330 --> 01:08:37.569
Maybe call them y1 to yk.

01:08:37.569 --> 01:08:40.550
Alice has an input
x and the integer i.

01:08:40.550 --> 01:08:44.189
So now Alice, the
querier, knows what i is.

01:08:44.189 --> 01:08:45.250
But Bob doesn't.

01:08:45.250 --> 01:08:47.200
The data structure
doesn't know it.

01:08:47.200 --> 01:08:48.850
So it just reversed everything.

01:08:48.850 --> 01:08:52.180
So let me state it over here.

01:08:52.180 --> 01:08:55.330
Bob has input.

01:08:55.330 --> 01:08:58.750
Now, in general, what Bob
knows is a bunch of integers,

01:08:58.750 --> 01:08:59.380
n integers.

01:08:59.380 --> 01:09:01.270
In general, n prime
integers, because n

01:09:01.270 --> 01:09:04.390
is also going to increase.

01:09:04.390 --> 01:09:09.970
And let's say each of
them is w prime bits.

01:09:09.970 --> 01:09:11.859
That's what Bob knows.

01:09:11.859 --> 01:09:12.634
That's y.

01:09:16.029 --> 01:09:19.420
So what do we have to do to
phrase an f to the k problem?

01:09:19.420 --> 01:09:23.840
Well, just like this, we've
got to break that input into bt

01:09:23.840 --> 01:09:26.550
squared equal-sized chunks.

01:09:26.550 --> 01:09:28.300
So let's just think
about what that means.

01:09:44.970 --> 01:09:50.499
So it's going to be y1 up to yk.

01:09:50.499 --> 01:09:53.299
k is theta bt squared.

01:09:53.299 --> 01:09:55.090
Again, we want bt
squared, because then you

01:09:55.090 --> 01:09:57.910
plug it into this
root b over k formula.

01:09:57.910 --> 01:10:03.430
And you get that
the error increase

01:10:03.430 --> 01:10:07.360
is order 1 over t,
which is what we need

01:10:07.360 --> 01:10:11.180
to do t round eliminations.

01:10:11.180 --> 01:10:14.180
So before, our input
was a single word.

01:10:14.180 --> 01:10:16.880
Now, our input is a
whole bunch of integers.

01:10:16.880 --> 01:10:19.750
So it's natural to split it
up into different integers.

01:10:24.487 --> 01:10:25.820
Here's how I'm going to do that.

01:10:30.000 --> 01:10:32.630
I'll just draw the
picture over here.

01:10:32.630 --> 01:10:38.020
So this was the Alice
to Bob elimination.

01:10:38.020 --> 01:10:41.710
Now, we'll do the
Bob to Alice picture.

01:10:44.980 --> 01:10:47.344
So what I'd like to do is
split up all the inputs.

01:10:47.344 --> 01:10:48.760
What's the natural
way to do that?

01:10:48.760 --> 01:10:53.486
Well, have a tree at the top.

01:10:53.486 --> 01:10:54.860
Let's just make
it a binary tree.

01:10:54.860 --> 01:10:57.760
So I'm drawing the usual
picture I draw when

01:10:57.760 --> 01:11:00.960
I draw a bunch of elements.

01:11:00.960 --> 01:11:04.537
And then there's some
elements left over

01:11:04.537 --> 01:11:05.620
in each of these subtrees.

01:11:11.710 --> 01:11:14.630
Each of these is a
predecessor problem.

01:11:14.630 --> 01:11:19.705
I'm constraining the
inputs to sort of differ

01:11:19.705 --> 01:11:21.970
a lot in the beginning,
in the early bits,

01:11:21.970 --> 01:11:23.060
the most significant bits.

01:11:23.060 --> 01:11:25.226
This is the most significant,
next most significant,

01:11:25.226 --> 01:11:26.110
and so on, bits.

01:11:26.110 --> 01:11:31.822
I want the number of leaves
here to be theta bt squared--

01:11:31.822 --> 01:11:34.030
or not leaves, but number
of intermediate nodes here.

01:11:36.940 --> 01:11:41.410
And, therefore, there are
theta bt squared subtrees.

01:11:41.410 --> 01:11:44.490
And I want to set up the n items
to be uniformly distributed.

01:11:44.490 --> 01:11:50.920
So each of these has n
divided by bt squared items.

01:11:50.920 --> 01:11:54.220
So we used a few
bits up top here.

01:11:54.220 --> 01:11:58.639
How many bits was this,
just log of bt squared?

01:11:58.639 --> 01:12:00.430
So if you look at one
of these subproblems,

01:12:00.430 --> 01:12:05.230
we have reduced w slightly, but
only by this additive amount.

01:12:05.230 --> 01:12:07.162
It basically won't matter.

01:12:07.162 --> 01:12:08.620
The big thing we
changed is we used

01:12:08.620 --> 01:12:11.950
to have n items to search among,
or this is n prime, I guess.

01:12:11.950 --> 01:12:15.480
Now, we have and divided
by bt squared items.

01:12:15.480 --> 01:12:17.990
So we reduced n substantially.

01:12:17.990 --> 01:12:19.830
And this is what
fusion trees do, right?

01:12:19.830 --> 01:12:22.570
They look at the root
first, and say, OK, look, I

01:12:22.570 --> 01:12:25.210
can distinguish w to the
epsilon, different things

01:12:25.210 --> 01:12:26.710
in constant time.

01:12:26.710 --> 01:12:28.090
Here, it's on w to the epsilon.

01:12:28.090 --> 01:12:29.530
But it's b to bt squared.

01:12:29.530 --> 01:12:31.930
And we haven't figured
out what bt are yet.

01:12:31.930 --> 01:12:33.170
I mean, b is w.

01:12:33.170 --> 01:12:39.700
So that's close to a w
factor, w folds search.

01:12:39.700 --> 01:12:42.290
And then you determine, am I in
this subtree, or this subtree?

01:12:42.290 --> 01:12:45.319
This is exactly what
fusion trees do.

01:12:45.319 --> 01:12:47.110
But we're setting up
a lower bound instance

01:12:47.110 --> 01:12:48.910
where that is the
right thing to do,

01:12:48.910 --> 01:12:52.510
where you have to know
what the early bits are

01:12:52.510 --> 01:12:57.640
before you can figure out
which subtree you're in.

01:12:57.640 --> 01:13:00.550
Now, reverse roles.

01:13:00.550 --> 01:13:04.666
Alice knows what these bits are.

01:13:04.666 --> 01:13:06.040
The data structure
of relevance--

01:13:06.040 --> 01:13:08.350
let's suppose you fit
here, because you are 0111

01:13:08.350 --> 01:13:10.540
in the beginning.

01:13:10.540 --> 01:13:14.770
This is our yi problem.

01:13:14.770 --> 01:13:16.060
Alice knows what i is.

01:13:16.060 --> 01:13:18.250
Alice knows what those
leading bits are.

01:13:18.250 --> 01:13:20.230
Bob doesn't.

01:13:20.230 --> 01:13:22.090
And Bob is speaking first.

01:13:22.090 --> 01:13:24.710
So Bob could try to summarize
this entire data structure.

01:13:24.710 --> 01:13:26.500
But there's basically no hope.

01:13:26.500 --> 01:13:29.180
Bob needs to know, what
are those early bits?

01:13:29.180 --> 01:13:31.502
So the first message from
Bob is going to be useless.

01:13:31.502 --> 01:13:32.710
Alice can then send the bits.

01:13:32.710 --> 01:13:34.293
And then you can
restrict your problem

01:13:34.293 --> 01:13:36.480
to a regular predecessor
problem in here.

01:13:36.480 --> 01:13:38.732
And so you've eliminated
this sort of root node.

01:13:38.732 --> 01:13:40.940
Your entire problem is a
predecessor problem in here.

01:13:40.940 --> 01:13:44.560
So, again, we have an f
to the k style problem.

01:13:44.560 --> 01:13:47.680
We reduce to a predecessor
problem on a smaller--

01:13:47.680 --> 01:13:50.520
in this case, y
and x are reversed.

01:13:50.520 --> 01:13:52.840
But we end up with
a smaller problem,

01:13:52.840 --> 01:13:55.740
mainly in the fact
that n got smaller.

01:13:55.740 --> 01:14:00.010
So let me go back to this board.

01:14:00.010 --> 01:14:05.490
What happens is that
we reduce n prime

01:14:05.490 --> 01:14:10.810
to n prime, divided
by bt squared.

01:14:14.060 --> 01:14:17.020
There's theta.

01:14:17.020 --> 01:14:20.200
We also reduce w prime.

01:14:20.200 --> 01:14:27.050
w prime becomes w prime,
minus log bt squared.

01:14:27.050 --> 01:14:28.810
OK, but in particular,
this is going

01:14:28.810 --> 01:14:33.910
to be at least w prime
over 2 most of the time,

01:14:33.910 --> 01:14:35.020
till the very end.

01:14:35.020 --> 01:14:38.950
And over here, we're reducing
w prime by a big factor,

01:14:38.950 --> 01:14:40.540
something bigger
than a constant.

01:14:40.540 --> 01:14:42.760
So we don't really care
about losing a factor of 2,

01:14:42.760 --> 01:14:44.520
because we alternate
between these.

01:14:44.520 --> 01:14:47.990
And so a factor of 2 just
falls into the theta.

01:14:47.990 --> 01:14:53.140
So it's going to be at least
that, as long as w prime is,

01:14:53.140 --> 01:14:54.940
I don't know, at least log--

01:15:00.570 --> 01:15:01.950
log w would be enough.

01:15:07.610 --> 01:15:09.390
Yeah, this is log w.

01:15:09.390 --> 01:15:12.660
t squared is going to
be, at most, log w.

01:15:12.660 --> 01:15:18.045
So this is going to be
log w, plus log log w.

01:15:18.045 --> 01:15:20.460
Log log w goes away.

01:15:20.460 --> 01:15:21.435
What is t?

01:15:21.435 --> 01:15:23.550
The whole point is to
prove a lower bound on t.

01:15:23.550 --> 01:15:25.383
But we know from an
upper bound perspective,

01:15:25.383 --> 01:15:28.655
t is, at most, log w,
because Van Emde Boas exists.

01:15:28.655 --> 01:15:31.910
So we're never going to prove a
lower bound bigger than log w.

01:15:31.910 --> 01:15:34.680
So whatever t is going
to be is, at most, log w.

01:15:34.680 --> 01:15:37.470
So this is just log w.

01:15:37.470 --> 01:15:39.880
So you're fine.

01:15:39.880 --> 01:15:42.720
But at some point,
we have to stop.

01:15:42.720 --> 01:15:45.000
We start with some w and n.

01:15:45.000 --> 01:15:47.630
n prime is racing
down at this rate.

01:15:47.630 --> 01:15:51.050
w prime is racing
down at this rate.

01:15:51.050 --> 01:15:56.820
When w prime gets down to
log w, then we're in trouble.

01:15:56.820 --> 01:16:02.740
If n prime gets down to 2,
1, 0, one of those constants,

01:16:02.740 --> 01:16:05.250
when n prime gets down to a
constant, we're in trouble.

01:16:05.250 --> 01:16:08.120
You can no longer
evenly divide things.

01:16:08.120 --> 01:16:09.980
So when does that happen?

01:16:09.980 --> 01:16:13.290
This is where we're going
to get a lower bound.

01:16:13.290 --> 01:16:14.940
So I guess we can
go back over here.

01:16:20.060 --> 01:16:23.130
So if you do a
round elimination,

01:16:23.130 --> 01:16:26.130
you decrease w prime
by this factor.

01:16:26.130 --> 01:16:28.530
And you decrease n
prime by this factor,

01:16:28.530 --> 01:16:30.820
both of these in sequence.

01:16:30.820 --> 01:16:34.020
Then you repeat it.

01:16:34.020 --> 01:16:43.620
So we have to stop when
w prime hits log w,

01:16:43.620 --> 01:16:50.134
or when n prime
hits 2, let's say.

01:16:50.134 --> 01:16:52.050
This is not really going
to make a difference.

01:16:52.050 --> 01:16:53.258
You can also think that as 2.

01:16:56.700 --> 01:17:01.394
So, in fact, I'll just
do that, just to clean

01:17:01.394 --> 01:17:02.560
the arithmetic a little bit.

01:17:07.390 --> 01:17:10.660
Now, if we succeed in
eliminating all the messages,

01:17:10.660 --> 01:17:13.000
we get a contradiction,
because we set up our errors

01:17:13.000 --> 01:17:15.760
to add up to only 1/3.

01:17:15.760 --> 01:17:18.430
And we know the error
has to be at least 1/2,

01:17:18.430 --> 01:17:21.070
if you eliminate all the
messages, which means we didn't

01:17:21.070 --> 01:17:22.900
eliminate all the messages.

01:17:22.900 --> 01:17:28.720
Now, we will not eliminate all
the messages if t is large.

01:17:28.720 --> 01:17:31.420
So when would we eliminate
all the messages?

01:17:31.420 --> 01:17:32.460
Let's compute that.

01:17:32.460 --> 01:17:37.210
And then t has to be
at least that big.

01:17:37.210 --> 01:17:40.735
t has to be greater than
the point at which we would

01:17:40.735 --> 01:17:41.860
eliminate all the messages.

01:17:41.860 --> 01:17:44.390
Otherwise, we would
get a contradiction.

01:17:44.390 --> 01:17:48.280
So this is where we get a
min, which is kind of cool,

01:17:48.280 --> 01:17:50.800
because we have to stop
when w prime gets too small,

01:17:50.800 --> 01:17:54.820
or when n prime gets too
small, whichever happens first.

01:17:54.820 --> 01:17:59.170
And so it's going to be a
min of the thing relating

01:17:59.170 --> 01:18:06.280
to w, which is going to be
log base at squared of w,

01:18:06.280 --> 01:18:08.580
because we're reducing w
by a factor of at squared

01:18:08.580 --> 01:18:09.500
in each step.

01:18:09.500 --> 01:18:11.440
So that's when it's
going to bottom out.

01:18:11.440 --> 01:18:13.300
And then there's the
thing for n, which

01:18:13.300 --> 01:18:17.437
is log base bt squared of n.

01:18:17.437 --> 01:18:19.270
That's when n will
bottom out, because we're

01:18:19.270 --> 01:18:21.390
reducing by a factor of
bt square at each step.

01:18:24.010 --> 01:18:28.790
So that's the bound we get.

01:18:28.790 --> 01:18:32.960
Now, it doesn't look quite
like the bound that we want,

01:18:32.960 --> 01:18:36.740
which is this very simple log,
base of w, log base b of n.

01:18:36.740 --> 01:18:38.800
But I claim that it's
basically the same thing.

01:18:50.350 --> 01:18:51.880
Why?

01:18:51.880 --> 01:18:54.490
I got to cheat here, speed
things up a little bit.

01:18:54.490 --> 01:19:01.090
I claim that at squared
is order a cubed.

01:19:01.090 --> 01:19:02.890
Why?

01:19:02.890 --> 01:19:06.480
Because a is at least log n.

01:19:06.480 --> 01:19:07.885
Remember, a is log of space.

01:19:10.430 --> 01:19:11.800
You have to store the data.

01:19:11.800 --> 01:19:14.440
So you need at least
n words of space.

01:19:14.440 --> 01:19:15.540
a is log of that.

01:19:15.540 --> 01:19:17.200
So it's got to be
at least log n.

01:19:17.200 --> 01:19:20.290
And, furthermore,
t is at most log n.

01:19:22.910 --> 01:19:23.410
Why?

01:19:23.410 --> 01:19:25.618
Because there is a predecessor
or data structure that

01:19:25.618 --> 01:19:27.640
runs in log n time
operation, namely

01:19:27.640 --> 01:19:29.500
balance binary search trees.

01:19:29.500 --> 01:19:33.250
There's a theta here,
order, big O. But at squared

01:19:33.250 --> 01:19:37.360
is, at most, a cubed.

01:19:37.360 --> 01:19:45.190
On the other hand, this thing,
bt square, is order b cubed.

01:19:45.190 --> 01:19:45.950
Why?

01:19:45.950 --> 01:19:56.460
Because t is order
log w. b is w.

01:19:56.460 --> 01:19:58.260
That's the definition of b.

01:19:58.260 --> 01:20:01.230
T is order log w, because
Van Emde Boas exists.

01:20:01.230 --> 01:20:05.230
And so this is actually, at
most, w times log squared w.

01:20:05.230 --> 01:20:07.980
But, in particular,
it's at most w cubed,

01:20:07.980 --> 01:20:11.336
because we don't care about that
constant because it's in a log.

01:20:11.336 --> 01:20:12.210
So it just comes out.

01:20:12.210 --> 01:20:13.960
It's a factor of
1/3 or whatever.

01:20:13.960 --> 01:20:21.030
And so this is the same
thing as log base a of w,

01:20:21.030 --> 01:20:24.480
log base b of n, minned.

01:20:24.480 --> 01:20:26.190
And that's the nice,
symmetric bound

01:20:26.190 --> 01:20:30.232
that you get our of round
elimination, pretty cool.

01:20:30.232 --> 01:20:31.690
Now, if you look
in the notes, I'll

01:20:31.690 --> 01:20:33.190
just mentioned this
briefly, there's

01:20:33.190 --> 01:20:36.780
a couple of more pages that
give you a little bit of flavor

01:20:36.780 --> 01:20:40.620
of what the round elimination
and that vague proof sketch,

01:20:40.620 --> 01:20:44.980
which gave the wrong answer by
a square root, really means.

01:20:44.980 --> 01:20:48.870
We said, oh, Alice
can only communicate,

01:20:48.870 --> 01:20:54.510
in some sense, a
over k bits about xi.

01:20:54.510 --> 01:20:56.970
And you can formalize that by
talking about distributions

01:20:56.970 --> 01:21:04.230
of inputs, and talking about
the expected amount of entropy

01:21:04.230 --> 01:21:11.280
of xi communicated by
that first message.

01:21:11.280 --> 01:21:13.530
And so it's very simple
definitions of entropy,

01:21:13.530 --> 01:21:15.780
and shared information,
but just in terms

01:21:15.780 --> 01:21:17.130
of probabilistic quantities.

01:21:17.130 --> 01:21:18.810
And it at least
gives you a sense

01:21:18.810 --> 01:21:20.670
of how you might prove
something like this.

01:21:20.670 --> 01:21:22.920
Whereas talking about
information about something

01:21:22.920 --> 01:21:25.300
is kind of vague,
talking about entropy,

01:21:25.300 --> 01:21:27.240
which is about sum
of probabilities

01:21:27.240 --> 01:21:30.420
times log of probabilities is
a clean, probabilistic notion.

01:21:30.420 --> 01:21:35.190
So it becomes a purely
probabilistic statements

01:21:35.190 --> 01:21:36.510
about error probabilities.

01:21:36.510 --> 01:21:38.040
And that's how you argue this.

01:21:38.040 --> 01:21:40.440
But even these notes
do not give a proof.

01:21:40.440 --> 01:21:43.200
They just give a hint of how
you formalize what this means,

01:21:43.200 --> 01:21:45.420
or what the proof sketch means.

01:21:45.420 --> 01:21:48.340
And it's several pages
to actually prove it,

01:21:48.340 --> 01:21:50.550
so a bit beyond this class.

01:21:50.550 --> 01:21:53.070
But once you have it--
because it's very clean,

01:21:53.070 --> 01:21:56.010
I'd say kind of beautiful
lower bound, not quite

01:21:56.010 --> 01:21:58.110
the right answer, but
up to log log factors,

01:21:58.110 --> 01:22:00.610
the right answer
for predecessor.

01:22:00.610 --> 01:22:04.250
So that's the end of
predecessor in this class.