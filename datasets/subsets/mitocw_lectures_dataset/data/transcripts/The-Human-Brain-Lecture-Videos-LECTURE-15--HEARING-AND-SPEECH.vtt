WEBVTT

00:00:00.000 --> 00:00:01.948
[SQUEAKING]

00:00:01.948 --> 00:00:04.383
[RUSTLING]

00:00:04.383 --> 00:00:10.240
[CLICKING]

00:00:10.240 --> 00:00:12.880
NANCY KANWISHER: We are turning
from our various other topics

00:00:12.880 --> 00:00:14.590
to talk about hearing today.

00:00:14.590 --> 00:00:18.190
And let's start by thinking
about all the cool stuff

00:00:18.190 --> 00:00:21.940
that you can do
just by listening.

00:00:21.940 --> 00:00:25.510
So just by listening, you
can identify the scene

00:00:25.510 --> 00:00:27.310
that you're in and
what's going on

00:00:27.310 --> 00:00:30.025
in it, like for example, this.

00:00:30.025 --> 00:00:35.887
[AUDIO PLAYBACK]

00:00:35.887 --> 00:00:36.470
[END PLAYBACK]

00:00:36.470 --> 00:00:38.780
OK, so you know what kind of
room you're in and roughly

00:00:38.780 --> 00:00:42.680
what's going on, just from
that little bit of sound.

00:00:42.680 --> 00:00:45.860
You can localize events
and people and objects.

00:00:45.860 --> 00:00:47.510
So close your eyes, everyone.

00:00:47.510 --> 00:00:48.810
Keep them closed.

00:00:48.810 --> 00:00:52.130
And if you just
listen to me talking,

00:00:52.130 --> 00:00:54.650
it's really very
vivid, isn't it,

00:00:54.650 --> 00:00:58.040
exactly how obvious
it is where I am.

00:00:58.040 --> 00:01:00.560
And I will refrain
from the temptation

00:01:00.560 --> 00:01:03.162
of coming up and speaking
in somebody's ears

00:01:03.162 --> 00:01:04.370
because it's just too creepy.

00:01:04.370 --> 00:01:05.540
OK, you can open your eyes.

00:01:05.540 --> 00:01:06.620
It's very vivid.

00:01:06.620 --> 00:01:10.340
Just from listening, you know
where the sound source is.

00:01:10.340 --> 00:01:13.700
You can recognize sound
sources, so for example,

00:01:13.700 --> 00:01:14.870
sounds like this--

00:01:14.870 --> 00:01:18.072
[GLASS BREAKING]

00:01:18.072 --> 00:01:19.280
You know what happened there.

00:01:19.280 --> 00:01:21.260
It's a whole vivid event
just unfolded there

00:01:21.260 --> 00:01:24.860
in a whole second and a
half, or a random series

00:01:24.860 --> 00:01:25.753
of sounds like this.

00:01:25.753 --> 00:01:26.420
[AUDIO PLAYBACK]

00:01:26.420 --> 00:01:29.729
- It's supposed to
either rain or snow.

00:01:29.729 --> 00:01:34.776
[RANDOM SOUNDS]

00:01:34.776 --> 00:01:37.531
- Hannah is good
at compromising.

00:01:37.531 --> 00:01:41.950
[RANDOM SOUNDS]

00:01:41.950 --> 00:01:44.337
[LAUGHTER]

00:01:44.337 --> 00:01:44.920
[END PLAYBACK]

00:01:44.920 --> 00:01:45.910
NANCY KANWISHER: Anyway,
every one of those

00:01:45.910 --> 00:01:47.635
sounds you
immediately recognize.

00:01:47.635 --> 00:01:50.680
You know exactly what it is.

00:01:50.680 --> 00:01:52.570
And that's environmental
sounds, things

00:01:52.570 --> 00:01:55.540
that happen outdoors,
speech, what is being said,

00:01:55.540 --> 00:01:57.700
voices, who is saying it.

00:01:57.700 --> 00:02:00.320
If you don't know the person,
if they're male or female,

00:02:00.320 --> 00:02:02.140
young or old, much like faces--

00:02:02.140 --> 00:02:05.980
if you know them, you'll
recognize them pretty fast.

00:02:05.980 --> 00:02:09.449
You can selectively attend
to one sound among others.

00:02:09.449 --> 00:02:14.460
Like if you had a little, hidden
earphone that I didn't see,

00:02:14.460 --> 00:02:16.543
and you wanted to listen
to your favorite podcast,

00:02:16.543 --> 00:02:18.293
you could listen to
that occasionally when

00:02:18.293 --> 00:02:19.170
I was getting boring.

00:02:19.170 --> 00:02:21.690
And then you could turn
back and listen to me.

00:02:21.690 --> 00:02:23.490
And you could just
selectively choose

00:02:23.490 --> 00:02:27.870
which of those different
audio inputs to listen to.

00:02:27.870 --> 00:02:30.810
And we'll talk more in a moment
about this classic problem

00:02:30.810 --> 00:02:33.570
in hearing, which is known as
the "cocktail party effect."

00:02:33.570 --> 00:02:36.978
I guess it was named in the '50s
when cocktail parties were big.

00:02:36.978 --> 00:02:38.520
And it consists in
the fact that when

00:02:38.520 --> 00:02:41.010
there are multiple sound
sources, such as many people

00:02:41.010 --> 00:02:44.370
talking in a room, you
can tune in one channel

00:02:44.370 --> 00:02:46.620
and then tune in
another channel.

00:02:46.620 --> 00:02:48.570
And you can just
selectively attend

00:02:48.570 --> 00:02:51.000
to one of many
sound sources, even

00:02:51.000 --> 00:02:54.270
though those sound sources are
massively overlapping on top

00:02:54.270 --> 00:02:55.590
of each other in the input.

00:02:55.590 --> 00:02:57.660
And it's a big
computational challenge,

00:02:57.660 --> 00:03:00.960
as we'll talk about
shortly, to do that.

00:03:00.960 --> 00:03:03.610
You can enjoy music.

00:03:03.610 --> 00:03:05.898
And you can determine
what things are made of.

00:03:05.898 --> 00:03:08.440
So close your eyes and I'm going
to drop things on the table.

00:03:08.440 --> 00:03:08.940
Don't look.

00:03:08.940 --> 00:03:10.648
I'm going to do various
things and you're

00:03:10.648 --> 00:03:11.710
going to identify them.

00:03:14.290 --> 00:03:17.000
So let's see-- don't
open your eyes.

00:03:17.000 --> 00:03:19.640
See if you can tell what's
being dropped on the table,

00:03:19.640 --> 00:03:20.890
or at least what it's made of.

00:03:20.890 --> 00:03:21.950
Close your eyes.

00:03:21.950 --> 00:03:23.750
That's cheating.

00:03:23.750 --> 00:03:24.720
Wood, exactly.

00:03:24.720 --> 00:03:25.220
Very good.

00:03:25.220 --> 00:03:26.120
OK, what is this?

00:03:26.120 --> 00:03:27.260
Keep your eyes closed.

00:03:27.260 --> 00:03:30.180
What is this made of?

00:03:30.180 --> 00:03:30.680
- Plastic.

00:03:30.680 --> 00:03:31.310
NANCY KANWISHER: Yeah, good.

00:03:31.310 --> 00:03:32.240
Keep your eyes closed.

00:03:32.240 --> 00:03:34.730
What is this made of?

00:03:34.730 --> 00:03:36.843
STUDENT: [INAUDIBLE]

00:03:36.843 --> 00:03:37.760
NANCY KANWISHER: Yeah.

00:03:37.760 --> 00:03:39.080
OK, keep your eyes closed.

00:03:39.080 --> 00:03:42.044
What's this made of?

00:03:42.044 --> 00:03:42.990
STUDENT: [INAUDIBLE].

00:03:42.990 --> 00:03:43.410
NANCY KANWISHER: Awesome.

00:03:43.410 --> 00:03:44.790
OK, you can open your eyes.

00:03:44.790 --> 00:03:45.420
Perfect.

00:03:45.420 --> 00:03:47.050
You guys are awesome.

00:03:47.050 --> 00:03:49.530
I just dropped these objects
that I found from my kitchen

00:03:49.530 --> 00:03:52.740
this morning and you guys could
tell what they're made of.

00:03:52.740 --> 00:03:55.860
That's amazing.

00:03:55.860 --> 00:03:59.160
OK, all of this that
you guys just did

00:03:59.160 --> 00:04:02.125
happens from the
simplest possible signal.

00:04:02.125 --> 00:04:04.500
We'll talk about what that
signal is exactly in a moment,

00:04:04.500 --> 00:04:07.870
but it's just sound compression
coming through the air.

00:04:07.870 --> 00:04:11.140
And it tells you all this rich
stuff about your environment.

00:04:11.140 --> 00:04:13.540
So the question is,
how do we do that?

00:04:13.540 --> 00:04:15.750
And the first
question is, how do we

00:04:15.750 --> 00:04:17.970
start to think about
how hearing works, how

00:04:17.970 --> 00:04:19.500
you're able to do all of that?

00:04:19.500 --> 00:04:23.400
And you guys know we start
with computational theory--

00:04:23.400 --> 00:04:26.850
considering what the inputs
are, what the outputs are,

00:04:26.850 --> 00:04:30.180
the physics of sound,
what would be involved

00:04:30.180 --> 00:04:33.090
if we tried to code up a machine
to take those audio input

00:04:33.090 --> 00:04:35.070
and deliver the output
that you guys all

00:04:35.070 --> 00:04:37.860
just delivered with
no trouble whatsoever.

00:04:37.860 --> 00:04:39.600
What cues are in the stimulus?

00:04:39.600 --> 00:04:41.760
What are the key
computational challenges?

00:04:41.760 --> 00:04:45.222
And what makes those aspects
of hearing challenging?

00:04:45.222 --> 00:04:46.680
And then after we
do all that stuff

00:04:46.680 --> 00:04:48.480
at the level of
computational theory,

00:04:48.480 --> 00:04:51.170
we can, of course, study
hearing in other ways,

00:04:51.170 --> 00:04:52.420
like studying it behaviorally.

00:04:52.420 --> 00:04:53.850
What can people do and not do?

00:04:53.850 --> 00:04:54.360
What's hard?

00:04:54.360 --> 00:04:56.040
What's less hard?

00:04:56.040 --> 00:04:57.880
And we can measure
neural responses.

00:04:57.880 --> 00:04:59.800
So we'll talk about all of that.

00:04:59.800 --> 00:05:03.900
But let's start with a
little more on what sound is.

00:05:03.900 --> 00:05:07.620
So sound is just a
single univariate signal

00:05:07.620 --> 00:05:08.837
coming into the ears.

00:05:08.837 --> 00:05:10.420
We'll say more about
that in a second,

00:05:10.420 --> 00:05:12.900
but it's really, really simple.

00:05:12.900 --> 00:05:16.950
And from that, you get
all this rich experience.

00:05:16.950 --> 00:05:18.480
And so the question
is, what goes

00:05:18.480 --> 00:05:20.700
on in that magic
box in the middle

00:05:20.700 --> 00:05:23.250
to enable you to extract
this kind of information

00:05:23.250 --> 00:05:26.070
from this really simple signal?

00:05:26.070 --> 00:05:28.470
So let's start
with what is sound.

00:05:28.470 --> 00:05:32.640
Sound is just a set of
longitudinal compressions

00:05:32.640 --> 00:05:36.600
and decompressions of the
air coming from the source

00:05:36.600 --> 00:05:38.580
into your ear.

00:05:38.580 --> 00:05:41.340
So these waves travel
from the source

00:05:41.340 --> 00:05:44.590
to the ear in little
waves of compression

00:05:44.590 --> 00:05:46.950
where the air is
just compressed,

00:05:46.950 --> 00:05:49.890
and rarefaction where
the air is spread out.

00:05:52.530 --> 00:05:55.050
And just to give you a sense
of how physical sound is,

00:05:55.050 --> 00:05:56.430
there's a silly video here.

00:05:56.430 --> 00:05:58.800
It's a speaker in a sink
with a bunch of paint.

00:05:58.800 --> 00:06:01.320
And you can just see that
the movement of the speaker--

00:06:01.320 --> 00:06:04.230
normally, it makes those
compressions and rarefactions

00:06:04.230 --> 00:06:05.860
of air, but if you
stick paint on it.

00:06:05.860 --> 00:06:08.190
It's going to shove the
paint up in the air,

00:06:08.190 --> 00:06:12.000
too, just to show you
how physical it is.

00:06:12.000 --> 00:06:15.030
There's something called
Schlieren photography,

00:06:15.030 --> 00:06:17.850
which is totally cool, and which
is a way to visualize those

00:06:17.850 --> 00:06:20.603
compressions of the air
to show you what's--

00:06:20.603 --> 00:06:21.270
[VIDEO PLAYBACK]

00:06:21.270 --> 00:06:22.103
[INTERPOSING VOICES]

00:06:22.103 --> 00:06:23.910
- --use it to study
aerodynamic flow.

00:06:23.910 --> 00:06:27.390
And sound-- well, that's just
another change in air density,

00:06:27.390 --> 00:06:29.250
a traveling compression wave.

00:06:29.250 --> 00:06:32.520
So Schlieren visualization,
along with a high-speed camera,

00:06:32.520 --> 00:06:35.530
can be used to see it as well.

00:06:35.530 --> 00:06:38.430
Here's a book
landing on a table,

00:06:38.430 --> 00:06:41.640
the end of a towel
being snapped,

00:06:41.640 --> 00:06:49.260
a firecracker, an AK-47,
and of course, a clap.

00:06:52.260 --> 00:06:53.160
[END PLAYBACK]

00:06:53.160 --> 00:06:55.440
NANCY KANWISHER: OK, so
just compressions of air

00:06:55.440 --> 00:07:00.000
traveling from the source to
your ears-- that's all it is.

00:07:00.000 --> 00:07:03.030
So natural sounds happen at
lots of different frequencies.

00:07:03.030 --> 00:07:05.760
And one of the ways
we describe sounds

00:07:05.760 --> 00:07:08.290
is by looking at
those frequencies.

00:07:08.290 --> 00:07:12.820
So there's an awesome website
that is here on your slides.

00:07:12.820 --> 00:07:14.160
You can play with it offline.

00:07:14.160 --> 00:07:17.040
But meanwhile, we're going to
play with it a little bit right

00:07:17.040 --> 00:07:22.283
now because it is so cool.

00:07:22.283 --> 00:07:23.700
So what we're going
to do is we're

00:07:23.700 --> 00:07:26.640
going to look at spectrograms
of different sounds.

00:07:26.640 --> 00:07:28.710
Let's start with a
person whistling.

00:07:28.710 --> 00:07:32.340
[WHISTLING]

00:07:32.340 --> 00:07:35.010
OK, so frequency
is on this axis,

00:07:35.010 --> 00:07:38.070
higher frequencies up here,
lower frequencies there.

00:07:38.070 --> 00:07:40.276
And it's going by in time.

00:07:40.276 --> 00:07:45.080
[WHISTLING]

00:07:45.080 --> 00:07:48.530
So whistling is unusual
in that it's pretty much

00:07:48.530 --> 00:07:50.630
a single frequency at a time.

00:07:50.630 --> 00:07:52.630
Many natural sounds
are not like that.

00:07:52.630 --> 00:07:55.370
So you see not single,
but a small, narrow band

00:07:55.370 --> 00:07:56.540
of frequencies at a time.

00:07:56.540 --> 00:07:58.700
OK, that's enough.

00:07:58.700 --> 00:08:00.260
[WHISTLING]

00:08:00.260 --> 00:08:01.310
Stop.

00:08:01.310 --> 00:08:01.910
All right.

00:08:01.910 --> 00:08:02.759
OK.

00:08:02.759 --> 00:08:10.830
[TROMBONE PLAYING]

00:08:10.830 --> 00:08:13.650
OK, so you see how
with the trombone,

00:08:13.650 --> 00:08:16.770
there were many different
bands of frequencies.

00:08:16.770 --> 00:08:18.940
In contrast-- this is
me talking, by the way.

00:08:18.940 --> 00:08:20.950
We'll talk about
that in a second.

00:08:20.950 --> 00:08:24.420
But with the whistling, you saw
just a single band at a time.

00:08:24.420 --> 00:08:27.150
With the trombone, it has
all of these harmonics,

00:08:27.150 --> 00:08:32.850
these parallel lines of
multiples of frequencies.

00:08:32.850 --> 00:08:34.659
Those are called
"pitched sounds."

00:08:34.659 --> 00:08:37.260
Sounds that have a pitch where
you could sing back the tune

00:08:37.260 --> 00:08:40.605
have those bands of
frequencies like that.

00:08:40.605 --> 00:08:42.480
And so that's what you
see with the trombone.

00:08:42.480 --> 00:08:45.270
You see a little bit of this
with natural speech here.

00:08:45.270 --> 00:08:48.030
You can see sets of
bands, but mostly, you

00:08:48.030 --> 00:08:49.530
see vertical stripes.

00:08:49.530 --> 00:08:52.590
That's because I'm talking fast
and mostly what's coming out

00:08:52.590 --> 00:08:53.640
is consonants.

00:08:53.640 --> 00:09:01.260
If I slowed down and
stretched out the vowels,

00:09:01.260 --> 00:09:08.537
you would see more
of those harmonics.

00:09:08.537 --> 00:09:09.120
Fun and games.

00:09:11.640 --> 00:09:13.430
OK, so that's what
sound looks like.

00:09:13.430 --> 00:09:15.097
So everybody has to
have a sense of this

00:09:15.097 --> 00:09:17.790
is showing you the energy
at each frequency over time

00:09:17.790 --> 00:09:19.590
in response to natural speech.

00:09:19.590 --> 00:09:23.130
We'll play with this a little
bit more later in the lecture.

00:09:31.360 --> 00:09:32.830
So we did all that.

00:09:32.830 --> 00:09:34.570
We'll do some of that
other stuff later.

00:09:34.570 --> 00:09:37.240
So now that we have some
sense of what sound is

00:09:37.240 --> 00:09:40.630
and what that input
is, how are we

00:09:40.630 --> 00:09:43.840
going to think about how to
extract information from it?

00:09:43.840 --> 00:09:45.970
What we want to do is
think about how is it?

00:09:45.970 --> 00:09:48.910
Why is it challenging to
get to that from this?

00:09:53.140 --> 00:09:55.360
There are several reasons
that's challenging.

00:09:55.360 --> 00:09:58.120
First is invariance
problems, much like we've

00:09:58.120 --> 00:10:00.910
discussed in the domain of
vision and other domains

00:10:00.910 --> 00:10:03.980
already in this class.

00:10:03.980 --> 00:10:06.790
And so the way to think about
that here is that a given sound

00:10:06.790 --> 00:10:10.930
source sounds really different
in different situations.

00:10:10.930 --> 00:10:13.900
So if we have different
people saying the same word,

00:10:13.900 --> 00:10:16.660
that will look very different
on those spectrograms.

00:10:16.660 --> 00:10:18.250
The stimulus is
actually different,

00:10:18.250 --> 00:10:21.730
even though we want to just
know what word is being said.

00:10:21.730 --> 00:10:24.730
And conversely, if we
have the same person

00:10:24.730 --> 00:10:27.620
saying two different words,
that will look really different.

00:10:27.620 --> 00:10:29.990
And even if we want to
know just who's speaking,

00:10:29.990 --> 00:10:32.740
we have to deal with the
invariance of generalizing

00:10:32.740 --> 00:10:35.680
across those very different
ways, very different

00:10:35.680 --> 00:10:38.410
sounds that they produce when
they say different things.

00:10:38.410 --> 00:10:40.570
So those are kind of
flips of each other.

00:10:40.570 --> 00:10:43.840
To recognize voices, we
want invariance of the voice

00:10:43.840 --> 00:10:45.130
with respect to the words.

00:10:45.130 --> 00:10:47.830
To recognize words, we want
invariance for the words

00:10:47.830 --> 00:10:49.690
independent of the voice.

00:10:49.690 --> 00:10:51.385
And those are all
tied up together.

00:10:54.040 --> 00:10:56.530
So we need to appreciate the
sameness of those stimuli

00:10:56.530 --> 00:10:59.620
across those changes.

00:10:59.620 --> 00:11:01.777
Here's another reason that
hearing is challenging--

00:11:01.777 --> 00:11:02.860
I mentioned this briefly--

00:11:05.890 --> 00:11:08.387
in normal situations-- it's
pretty quiet in the room.

00:11:08.387 --> 00:11:09.970
There's some background
noise, but not

00:11:09.970 --> 00:11:11.720
a whole lot of other
noise, so it's mostly

00:11:11.720 --> 00:11:13.900
just me making
the noise in here.

00:11:13.900 --> 00:11:17.600
But in many situations, there
are multiple sound sources.

00:11:17.600 --> 00:11:19.090
For example, listen to this.

00:11:19.090 --> 00:11:20.050
[AUDIO PLAYBACK]

00:11:20.050 --> 00:11:21.280
[INTERPOSING VOICES]

00:11:21.280 --> 00:11:23.320
- All right, Debbie
Whittaker, Sterling James,

00:11:23.320 --> 00:11:24.160
wrapping things up.

00:11:24.160 --> 00:11:24.850
[END PLAYBACK]

00:11:24.850 --> 00:11:27.058
NANCY KANWISHER: OK, little
segment of radio, there's

00:11:27.058 --> 00:11:28.960
music, and a person
speaking both at once.

00:11:28.960 --> 00:11:30.970
And you had no
problem hearing what

00:11:30.970 --> 00:11:32.680
the person was
saying and knowing

00:11:32.680 --> 00:11:35.830
something about the gender
and age of that person.

00:11:35.830 --> 00:11:38.080
You recognize the voice,
the content of the speech,

00:11:38.080 --> 00:11:41.410
even though the music
is right on top of it.

00:11:41.410 --> 00:11:45.040
So the music might be like
this and the speech like that.

00:11:45.040 --> 00:11:47.770
And what you hear is
this, with those things

00:11:47.770 --> 00:11:50.590
right on top of each other.

00:11:50.590 --> 00:11:53.200
So you need to go backwards
to hear these things,

00:11:53.200 --> 00:11:55.962
even though that's all you get.

00:11:55.962 --> 00:11:57.670
Everybody see how
that's a big challenge?

00:11:57.670 --> 00:12:00.430
If you had to write the code
to take this and recover

00:12:00.430 --> 00:12:02.860
that, best of luck to you.

00:12:02.860 --> 00:12:04.630
Yeah, question?

00:12:04.630 --> 00:12:07.480
STUDENT: How does intensity or
volume come into this picture

00:12:07.480 --> 00:12:08.923
again?

00:12:08.923 --> 00:12:10.840
NANCY KANWISHER: It's
not really well depicted

00:12:10.840 --> 00:12:11.890
on these diagrams.

00:12:11.890 --> 00:12:14.470
This is just showing
you the entire source.

00:12:14.470 --> 00:12:17.950
So the intensity I showed before
essentially takes this and does

00:12:17.950 --> 00:12:20.860
a Fourier analysis of it so that
it gives you the energy at each

00:12:20.860 --> 00:12:22.370
of those frequencies.

00:12:22.370 --> 00:12:24.370
So you could just do a
Fourier analysis on this

00:12:24.370 --> 00:12:25.770
and you get a spectrogram.

00:12:29.460 --> 00:12:31.530
So the listener's
usually interested

00:12:31.530 --> 00:12:34.200
in individual sources
even though they're

00:12:34.200 --> 00:12:36.220
superimposed on other sources.

00:12:36.220 --> 00:12:37.720
And that's a real problem.

00:12:37.720 --> 00:12:38.970
So this is the input.

00:12:38.970 --> 00:12:43.890
They get added together, and the
brain has to pull them apart.

00:12:43.890 --> 00:12:46.770
So this is a classic,
ill-posed problem.

00:12:46.770 --> 00:12:52.140
That means just given
this, we have no way

00:12:52.140 --> 00:12:55.260
to go backwards to that
if that's all we have,

00:12:55.260 --> 00:12:58.170
because there's multiple
possible solutions.

00:12:58.170 --> 00:13:01.080
It's like saying,
"x plus y equals 9,

00:13:01.080 --> 00:13:03.030
now solve for x and y."

00:13:03.030 --> 00:13:05.550
And whenever we're
in that situation

00:13:05.550 --> 00:13:09.060
of an ill-posed problem with
multiple possible solutions,

00:13:09.060 --> 00:13:12.810
only one of which is right in
any situation in the world,

00:13:12.810 --> 00:13:15.900
the usual answer is
that we need to bring

00:13:15.900 --> 00:13:18.720
in some other assumptions or
world knowledge or something,

00:13:18.720 --> 00:13:21.960
to constrain that problem and
narrow that large, usually

00:13:21.960 --> 00:13:23.850
infinite, space of
possible answers

00:13:23.850 --> 00:13:26.820
down to the one correct one.

00:13:26.820 --> 00:13:28.770
So this is a classic
problem that people

00:13:28.770 --> 00:13:31.350
have talked about in
audition for many decades.

00:13:31.350 --> 00:13:33.840
Josh McDermott in this
department does a lot of work

00:13:33.840 --> 00:13:35.220
on it.

00:13:35.220 --> 00:13:38.460
And you can solve it
in part by knowledge

00:13:38.460 --> 00:13:43.770
of natural sounds, which I
won't talk about in detail here.

00:13:43.770 --> 00:13:48.180
One more challenge for
solving problems in audition

00:13:48.180 --> 00:13:49.860
comes from the fact
that real world

00:13:49.860 --> 00:13:53.310
sounds, including the sound
of my voice right now,

00:13:53.310 --> 00:13:54.330
have reverb.

00:13:54.330 --> 00:13:56.490
So "reverb" means--
this is an aerial view.

00:13:56.490 --> 00:13:59.530
That's a person, kind of hard
to see in an aerial view.

00:13:59.530 --> 00:14:01.020
And that's a sound source.

00:14:01.020 --> 00:14:04.260
And some of the sound comes
straight from the sound source

00:14:04.260 --> 00:14:06.030
to the person's ears.

00:14:06.030 --> 00:14:09.900
But a lot of the sound goes
and ricochets off the walls,

00:14:09.900 --> 00:14:12.630
god knows how many times,
before it hits the ears.

00:14:12.630 --> 00:14:15.360
And all of those
different paths of sound

00:14:15.360 --> 00:14:17.730
are all kind of
superimposed at the ears.

00:14:17.730 --> 00:14:19.590
And they arrive at
different times,

00:14:19.590 --> 00:14:22.290
making a hell of a mess
of the input sound.

00:14:22.290 --> 00:14:25.590
So instead of that nice,
clean, straightforward input,

00:14:25.590 --> 00:14:28.170
you have the input plus
a slightly delayed input,

00:14:28.170 --> 00:14:31.140
a more delayed input, another
delayed input, all superimposed

00:14:31.140 --> 00:14:32.220
on top of each other.

00:14:32.220 --> 00:14:33.810
That's reverb.

00:14:33.810 --> 00:14:35.550
Is that clear, what
the problem is?

00:14:35.550 --> 00:14:38.400
So now we have this
really messed-up signal

00:14:38.400 --> 00:14:40.860
that we're trying to go
backwards and understand

00:14:40.860 --> 00:14:42.610
what the input is.

00:14:42.610 --> 00:14:44.080
So I'll give you an example.

00:14:44.080 --> 00:14:46.770
This is a recording of
what's known as "dry speech."

00:14:46.770 --> 00:14:48.450
That means speech
with no reverb.

00:14:48.450 --> 00:14:49.398
Sorry, question?

00:14:49.398 --> 00:14:51.690
STUDENT: I'm just having a
little trouble understanding

00:14:51.690 --> 00:14:54.210
why reverb poses a problem.

00:14:54.210 --> 00:14:57.870
The stimulus isn't changing,
it's just delayed over time.

00:14:57.870 --> 00:14:59.460
NANCY KANWISHER: Yeah, OK.

00:14:59.460 --> 00:15:01.418
Let's do a vision example.

00:15:01.418 --> 00:15:03.460
This is a little crazy,
but let me just try this.

00:15:03.460 --> 00:15:05.842
Suppose we had a
photograph of my face

00:15:05.842 --> 00:15:07.050
and you have to recognize it.

00:15:07.050 --> 00:15:07.560
OK, fine.

00:15:07.560 --> 00:15:09.580
Various visual
algorithms can do that.

00:15:09.580 --> 00:15:12.360
But now suppose we took that
photograph and we moved it

00:15:12.360 --> 00:15:16.207
over 10%, and we superimposed
it and added them together,

00:15:16.207 --> 00:15:18.540
and then we moved it over
again and added them together,

00:15:18.540 --> 00:15:20.457
and moved it over again
and add them together.

00:15:20.457 --> 00:15:22.710
Pretty soon you
have a blurry mess.

00:15:22.710 --> 00:15:24.900
And those things are all
on top of each other,

00:15:24.900 --> 00:15:28.372
just as two people talking at
once are on top of each other.

00:15:28.372 --> 00:15:30.330
And so you have a real
problem going backwards.

00:15:30.330 --> 00:15:31.470
Does that make sense?

00:15:31.470 --> 00:15:32.400
OK.

00:15:32.400 --> 00:15:34.883
OK, so here's dry
speech with no reverb.

00:15:34.883 --> 00:15:35.550
[AUDIO PLAYBACK]

00:15:35.550 --> 00:15:37.170
- They ate the lemon pie.

00:15:37.170 --> 00:15:38.790
Father forgot the bread.

00:15:38.790 --> 00:15:39.390
[END PLAYBACK]

00:15:39.390 --> 00:15:41.182
NANCY KANWISHER: OK,
here's the same speech

00:15:41.182 --> 00:15:42.220
but with lots of reverb.

00:15:42.220 --> 00:15:43.830
- They ate the lemon pie.

00:15:43.830 --> 00:15:45.184
Father forgot the bread.

00:15:45.184 --> 00:15:46.150
[END PLAYBACK]

00:15:46.150 --> 00:15:47.733
NANCY KANWISHER: OK,
now you can still

00:15:47.733 --> 00:15:49.840
hear it because your
auditory system knows

00:15:49.840 --> 00:15:52.120
how to solve this problem.

00:15:52.120 --> 00:15:54.160
But look what happens
to the spectrogram.

00:15:54.160 --> 00:15:57.040
Here-- this is time this
way, frequency this way,

00:15:57.040 --> 00:15:59.740
and the dark bits are where the
energy is, where the power is.

00:15:59.740 --> 00:16:03.310
In the dry speech, you see all
these nice, vertical things,

00:16:03.310 --> 00:16:06.910
and here you see a blurry mess.

00:16:06.910 --> 00:16:09.160
Nonetheless, you
can hear it fine.

00:16:09.160 --> 00:16:12.280
And further, what else could
you tell from the reverb?

00:16:16.890 --> 00:16:18.140
STUDENT: The size of the room.

00:16:18.140 --> 00:16:21.350
NANCY KANWISHER: Yeah, it's in
a cathedral or something, right?

00:16:21.350 --> 00:16:25.370
So it's not just that
it causes a problem.

00:16:25.370 --> 00:16:27.920
Reverb also tells us
something about the location

00:16:27.920 --> 00:16:30.180
we're in, if we know
how to extract it,

00:16:30.180 --> 00:16:32.790
which you guys' visual--

00:16:32.790 --> 00:16:34.310
auditory systems do.

00:16:34.310 --> 00:16:37.070
You can see I'm a
vision scientist.

00:16:37.070 --> 00:16:39.260
So how to study this?

00:16:39.260 --> 00:16:41.390
There's a very beautiful
paper that Josh McDermott

00:16:41.390 --> 00:16:42.660
published a few years ago.

00:16:42.660 --> 00:16:44.300
And I'm going to try to give
you the gist of the paper

00:16:44.300 --> 00:16:45.770
without all the
technical details,

00:16:45.770 --> 00:16:47.810
because I think
it's just brilliant.

00:16:47.810 --> 00:16:52.520
So they wanted to characterize
what exactly is reverb.

00:16:52.520 --> 00:16:54.800
And reverb is going to
vary for different sounds.

00:16:54.800 --> 00:16:57.195
You heard the reverb in
that cathedral-like space.

00:16:57.195 --> 00:16:59.570
That's very different from
the reverb in this room, which

00:16:59.570 --> 00:17:00.230
also happens.

00:17:00.230 --> 00:17:02.300
It's harder to hear
because it's less obvious.

00:17:02.300 --> 00:17:04.010
But you can tell a
lot about the space

00:17:04.010 --> 00:17:06.530
you're in because the reverb
properties are different.

00:17:06.530 --> 00:17:08.270
The distance to the
walls are different.

00:17:08.270 --> 00:17:11.190
The reflective
properties are different.

00:17:11.190 --> 00:17:12.849
And so there's
information there.

00:17:12.849 --> 00:17:16.730
So you can characterize the
nature of the reverb in any one

00:17:16.730 --> 00:17:20.990
location by making an
instantaneous, brief click

00:17:20.990 --> 00:17:24.020
sound in that environment
and recording what

00:17:24.020 --> 00:17:25.079
happens after that.

00:17:25.079 --> 00:17:29.090
And then you can collect all
the reverberant reflections

00:17:29.090 --> 00:17:31.160
of that sound off the walls.

00:17:31.160 --> 00:17:32.660
So what they did
is they went around

00:17:32.660 --> 00:17:35.720
to lots of natural locations and
they played a click like this.

00:17:35.720 --> 00:17:36.780
[CLICK]

00:17:36.780 --> 00:17:38.150
That's it, just a click.

00:17:38.150 --> 00:17:40.130
And then they recorded.

00:17:40.130 --> 00:17:42.380
So this is the initial
click, but this

00:17:42.380 --> 00:17:46.410
is what you record in a single
location, all this stuff.

00:17:46.410 --> 00:17:49.370
And those are all the
reverberant reflections

00:17:49.370 --> 00:17:51.470
of that sound off the
walls-- make sense?

00:17:51.470 --> 00:17:54.780
For one location.

00:17:54.780 --> 00:17:58.850
So then they did that in a
whole bunch of locations.

00:17:58.850 --> 00:18:02.180
And the idea is that
here is a description

00:18:02.180 --> 00:18:05.640
of the basic problems, just
the same thing I said before,

00:18:05.640 --> 00:18:08.180
but slightly more detailed.

00:18:08.180 --> 00:18:11.915
So a sound source would
be something like this.

00:18:11.915 --> 00:18:14.510
This looks like a
person speaking,

00:18:14.510 --> 00:18:17.510
with those nice, harmonic,
parallel bands, like you

00:18:17.510 --> 00:18:18.858
saw when I was speaking.

00:18:18.858 --> 00:18:19.775
Maybe it's a trombone.

00:18:22.520 --> 00:18:23.270
So that's time.

00:18:23.270 --> 00:18:24.330
That's the source.

00:18:24.330 --> 00:18:26.480
That's what you want to know.

00:18:26.480 --> 00:18:28.850
This is now the impulse
response function

00:18:28.850 --> 00:18:31.760
for the location where
that sound is being played,

00:18:31.760 --> 00:18:34.310
determined by doing that
click and recording.

00:18:34.310 --> 00:18:37.070
I showed you just
you do a Fourier

00:18:37.070 --> 00:18:40.550
analysis of that black
curve in the previous slide

00:18:40.550 --> 00:18:42.080
and you get something like this.

00:18:42.080 --> 00:18:44.450
And that shows you all
the echoes that happen

00:18:44.450 --> 00:18:45.770
in that sound in that location.

00:18:45.770 --> 00:18:47.270
And there are
different time delays,

00:18:47.270 --> 00:18:51.440
and different intensities,
and frequency dependence.

00:18:51.440 --> 00:18:55.655
What comes to your ear is
basically this times that.

00:18:58.450 --> 00:19:00.820
So you're given
this and you have

00:19:00.820 --> 00:19:03.430
to go backwards
and solve for that.

00:19:03.430 --> 00:19:06.250
Everybody see the problem?

00:19:06.250 --> 00:19:13.028
So what McDermott and
Traer showed is that--

00:19:13.028 --> 00:19:15.070
just to state the problem
a little more clearly--

00:19:15.070 --> 00:19:18.113
you're interested in the
source and/or the environment.

00:19:18.113 --> 00:19:19.780
You might want to
know what kind of room

00:19:19.780 --> 00:19:22.180
am I, if somebody is dragging
you around blindfolded.

00:19:22.180 --> 00:19:24.347
You might want to know if
you're outside, or inside,

00:19:24.347 --> 00:19:27.880
or in a cathedral,
or a closet, or what.

00:19:27.880 --> 00:19:30.910
And now this should seem
very analogous to the problem

00:19:30.910 --> 00:19:32.440
of color vision.

00:19:32.440 --> 00:19:34.870
Remember the problem
of color vision?

00:19:34.870 --> 00:19:38.980
We want to know the color
of this object right here.

00:19:38.980 --> 00:19:40.600
So this little,
purple patch here,

00:19:40.600 --> 00:19:44.080
we want to know that, but all
we have is the light coming

00:19:44.080 --> 00:19:47.192
to our eyes from that patch.

00:19:47.192 --> 00:19:49.150
And the light coming to
our eyes from the patch

00:19:49.150 --> 00:19:52.270
is a function not just of
the property of the object,

00:19:52.270 --> 00:19:54.850
but whatever light happens
to be coming onto it

00:19:54.850 --> 00:19:58.180
and then reflecting to our eyes.

00:19:58.180 --> 00:19:59.830
And so in color
vision, we have one set

00:19:59.830 --> 00:20:01.720
of tricks to try to
solve that problem

00:20:01.720 --> 00:20:04.360
and recover the actual
properties of the object,

00:20:04.360 --> 00:20:06.430
even though it's totally
confounded in the input

00:20:06.430 --> 00:20:08.740
with the properties
of the incident light.

00:20:08.740 --> 00:20:10.900
This is extremely analogous.

00:20:10.900 --> 00:20:14.110
We're trying to solve for
what is the sound source.

00:20:14.110 --> 00:20:16.210
And we have to deal
with this problem that

00:20:16.210 --> 00:20:19.340
is completely confounded with
the reverberation of the room

00:20:19.340 --> 00:20:19.840
it's in.

00:20:19.840 --> 00:20:21.340
Does everybody see that analogy?

00:20:21.340 --> 00:20:26.790
They're both classic ill-posed
problems in perception.

00:20:26.790 --> 00:20:29.670
So here's another way of
putting it-- we're given that

00:20:29.670 --> 00:20:31.350
and we want to solve
for at least one

00:20:31.350 --> 00:20:34.110
of these, ideally both of those.

00:20:34.110 --> 00:20:36.430
And you can't do
that with just this.

00:20:36.430 --> 00:20:38.400
So we need to make
assumptions about the room.

00:20:38.400 --> 00:20:40.680
And what Traer and
McDermott showed

00:20:40.680 --> 00:20:43.290
is that first, they measured
those impulse response

00:20:43.290 --> 00:20:45.000
functions in
natural environments

00:20:45.000 --> 00:20:48.540
to characterize reverb in
different environments.

00:20:48.540 --> 00:20:52.260
And they found that there's some
systematic properties of reverb

00:20:52.260 --> 00:20:54.540
having to do with
the decay function

00:20:54.540 --> 00:20:56.040
as a function of frequency.

00:20:56.040 --> 00:20:59.100
And those systematic
properties are preserved

00:20:59.100 --> 00:21:02.260
across different environments.

00:21:02.260 --> 00:21:07.440
And then they showed that
your auditory system knows

00:21:07.440 --> 00:21:10.290
about the way reverb
works, in the sense

00:21:10.290 --> 00:21:15.040
that if you make up a different,
non-physical reverb property

00:21:15.040 --> 00:21:18.450
and you play it to people,
it sounds weird, number one.

00:21:18.450 --> 00:21:22.020
And two, they can't
recover the sound source.

00:21:22.020 --> 00:21:25.410
And what that means is, built
into your auditory system

00:21:25.410 --> 00:21:27.660
is knowledge of the
physics of sound,

00:21:27.660 --> 00:21:31.050
and in particular about the
particulars of the decay

00:21:31.050 --> 00:21:33.420
function of reverb,
such that you

00:21:33.420 --> 00:21:35.340
can use that knowledge
of how reverb

00:21:35.340 --> 00:21:38.100
works in general to
undo this problem,

00:21:38.100 --> 00:21:41.133
and constrain this problem,
and solve for the sound source.

00:21:41.133 --> 00:21:42.550
I didn't give you
all the details.

00:21:42.550 --> 00:21:43.890
But I want you to get the gist.

00:21:43.890 --> 00:21:47.110
Do you get the kind of idea?

00:21:47.110 --> 00:21:47.610
OK.

00:21:50.640 --> 00:21:53.640
But as I said, that's
only true for reverb

00:21:53.640 --> 00:21:57.240
that has the reverb properties
of real-world sound.

00:21:57.240 --> 00:21:59.760
If you make up fake
reverb, it doesn't work.

00:21:59.760 --> 00:22:01.590
And people can't
solve this problem.

00:22:01.590 --> 00:22:03.690
That tells you they're
using their knowledge.

00:22:03.690 --> 00:22:06.510
Doesn't tell us whether that
knowledge is built in innately,

00:22:06.510 --> 00:22:08.250
or whether they
learned it, or what.

00:22:13.110 --> 00:22:14.100
All right, good.

00:22:14.100 --> 00:22:17.220
So in other words, we
solve the ill-posed problem

00:22:17.220 --> 00:22:20.070
of recovering the sound
source despite reverb

00:22:20.070 --> 00:22:23.070
by building a knowledge of
the physics of the world

00:22:23.070 --> 00:22:24.810
into our auditory
system and using

00:22:24.810 --> 00:22:28.410
it to constrain the problem.

00:22:28.410 --> 00:22:32.160
So we just said, why is this
computationally challenging?

00:22:32.160 --> 00:22:34.620
Invariance problems,
appreciating the sameness

00:22:34.620 --> 00:22:36.630
of a voice across
different words,

00:22:36.630 --> 00:22:42.060
appreciating the sameness of a
word across different voices.

00:22:42.060 --> 00:22:44.310
Separating multiple
sound sources that come

00:22:44.310 --> 00:22:47.100
in simultaneously and are
just massively superimposed

00:22:47.100 --> 00:22:51.840
on the input-- the cocktail
party problem, also ill-posed--

00:22:51.840 --> 00:22:53.670
and the reverb problem.

00:22:53.670 --> 00:22:56.580
So everybody see how these are
three really big challenges

00:22:56.580 --> 00:22:57.210
for audition?

00:22:57.210 --> 00:22:57.710
Yeah.

00:22:57.710 --> 00:23:01.890
STUDENT: So was brain imaging as
well a part of the [INAUDIBLE]??

00:23:01.890 --> 00:23:03.470
NANCY KANWISHER: Nope.

00:23:03.470 --> 00:23:05.065
One could do that
and ask questions

00:23:05.065 --> 00:23:06.690
about where that's
solved in the brain.

00:23:06.690 --> 00:23:08.580
But the beauty of
that study is that

00:23:08.580 --> 00:23:10.260
in a way, who cares
where it's solved?

00:23:10.260 --> 00:23:11.635
I mean, it's kind
of interesting,

00:23:11.635 --> 00:23:14.970
but it's such a beautiful story
already just from actually,

00:23:14.970 --> 00:23:17.430
a big part of their study
was measuring reverb.

00:23:17.430 --> 00:23:19.180
Nobody had done it before.

00:23:19.180 --> 00:23:21.120
They sent people
out with speakers,

00:23:21.120 --> 00:23:24.750
and recording devices,
and little random timers

00:23:24.750 --> 00:23:25.920
on their iPhones.

00:23:25.920 --> 00:23:27.060
And at random times--

00:23:27.060 --> 00:23:28.560
how did this go--
oh, yeah, they had

00:23:28.560 --> 00:23:31.920
people had to mark the location
they were in using their iPhone

00:23:31.920 --> 00:23:34.410
GPS and then that's right--
they didn't send people out

00:23:34.410 --> 00:23:35.370
with recording devices.

00:23:35.370 --> 00:23:36.070
It's too hard.

00:23:36.070 --> 00:23:38.160
And so then they sampled
what kind of places

00:23:38.160 --> 00:23:39.480
do people hang out in.

00:23:39.480 --> 00:23:43.170
And then they went back
with their impulse sound

00:23:43.170 --> 00:23:45.030
source and the recording
device, and they

00:23:45.030 --> 00:23:46.950
measured that impulse
response function

00:23:46.950 --> 00:23:49.050
in lots and lots of
different natural sounds

00:23:49.050 --> 00:23:51.900
in order to characterize
what is the nature of reverb

00:23:51.900 --> 00:23:52.510
in the world.

00:23:52.510 --> 00:23:53.850
Nobody had done that before.

00:23:53.850 --> 00:23:55.560
So that's why I tell you
this, is that to me, it's

00:23:55.560 --> 00:23:57.185
just one of the most
beautiful examples

00:23:57.185 --> 00:23:58.500
of computational theory--

00:23:58.500 --> 00:24:00.600
no measurement in the brain.

00:24:00.600 --> 00:24:02.670
A big part of the study
was just characterizing

00:24:02.670 --> 00:24:05.340
the physics of
sound, and then some

00:24:05.340 --> 00:24:07.020
psychophysics to say
actually, do people

00:24:07.020 --> 00:24:09.660
use that knowledge of how
reverb works in the world?

00:24:09.660 --> 00:24:10.530
And yes, they do.

00:24:15.580 --> 00:24:17.810
So I've been talking
about hearing in general,

00:24:17.810 --> 00:24:20.980
but let's talk about one of
the most interesting examples

00:24:20.980 --> 00:24:24.070
of hearing, the one you're doing
right now-- speech perception.

00:24:26.590 --> 00:24:28.440
So what do speech
sounds look like?

00:24:28.440 --> 00:24:30.900
You saw a few of
them briefly before.

00:24:30.900 --> 00:24:32.350
Here are a few spectra.

00:24:32.350 --> 00:24:34.740
So just to remind you,
each one of these things

00:24:34.740 --> 00:24:38.670
has time going along the
x-axis, frequency here.

00:24:38.670 --> 00:24:42.600
And the color shows you
the intensity of energy

00:24:42.600 --> 00:24:44.670
at that frequency band.

00:24:44.670 --> 00:24:48.630
So this is a person saying,
"hot," and "hat," and "hit,"

00:24:48.630 --> 00:24:50.940
and "head."

00:24:50.940 --> 00:24:54.540
That's the same person saying
these four things, a person

00:24:54.540 --> 00:24:56.100
with a high-pitched voice.

00:24:56.100 --> 00:24:58.920
And here's a person with a
slightly lower-pitched voice

00:24:58.920 --> 00:25:00.390
saying the same things.

00:25:04.080 --> 00:25:06.270
So what do we notice here?

00:25:06.270 --> 00:25:10.460
Well, first of all, we see
that vowels have regularly

00:25:10.460 --> 00:25:11.760
spaced harmonics.

00:25:11.760 --> 00:25:12.733
That's the red stripes.

00:25:12.733 --> 00:25:14.150
This is a vowel
sound right there.

00:25:14.150 --> 00:25:17.240
See those perfectly
regularly spaced harmonics?

00:25:17.240 --> 00:25:20.780
That makes a pitchy sound,
so voices are pitchy.

00:25:20.780 --> 00:25:22.970
You may not think that
there's a pitch to my voice

00:25:22.970 --> 00:25:24.860
right now because I'm
talking, not singing,

00:25:24.860 --> 00:25:26.240
but there is a pitch.

00:25:26.240 --> 00:25:28.370
And you use that,
actually, in the intonation

00:25:28.370 --> 00:25:31.220
of speech, as you guys
read about in the assigned

00:25:31.220 --> 00:25:33.680
reading for yesterday.

00:25:33.680 --> 00:25:36.530
So each of these things
with the stacked harmonics

00:25:36.530 --> 00:25:38.420
is a vowel sound.

00:25:38.420 --> 00:25:42.440
It's got a pitch and it
lasts over a chunk of time.

00:25:42.440 --> 00:25:45.650
And the consonants are
these kind of muckier things

00:25:45.650 --> 00:25:47.240
that happen before and after.

00:25:47.240 --> 00:25:49.310
And consonants don't have pitch.

00:25:49.310 --> 00:25:50.540
They don't have harmonics.

00:25:50.540 --> 00:25:51.740
They have kind of muck.

00:25:55.610 --> 00:25:57.680
So there are certain
band-- people

00:25:57.680 --> 00:26:00.200
who study speech spend a lot
of time staring at these things

00:26:00.200 --> 00:26:01.670
and characterizing them.

00:26:01.670 --> 00:26:07.160
And they like to talk about
bands of frequency, of power.

00:26:07.160 --> 00:26:09.950
And so this band
down here that's

00:26:09.950 --> 00:26:13.820
present in all of
these speech sounds

00:26:13.820 --> 00:26:15.740
here is called a "formant."

00:26:15.740 --> 00:26:18.527
It's just a chunk of
the frequency spectrum

00:26:18.527 --> 00:26:19.610
that you hear with speech.

00:26:23.030 --> 00:26:25.550
So that's a formant.

00:26:25.550 --> 00:26:29.390
And some of those
frequency bands or formants

00:26:29.390 --> 00:26:33.680
are particularly diagnostic
for different vowels.

00:26:33.680 --> 00:26:35.780
So if you look in
this range here,

00:26:35.780 --> 00:26:39.140
only in that mid-range
here, only for "hat"

00:26:39.140 --> 00:26:41.210
and a little bit
for "F" sound do

00:26:41.210 --> 00:26:44.270
you get an energy in
that frequency band,

00:26:44.270 --> 00:26:47.700
not for "hot" or "hit."

00:26:47.700 --> 00:26:49.820
And that's true both for
the high-pitched voice

00:26:49.820 --> 00:26:51.050
and the low-pitched voice.

00:26:51.050 --> 00:26:54.018
This frequency band here is
really diagnostic to which

00:26:54.018 --> 00:26:55.310
of those vowels you're hearing.

00:26:59.790 --> 00:27:03.690
So we're going to play
with that spectrogram

00:27:03.690 --> 00:27:08.800
again a little bit
more, although I now

00:27:08.800 --> 00:27:09.940
have learned avoidance.

00:27:13.350 --> 00:27:20.110
So this is me speaking
again, as you saw before.

00:27:20.110 --> 00:27:29.300
So I'm going to say
an A, an E, an I--

00:27:29.300 --> 00:27:34.460
look how different
that one is, O, and U.

00:27:34.460 --> 00:27:35.840
And there's lots
of other vowels.

00:27:35.840 --> 00:27:37.573
Do you see how that
energy moves around

00:27:37.573 --> 00:27:38.615
for the different vowels?

00:27:41.330 --> 00:27:47.950
Now as I said before, if I
do a long vowel like this,

00:27:47.950 --> 00:27:51.950
it makes a big, long
bunch of harmonics.

00:27:51.950 --> 00:27:55.460
But a lot of the time, they're
just these vertical lines.

00:27:55.460 --> 00:28:00.890
The vertical lines are
consonants, t, p, k, r.

00:28:00.890 --> 00:28:03.288
If I don't say a vowel, you
just see a vertical line.

00:28:03.288 --> 00:28:04.580
It's not quite a vertical line.

00:28:04.580 --> 00:28:08.450
They are different from each
other in ways you can tell.

00:28:08.450 --> 00:28:12.440
So the consonants are
those bands of energy

00:28:12.440 --> 00:28:14.120
that go vertically.

00:28:14.120 --> 00:28:16.760
And the vowels are the big,
long harmonic structures

00:28:16.760 --> 00:28:17.990
that stretch between them.

00:28:20.580 --> 00:28:22.457
Now, I'm not sure you'll
be able to do this.

00:28:22.457 --> 00:28:24.540
I'm going to need a volunteer
in a second, and I'm

00:28:24.540 --> 00:28:26.280
going to pick on
[? Iadun, ?] because he's

00:28:26.280 --> 00:28:27.447
most accessible right there.

00:28:27.447 --> 00:28:28.330
So come on up here.

00:28:28.330 --> 00:28:32.970
You know it won't be
horrible or embarrassing.

00:28:32.970 --> 00:28:34.870
So you can stand
here for a second.

00:28:34.870 --> 00:28:38.640
I'm going to say "ba's" and
"pa's" and I'll tell you

00:28:38.640 --> 00:28:39.900
in a moment what to say.

00:28:39.900 --> 00:28:41.358
I'm not sure this
is going to work.

00:28:41.358 --> 00:28:42.540
I tried it before.

00:28:42.540 --> 00:28:44.550
We're going to look at
two different formants

00:28:44.550 --> 00:28:48.240
when I say "ba."

00:28:48.240 --> 00:28:56.790
Actually, I'm going to do
it rising-- ba, pa, ba, pa.

00:28:56.790 --> 00:28:58.920
So there's two different
formants, here and here,

00:28:58.920 --> 00:28:59.820
with both of those.

00:28:59.820 --> 00:29:00.960
I'm going to do it again.

00:29:00.960 --> 00:29:03.030
And there's just a
tiny, little difference

00:29:03.030 --> 00:29:04.770
between a ba and a pa.

00:29:04.770 --> 00:29:08.370
And it has to do with the
interval between the consonant,

00:29:08.370 --> 00:29:09.870
which is the first
vertical thing,

00:29:09.870 --> 00:29:11.760
and the vowel, which is
the horizontal stuff.

00:29:11.760 --> 00:29:13.260
So let's see if we
can see it again.

00:29:13.260 --> 00:29:14.880
Here we go.

00:29:14.880 --> 00:29:20.740
Ba, pa-- do you see how
the pa starts earlier there

00:29:20.740 --> 00:29:23.230
and the ba is slightly delayed?

00:29:23.230 --> 00:29:25.370
I'll show you diagrams
that show you more clearly.

00:29:25.370 --> 00:29:26.020
OK, great.

00:29:26.020 --> 00:29:28.240
Don't go away.

00:29:28.240 --> 00:29:30.040
So we're going to do
the cocktail party

00:29:30.040 --> 00:29:33.430
thing with the
recording devices here.

00:29:33.430 --> 00:29:34.345
What is this?

00:29:34.345 --> 00:29:36.220
This is just some boring
administrative thing

00:29:36.220 --> 00:29:37.650
you can just read.

00:29:37.650 --> 00:29:40.150
I actually brought it to crumple
and make a crumpling sound,

00:29:40.150 --> 00:29:42.580
but we'll do that afterwards.

00:29:42.580 --> 00:29:45.370
Right now, you
will read from that

00:29:45.370 --> 00:29:47.740
and I will recite
something boring.

00:29:47.740 --> 00:29:49.282
And we'll just do
it simultaneously.

00:29:49.282 --> 00:29:50.740
So just focus on
what you're doing.

00:29:50.740 --> 00:29:51.670
And everybody watch.

00:29:51.670 --> 00:29:53.600
You can see my voice here.

00:29:53.600 --> 00:29:56.830
And let's see what happens when
we're both talking at once.

00:29:56.830 --> 00:29:58.610
OK, here we go.

00:29:58.610 --> 00:30:01.160
Four score and seven years ago--

00:30:01.160 --> 00:30:03.980
oh, geez, I forget how it
goes after that, so I'll just

00:30:03.980 --> 00:30:05.840
have to make up some
other random garbage.

00:30:05.840 --> 00:30:06.170
[INTERPOSING VOICES]

00:30:06.170 --> 00:30:06.920
STUDENT: --outstanding--

00:30:06.920 --> 00:30:07.400
NANCY KANWISHER: OK.

00:30:07.400 --> 00:30:08.840
STUDENT: --review the
student's course--

00:30:08.840 --> 00:30:09.140
[INTERPOSING VOICES]

00:30:09.140 --> 00:30:10.460
NANCY KANWISHER: That's great.

00:30:10.460 --> 00:30:11.510
That's great.

00:30:11.510 --> 00:30:12.787
Don't go away.

00:30:12.787 --> 00:30:14.870
I don't know if you could
tell that it got muckier

00:30:14.870 --> 00:30:16.610
when we were both talking.

00:30:16.610 --> 00:30:20.370
Maybe it's mucky enough with
me talking fast to begin with.

00:30:20.370 --> 00:30:21.770
Let's try a few other things.

00:30:21.770 --> 00:30:23.780
Let's have me say words
and you say words.

00:30:23.780 --> 00:30:25.430
And let's see how
different they look.

00:30:25.430 --> 00:30:31.950
OK, so I'm going
to say "mousetrap."

00:30:31.950 --> 00:30:32.805
STUDENT: Mousetrap.

00:30:32.805 --> 00:30:34.930
NANCY KANWISHER: You can
see some similarity there,

00:30:34.930 --> 00:30:35.430
can't you?

00:30:35.430 --> 00:30:37.540
Let's do it again.

00:30:37.540 --> 00:30:39.260
Mousetrap.

00:30:39.260 --> 00:30:40.085
STUDENT: Mousetrap.

00:30:40.085 --> 00:30:41.460
NANCY KANWISHER:
OK, that's good.

00:30:41.460 --> 00:30:43.230
It's funny, I see more
low-frequency band here.

00:30:43.230 --> 00:30:44.910
I'm sure your voice
is lower than mine.

00:30:44.910 --> 00:30:49.740
Pitch, interestingly, isn't just
about how low the energy goes.

00:30:49.740 --> 00:30:52.320
It's an interesting,
complicated property

00:30:52.320 --> 00:30:56.452
of the lowest common denominator
of that whole frequency stack.

00:30:56.452 --> 00:30:57.660
So I'm not going to do pitch.

00:30:57.660 --> 00:31:00.550
It's complicated.

00:31:00.550 --> 00:31:01.810
What else do we want to do?

00:31:01.810 --> 00:31:06.167
Let's try some ba's and pa's.

00:31:06.167 --> 00:31:08.000
But let's stick them
on the fronts of words.

00:31:08.000 --> 00:31:09.125
Maybe that'll work better--

00:31:12.990 --> 00:31:16.960
pat, bat.

00:31:16.960 --> 00:31:18.278
STUDENT: Pat, bat.

00:31:18.278 --> 00:31:20.570
NANCY KANWISHER: Oh, I could
see the commonality there.

00:31:20.570 --> 00:31:22.090
Could you guys see that?

00:31:22.090 --> 00:31:24.010
Let's do it again.

00:31:24.010 --> 00:31:27.130
Pat, bat.

00:31:27.130 --> 00:31:29.352
STUDENT: Pat, bat.

00:31:29.352 --> 00:31:31.310
NANCY KANWISHER: Well,
yours look more similar.

00:31:31.310 --> 00:31:31.850
All right.

00:31:31.850 --> 00:31:32.630
Anyway, thank you.

00:31:32.630 --> 00:31:33.130
That's good.

00:31:33.130 --> 00:31:35.990
That's all I need, just to
show you how hard this is,

00:31:35.990 --> 00:31:38.360
and how there's variability
across speakers saying

00:31:38.360 --> 00:31:42.140
the same thing, and very,
very subtle differences

00:31:42.140 --> 00:31:46.700
between sounds that sound
totally different to us.

00:31:46.700 --> 00:31:50.745
So back to lecture.

00:31:56.620 --> 00:32:01.990
So you saw the harmonics, those
red stripes, during the vowels.

00:32:01.990 --> 00:32:07.330
You noticed that I
showed the consonants

00:32:07.330 --> 00:32:08.277
and the ba's and pa's.

00:32:08.277 --> 00:32:09.110
So here's a diagram.

00:32:09.110 --> 00:32:10.540
I'm sorry, this
is very abstracted

00:32:10.540 --> 00:32:11.998
away from those
spectrograms, which

00:32:11.998 --> 00:32:13.930
are messy, as you can see.

00:32:13.930 --> 00:32:18.940
The idea is that a consonant
vowel sound, a single syllable

00:32:18.940 --> 00:32:21.010
like ba or pa--

00:32:21.010 --> 00:32:23.890
this is time this way-- has
this big, long formant which

00:32:23.890 --> 00:32:27.100
is a band of energy that's
the vowel, the ah sound.

00:32:27.100 --> 00:32:29.740
And it's these transitions
that happen just before

00:32:29.740 --> 00:32:33.400
that that make the difference
for different consonants.

00:32:33.400 --> 00:32:36.580
And in particular,
the difference

00:32:36.580 --> 00:32:38.620
between a ba and a pa--

00:32:38.620 --> 00:32:40.900
this is a ba, that's a pa--

00:32:40.900 --> 00:32:43.420
the difference we were looking
for that didn't show up that

00:32:43.420 --> 00:32:45.190
clearly, but you
can try it at home,

00:32:45.190 --> 00:32:48.040
maybe you can get it clearer
than I just got it now--

00:32:48.040 --> 00:32:51.160
has to do with that transition
onto the first formant.

00:32:51.160 --> 00:32:55.810
So with a ba, the transitions
happen in parallel.

00:32:55.810 --> 00:32:58.840
And with a pa, this
transition happens

00:32:58.840 --> 00:33:00.700
before that lower formant.

00:33:00.700 --> 00:33:04.750
So that tiny, little--
it's a 65 millisecond delay

00:33:04.750 --> 00:33:08.440
in the case of pa that you
don't have in the case of ba,

00:33:08.440 --> 00:33:10.430
is how you tell that difference.

00:33:10.430 --> 00:33:11.740
It's very, very subtle.

00:33:14.950 --> 00:33:17.860
So there's lots of
different kinds of phonemes.

00:33:17.860 --> 00:33:21.160
We've been talking about
vowels and consonants.

00:33:21.160 --> 00:33:25.570
Each vowel or consonant
sound is called a "phoneme"

00:33:25.570 --> 00:33:29.070
if a distinction
in that sound makes

00:33:29.070 --> 00:33:30.820
the difference between
two different words

00:33:30.820 --> 00:33:33.140
in your language.

00:33:33.140 --> 00:33:36.160
And that means that what counts
as a phoneme in one language

00:33:36.160 --> 00:33:38.440
may not be a phoneme
in another language,

00:33:38.440 --> 00:33:39.910
because it won't
make a distinction

00:33:39.910 --> 00:33:42.220
between different words.

00:33:42.220 --> 00:33:45.760
Many of the phonemes are shared
across languages, but not all.

00:33:45.760 --> 00:33:49.630
We've talked about R and L that
aren't distinguished in Japan,

00:33:49.630 --> 00:33:51.340
and two different
D sounds that sound

00:33:51.340 --> 00:33:54.430
the same to me that are
distinguished in Hindi,

00:33:54.430 --> 00:33:56.440
and lots of others.

00:33:56.440 --> 00:33:59.320
And so those are just variations
across natural languages

00:33:59.320 --> 00:34:02.020
on which of those phonemes,
which of those sounds,

00:34:02.020 --> 00:34:03.820
are used to discriminate
different words,

00:34:03.820 --> 00:34:07.250
and hence count as
phonemes in that language.

00:34:07.250 --> 00:34:11.170
So there's some particularly
awesome phonemes

00:34:11.170 --> 00:34:13.120
that use a particular
kind of consonant

00:34:13.120 --> 00:34:15.100
known as a click consonant.

00:34:15.100 --> 00:34:19.360
And these are common in some
Southern African languages.

00:34:19.360 --> 00:34:23.530
And a year ago, I was traveling
in Mozambique, which was just

00:34:23.530 --> 00:34:25.030
hit by a devastating flood.

00:34:25.030 --> 00:34:25.940
It's really awful.

00:34:25.940 --> 00:34:28.150
But anyway, I was there
visiting a game park

00:34:28.150 --> 00:34:30.409
seeing all kinds of animals.

00:34:30.409 --> 00:34:35.080
And I met this guy, Test.

00:34:35.080 --> 00:34:35.857
And he's amazing.

00:34:35.857 --> 00:34:37.690
I mean, his knowledge
of the natural history

00:34:37.690 --> 00:34:39.409
was mind blowing,
but he also speaks,

00:34:39.409 --> 00:34:42.370
I think, six different
languages fluently, one of which

00:34:42.370 --> 00:34:45.500
is Xhosa, or as he would say,
[SPEAKING Xhosa] or something

00:34:45.500 --> 00:34:46.000
like that.

00:34:46.000 --> 00:34:47.690
You'll hear him
say it in a moment.

00:34:47.690 --> 00:34:49.690
And so he was illustrating
click languages.

00:34:49.690 --> 00:34:51.505
And I'll play this
for you in a second.

00:34:51.505 --> 00:34:53.380
And he says there's a
sentence in Xhosa which

00:34:53.380 --> 00:34:55.810
is a little bit crazy, but
has all the different clicks.

00:34:55.810 --> 00:34:58.570
And it means, basically,
"the skunk was rolling

00:34:58.570 --> 00:35:00.430
and accidentally got
cut by the throat."

00:35:00.430 --> 00:35:02.200
Doesn't mean a whole
lot, but listen

00:35:02.200 --> 00:35:04.930
to Test saying the
sentence, first in English

00:35:04.930 --> 00:35:06.240
and then in Xhosa.

00:35:06.240 --> 00:35:07.120
[AUDIO PLAYBACK]

00:35:07.120 --> 00:35:09.940
- The phrase in
English, it says skunk

00:35:09.940 --> 00:35:14.680
was rolling and accidentally
got cut by the throat.

00:35:14.680 --> 00:35:19.667
In Xhosa, or in east
Xhosa, [SPEAKING Xhosa].

00:35:23.514 --> 00:35:24.340
[END PLAYBACK]

00:35:24.340 --> 00:35:25.120
NANCY KANWISHER:
Isn't that awesome?

00:35:25.120 --> 00:35:27.120
I think we just have to
crank it up a little bit

00:35:27.120 --> 00:35:28.390
and hear him again.

00:35:28.390 --> 00:35:33.970
[AUDIO PLAYBACK]

00:35:33.970 --> 00:35:37.420
- The phrase in English, it
says the skunk was rolling

00:35:37.420 --> 00:35:41.500
and accidentally get
cut by the throat.

00:35:41.500 --> 00:35:46.296
In Xhosa or in east
Xhosa, [SPEAKING Xhosa].

00:35:51.540 --> 00:35:52.410
[END PLAYBACK]

00:35:52.410 --> 00:35:54.035
NANCY KANWISHER: OK,
for the most part,

00:35:54.035 --> 00:35:56.620
we don't have click
consonants in English

00:35:56.620 --> 00:35:58.090
that count as
phonemes in the sense

00:35:58.090 --> 00:35:59.680
of distinguishing
different words.

00:35:59.680 --> 00:36:03.340
But we do have click consonants
that we use in other domains.

00:36:03.340 --> 00:36:06.520
Anybody know what we use
click consonants for?

00:36:06.520 --> 00:36:09.230
There's at least two.

00:36:09.230 --> 00:36:10.932
Know any click consonants?

00:36:10.932 --> 00:36:12.260
STUDENT: [INAUDIBLE]

00:36:12.260 --> 00:36:13.427
NANCY KANWISHER: Yeah, what?

00:36:13.427 --> 00:36:14.825
STUDENT: [INAUDIBLE] That's--

00:36:14.825 --> 00:36:15.950
NANCY KANWISHER: Like what?

00:36:15.950 --> 00:36:18.200
STUDENT: [INAUDIBLE]

00:36:18.200 --> 00:36:21.047
NANCY KANWISHER: Yes, but
that's a regular consonant.

00:36:21.047 --> 00:36:22.130
It's actually not a click.

00:36:22.130 --> 00:36:25.250
It's just a regular consonant.

00:36:25.250 --> 00:36:30.830
Well, one is when you go, tsk,
tsk, tsk, the scolding sound.

00:36:30.830 --> 00:36:31.790
It's not a phoneme.

00:36:31.790 --> 00:36:35.440
It's not a word, but it has
a very particular meaning.

00:36:35.440 --> 00:36:41.722
Another one is how you get a
horse to giddy up. (CLICKS)

00:36:41.722 --> 00:36:43.930
So those are the click
consonants we have in English.

00:36:43.930 --> 00:36:46.120
They're not phonemes,
but we have them,

00:36:46.120 --> 00:36:48.340
and he's got a whole lot more.

00:36:48.340 --> 00:36:50.770
That was just for fun.

00:36:50.770 --> 00:36:54.430
So why is speech
perception challenging?

00:36:54.430 --> 00:36:58.060
Well, one is the essence of
it is that a given speech

00:36:58.060 --> 00:37:00.310
sound is highly variable.

00:37:00.310 --> 00:37:02.200
One way it's
variable is that when

00:37:02.200 --> 00:37:05.140
you speak at different
rates, all the frequencies

00:37:05.140 --> 00:37:07.030
go up and down and
haywire, making

00:37:07.030 --> 00:37:10.720
them very different across
different talking rates.

00:37:10.720 --> 00:37:13.460
Another is the context.

00:37:13.460 --> 00:37:17.500
So a given phoneme, like a
ba, or a pa sound, or a vowel,

00:37:17.500 --> 00:37:19.930
sounds totally different
depending on what phonemes

00:37:19.930 --> 00:37:21.400
come before and after it.

00:37:21.400 --> 00:37:24.490
They're not little punctate,
one at a time things.

00:37:24.490 --> 00:37:29.435
They all overlap and affect
each other in a big mess.

00:37:29.435 --> 00:37:31.310
And the third is one
we've already mentioned,

00:37:31.310 --> 00:37:33.670
which is the big
differences across speakers

00:37:33.670 --> 00:37:35.140
in the language.

00:37:35.140 --> 00:37:39.190
So you have to
recognize a ba sound

00:37:39.190 --> 00:37:41.410
even though it sounds
quite different when

00:37:41.410 --> 00:37:44.230
spoken by different speakers.

00:37:44.230 --> 00:37:46.900
So all of these things make
it very computationally

00:37:46.900 --> 00:37:48.820
challenging to
understand speech.

00:37:48.820 --> 00:37:53.170
Here's an illustration of
that talker variability.

00:37:53.170 --> 00:37:56.920
So what's shown here is not
a whole spectrogram, but just

00:37:56.920 --> 00:37:59.320
the intensity of
the first formant

00:37:59.320 --> 00:38:02.380
and the second formant,
those bands of energy

00:38:02.380 --> 00:38:04.090
that I showed you
in the spectrogram.

00:38:04.090 --> 00:38:09.550
And so each dot here
is a different person

00:38:09.550 --> 00:38:11.890
pronouncing a vowel.

00:38:11.890 --> 00:38:15.040
And each color-- this is
one vowel here in green,

00:38:15.040 --> 00:38:17.260
in that green ellipse, with
lots of different people

00:38:17.260 --> 00:38:19.210
saying that vowel.

00:38:19.210 --> 00:38:21.040
Here's another vowel
up here in red,

00:38:21.040 --> 00:38:24.670
with lots of different
people saying that vowel.

00:38:24.670 --> 00:38:29.650
And what you see is
they're really overlapping.

00:38:29.650 --> 00:38:32.290
So that means you can't just
go from the energy at those two

00:38:32.290 --> 00:38:35.500
formants, a point in that space,
and know what the vowel is.

00:38:35.500 --> 00:38:37.000
What if you were right there?

00:38:37.000 --> 00:38:41.050
Well, then it could be any
of four different vowels.

00:38:41.050 --> 00:38:43.820
So that's the problem of
talker variability illustrated

00:38:43.820 --> 00:38:44.320
with vowels.

00:38:44.320 --> 00:38:45.195
Does that make sense?

00:38:48.260 --> 00:38:50.540
I think I just said all of
this, blah, blah, blah--

00:38:50.540 --> 00:38:53.210
another classic ill-posed
problem in perception.

00:38:53.210 --> 00:38:54.870
You're given a
point in this space.

00:38:54.870 --> 00:38:57.890
How do you tell
which vowel it is?

00:38:57.890 --> 00:39:02.300
So one way we solve that is that
we learn each other's voices.

00:39:02.300 --> 00:39:06.290
And we know how a
given person pronounces

00:39:06.290 --> 00:39:08.720
a given set of vowels or words.

00:39:08.720 --> 00:39:11.300
And we use that to constrain
what they're saying.

00:39:11.300 --> 00:39:12.740
Have you ever
noticed, especially

00:39:12.740 --> 00:39:14.865
if you meet somebody new--
well, actually, you just

00:39:14.865 --> 00:39:16.250
experience this with Test.

00:39:16.250 --> 00:39:18.560
When he first speaks,
his English is beautiful,

00:39:18.560 --> 00:39:21.170
but he's from Zimbabwe and
he has kind of Zimbabwe,

00:39:21.170 --> 00:39:22.520
British-type accent.

00:39:22.520 --> 00:39:24.770
And at first it's hard to
understand what he's saying.

00:39:24.770 --> 00:39:26.570
Did you all experience
that briefly?

00:39:26.570 --> 00:39:29.108
I mean, that's why I put
the text on the slide,

00:39:29.108 --> 00:39:31.400
so you would get used to his
English and understand it.

00:39:31.400 --> 00:39:33.800
If I hadn't, you probably
wouldn't have understood

00:39:33.800 --> 00:39:35.060
that sentence he spoke first.

00:39:35.060 --> 00:39:37.430
That's because we don't
know his voice yet.

00:39:37.430 --> 00:39:39.870
But did you notice, after
even just a few words,

00:39:39.870 --> 00:39:42.620
you start to like tune right
in and you can understand him?

00:39:42.620 --> 00:39:45.500
So learning about an
individual's voice

00:39:45.500 --> 00:39:48.230
helps you pull apart the
properties of the voice,

00:39:48.230 --> 00:39:50.270
and unconfound
them from the sound

00:39:50.270 --> 00:39:52.550
so you can understand what
that person is saying.

00:39:56.180 --> 00:39:59.500
So that's part of how we
solve this ill-posed problem.

00:39:59.500 --> 00:40:02.500
And so evidence
that we do that is

00:40:02.500 --> 00:40:07.090
that if you have people listen
to voices they don't know

00:40:07.090 --> 00:40:09.460
or voices that are
changing from word to word,

00:40:09.460 --> 00:40:11.650
it's much harder to
understand speech.

00:40:11.650 --> 00:40:14.350
So you imagine you took the
sentence I'm saying right now,

00:40:14.350 --> 00:40:17.050
and you spliced in a different
person saying each word.

00:40:17.050 --> 00:40:19.540
Actually, I should
make that demo.

00:40:19.540 --> 00:40:21.010
One of you guys
send me an email--

00:40:21.010 --> 00:40:22.980
make that demo of a
different person speaking

00:40:22.980 --> 00:40:23.980
each word in a sentence.

00:40:23.980 --> 00:40:25.810
It'd be really
hard to understand.

00:40:25.810 --> 00:40:27.570
Because you wouldn't
have been able to fix

00:40:27.570 --> 00:40:28.945
this was a property
of the voice,

00:40:28.945 --> 00:40:31.030
now we can kind of separate
that from everything else.

00:40:31.030 --> 00:40:33.238
Because the damn voice will
be changing on each word.

00:40:33.238 --> 00:40:34.930
It'll be a mess.

00:40:34.930 --> 00:40:37.700
So that's one problem.

00:40:37.700 --> 00:40:42.470
So it turns out that the
opposite is true, as well.

00:40:42.470 --> 00:40:47.320
And that is, your ability to
recognize somebody's voice

00:40:47.320 --> 00:40:51.230
is a function of what you
know about that language.

00:40:51.230 --> 00:40:54.320
So you can recognize
voices better in a language

00:40:54.320 --> 00:40:57.740
you know than a language
you don't because you're

00:40:57.740 --> 00:40:58.700
doing the opposite.

00:40:58.700 --> 00:41:01.310
You're using knowledge of
the language and its speech

00:41:01.310 --> 00:41:05.450
properties that you
already know to constrain

00:41:05.450 --> 00:41:08.255
the problem of figuring out
who is this person's voice.

00:41:08.255 --> 00:41:09.380
So does everybody get this?

00:41:09.380 --> 00:41:11.990
These two things are affecting
each other-- the speaker

00:41:11.990 --> 00:41:13.490
and what's being said.

00:41:13.490 --> 00:41:15.290
And because they're
so confounded,

00:41:15.290 --> 00:41:17.300
massively confounded
in the stimulus,

00:41:17.300 --> 00:41:20.660
to solve that, the more
you know about the speaker,

00:41:20.660 --> 00:41:22.820
the better you can
understand what's being said.

00:41:22.820 --> 00:41:25.610
And the more you know about the
language and its properties,

00:41:25.610 --> 00:41:27.830
the more you can
recognize the voice.

00:41:27.830 --> 00:41:30.020
Each one is a source of
information about one

00:41:30.020 --> 00:41:34.100
of those two
confounded variables.

00:41:34.100 --> 00:41:37.340
And so people have shown
that psychophysically.

00:41:37.340 --> 00:41:39.050
And I think I have
time to do this.

00:41:39.050 --> 00:41:41.840
Here's a kind of cool
corollary of this,

00:41:41.840 --> 00:41:46.730
and that is, it's commonly
thought that dyslexia is most

00:41:46.730 --> 00:41:49.880
fundamentally a problem of
auditory speech perception, not

00:41:49.880 --> 00:41:50.730
a visual problem.

00:41:50.730 --> 00:41:52.640
There may also be a bit
of a visual problem,

00:41:52.640 --> 00:41:54.740
but it's thought
that at core, it's

00:41:54.740 --> 00:41:58.760
a problem of auditory
speech perception.

00:41:58.760 --> 00:42:01.430
So if that's true,
then you might

00:42:01.430 --> 00:42:05.150
think that this ability to
use knowledge of the language

00:42:05.150 --> 00:42:08.570
and its sounds to
constrain voice recognition

00:42:08.570 --> 00:42:11.330
would be reduced in
people with dyslexia,

00:42:11.330 --> 00:42:15.660
because they are less good
at processing speech sounds.

00:42:15.660 --> 00:42:17.470
And it turns out that's true.

00:42:17.470 --> 00:42:21.190
So here's a beautiful study from
Gabrielli Lab a few years ago.

00:42:21.190 --> 00:42:23.560
So first look at
the bars in blue.

00:42:23.560 --> 00:42:26.380
So this is accuracy
at voice recognition,

00:42:26.380 --> 00:42:28.920
which person is speaking.

00:42:28.920 --> 00:42:30.780
And this is native
English speakers

00:42:30.780 --> 00:42:32.100
who don't speak Chinese.

00:42:32.100 --> 00:42:34.230
They are much more
accurate recognizing

00:42:34.230 --> 00:42:36.480
who's speaking when they're
speaking English than when

00:42:36.480 --> 00:42:39.210
they're speaking Chinese.

00:42:39.210 --> 00:42:40.290
So that's kind of cool.

00:42:40.290 --> 00:42:41.970
That shows you the
way in which you

00:42:41.970 --> 00:42:45.480
use knowledge of the language
to constrain recognition

00:42:45.480 --> 00:42:46.890
of the voice.

00:42:46.890 --> 00:42:49.200
But now look what happens
in the dyslexics--

00:42:49.200 --> 00:42:52.350
no effect, exactly
as they predicted.

00:42:52.350 --> 00:42:55.380
Given that the dyslexics have a
problem with speech perception,

00:42:55.380 --> 00:42:57.150
they're apparently
not able to use

00:42:57.150 --> 00:43:00.150
that knowledge of the
phonemes of the language

00:43:00.150 --> 00:43:02.280
to constrain the problem
of voice recognition.

00:43:02.280 --> 00:43:05.220
They're just as bad
at voice recognition--

00:43:05.220 --> 00:43:07.890
I'm sorry, they're no
better at voice recognition

00:43:07.890 --> 00:43:11.130
in their native language
than in a foreign language.

00:43:11.130 --> 00:43:14.070
They can't use that knowledge
to constrain voice recognition.

00:43:14.070 --> 00:43:16.220
Does that make sense?

00:43:16.220 --> 00:43:18.920
Yeah, I love that study.

00:43:18.920 --> 00:43:23.240
So we haven't done any
brain stuff so far.

00:43:23.240 --> 00:43:26.090
We were just thinking about the
problem of hearing and speech

00:43:26.090 --> 00:43:28.550
perception, and what
we know from behavior.

00:43:28.550 --> 00:43:30.350
And we've learned a
lot already, but we'll

00:43:30.350 --> 00:43:32.600
learn more by looking at
the brain, and the meat,

00:43:32.600 --> 00:43:33.390
and all of that.

00:43:33.390 --> 00:43:36.230
So let's start with the ear.

00:43:36.230 --> 00:43:41.150
Again, remember, compressions
of air come into the ear.

00:43:41.150 --> 00:43:43.040
They travel through
the ear canal.

00:43:43.040 --> 00:43:45.230
They hit the tympanic membrane.

00:43:45.230 --> 00:43:49.160
They go through a whole series
of transducers, these three

00:43:49.160 --> 00:43:50.780
little ear bones
here that connect

00:43:50.780 --> 00:43:56.170
to this snail-shaped thing,
which is called the "cochlea."

00:43:56.170 --> 00:43:57.530
Cochlea is really important.

00:43:57.530 --> 00:43:58.780
You should remember that word.

00:43:58.780 --> 00:44:03.130
It's the place where you
transduce incoming sound

00:44:03.130 --> 00:44:08.890
into neural impulses,
way in there.

00:44:08.890 --> 00:44:10.945
And the cochlea is really cool.

00:44:13.600 --> 00:44:16.360
It's this, as I said,
a snail-shaped thing.

00:44:16.360 --> 00:44:20.500
And there are nerve endings
all the way along this thing.

00:44:20.500 --> 00:44:23.240
And because of the
physics of the cochlea,

00:44:23.240 --> 00:44:26.920
there are different resonant
frequencies at different parts

00:44:26.920 --> 00:44:28.700
of this snail.

00:44:28.700 --> 00:44:34.520
So basically, here are some
low-frequency sound waves.

00:44:34.520 --> 00:44:37.600
This is the cochlea stretched
out with the base and the apex.

00:44:37.600 --> 00:44:38.560
This is the base.

00:44:38.560 --> 00:44:39.880
That's the apex.

00:44:39.880 --> 00:44:45.070
And what you see is
the low frequencies

00:44:45.070 --> 00:44:48.850
have transduced some energy
at the base of the cochlea,

00:44:48.850 --> 00:44:51.550
and also at the apex.

00:44:51.550 --> 00:44:55.030
But midway range frequencies
and high frequencies

00:44:55.030 --> 00:44:56.470
do nothing at the apex.

00:44:56.470 --> 00:45:00.010
This business, there's
only physical fluctuations

00:45:00.010 --> 00:45:04.150
happening up here for
low frequency sounds.

00:45:04.150 --> 00:45:05.650
So there's little
nerve endings here

00:45:05.650 --> 00:45:07.420
that detect those
fluctuations up there

00:45:07.420 --> 00:45:10.060
and send those signals
up into the brain

00:45:10.060 --> 00:45:11.480
through the auditory nerve.

00:45:11.480 --> 00:45:15.970
And so in the middle,
here or something,

00:45:15.970 --> 00:45:18.820
you have sensitivity to
mid-range frequencies,

00:45:18.820 --> 00:45:21.770
not high or low.

00:45:21.770 --> 00:45:25.600
And at the base, it's sensitive
more to high frequencies

00:45:25.600 --> 00:45:27.520
than mid or low.

00:45:27.520 --> 00:45:28.570
So everybody get that?

00:45:28.570 --> 00:45:31.780
So basically, the cochlea
is doing a Fourier transform

00:45:31.780 --> 00:45:34.090
on the acoustic signal.

00:45:34.090 --> 00:45:39.370
It's taking these compressions
of air, and it's just saying,

00:45:39.370 --> 00:45:41.830
let's separate those out
into different frequencies,

00:45:41.830 --> 00:45:44.200
just with this physical device.

00:45:44.200 --> 00:45:47.620
It's like a physical Fourier
transform that's saying,

00:45:47.620 --> 00:45:50.110
let's just physically
separate the energy

00:45:50.110 --> 00:45:53.470
at each frequency range along
the length of the cochlea.

00:45:53.470 --> 00:45:55.520
Does that make sense?

00:45:55.520 --> 00:45:58.030
And then once you get different
parts of the cochlea that

00:45:58.030 --> 00:46:00.580
are sensitive to different
frequencies oscillating

00:46:00.580 --> 00:46:03.700
to different degrees, then you
stick some nerve cells there

00:46:03.700 --> 00:46:07.120
to pick up those oscillations,
go up the auditory nerve,

00:46:07.120 --> 00:46:08.980
and travel into the brain.

00:46:08.980 --> 00:46:10.815
Everybody have a gist
of how this works?

00:46:13.560 --> 00:46:14.580
So that's cool.

00:46:14.580 --> 00:46:16.380
But now, let's go
up to the brain.

00:46:16.380 --> 00:46:18.880
So now, this is
a view like this.

00:46:18.880 --> 00:46:20.710
And so here are the cochleae--

00:46:20.710 --> 00:46:22.380
I guess that's the plural--

00:46:22.380 --> 00:46:25.800
on each side-- ears,
ear canal, cochleae.

00:46:25.800 --> 00:46:28.530
And the first thing to
know, which is important,

00:46:28.530 --> 00:46:32.250
is that the path between the
cochlea and the first step

00:46:32.250 --> 00:46:35.640
up in the cortex is much
more complicated in hearing

00:46:35.640 --> 00:46:38.040
than it is in vision.

00:46:38.040 --> 00:46:40.530
Look at all these
nuclei deep down

00:46:40.530 --> 00:46:43.050
in the basement of the brain.

00:46:43.050 --> 00:46:46.380
In contrast, in vision,
how many synapses

00:46:46.380 --> 00:46:48.120
do you have to make
between the retina

00:46:48.120 --> 00:46:49.365
and primary visual cortex?

00:46:53.160 --> 00:46:54.480
Sorry.

00:46:54.480 --> 00:46:55.470
One synapse.

00:46:55.470 --> 00:46:56.110
Right?

00:46:56.110 --> 00:46:57.030
STUDENT: Well, I was thinking--

00:46:57.030 --> 00:46:58.560
NANCY KANWISHER:
Yeah, two, that's

00:46:58.560 --> 00:47:01.290
right, so retinal ganglion
cells send their axons straight

00:47:01.290 --> 00:47:04.290
into the LGN in the
thalamus, make a synapse.

00:47:04.290 --> 00:47:06.480
And then those LGN
neurons go straight

00:47:06.480 --> 00:47:09.420
up to primary visual cortex,
just one stop on the way.

00:47:09.420 --> 00:47:13.380
Look at all the stops
on the way here.

00:47:13.380 --> 00:47:15.420
So audition is a
really different beast

00:47:15.420 --> 00:47:17.460
from hearing in many ways.

00:47:17.460 --> 00:47:20.040
Next time, we'll talk
about how audition--

00:47:20.040 --> 00:47:22.950
not these parts of it, but
after you get up to the cortex--

00:47:22.950 --> 00:47:26.970
audition, we in my lab
and a few other labs

00:47:26.970 --> 00:47:28.560
are really starting
to suspect, is

00:47:28.560 --> 00:47:33.880
profoundly different in humans
from any non-human animal.

00:47:33.880 --> 00:47:35.880
And I think that's for
very interesting reasons,

00:47:35.880 --> 00:47:38.160
but this part is pretty
similar in animals,

00:47:38.160 --> 00:47:40.560
just getting information
up to the cortex.

00:47:40.560 --> 00:47:43.740
And audition is already very
different from vision just

00:47:43.740 --> 00:47:46.600
in the number of relays
going up to the brain.

00:47:46.600 --> 00:47:50.400
So those structures down there
do all kinds of awesome things.

00:47:50.400 --> 00:47:52.110
And last year, I
talked at great length

00:47:52.110 --> 00:47:54.570
about how we detect the
locations of sounds.

00:47:54.570 --> 00:47:57.480
It's absolutely beautiful
work, and elegant, and fun,

00:47:57.480 --> 00:48:00.060
but I decided that was a
little too much behavior.

00:48:00.060 --> 00:48:01.680
We should get on to the brain.

00:48:01.680 --> 00:48:05.430
But I recommend 9.35 if you want
to learn more about audition--

00:48:05.430 --> 00:48:06.270
awesome course.

00:48:06.270 --> 00:48:07.110
Did you take it?

00:48:07.110 --> 00:48:08.370
Really awesome course.

00:48:08.370 --> 00:48:09.700
Yeah, exactly.

00:48:09.700 --> 00:48:12.450
And so you'll learn more
about all that stuff.

00:48:12.450 --> 00:48:14.820
So instead, we will
just skip all that

00:48:14.820 --> 00:48:18.030
and go straight up to cortex.

00:48:18.030 --> 00:48:20.940
So the first place that
auditory information

00:48:20.940 --> 00:48:23.790
hits the cortex coming
up from the cochleae

00:48:23.790 --> 00:48:26.910
is primary auditory cortex,
just like the first place

00:48:26.910 --> 00:48:30.010
visual information hits the
cortex coming up from the eyes

00:48:30.010 --> 00:48:32.730
is primary visual cortex.

00:48:32.730 --> 00:48:37.590
So you can see in here that in a
cross-sectional view like that,

00:48:37.590 --> 00:48:39.280
this is primary auditory cortex.

00:48:39.280 --> 00:48:41.592
It's in that sulcus right there.

00:48:41.592 --> 00:48:43.050
That's kind of a
drag, because when

00:48:43.050 --> 00:48:46.110
we get occasional
opportunities to test patients

00:48:46.110 --> 00:48:50.010
who have grids of electrodes
on the surface of their brain,

00:48:50.010 --> 00:48:51.570
the grids don't
usually go in there

00:48:51.570 --> 00:48:54.900
and we can't see
primary auditory cortex.

00:48:54.900 --> 00:48:56.400
Although there are
new methods where

00:48:56.400 --> 00:48:58.110
they stick depth
electrodes, which

00:48:58.110 --> 00:49:00.570
is surprisingly, apparently,
better on the patients.

00:49:00.570 --> 00:49:03.180
And right now your TA,
Dana [? Bobinger, ?]

00:49:03.180 --> 00:49:08.160
is over at Children's Hospital
recording from a 19-year-old

00:49:08.160 --> 00:49:13.250
who has bad epilepsy and who has
depth electrodes in his brain.

00:49:13.250 --> 00:49:15.000
And he's listening to
all kinds of sounds.

00:49:15.000 --> 00:49:17.970
And she's recorded his neural
activity with depth electrodes.

00:49:17.970 --> 00:49:20.287
And so we are
hopeful, one, that we

00:49:20.287 --> 00:49:21.870
can find some
information that will be

00:49:21.870 --> 00:49:23.670
relevant to the neurosurgeons--

00:49:23.670 --> 00:49:25.140
I don't know about that--

00:49:25.140 --> 00:49:27.568
but two, that we'll
get some information

00:49:27.568 --> 00:49:29.610
from those deep structures
that you can't usually

00:49:29.610 --> 00:49:31.777
see when you have just grids
sitting on the surface.

00:49:34.650 --> 00:49:37.390
So back to functional MRI--

00:49:37.390 --> 00:49:39.750
so this is primary
auditory cortex.

00:49:39.750 --> 00:49:40.937
It's quite stylized.

00:49:40.937 --> 00:49:42.270
Let me remind you where you are.

00:49:42.270 --> 00:49:45.180
This is an inflated view
of the right hemisphere--

00:49:45.180 --> 00:49:48.420
back of the head, front of the
head, temporal lobe, all funny

00:49:48.420 --> 00:49:50.520
looking because it's been
mathematically unfolded

00:49:50.520 --> 00:49:53.460
so you can see stuff in the
sulcus where I just showed you.

00:49:53.460 --> 00:49:55.500
Primary auditory cortex
is in the sulcus.

00:49:55.500 --> 00:49:57.850
But we've inflated
it so you can see it.

00:49:57.850 --> 00:50:02.110
And so this is primary auditory
cortex, this whole thing here.

00:50:02.110 --> 00:50:04.810
And it shows you a property
we've talked about before.

00:50:04.810 --> 00:50:09.340
It's got a map, but the map
in primary auditory cortex

00:50:09.340 --> 00:50:12.010
is not a map of space
like it is in the retina

00:50:12.010 --> 00:50:13.490
for visual information.

00:50:13.490 --> 00:50:16.280
It's a map of frequency.

00:50:16.280 --> 00:50:18.770
And that makes sense
because the input transducer

00:50:18.770 --> 00:50:21.440
is a cochlea, which
already physically creates

00:50:21.440 --> 00:50:22.568
a map of frequency.

00:50:22.568 --> 00:50:24.110
And so that gets
traveled through all

00:50:24.110 --> 00:50:26.068
those intermediate stages
down in the basement,

00:50:26.068 --> 00:50:27.710
and it comes up to
the brain, and makes

00:50:27.710 --> 00:50:30.300
a map of frequency space.

00:50:30.300 --> 00:50:32.210
So what this means,
actually-- so here's

00:50:32.210 --> 00:50:34.470
sensitivity to
different frequencies.

00:50:34.470 --> 00:50:37.760
And so the classic structure
of primary auditory cortex

00:50:37.760 --> 00:50:41.330
in humans is high, low,
high-- high frequencies,

00:50:41.330 --> 00:50:43.010
low frequencies,
high frequencies,

00:50:43.010 --> 00:50:45.950
in that V-shaped pattern.

00:50:45.950 --> 00:50:47.323
So this is the right hemisphere.

00:50:47.323 --> 00:50:48.740
This is the left
hemisphere that's

00:50:48.740 --> 00:50:51.110
been mirror flipped so you
can compare them directly.

00:50:51.110 --> 00:50:54.710
And you can see this highly
stereotyped pattern of high,

00:50:54.710 --> 00:50:55.310
low, high.

00:50:55.310 --> 00:50:56.720
That's a tonotopic map.

00:50:56.720 --> 00:50:59.810
Everybody clear on what
a tonotopic map is?

00:50:59.810 --> 00:51:02.510
And we've just discretized
it into two chunks,

00:51:02.510 --> 00:51:05.810
but it's actually a gradient
of high to low to high,

00:51:05.810 --> 00:51:08.150
which you can kind of see
by those intermediate colors

00:51:08.150 --> 00:51:08.870
in there.

00:51:08.870 --> 00:51:10.690
Yeah.

00:51:10.690 --> 00:51:18.042
STUDENT: [INAUDIBLE] why
does the [INAUDIBLE]??

00:51:22.240 --> 00:51:24.970
NANCY KANWISHER: Yeah,
everything in the brain

00:51:24.970 --> 00:51:28.390
rearranges everything in
the input in multiple ways.

00:51:28.390 --> 00:51:30.550
So we didn't talk about
this, but in visual cortex,

00:51:30.550 --> 00:51:31.360
you have--

00:51:31.360 --> 00:51:33.520
I don't know what the latest
count is, at least 10,

00:51:33.520 --> 00:51:36.640
probably more than that,
separate retinotopic maps

00:51:36.640 --> 00:51:39.460
in different patches of cortex--
map, map, map, map, loads

00:51:39.460 --> 00:51:40.630
of them.

00:51:40.630 --> 00:51:43.960
And so there's all kinds
of transformations.

00:51:43.960 --> 00:51:48.700
And so much less is known about
the functional responses and

00:51:48.700 --> 00:51:50.920
functional organization
of auditory cortex

00:51:50.920 --> 00:51:55.000
than visual cortex, especially
in humans where we really

00:51:55.000 --> 00:51:57.010
don't know a lot, in fact.

00:51:57.010 --> 00:51:59.770
So there's no real
answer to that, other

00:51:59.770 --> 00:52:04.965
than it's not that
shocking, in a way,

00:52:04.965 --> 00:52:07.090
because you see that in
vision and in other domains

00:52:07.090 --> 00:52:10.120
anyway, with multiple maps
that differentially represent

00:52:10.120 --> 00:52:12.380
different parts of space.

00:52:12.380 --> 00:52:14.080
And so yeah, I didn't
say this, but many

00:52:14.080 --> 00:52:18.160
of those dozen or so
maps in visual cortex

00:52:18.160 --> 00:52:21.988
have differential representation
of different parts of space.

00:52:21.988 --> 00:52:23.530
Some focus on the
upper visual field,

00:52:23.530 --> 00:52:25.150
some on the lower visual field.

00:52:25.150 --> 00:52:29.470
And the whole question of
is that really one thing

00:52:29.470 --> 00:52:31.030
or is it two--

00:52:31.030 --> 00:52:33.100
this is all now
getting into the kind

00:52:33.100 --> 00:52:38.130
of cutting-edge, ambiguous
state that we don't know.

00:52:38.130 --> 00:52:40.540
All right, everybody clear
on tonotopy, primary auditory

00:52:40.540 --> 00:52:41.040
cortex?

00:52:41.040 --> 00:52:42.300
OK, good.

00:52:42.300 --> 00:52:46.590
All right, the standard
view from recording neurons

00:52:46.590 --> 00:52:50.130
in primary auditory
cortex in animals--

00:52:50.130 --> 00:52:54.030
monkeys, ferrets are big
in auditory neuroscience,

00:52:54.030 --> 00:52:56.280
other animals--

00:52:56.280 --> 00:52:58.770
is that the receptive
fields of individual

00:52:58.770 --> 00:53:01.110
neurons in primary
auditory cortex

00:53:01.110 --> 00:53:04.680
are linear filters in
the following sense--

00:53:04.680 --> 00:53:06.780
so here's a
spectrogram of a sound.

00:53:06.780 --> 00:53:08.700
This is just a description
of the stimulus.

00:53:08.700 --> 00:53:11.790
As usual, time, frequency.

00:53:11.790 --> 00:53:14.430
So it looks like it could be a
speech sound with some vowels

00:53:14.430 --> 00:53:14.670
there.

00:53:14.670 --> 00:53:15.920
Or it might be something else.

00:53:15.920 --> 00:53:17.850
Who knows.

00:53:17.850 --> 00:53:19.800
So that's a sound.

00:53:19.800 --> 00:53:25.020
So now, imagine an
electrode sitting

00:53:25.020 --> 00:53:28.770
next to a single neuron
in primary auditory

00:53:28.770 --> 00:53:32.100
cortex in, say, a ferret
listening to that sound,

00:53:32.100 --> 00:53:35.310
and characterizing what
does that neuron respond to.

00:53:35.310 --> 00:53:37.740
Well, the typical
finding is that neurons

00:53:37.740 --> 00:53:40.890
in primary auditory
cortex are what's

00:53:40.890 --> 00:53:45.090
known as spectral temporal
receptive fields, or STRFs

00:53:45.090 --> 00:53:47.160
to their friends.

00:53:47.160 --> 00:53:48.490
So what does that mean?

00:53:48.490 --> 00:53:51.870
Here's an example of
the receptive field that

00:53:51.870 --> 00:53:55.830
is the response dependence
of a given auditory cell,

00:53:55.830 --> 00:53:59.680
again, with time on this axis
and frequency on that axis.

00:53:59.680 --> 00:54:03.795
So what kind of sound
does that cell like?

00:54:09.239 --> 00:54:11.168
Can you see just
by looking at this?

00:54:11.168 --> 00:54:11.960
What kind of sound?

00:54:14.550 --> 00:54:15.840
STUDENT: Increasing frequency.

00:54:15.840 --> 00:54:17.970
NANCY KANWISHER:
Increasing frequency, yeah,

00:54:17.970 --> 00:54:20.280
something like that right.

00:54:20.280 --> 00:54:22.950
Here's one that also likes
increasing frequency,

00:54:22.950 --> 00:54:26.550
but slower, shallower
increasing frequency.

00:54:26.550 --> 00:54:28.890
Here's one that likes
decreasing frequency.

00:54:28.890 --> 00:54:31.080
Now, you may be wondering
what the stripes are.

00:54:31.080 --> 00:54:33.070
We didn't talk about
this in visual cortex,

00:54:33.070 --> 00:54:35.010
but this is a common
property, that it

00:54:35.010 --> 00:54:37.320
likes this particular
set of frequencies

00:54:37.320 --> 00:54:41.910
here, but is inhibited
by adjacent frequencies.

00:54:41.910 --> 00:54:45.660
So you also see something like
that with orientation tuning

00:54:45.660 --> 00:54:49.440
in primary visual cortex.

00:54:49.440 --> 00:54:53.700
And so here, these ones
are changing faster,

00:54:53.700 --> 00:54:55.750
both increasing and decreasing.

00:54:55.750 --> 00:54:58.950
So the idea is primary
auditory cortex in animals,

00:54:58.950 --> 00:55:02.580
and presumably in humans,
is full of a bunch

00:55:02.580 --> 00:55:05.580
of cells that are basically
spectrotemporal filters

00:55:05.580 --> 00:55:06.600
like this.

00:55:06.600 --> 00:55:09.300
They are picking out
changes in frequency

00:55:09.300 --> 00:55:12.420
over time that happen
to different degrees,

00:55:12.420 --> 00:55:17.802
and at different rates, and
in different frequency ranges.

00:55:17.802 --> 00:55:19.260
Does that make
sense, more or less?

00:55:19.260 --> 00:55:20.385
Yes, [INAUDIBLE]

00:55:20.385 --> 00:55:21.510
STUDENT: I have a question.

00:55:21.510 --> 00:55:22.427
NANCY KANWISHER: Yeah.

00:55:22.427 --> 00:55:29.955
STUDENT: [INAUDIBLE] how would
you tell that was [INAUDIBLE]??

00:55:29.955 --> 00:55:32.080
NANCY KANWISHER: Yeah, how
do they figure that out?

00:55:32.080 --> 00:55:34.240
I usually spend all this
time talking about the design

00:55:34.240 --> 00:55:34.990
of the experiment.

00:55:34.990 --> 00:55:37.970
I just skipped straight
to the answer here.

00:55:37.970 --> 00:55:40.720
Well, I don't know exactly
what you do, but you probably--

00:55:40.720 --> 00:55:42.310
I mean, this has
been a whole thing

00:55:42.310 --> 00:55:44.630
that went on for decades
for people to get at this.

00:55:44.630 --> 00:55:46.380
So I'm guessing
that somehow, they

00:55:46.380 --> 00:55:48.130
got into that general
space, and then they

00:55:48.130 --> 00:55:51.053
generated stimuli that make
all these different sounds.

00:55:51.053 --> 00:55:52.720
And they just run
through them, and they

00:55:52.720 --> 00:55:55.620
find, for a given cell, you
play all these different sounds.

00:55:55.620 --> 00:56:01.210
You go-- [MAKES SOUNDS],,
et cetera.

00:56:01.210 --> 00:56:03.250
I'll spare you more imitations.

00:56:03.250 --> 00:56:05.830
You play all these different
sounds to the animal

00:56:05.830 --> 00:56:07.690
and you record the
response of that neuron.

00:56:07.690 --> 00:56:09.400
And you would find,
for example, that it

00:56:09.400 --> 00:56:11.380
responds much more when you
play that sound than any

00:56:11.380 --> 00:56:11.963
of the others.

00:56:15.305 --> 00:56:16.180
Does that make sense?

00:56:16.180 --> 00:56:17.897
STUDENT: No, it makes sense.

00:56:17.897 --> 00:56:19.980
NANCY KANWISHER: But how
do they ever hit on that?

00:56:19.980 --> 00:56:22.640
STUDENT: No, what
I was asking is

00:56:22.640 --> 00:56:26.460
that are they using
separate [INAUDIBLE]??

00:56:29.430 --> 00:56:31.740
NANCY KANWISHER: Oh,
the red and the blue?

00:56:31.740 --> 00:56:34.860
How exactly they got-- rather
than just the simple thing with

00:56:34.860 --> 00:56:36.360
just that--

00:56:36.360 --> 00:56:39.870
how exactly they arrived on
that, I'm not totally sure.

00:56:42.770 --> 00:56:45.380
I mean, there are mathematical
reasons why it makes sense

00:56:45.380 --> 00:56:50.600
to have that whole thing rather
than just a single stripe,

00:56:50.600 --> 00:56:51.978
that I think are
beyond the scope

00:56:51.978 --> 00:56:53.270
of this lecture for the moment.

00:56:53.270 --> 00:56:57.770
But anyway, it wasn't just a
totally arbitrary thing to try.

00:56:57.770 --> 00:56:59.990
Those are particularly
useful kind

00:56:59.990 --> 00:57:05.490
of receptive fields for
representing the input.

00:57:05.490 --> 00:57:07.800
So everybody sort of
clear, approximately,

00:57:07.800 --> 00:57:08.550
what this idea is?

00:57:08.550 --> 00:57:10.920
So it's very
low-level basic, just

00:57:10.920 --> 00:57:13.590
are the frequencies going
up or down, and which range,

00:57:13.590 --> 00:57:14.472
and how fast?

00:57:14.472 --> 00:57:15.930
That's what primary
auditory cortex

00:57:15.930 --> 00:57:20.055
does organized in this
map, this tonotopic map.

00:57:25.770 --> 00:57:28.500
So think of primary
auditory cortex as just

00:57:28.500 --> 00:57:30.570
this bank, this big
set of linear filters

00:57:30.570 --> 00:57:34.170
for particular frequency
changes over time.

00:57:34.170 --> 00:57:38.220
So that's all based
on data from animals,

00:57:38.220 --> 00:57:40.290
from recording
individual neurons.

00:57:40.290 --> 00:57:43.260
But we want to know about
humans, not just because that's

00:57:43.260 --> 00:57:46.020
what this course is about, but
we want to know about humans.

00:57:46.020 --> 00:57:48.960
I mean, ferrets are
nice, but really!

00:57:48.960 --> 00:57:51.820
So is that true for humans.

00:57:51.820 --> 00:57:54.900
Well, Josh McDermott
and Sam Norman-Haignere

00:57:54.900 --> 00:57:57.300
just published a
paper a few months ago

00:57:57.300 --> 00:57:59.010
in which they
addressed this question

00:57:59.010 --> 00:58:00.922
in a really interesting way.

00:58:00.922 --> 00:58:03.130
So here's the logic-- this
is a little bit technical.

00:58:03.130 --> 00:58:04.140
I'm trying to give you the gist.

00:58:04.140 --> 00:58:05.010
I hope it works.

00:58:05.010 --> 00:58:06.190
Give it a try.

00:58:06.190 --> 00:58:09.690
So they generated
synthetically, computationally,

00:58:09.690 --> 00:58:12.550
what they call
"model-matched stimuli."

00:58:12.550 --> 00:58:13.950
So the idea is this--

00:58:13.950 --> 00:58:16.470
the idea is if you
present a natural sound--

00:58:16.470 --> 00:58:19.620
like a dog barking, or a person
speaking, or a toilet flushing,

00:58:19.620 --> 00:58:22.680
just some sound that
you would hear in life--

00:58:22.680 --> 00:58:26.490
and then what they do is they
make a synthetic signal that

00:58:26.490 --> 00:58:29.940
matches that sound with
respect to those STRFs

00:58:29.940 --> 00:58:31.260
I just showed you.

00:58:31.260 --> 00:58:34.170
That is, if you fed
the original sound

00:58:34.170 --> 00:58:37.170
and you fed this synthetic
sound into the STRFs,

00:58:37.170 --> 00:58:40.260
you'd get the same
thing in the STRFs.

00:58:40.260 --> 00:58:42.330
So this is a way
of saying, we're

00:58:42.330 --> 00:58:44.940
assuming that those STRFs are
a good description of what

00:58:44.940 --> 00:58:47.370
goes on in A1, so
let's test that

00:58:47.370 --> 00:58:49.572
by taking a big, fancy,
real-world sound that

00:58:49.572 --> 00:58:51.030
has meaning and
people know what it

00:58:51.030 --> 00:58:54.330
is, and let's make
a control sound that

00:58:54.330 --> 00:58:58.710
matches the in-STRF properties.

00:58:58.710 --> 00:59:00.660
And let's see if we
get the same response

00:59:00.660 --> 00:59:03.870
in the brain in that region.

00:59:03.870 --> 00:59:05.610
If that model is
a good description

00:59:05.610 --> 00:59:07.170
of what that region
does, then you

00:59:07.170 --> 00:59:08.820
should get a very
similar response

00:59:08.820 --> 00:59:11.790
when you give the synthetic
sound and the original sound

00:59:11.790 --> 00:59:13.830
that you recorded in the world.

00:59:13.830 --> 00:59:17.010
So they tested this
on a STRF-like model,

00:59:17.010 --> 00:59:19.763
like this thing I
just described before.

00:59:19.763 --> 00:59:21.930
And so just to show you
what these sounds are like--

00:59:21.930 --> 00:59:24.032
so here's an original
sound just recorded

00:59:24.032 --> 00:59:25.365
in the world of somebody typing.

00:59:25.365 --> 00:59:26.032
[AUDIO PLAYBACK]

00:59:26.032 --> 00:59:30.087
[TYPING]

00:59:30.087 --> 00:59:30.670
[END PLAYBACK]

00:59:30.670 --> 00:59:32.830
OK, OK, OK, that's enough.

00:59:32.830 --> 00:59:35.890
I know it's riveting,
but so then they run that

00:59:35.890 --> 00:59:37.000
through their STRF model.

00:59:37.000 --> 00:59:39.010
They get a STRF
description and they

00:59:39.010 --> 00:59:42.900
generate a matched stimulus
from their STRF description.

00:59:42.900 --> 00:59:43.900
And it sounds like this.

00:59:43.900 --> 00:59:44.567
[AUDIO PLAYBACK]

00:59:44.567 --> 00:59:46.190
[TYPING]

00:59:46.690 --> 00:59:47.758
Pretty good.

00:59:47.758 --> 00:59:49.300
It's kind of hard
to tell them apart.

00:59:51.850 --> 00:59:53.320
Sorry, enough.

00:59:53.320 --> 00:59:54.640
[END PLAYBACK]

00:59:54.640 --> 00:59:56.733
All right.

00:59:56.733 --> 00:59:58.150
And you can see
their spectrograms

00:59:58.150 --> 01:00:00.070
are really similar.

01:00:00.070 --> 01:00:02.140
So for a textury
thing like typing,

01:00:02.140 --> 01:00:04.630
it really captures the
essence of what's being heard.

01:00:04.630 --> 01:00:07.420
We're just telling you
what these control stimuli

01:00:07.420 --> 01:00:08.373
sound like.

01:00:08.373 --> 01:00:10.540
Let's take another sound,
a person walking in heels.

01:00:10.540 --> 01:00:12.310
And you can see all
those verticals.

01:00:12.310 --> 01:00:13.300
Those are the clicks.

01:00:13.300 --> 01:00:17.135
Clicks have energy across
lots of different frequencies.

01:00:17.135 --> 01:00:18.760
And that's what a
vertical line means--

01:00:18.760 --> 01:00:20.170
it means all those
different-- remember,

01:00:20.170 --> 01:00:21.462
this is frequency on this axis.

01:00:21.462 --> 01:00:23.440
So a vertical line
means energy at lots

01:00:23.440 --> 01:00:27.580
of different frequencies
not organized in harmonics,

01:00:27.580 --> 01:00:29.050
so it's not pitchy.

01:00:29.050 --> 01:00:29.590
Here we go.

01:00:29.590 --> 01:00:30.516
[AUDIO PLAYBACK]

01:00:30.516 --> 01:00:36.747
[HEELS CLICKING]

01:00:36.747 --> 01:00:37.330
[END PLAYBACK]

01:00:37.330 --> 01:00:40.360
OK, here's the STRF version,
the control stimulus.

01:00:40.360 --> 01:00:41.211
[AUDIO PLAYBACK]

01:00:41.211 --> 01:00:47.594
[CLICKING]

01:00:47.594 --> 01:00:48.590
[END PLAYBACK]

01:00:48.590 --> 01:00:52.330
So it captures some of
it, but not all of it.

01:00:52.330 --> 01:00:54.130
It captures the
sound of each click,

01:00:54.130 --> 01:00:56.110
but not the spacing between.

01:00:56.110 --> 01:00:58.150
So it's getting the
local properties, but not

01:00:58.150 --> 01:01:00.340
all of the properties.

01:01:00.340 --> 01:01:01.070
Yeah.

01:01:01.070 --> 01:01:02.730
STUDENT: How did you say--

01:01:02.730 --> 01:01:03.910
like just the [INAUDIBLE]?

01:01:03.910 --> 01:01:05.452
NANCY KANWISHER:
How do they make it?

01:01:05.452 --> 01:01:07.360
I didn't tell you
because it's complicated.

01:01:07.360 --> 01:01:10.360
They basically start with
pink noise, or white noise,

01:01:10.360 --> 01:01:11.440
or some kind of noise.

01:01:11.440 --> 01:01:13.365
They run it through
their STRF thing.

01:01:13.365 --> 01:01:15.490
They run the original sound
through the STRF thing.

01:01:15.490 --> 01:01:16.270
They compare them.

01:01:16.270 --> 01:01:17.620
And they say, how are we
going to adjust the noise

01:01:17.620 --> 01:01:18.703
to make it more like that?

01:01:18.703 --> 01:01:24.000
And they just iterate a lot, and
they end up with these stimuli.

01:01:24.000 --> 01:01:25.800
And you can see
just looking at it,

01:01:25.800 --> 01:01:27.258
they ended up with
something that's

01:01:27.258 --> 01:01:30.090
pretty similar in terms
of the spectrogram.

01:01:30.090 --> 01:01:32.037
Let's listen to a
person speaking.

01:01:32.037 --> 01:01:33.120
Here's the original sound.

01:01:33.120 --> 01:01:33.787
[AUDIO PLAYBACK]

01:01:33.787 --> 01:01:36.840
- Is that art offers a time
warp to the past, as well

01:01:36.840 --> 01:01:37.365
as insight.

01:01:37.365 --> 01:01:37.740
[END PLAYBACK]

01:01:37.740 --> 01:01:39.540
NANCY KANWISHER: OK, now
I'm going to turn it off.

01:01:39.540 --> 01:01:40.950
Here's the synthetic version.

01:01:40.950 --> 01:01:41.617
[AUDIO PLAYBACK]

01:01:41.617 --> 01:01:46.428
[INAUDIBLE]

01:01:46.428 --> 01:01:47.370
[END PLAYBACK]

01:01:47.370 --> 01:01:49.000
OK, now we've lost something.

01:01:49.000 --> 01:01:51.780
So does everybody see
how with keyboard typing,

01:01:51.780 --> 01:01:54.450
it really sounds the same,
the synthetic version?

01:01:54.450 --> 01:01:56.280
With walking in heels,
kind of, sort of,

01:01:56.280 --> 01:01:59.460
at least locally, but not
globally, and with speech,

01:01:59.460 --> 01:02:01.690
we've just totally lost it.

01:02:01.690 --> 01:02:03.690
The stuff that you can
capture with a STRF model

01:02:03.690 --> 01:02:06.420
does not capture the
full richness of speech.

01:02:06.420 --> 01:02:08.970
There's something more
in a speech stimulus

01:02:08.970 --> 01:02:11.812
than you can capture with
that just simple STRF model.

01:02:11.812 --> 01:02:13.020
OK, let's listen to a violin.

01:02:13.020 --> 01:02:13.687
[AUDIO PLAYBACK]

01:02:13.687 --> 01:02:18.907
[MUSIC PLAYING]

01:02:18.907 --> 01:02:19.490
[END PLAYBACK]

01:02:19.490 --> 01:02:21.410
OK, what does the STRF
model do with that?

01:02:21.410 --> 01:02:22.077
[AUDIO PLAYBACK]

01:02:22.077 --> 01:02:25.147
[MUDDY MUSIC PLAYING]

01:02:25.147 --> 01:02:25.730
[END PLAYBACK]

01:02:25.730 --> 01:02:26.230
I love that.

01:02:26.230 --> 01:02:29.460
It sounds like a
sea lion colony.

01:02:29.460 --> 01:02:32.870
Anyway, so what you see
is the STRF model totally

01:02:32.870 --> 01:02:35.150
fails to capture
speech and music,

01:02:35.150 --> 01:02:38.210
but it captures textury
sounds like that.

01:02:38.210 --> 01:02:44.630
And it loses some of the broader
temporal scale information.

01:02:44.630 --> 01:02:48.470
So that's the stimuli.

01:02:48.470 --> 01:02:51.890
Then you scan people
listening to these sounds.

01:02:51.890 --> 01:02:55.710
Just pop them in the scanner
and play those sounds.

01:02:55.710 --> 01:02:58.910
And so then what they
do is they just ask.

01:02:58.910 --> 01:03:01.730
So this is, again, the white
outline is primary auditory

01:03:01.730 --> 01:03:04.190
cortex where you have
that frequency map, mapped

01:03:04.190 --> 01:03:06.800
in a separate experiment, and
just plunk down on the brain

01:03:06.800 --> 01:03:07.300
here.

01:03:07.300 --> 01:03:09.890
We're zooming in on that part
of the top of the temporal lobe.

01:03:09.890 --> 01:03:13.250
And so what's shown
here is, for each voxel,

01:03:13.250 --> 01:03:16.850
they're showing the correlation
of the response of that voxel

01:03:16.850 --> 01:03:21.470
to the original sound and
the synthetic, STRF-y sound.

01:03:21.470 --> 01:03:23.360
And what you see is
those correlations

01:03:23.360 --> 01:03:26.510
are really high in
primary auditory cortex.

01:03:26.510 --> 01:03:29.090
In other words,
primary auditory cortex

01:03:29.090 --> 01:03:32.150
responds pretty much the
same to the original sound

01:03:32.150 --> 01:03:33.560
and the synthetic sound.

01:03:33.560 --> 01:03:35.630
It doesn't detect
that difference.

01:03:35.630 --> 01:03:38.810
But as soon as you get outside
of primary auditory cortex,

01:03:38.810 --> 01:03:41.070
you get something
totally different.

01:03:41.070 --> 01:03:43.190
And so that was
exactly the prediction,

01:03:43.190 --> 01:03:45.470
is that model that's
being tested here

01:03:45.470 --> 01:03:47.690
is a model of how they
thought primary auditory

01:03:47.690 --> 01:03:48.730
cortex worked--

01:03:48.730 --> 01:03:50.420
a bank of linear filters.

01:03:50.420 --> 01:03:53.510
They test that model by
generating a new set of stimuli

01:03:53.510 --> 01:03:56.180
that are matched for
those linear filters,

01:03:56.180 --> 01:03:58.100
and they get pretty
much the same response

01:03:58.100 --> 01:03:59.730
in primary auditory cortex.

01:03:59.730 --> 01:04:02.850
So check-- that's a good model
of primary auditory cortex.

01:04:02.850 --> 01:04:04.815
But also, the blue
shows you much lower

01:04:04.815 --> 01:04:05.690
correlation out here.

01:04:05.690 --> 01:04:08.795
It is not a good model of stuff
outside of auditory cortex.

01:04:08.795 --> 01:04:09.930
Josh.

01:04:09.930 --> 01:04:12.410
STUDENT: So isn't this
kind of self-fulfilling,

01:04:12.410 --> 01:04:17.030
in the sense that I build
my synthetic stimuli based

01:04:17.030 --> 01:04:19.340
on these kind of
models, and then--

01:04:19.340 --> 01:04:22.190
NANCY KANWISHER: It is,
except the models were all

01:04:22.190 --> 01:04:24.890
based on animal work and
this is human brains.

01:04:24.890 --> 01:04:26.128
So this is a way--

01:04:26.128 --> 01:04:27.170
but that's exactly right.

01:04:27.170 --> 01:04:32.000
It's a way of saying all this
work from animals precisely

01:04:32.000 --> 01:04:33.800
characterizing
response properties

01:04:33.800 --> 01:04:36.350
of individual neurons, which
you can do in animals and mostly

01:04:36.350 --> 01:04:38.600
not in humans, do
we think that's

01:04:38.600 --> 01:04:40.460
true of human primary
auditory cortex?

01:04:40.460 --> 01:04:41.135
And yes, it is.

01:04:41.135 --> 01:04:43.010
Does everybody get at
least the gist of that?

01:04:43.010 --> 01:04:45.098
I realize I skipped
over lots of details

01:04:45.098 --> 01:04:47.015
because I want you to
get the general picture.

01:04:49.610 --> 01:04:51.170
Yeah.

01:04:51.170 --> 01:04:54.080
STUDENT: What are they
trying to achieve by doing

01:04:54.080 --> 01:04:55.440
this type of [INAUDIBLE]?

01:04:55.440 --> 01:04:58.130
I mean, the hypothesis
is that the human

01:04:58.130 --> 01:05:02.673
and the animal auditory
cortex is the same?

01:05:02.673 --> 01:05:04.590
NANCY KANWISHER: Primary
auditory cortex, yes.

01:05:04.590 --> 01:05:05.540
Yes.

01:05:05.540 --> 01:05:07.850
They're basically testing--
you derive that model

01:05:07.850 --> 01:05:11.880
from the animal work, then
you design a test of it,

01:05:11.880 --> 01:05:14.480
which is making those
synthetic stimuli.

01:05:14.480 --> 01:05:16.280
And I left this out
because actually, I

01:05:16.280 --> 01:05:18.500
don't think they've done
that, but presumably,

01:05:18.500 --> 01:05:21.112
if you test those stimuli
with single units in ferrets,

01:05:21.112 --> 01:05:22.070
you get the same thing.

01:05:22.070 --> 01:05:23.810
You get very, very
similar responses

01:05:23.810 --> 01:05:26.630
in primary auditory cortex
to the original sound

01:05:26.630 --> 01:05:29.680
and the synthetic version of
it based on the STRF model.

01:05:29.680 --> 01:05:32.180
STUDENT: It's predicated on the
assumption that both of them

01:05:32.180 --> 01:05:33.263
are structurally the same.

01:05:33.263 --> 01:05:34.763
NANCY KANWISHER:
Well, it's testing.

01:05:34.763 --> 01:05:36.080
It's asking that question.

01:05:36.080 --> 01:05:38.400
It's asking that question.

01:05:38.400 --> 01:05:42.110
Because I've occasionally in
here lamented about how crappy

01:05:42.110 --> 01:05:44.120
our methods are in human
cognitive neuroscience.

01:05:44.120 --> 01:05:44.953
I mean, they're fun.

01:05:44.953 --> 01:05:47.340
We can do something, but
we hit a wall pretty fast.

01:05:47.340 --> 01:05:49.190
We want to see the
actual neural code.

01:05:49.190 --> 01:05:51.530
We don't have spatial
and temporal resolution

01:05:51.530 --> 01:05:52.820
at the same time.

01:05:52.820 --> 01:05:54.800
We pretty much only
get that in animals.

01:05:54.800 --> 01:05:58.380
We can pretty much only do
really careful causal tests

01:05:58.380 --> 01:05:58.880
in animals.

01:05:58.880 --> 01:06:01.790
We can pretty much only see
connectivity in a precise way.

01:06:01.790 --> 01:06:04.110
And all these things we
can do only in animals.

01:06:04.110 --> 01:06:07.220
And so we need to know if
those animal models are

01:06:07.220 --> 01:06:08.232
good models for humans.

01:06:08.232 --> 01:06:09.440
And this is a way to test it.

01:06:09.440 --> 01:06:11.750
And it passed with
flying colors.

01:06:11.750 --> 01:06:13.850
Make sense?

01:06:13.850 --> 01:06:17.030
So primary auditory
cortex seems in humans

01:06:17.030 --> 01:06:18.860
that it's much like
it is in ferrets,

01:06:18.860 --> 01:06:21.980
a bank of linear filters
with STRF-y properties.

01:06:25.420 --> 01:06:27.520
What about everything else?

01:06:27.520 --> 01:06:30.220
After all, you guys
can hear the difference

01:06:30.220 --> 01:06:33.250
between the original version
and the synthetic version

01:06:33.250 --> 01:06:35.410
of the woman talking
and the violin.

01:06:35.410 --> 01:06:37.180
And if I played you
all the other stimuli

01:06:37.180 --> 01:06:39.347
of real-world sounds, you
could hear the differences

01:06:39.347 --> 01:06:40.880
in many of the
other ones as well.

01:06:40.880 --> 01:06:42.190
So what are you doing?

01:06:42.190 --> 01:06:44.140
Well, there's lots
of auditory cortex

01:06:44.140 --> 01:06:46.900
beyond primary auditory
cortex that could

01:06:46.900 --> 01:06:48.180
represent that difference.

01:06:48.180 --> 01:06:49.930
And what this is
suggesting is, whatever's

01:06:49.930 --> 01:06:52.330
going on out here is doing
something really different

01:06:52.330 --> 01:06:53.080
with those sounds.

01:06:53.080 --> 01:06:53.818
It is not fooled.

01:06:53.818 --> 01:06:56.110
It does not think the synthetic
thing is the same thing

01:06:56.110 --> 01:06:57.242
as the original thing.

01:06:57.242 --> 01:06:58.825
That's what the low
correlation means.

01:07:02.210 --> 01:07:04.880
So I'll tell you about just
one little patch of cortex

01:07:04.880 --> 01:07:06.410
out there.

01:07:06.410 --> 01:07:10.220
And that is-- again, this
is just for reference.

01:07:10.220 --> 01:07:13.580
We've zoomed in again on
this is the little code

01:07:13.580 --> 01:07:16.280
for separate mapping of high,
low, high, primary auditory

01:07:16.280 --> 01:07:17.990
cortex right there.

01:07:17.990 --> 01:07:22.707
And what the yellow bands are is
selective responses to speech.

01:07:22.707 --> 01:07:24.290
So you compare a
whole bunch of speech

01:07:24.290 --> 01:07:26.840
sounds to a whole bunch
of non-speech sounds,

01:07:26.840 --> 01:07:29.720
and you get a band
of activation right

01:07:29.720 --> 01:07:31.910
below primary auditory cortex.

01:07:31.910 --> 01:07:32.884
Yes.

01:07:32.884 --> 01:07:34.884
STUDENT: I thought the
separation was low, high,

01:07:34.884 --> 01:07:37.320
medium [INAUDIBLE].

01:07:37.320 --> 01:07:39.410
NANCY KANWISHER:
High, low, high--

01:07:39.410 --> 01:07:40.863
I probably said it backwards.

01:07:40.863 --> 01:07:41.780
That would be like me.

01:07:41.780 --> 01:07:44.510
But it's-- wait, wait.

01:07:44.510 --> 01:07:46.880
What the hell is it?

01:07:46.880 --> 01:07:48.620
I'm pretty sure it's
high, low, high.

01:07:48.620 --> 01:07:50.938
Let's go back and look.

01:07:50.938 --> 01:07:53.480
I might have screwed it up on
the slide or said it backwards,

01:07:53.480 --> 01:07:59.360
but I'm pretty sure
it's high, low, high.

01:07:59.360 --> 01:08:02.495
STUDENT: So the low
frequency is the [INAUDIBLE]..

01:08:02.495 --> 01:08:05.120
NANCY KANWISHER: Yeah, just like
that's the code for frequency,

01:08:05.120 --> 01:08:07.760
right there.

01:08:07.760 --> 01:08:09.400
But ask me those
questions because I'm

01:08:09.400 --> 01:08:11.900
very capable of getting things
backwards, as you've probably

01:08:11.900 --> 01:08:12.567
already noticed.

01:08:16.069 --> 01:08:20.300
So there is a band of
speech-selective cortex just

01:08:20.300 --> 01:08:22.460
outside of primary
auditory cortex,

01:08:22.460 --> 01:08:26.479
in that region that we just
saw responds differently

01:08:26.479 --> 01:08:31.310
to the original sound and the
model-matched synthetic sound.

01:08:31.310 --> 01:08:32.390
So that's pretty cool.

01:08:32.390 --> 01:08:35.029
What do I mean by
"speech-selective cortex?"

01:08:35.029 --> 01:08:37.880
What I mean is--

01:08:37.880 --> 01:08:39.255
this is some of our data.

01:08:39.255 --> 01:08:40.880
I tried to find you
someone else's data

01:08:40.880 --> 01:08:44.083
and I went down a
45-minute rabbit hole

01:08:44.083 --> 01:08:45.250
trying to find a nice slide.

01:08:45.250 --> 01:08:46.790
And I just couldn't
find a good picture.

01:08:46.790 --> 01:08:48.370
I finally said, screw it,
I'll show you my data,

01:08:48.370 --> 01:08:49.495
even though I'm trying to--

01:08:49.495 --> 01:08:51.245
we're not the only
ones who've shown this.

01:08:51.245 --> 01:08:52.390
We just have the best data.

01:08:52.390 --> 01:08:55.510
Other people had tested it with
four, five, six conditions.

01:08:55.510 --> 01:08:59.830
We tested it with 165 sounds.

01:08:59.830 --> 01:09:01.870
So this is the
magnitude of response

01:09:01.870 --> 01:09:06.790
in that yellow region to 165
different sounds, color coded

01:09:06.790 --> 01:09:09.088
by condition shown down here.

01:09:09.088 --> 01:09:10.630
And so what you see
if you look at it

01:09:10.630 --> 01:09:16.040
is all the top sounds are
light green and dark green.

01:09:16.040 --> 01:09:21.970
Speech-- notice, importantly,
that the response

01:09:21.970 --> 01:09:25.840
is very similar to English
speech and foreign speech

01:09:25.840 --> 01:09:29.670
which our subjects
do not understand.

01:09:29.670 --> 01:09:32.810
So that tells us that this
is not about language.

01:09:32.810 --> 01:09:35.840
This is not about the meaning
of a sentence, or syntax, or any

01:09:35.840 --> 01:09:37.170
of that stuff.

01:09:37.170 --> 01:09:39.470
This is about phonemes,
the difference

01:09:39.470 --> 01:09:42.157
between a ba and a pa, which you
can do on a foreign language,

01:09:42.157 --> 01:09:44.240
even if there's a few
phonemes that are different.

01:09:44.240 --> 01:09:45.475
You get most of them.

01:09:45.475 --> 01:09:46.850
Does everybody
get the difference

01:09:46.850 --> 01:09:50.760
between speech and language?

01:09:50.760 --> 01:09:53.010
Amazingly, the senior
author of the paper

01:09:53.010 --> 01:09:56.400
you read for last night does
not understand that difference.

01:09:56.400 --> 01:09:58.170
He published a beautiful paper.

01:09:58.170 --> 01:10:00.000
Every time he comes
here to speak,

01:10:00.000 --> 01:10:02.760
he talks about language,
language, language, language.

01:10:02.760 --> 01:10:07.080
And I say, Eddie, have you ever
presented a stimulus that's

01:10:07.080 --> 01:10:08.038
in a foreign language?

01:10:08.038 --> 01:10:10.080
He's, like, oh, no, that'd
be really interesting.

01:10:10.080 --> 01:10:13.290
It's like, Eddie, until you do
that, you don't know if you're

01:10:13.290 --> 01:10:14.730
studying language or speech.

01:10:14.730 --> 01:10:16.800
Oh, yeah, really interesting.

01:10:16.800 --> 01:10:19.112
And then he comes
back four years later

01:10:19.112 --> 01:10:21.570
and he doesn't seem to know
the difference between language

01:10:21.570 --> 01:10:22.070
and speech.

01:10:22.070 --> 01:10:23.750
I'm, like, hello.

01:10:23.750 --> 01:10:27.033
Anyway, he does beautiful
experiments, but it's just--

01:10:27.033 --> 01:10:28.950
it's a blind spot, or
it's a misuse of a word.

01:10:28.950 --> 01:10:30.908
I don't know what it is,
but it drives me nuts.

01:10:30.908 --> 01:10:32.550
Can you tell?

01:10:32.550 --> 01:10:34.800
Anyway, you guys get that
difference even if Eddie

01:10:34.800 --> 01:10:36.347
doesn't.

01:10:36.347 --> 01:10:37.680
Let's look at some other things.

01:10:37.680 --> 01:10:38.970
How about all this
light blue stuff?

01:10:38.970 --> 01:10:41.430
There's a lot of light blue
stuff that's almost as high.

01:10:41.430 --> 01:10:44.130
Oh, that's music
with people singing.

01:10:44.130 --> 01:10:46.190
That also has speech.

01:10:46.190 --> 01:10:47.930
The speech is slightly
less intelligible

01:10:47.930 --> 01:10:49.305
because it's
singing, and there's

01:10:49.305 --> 01:10:52.850
background instrumental music,
so it's a little bit lower.

01:10:52.850 --> 01:10:53.780
Oh, what's next?

01:10:53.780 --> 01:10:55.910
We've got some
light purple stuff

01:10:55.910 --> 01:10:58.310
and some dark purple stuff.

01:10:58.310 --> 01:11:00.560
This is non-speech
vocalizations.

01:11:00.560 --> 01:11:04.790
That's stuff like laughing,
and crying, and sighing--

01:11:04.790 --> 01:11:07.100
pretty similar to
speech but not speech.

01:11:07.100 --> 01:11:09.650
It's the next highest
thing, but it's well down

01:11:09.650 --> 01:11:12.150
from the speech sounds.

01:11:12.150 --> 01:11:14.122
And then we have dogs
barking, and geese,

01:11:14.122 --> 01:11:16.080
and stuff like that, that
are yet further down.

01:11:16.080 --> 01:11:18.630
And then we have all kinds
of other stuff down there--

01:11:18.630 --> 01:11:21.930
sirens, and toilets,
and stuff like that.

01:11:21.930 --> 01:11:22.620
Yeah.

01:11:22.620 --> 01:11:25.680
STUDENT: Is instrumental
music perceived as speech?

01:11:25.680 --> 01:11:27.622
I mean, I can't
make out the colors.

01:11:27.622 --> 01:11:28.455
NANCY KANWISHER: No.

01:11:28.455 --> 01:11:31.637
The instrumental music
is way down in here.

01:11:31.637 --> 01:11:32.970
Yeah, it's a little hard to see.

01:11:32.970 --> 01:11:35.640
That stuff up there is
non-speech vocalizations.

01:11:35.640 --> 01:11:38.250
It's not a perfect slide.

01:11:38.250 --> 01:11:43.080
So that's pretty strong evidence
that that band of cortex

01:11:43.080 --> 01:11:44.760
is pretty selective for speech.

01:11:44.760 --> 01:11:47.650
Everybody get that?

01:11:47.650 --> 01:11:48.150
Yeah.

01:11:48.150 --> 01:11:49.567
STUDENT: So you're
saying it's not

01:11:49.567 --> 01:11:51.540
like it doesn't process
like the other one,

01:11:51.540 --> 01:11:54.720
so the violin stuff would
still be that [INAUDIBLE]

01:11:54.720 --> 01:11:56.700
NANCY KANWISHER: Yeah, right.

01:11:56.700 --> 01:11:57.750
OK, good point.

01:11:57.750 --> 01:12:00.480
Remember when I first showed
you the fusiform face area,

01:12:00.480 --> 01:12:03.270
I showed you that time where
it's faces are like this,

01:12:03.270 --> 01:12:05.910
staring at dot is like
that, looking at objects

01:12:05.910 --> 01:12:07.322
is like this.

01:12:07.322 --> 01:12:09.780
So I said, OK, there's a little
bit of a response to things

01:12:09.780 --> 01:12:10.530
that aren't faces.

01:12:10.530 --> 01:12:12.360
It's just much more to faces.

01:12:12.360 --> 01:12:14.040
Now, you guys may
not have noticed this

01:12:14.040 --> 01:12:16.770
because it went by kind of
fast, but when I showed you

01:12:16.770 --> 01:12:19.740
intracranial data
from the fusiform face

01:12:19.740 --> 01:12:21.780
area in that patient who
got stimulated there,

01:12:21.780 --> 01:12:24.780
and saw the illusory faces,
the intracranial data

01:12:24.780 --> 01:12:28.660
showed zero response to
things that are not faces.

01:12:28.660 --> 01:12:32.580
So I think that that's
because functional MRI is

01:12:32.580 --> 01:12:35.430
the best we have in spatial
resolution in the human brain,

01:12:35.430 --> 01:12:37.800
except when we have
intracranial data.

01:12:37.800 --> 01:12:38.760
But it's still blurry.

01:12:38.760 --> 01:12:43.380
It's blurry because there's
blood flow and all of that.

01:12:43.380 --> 01:12:46.870
So I would guess
the same thing here.

01:12:46.870 --> 01:12:48.930
In fact, I guess it
isn't in the paper you

01:12:48.930 --> 01:12:51.390
read because he didn't
have any non-speech sounds,

01:12:51.390 --> 01:12:53.160
but I will show you.

01:12:53.160 --> 01:12:55.590
Dana's recording them right
now at Children's Hospital,

01:12:55.590 --> 01:12:58.320
and we have some other ones
that I will show you next time,

01:12:58.320 --> 01:13:00.540
of intracranial electrodes.

01:13:00.540 --> 01:13:03.030
And they will be even
more selective than that.

01:13:05.467 --> 01:13:06.800
But this is pretty good already.

01:13:06.800 --> 01:13:07.940
Yeah, Nava.

01:13:07.940 --> 01:13:09.770
STUDENT: What's the
human non-vocal?

01:13:09.770 --> 01:13:10.190
NANCY KANWISHER: I didn't hear.

01:13:10.190 --> 01:13:10.690
What?

01:13:10.690 --> 01:13:12.770
STUDENT: The human non-vocal?

01:13:12.770 --> 01:13:18.150
NANCY KANWISHER: Oh, that's
like clapping, and footsteps,

01:13:18.150 --> 01:13:22.177
and I forget what else, things
where you hear it and you know

01:13:22.177 --> 01:13:24.010
that's a person, but
it doesn't sound at all

01:13:24.010 --> 01:13:26.260
like speaking or speech.

01:13:26.260 --> 01:13:27.700
So if it was about
the meaning, it

01:13:27.700 --> 01:13:29.780
could have been all about
the meaning of people,

01:13:29.780 --> 01:13:31.420
could be something telling
you there's a person there.

01:13:31.420 --> 01:13:32.080
Deal with it.

01:13:32.080 --> 01:13:33.250
But no, apparently not.

01:13:36.720 --> 01:13:38.940
So we're not the first
ones to see this.

01:13:38.940 --> 01:13:40.690
We've just tested it
with more conditions.

01:13:40.690 --> 01:13:43.680
So our evidence for selectivity
is stronger than everyone

01:13:43.680 --> 01:13:45.600
else's.

01:13:45.600 --> 01:13:49.680
Given what I've told you today,
can you think of a stronger way

01:13:49.680 --> 01:13:51.450
to test this?

01:13:51.450 --> 01:13:55.220
For example, suppose
I was worried,

01:13:55.220 --> 01:13:58.500
maybe the frequency
composition of the speech

01:13:58.500 --> 01:14:01.200
is different than
the non-speech.

01:14:01.200 --> 01:14:02.880
After all, those
are just recordings

01:14:02.880 --> 01:14:05.820
of natural sounds in the world
that we went out and made,

01:14:05.820 --> 01:14:11.030
or mostly got off the
web, someone else made.

01:14:11.030 --> 01:14:14.100
And maybe they differ in
really low-level properties.

01:14:14.100 --> 01:14:16.340
And so how do we know
that that's really

01:14:16.340 --> 01:14:19.670
speech selectivity,
not just selectivity

01:14:19.670 --> 01:14:22.320
for certain frequencies
or frequency changes?

01:14:22.320 --> 01:14:22.820
Yes.

01:14:22.820 --> 01:14:25.490
STUDENT: You could run it
with the McDermott generate--

01:14:25.490 --> 01:14:27.740
NANCY KANWISHER:
Bingo, absolutely.

01:14:27.740 --> 01:14:28.610
Everybody get that?

01:14:31.810 --> 01:14:34.630
So then we'd know, because
those are beautifully

01:14:34.630 --> 01:14:38.320
designed to match all
those acoustic properties,

01:14:38.320 --> 01:14:43.820
match the spectrogram for all
those lower level properties.

01:14:43.820 --> 01:14:45.520
And McDermott and
Norman-Haigenere

01:14:45.520 --> 01:14:46.450
have done that.

01:14:46.450 --> 01:14:49.390
And this region does
not respond strongly

01:14:49.390 --> 01:14:51.310
to the model-matched
version, so it's not

01:14:51.310 --> 01:14:52.690
just the acoustic properties.

01:14:52.690 --> 01:14:53.860
Yeah.

01:14:53.860 --> 01:14:56.210
STUDENT: Can we also do
something like [INAUDIBLE] play

01:14:56.210 --> 01:14:57.610
speech backwards?

01:14:57.610 --> 01:15:00.670
NANCY KANWISHER: Yes,
people have done that, too.

01:15:00.670 --> 01:15:03.490
It's a little bit complicated,
because speech backward

01:15:03.490 --> 01:15:06.610
sounds a lot like speech.

01:15:06.610 --> 01:15:08.800
It's kind of in the
intermediate zone.

01:15:08.800 --> 01:15:11.320
So it balances many
things, but one, it

01:15:11.320 --> 01:15:14.020
doesn't balance all the
acoustic properties.

01:15:14.020 --> 01:15:16.480
So speech has certain
onset properties.

01:15:16.480 --> 01:15:18.770
I forget how it goes, but
if you play it backwards,

01:15:18.770 --> 01:15:19.478
there's lots of--

01:15:19.478 --> 01:15:23.050
[MAKING SOUNDS] You've heard
backward speech played, right?

01:15:23.050 --> 01:15:27.040
And so the STRF model
would respond differently

01:15:27.040 --> 01:15:30.730
to forward and backward speech,
whereas the STRF model responds

01:15:30.730 --> 01:15:34.000
the same to the original
and the synthetic speech.

01:15:34.000 --> 01:15:36.370
Make sense?

01:15:36.370 --> 01:15:39.700
So there's a very
speech-selective patch

01:15:39.700 --> 01:15:41.710
of cortex.

01:15:41.710 --> 01:15:46.840
And it's speech selective,
not language selective.

01:15:46.840 --> 01:15:48.310
And of course, we want to know--

01:15:48.310 --> 01:15:51.140
speech is lots of
different things.

01:15:51.140 --> 01:15:53.590
It's what words you're saying.

01:15:53.590 --> 01:15:55.660
It's who's saying it.

01:15:55.660 --> 01:15:57.760
It's your intonation--
are you making

01:15:57.760 --> 01:15:59.440
a statement, or a
question, or what

01:15:59.440 --> 01:16:01.270
are you emphasizing
in the sentence?

01:16:01.270 --> 01:16:03.100
And it's lots of other things.

01:16:03.100 --> 01:16:05.590
And the paper you read
asked that question.

01:16:05.590 --> 01:16:07.270
What's coded here about speech?

01:16:09.800 --> 01:16:11.810
And so I made a
whole bunch of slides

01:16:11.810 --> 01:16:15.005
to explain what the paper
said because I thought people

01:16:15.005 --> 01:16:16.130
would have trouble with it.

01:16:16.130 --> 01:16:17.390
And everyone nailed
it, so I'm not even

01:16:17.390 --> 01:16:18.432
going to go through them.

01:16:18.432 --> 01:16:20.450
Maybe I'll just
show one in closing.

01:16:20.450 --> 01:16:22.730
So one thing a few
of you got wrong--

01:16:22.730 --> 01:16:24.620
and I totally get why,
it didn't matter--

01:16:24.620 --> 01:16:26.940
is that here is
this is one patient,

01:16:26.940 --> 01:16:29.390
and this is the bank
of electrodes placed

01:16:29.390 --> 01:16:31.340
on the surface of the brain.

01:16:31.340 --> 01:16:33.530
The red bits are
the bits where you

01:16:33.530 --> 01:16:35.720
could account for the
neural responses in terms

01:16:35.720 --> 01:16:37.130
of any of those models--

01:16:37.130 --> 01:16:40.340
intonation, speaker
identity, sentence,

01:16:40.340 --> 01:16:42.948
or any of the interactions
between those things.

01:16:42.948 --> 01:16:44.990
And so that just says
that's where the action is,

01:16:44.990 --> 01:16:46.790
is those electrodes there.

01:16:46.790 --> 01:16:51.740
And that graph down here is
from only three different--

01:16:51.740 --> 01:16:54.200
each one is a single electrode,
just so you get this.

01:16:54.200 --> 01:16:58.610
So this critical graph here,
that shows electrode E1.

01:16:58.610 --> 01:17:01.670
That's one of those
electrodes in one patient.

01:17:01.670 --> 01:17:05.750
An electrode is typically
2 millimeters on a side.

01:17:05.750 --> 01:17:10.530
It's probably listening to a few
tens of thousands of neurons.

01:17:10.530 --> 01:17:12.590
So it's one or two
orders of magnitude

01:17:12.590 --> 01:17:14.383
better than a voxel
with functional MRI,

01:17:14.383 --> 01:17:15.800
but it's still
averaging over lots

01:17:15.800 --> 01:17:17.750
of neurons, not a single nerve.

01:17:17.750 --> 01:17:24.210
STUDENT: The
question [INAUDIBLE]

01:17:24.210 --> 01:17:30.120
averaging over [INAUDIBLE] but
it's averaged over [INAUDIBLE]..

01:17:30.120 --> 01:17:33.210
NANCY KANWISHER: Yeah, that was
the response of one electrode

01:17:33.210 --> 01:17:34.860
listening to male and female.

01:17:34.860 --> 01:17:37.440
I forget which is which.

01:17:37.440 --> 01:17:39.540
But other than that, you
guys totally nailed it.

01:17:39.540 --> 01:17:44.010
And notice how precise, and
specific, and fascinatingly

01:17:44.010 --> 01:17:46.530
separated the responses
of those electrodes

01:17:46.530 --> 01:17:52.110
are, segregated for pitch
contour, or speaker identity,

01:17:52.110 --> 01:17:54.390
or what sentence
was being spoken.

01:17:54.390 --> 01:17:56.400
Those things seem to
be segregated spatially

01:17:56.400 --> 01:17:58.010
in the brain at a fine grain.

01:17:58.010 --> 01:17:59.760
Whether you'd see it
with functional MRI--

01:17:59.760 --> 01:18:00.840
you might, might not.

01:18:00.840 --> 01:18:03.552
Many of you pointed out we might
have not have the resolution.

01:18:03.552 --> 01:18:05.010
Think about other
methods you might

01:18:05.010 --> 01:18:08.940
use to look for that, even if
we didn't have the resolution

01:18:08.940 --> 01:18:11.370
with a simple binary contrast.

01:18:11.370 --> 01:18:13.175
And it's 12:26 and
I'm going to stop.

01:18:13.175 --> 01:18:14.550
I will see you
guys on Wednesday,

01:18:14.550 --> 01:18:17.390
and we will talk about music.