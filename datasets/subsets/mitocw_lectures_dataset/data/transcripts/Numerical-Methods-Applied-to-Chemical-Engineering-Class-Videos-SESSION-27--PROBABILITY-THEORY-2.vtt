WEBVTT

00:00:01.540 --> 00:00:03.910
The following content is
provided under a Creative

00:00:03.910 --> 00:00:05.300
Commons license.

00:00:05.300 --> 00:00:07.510
Your support will help
MIT OpenCourseWare

00:00:07.510 --> 00:00:11.600
continue to offer high-quality
educational resources for free.

00:00:11.600 --> 00:00:14.140
To make a donation or to
view additional materials

00:00:14.140 --> 00:00:18.100
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:18.100 --> 00:00:19.310
at ocw.mit.edu.

00:00:22.702 --> 00:00:24.660
WILLIAM GREEN: All right,
so I know some of you

00:00:24.660 --> 00:00:27.030
have succeeded to do the
homework and some of you,

00:00:27.030 --> 00:00:28.380
I think, have not.

00:00:28.380 --> 00:00:29.520
Is this correct?

00:00:29.520 --> 00:00:30.144
AUDIENCE: Yeah.

00:00:30.144 --> 00:00:31.530
WILLIAM GREEN: OK.

00:00:31.530 --> 00:00:33.120
So I was wondering
if someone who

00:00:33.120 --> 00:00:34.536
has succeeded to
do their homework

00:00:34.536 --> 00:00:39.045
might comment on how small a
mesh do you need to converge.

00:00:39.045 --> 00:00:40.830
AUDIENCE: [INAUDIBLE]

00:00:40.830 --> 00:00:42.160
WILLIAM GREEN: It's about l?

00:00:42.160 --> 00:00:42.870
L?

00:00:42.870 --> 00:00:45.203
OK, so you need something on
the order of l to converge.

00:00:45.203 --> 00:00:45.969
Is that correct?

00:00:45.969 --> 00:00:47.760
So if you're trying to
do the problem using

00:00:47.760 --> 00:00:50.040
mesh much bigger than
l, you should probably

00:00:50.040 --> 00:00:52.323
try a tighter mesh.

00:00:52.323 --> 00:00:52.822
Yes?

00:00:52.822 --> 00:00:54.349
AUDIENCE: [INAUDIBLE]

00:00:54.349 --> 00:00:55.390
WILLIAM GREEN: All right.

00:00:55.390 --> 00:00:55.889
Yes?

00:00:55.889 --> 00:00:58.260
AUDIENCE: [INAUDIBLE]

00:00:58.260 --> 00:00:59.220
WILLIAM GREEN: Yes.

00:00:59.220 --> 00:01:01.510
Yes.

00:01:01.510 --> 00:01:02.440
All right.

00:01:02.440 --> 00:01:04.870
And has anyone managed to
get the [INAUDIBLE] solution

00:01:04.870 --> 00:01:08.480
to actually be consistent
with the [INAUDIBLE] solution?

00:01:08.480 --> 00:01:11.080
AUDIENCE: Something
like 3% or 4% or so.

00:01:11.080 --> 00:01:13.750
WILLIAM GREEN: 3% or 4%, OK.

00:01:13.750 --> 00:01:15.820
And I assume that the
[INAUDIBLE] is also

00:01:15.820 --> 00:01:18.790
using a mesh of similar size?

00:01:18.790 --> 00:01:19.599
Hard to tell?

00:01:19.599 --> 00:01:21.390
AUDIENCE: I used like
a triangular system--

00:01:21.390 --> 00:01:23.806
WILLIAM GREEN: Yeah, yeah, but
I mean, it's really, really

00:01:23.806 --> 00:01:24.946
tiny ones at the bottom?

00:01:24.946 --> 00:01:27.112
If you want me to blow it
up, I can just take a look

00:01:27.112 --> 00:01:29.800
and see to be sure.

00:01:29.800 --> 00:01:35.020
All right, and is backslash
able to handle a million

00:01:35.020 --> 00:01:36.395
by million matrix?

00:01:36.395 --> 00:01:40.040
AUDIENCE: Like 10
seconds with [INAUDIBLE]..

00:01:40.040 --> 00:01:41.290
WILLIAM GREEN: [INAUDIBLE] OK.

00:01:41.290 --> 00:01:44.220
So you need to-- so do
the sparse allocation.

00:01:44.220 --> 00:01:46.330
And MATLAB is so
smart that it just

00:01:46.330 --> 00:01:48.350
can handle it with a
million by million,

00:01:48.350 --> 00:01:49.766
which is pretty
amazing, actually.

00:01:49.766 --> 00:01:51.880
That's a pretty big matrix.

00:01:51.880 --> 00:01:55.720
All right, sorry,
this is too loud.

00:01:55.720 --> 00:02:00.020
All right, so last time, we were
doing some elementary things

00:02:00.020 --> 00:02:01.439
about probability.

00:02:01.439 --> 00:02:03.730
Actually, any more questions
about the homework problem

00:02:03.730 --> 00:02:04.290
before we get started?

00:02:04.290 --> 00:02:05.440
AUDIENCE: What's the answer?

00:02:05.440 --> 00:02:06.814
WILLIAM GREEN:
What's the answer?

00:02:06.814 --> 00:02:08.170
You could ask your classmates.

00:02:08.170 --> 00:02:09.963
Any other questions?

00:02:12.841 --> 00:02:13.340
All right.

00:02:17.420 --> 00:02:26.570
So I had you confused a
little bit with this formula

00:02:26.570 --> 00:02:31.047
probability of either
A or B. So I asked what

00:02:31.047 --> 00:02:31.880
the probability of--

00:02:31.880 --> 00:02:34.970
I flipped two coins-- that
one of them would be a head.

00:02:34.970 --> 00:02:36.890
And I could see a
lot of consternation.

00:02:36.890 --> 00:02:39.285
The general formula
for this is it's

00:02:39.285 --> 00:02:43.780
the probability of A
plus the probability of B

00:02:43.780 --> 00:02:51.822
minus the probability
of A and B.

00:02:51.822 --> 00:02:53.780
It can't just be the two
of them added together

00:02:53.780 --> 00:02:58.520
because if you have 50%
chance for head of the penny

00:02:58.520 --> 00:03:01.340
and the dime is 50% chance,
this would add up to 100% chance

00:03:01.340 --> 00:03:04.170
that you'll get a head, but
you know sometimes it's true.

00:03:04.170 --> 00:03:06.000
So this is the formula.

00:03:06.000 --> 00:03:11.810
And then the
probability of A and B

00:03:11.810 --> 00:03:15.920
is often written in terms of
the conditional probabilities,

00:03:15.920 --> 00:03:19.400
the probability of A times the
probability that B would happen

00:03:19.400 --> 00:03:22.171
given that A already
happened, which is also

00:03:22.171 --> 00:03:23.420
equal to the other way around.

00:03:30.020 --> 00:03:31.520
And this has to
be read carefully.

00:03:31.520 --> 00:03:33.845
It means B already
happened, and then you

00:03:33.845 --> 00:03:35.220
want to know the
probability of A

00:03:35.220 --> 00:03:37.000
given that B already happened.

00:03:37.000 --> 00:03:38.954
So it's sort of like this way--

00:03:38.954 --> 00:03:40.620
I don't know-- the
way I think about it.

00:03:40.620 --> 00:03:41.940
Like, this happened
first. and now I'm

00:03:41.940 --> 00:03:44.148
checking the probability
that that's going to happen.

00:03:47.730 --> 00:03:50.165
Now, a nice little
example of this

00:03:50.165 --> 00:03:51.540
is given in
[INAUDIBLE] textbook.

00:03:51.540 --> 00:03:53.460
And I think it's nice enough
that it's worthwhile to spend

00:03:53.460 --> 00:03:55.050
a few minutes talking about it.

00:03:55.050 --> 00:03:56.160
So he was--

00:03:56.160 --> 00:03:57.630
[INAUDIBLE] who
wrote the textbook,

00:03:57.630 --> 00:03:59.005
was not actually
a numerical guy.

00:03:59.005 --> 00:04:00.660
He was a polymer chemist.

00:04:00.660 --> 00:04:03.870
And so he gave a
nice polymer example.

00:04:03.870 --> 00:04:07.380
So if you have a
polymer and the monomers

00:04:07.380 --> 00:04:11.790
have some big molecule,
and at one side,

00:04:11.790 --> 00:04:14.816
they have a sort
of acceptor group,

00:04:14.816 --> 00:04:17.440
and the other side, some kind of
donor group-- we'll call it D,

00:04:17.440 --> 00:04:19.935
I guess.

00:04:19.935 --> 00:04:21.060
And these are the monomers.

00:04:21.060 --> 00:04:22.310
And so they can link together.

00:04:22.310 --> 00:04:23.940
The donor can react
to the acceptor.

00:04:23.940 --> 00:04:34.182
So you can end up with
things like this and so on.

00:04:34.182 --> 00:04:35.140
So this is the monomer.

00:04:35.140 --> 00:04:36.186
This is the dimer.

00:04:36.186 --> 00:04:38.060
Then you could keep on
[INAUDIBLE] like this.

00:04:38.060 --> 00:04:40.370
And many, many, many
of the materials

00:04:40.370 --> 00:04:42.920
you use every day, the
fabrics in the seats

00:04:42.920 --> 00:04:45.506
that you're sitting
on, the backs

00:04:45.506 --> 00:04:51.380
of the seats, your
clothing, the binder

00:04:51.380 --> 00:04:53.300
holding the chalk
together, all this stuff

00:04:53.300 --> 00:04:55.670
is made from polymers like this.

00:04:55.670 --> 00:04:58.220
So this is a pretty important,
actually, practical problem.

00:04:58.220 --> 00:05:03.620
And so you start
with the monomers,

00:05:03.620 --> 00:05:08.840
and they react where you have A
reacting plus D, over and over

00:05:08.840 --> 00:05:10.380
again.

00:05:10.380 --> 00:05:17.540
And we want to understand
the statistics of what

00:05:17.540 --> 00:05:21.802
chain lengths are going to
make, maybe what weight percent

00:05:21.802 --> 00:05:23.510
or what would the
average microweight be,

00:05:23.510 --> 00:05:26.093
something like that would be the
kind of things we care about.

00:05:29.030 --> 00:05:34.180
So a way to think about
it is if I've reacted this

00:05:34.180 --> 00:05:39.880
to some extent and I just
grab a random polymer

00:05:39.880 --> 00:05:44.850
chain, any molecule in there,
and I look and find, let's

00:05:44.850 --> 00:05:49.840
say, the unreacted D end--

00:05:49.840 --> 00:05:53.380
so any oligomer is going to
have one unreacted D end.

00:05:53.380 --> 00:05:55.100
You can see no matter
how long I make it,

00:05:55.100 --> 00:05:56.815
there will still be
one unreacted D end.

00:05:56.815 --> 00:05:58.690
And I'm neglecting the
possibility this might

00:05:58.690 --> 00:06:00.880
circle around and make a loop.

00:06:00.880 --> 00:06:04.180
So assuming no loops,
then any molecule I grab

00:06:04.180 --> 00:06:06.490
is going to have
one unreacted D end.

00:06:06.490 --> 00:06:08.380
So I grab a molecule.

00:06:08.380 --> 00:06:10.780
I start at the unreacted
D end, and I look

00:06:10.780 --> 00:06:12.350
at the A that's next to it.

00:06:12.350 --> 00:06:14.570
And I say, is that
A reacted or not?

00:06:14.570 --> 00:06:17.650
So if it's a monomer, I grab
the D. I look over here.

00:06:17.650 --> 00:06:18.880
The A is unreacted.

00:06:18.880 --> 00:06:25.760
So the probability
that it's a monomer

00:06:25.760 --> 00:06:27.890
is going to be equal
sort of like 1 minus P

00:06:27.890 --> 00:06:33.200
where P is the
probability that As react.

00:06:33.200 --> 00:06:36.380
So it didn't react,
so just like that.

00:06:36.380 --> 00:06:40.250
This one, the one next
to it has reacted.

00:06:40.250 --> 00:06:47.910
So this is just going to be
the probability of a dimer is

00:06:47.910 --> 00:07:00.540
the probability that my
nearest neighbor reacted

00:07:00.540 --> 00:07:14.286
and next neighbor
is unreacted, right?

00:07:14.286 --> 00:07:17.600
Is that OK?

00:07:17.600 --> 00:07:19.262
So I can write this way.

00:07:19.262 --> 00:07:20.720
I could say, what's
the probability

00:07:20.720 --> 00:07:34.510
that my nearest reacted times
a conditional probability, next

00:07:34.510 --> 00:07:42.480
unreacted if nearest is reacted?

00:07:46.770 --> 00:07:49.760
So far, so good?

00:07:49.760 --> 00:07:50.940
You guys are OK with this?

00:07:50.940 --> 00:07:52.400
So I grabbed a chain.

00:07:52.400 --> 00:07:54.370
I'm trying to see
if it's a dimer.

00:07:54.370 --> 00:07:56.100
I'm going to calculate
the probability

00:07:56.100 --> 00:07:59.580
that this next
acceptor group has

00:07:59.580 --> 00:08:02.015
been reacted to a donor group.

00:08:02.015 --> 00:08:03.390
If it has reacted,
then I'm going

00:08:03.390 --> 00:08:05.277
to check the next
one after that.

00:08:05.277 --> 00:08:06.610
So this is the nearest neighbor.

00:08:06.610 --> 00:08:07.970
This is the next
nearest neighbor.

00:08:07.970 --> 00:08:09.303
And I want this to be unreacted.

00:08:09.303 --> 00:08:12.320
If that's both true,
then I have a dimer.

00:08:12.320 --> 00:08:14.890
If either one of those
is false, [INAUDIBLE]..

00:08:14.890 --> 00:08:17.270
Is that OK?

00:08:17.270 --> 00:08:19.515
So now I need to
have a probability.

00:08:19.515 --> 00:08:22.250
So what's the probability that
the nearest one is reacted?

00:08:22.250 --> 00:08:24.560
There's some probability
that things have reacted.

00:08:24.560 --> 00:08:29.860
So this is going to be my
P, probability that things

00:08:29.860 --> 00:08:31.050
reacted.

00:08:31.050 --> 00:08:33.419
And I wanted this
to be unreacted.

00:08:33.419 --> 00:08:34.470
Now, there's a question.

00:08:34.470 --> 00:08:37.179
Are these correlated or not?

00:08:37.179 --> 00:08:39.976
Now, in reality, everything's
correlated to everything.

00:08:39.976 --> 00:08:41.309
So probably, they're correlated.

00:08:41.309 --> 00:08:44.850
But if we're trying to make
a model and think about it,

00:08:44.850 --> 00:08:47.100
the fact that this thing
reacted at this side

00:08:47.100 --> 00:08:48.750
doesn't really affect
this side if this

00:08:48.750 --> 00:08:52.500
is a big enough [INAUDIBLE]
So to a good approximation,

00:08:52.500 --> 00:08:55.140
this is independent of whether
or not it's reacted or not.

00:08:55.140 --> 00:08:58.530
So this is still going to
have the ordinary probability

00:08:58.530 --> 00:09:05.160
of being unreacted,
which would be 1 minus P.

00:09:05.160 --> 00:09:18.310
So I could write down that the
probability of being a monomer

00:09:18.310 --> 00:09:24.930
is equal to 1 minus P. The
probability of being a dimer

00:09:24.930 --> 00:09:28.940
is equal to P times 1 minus P.
What's the probability of being

00:09:28.940 --> 00:09:29.440
a trimer?

00:09:36.540 --> 00:09:44.630
P squared times 1 minus
P. And in general,

00:09:44.630 --> 00:09:50.370
the probability
of being an n-mer

00:09:50.370 --> 00:09:57.320
is equal to P n minus
1 times 1 minus P.

00:09:57.320 --> 00:10:01.880
So now you guys are
statistical polymer chemists.

00:10:01.880 --> 00:10:04.790
So this derivation was
derived by a guy named Flory.

00:10:04.790 --> 00:10:05.950
He got the Nobel Prize.

00:10:05.950 --> 00:10:07.329
He's a pretty important guy.

00:10:07.329 --> 00:10:08.870
If you want to learn
a lot about him,

00:10:08.870 --> 00:10:10.619
I think both Professor
Cohen and Professor

00:10:10.619 --> 00:10:12.920
Rutledge teach classes
that are basically, learn

00:10:12.920 --> 00:10:15.280
what Mr. Flory figured out.

00:10:15.280 --> 00:10:18.899
Well, maybe that's a little bit
too strong, but pretty much.

00:10:18.899 --> 00:10:20.440
There's another guy
named [INAUDIBLE]

00:10:20.440 --> 00:10:22.970
that did a bit too, so
[INAUDIBLE] and Flory.

00:10:22.970 --> 00:10:24.995
Basically everything
about polymers

00:10:24.995 --> 00:10:26.450
worked out by at these guys.

00:10:26.450 --> 00:10:27.640
And all they did was
just probability theory,

00:10:27.640 --> 00:10:28.723
so it was a piece of cake.

00:10:31.820 --> 00:10:36.480
And so this is the probability
that you have an n-mer.

00:10:36.480 --> 00:10:38.210
So now we can
compute things like,

00:10:38.210 --> 00:10:41.720
what is the expectation
value of the chain length?

00:10:41.720 --> 00:10:44.930
How many guys link together?

00:10:44.930 --> 00:10:50.870
And that's defined
to be the sum of n

00:10:50.870 --> 00:10:52.611
times the probability of n.

00:11:00.310 --> 00:11:09.435
So that, in this case, is
going to be sum of n times P

00:11:09.435 --> 00:11:14.970
to the n minus 1
times 1 minus P.

00:11:14.970 --> 00:11:19.880
Now, a lot of these kinds
of simple series summations,

00:11:19.880 --> 00:11:20.880
there's formulas for it.

00:11:20.880 --> 00:11:22.570
And maybe in high school, you
guys might have studied series.

00:11:22.570 --> 00:11:24.012
I don't know if you remember.

00:11:24.012 --> 00:11:24.970
And so you can look up.

00:11:24.970 --> 00:11:26.430
And some of these have
analytical formulas

00:11:26.430 --> 00:11:27.000
that are really simple.

00:11:27.000 --> 00:11:28.320
But you can just
leave it this way too,

00:11:28.320 --> 00:11:29.390
because you get a
value numerically

00:11:29.390 --> 00:11:30.390
with MATLAB, no trouble.

00:11:38.090 --> 00:11:44.990
You can also figure out what is
the concentration of oligomers

00:11:44.990 --> 00:11:48.400
with n units in them.

00:11:48.400 --> 00:12:00.440
And so that's going to be equal
to the total concentration

00:12:00.440 --> 00:12:08.010
of polymers times the
probability that it has n.

00:12:11.460 --> 00:12:13.200
So this one, we just worked out.

00:12:13.200 --> 00:12:18.540
The total concentration,
a way to figure

00:12:18.540 --> 00:12:24.680
that out is to
think about there's

00:12:24.680 --> 00:12:27.882
one monomer or one monomer--

00:12:27.882 --> 00:12:29.090
I'll call this a polymer too.

00:12:29.090 --> 00:12:30.860
This is a polymer with one unit.

00:12:30.860 --> 00:12:36.320
There's one polymer molecule per
unreacted end, unreacted D end.

00:12:36.320 --> 00:12:39.110
So it's really, I want to
know how many are unreacted.

00:12:39.110 --> 00:12:47.720
So that's going to be 1 minus
P times the amount of monomer

00:12:47.720 --> 00:12:50.130
I had to start with.

00:12:50.130 --> 00:12:53.870
It could be A or D.
It doesn't matter.

00:12:53.870 --> 00:12:55.950
It's like, how many of them--

00:12:55.950 --> 00:12:59.190
I started with a certain
amount of free ends.

00:12:59.190 --> 00:13:04.355
What fraction of them
have reacted based on 1

00:13:04.355 --> 00:13:08.330
minus P. Yeah, it's 1 minus P.

00:13:08.330 --> 00:13:09.680
So as P goes--

00:13:09.680 --> 00:13:11.040
well, yeah, it goes backwards.

00:13:11.040 --> 00:13:15.340
Yeah, as P goes to infinity--

00:13:15.340 --> 00:13:16.520
I think that's right.

00:13:19.060 --> 00:13:20.740
Yeah, when P is--

00:13:20.740 --> 00:13:23.510
well, I'm totally
confused here now.

00:13:23.510 --> 00:13:26.350
1 minus P sound right?

00:13:26.350 --> 00:13:27.850
Maybe I did the
reasoning backwards.

00:13:27.850 --> 00:13:29.391
This is definitely
the right formula.

00:13:29.391 --> 00:13:31.200
I'm just confusing
myself with my language.

00:13:31.200 --> 00:13:34.350
This is a, at least
for me, endemic problem

00:13:34.350 --> 00:13:37.505
with probability is you
could say things very glibly.

00:13:37.505 --> 00:13:39.380
You've got to think of
exactly what you mean.

00:13:39.380 --> 00:13:59.470
So the concentration
of unreacted ends,

00:13:59.470 --> 00:14:08.180
so initially, this was equal to
A. It was all unreacted ends.

00:14:08.180 --> 00:14:14.010
And as the process proceeds, as
P increases, then at the end,

00:14:14.010 --> 00:14:15.380
it's going to be very small.

00:14:15.380 --> 00:14:16.210
So this is right.

00:14:20.565 --> 00:14:22.190
And the concentration
of unreacted ends

00:14:22.190 --> 00:14:23.648
is equal to the
total concentration

00:14:23.648 --> 00:14:26.410
of polymers, the number
of polymers [INAUDIBLE]..

00:14:26.410 --> 00:14:31.332
So it's this times P n
minus 1 times [INAUDIBLE]..

00:14:36.120 --> 00:14:41.070
All right, and this is called
the Flory redistribution.

00:14:41.070 --> 00:14:43.680
And that gives the
concentrations of all

00:14:43.680 --> 00:14:46.410
your oligomers after
you do a polymerization

00:14:46.410 --> 00:14:49.540
if they're all uncorrelated
and you don't form any loops.

00:14:55.120 --> 00:14:57.410
It's often very
important to know

00:14:57.410 --> 00:14:58.975
the width of the distribution.

00:14:58.975 --> 00:15:01.100
If you make a polymer, you
want to make things have

00:15:01.100 --> 00:15:03.160
as monodisperse as possible.

00:15:03.160 --> 00:15:05.806
It's because you'd really like
to make this pure chemical.

00:15:05.806 --> 00:15:07.430
There's some polymer
chain length which

00:15:07.430 --> 00:15:09.410
is optimal for your purpose.

00:15:09.410 --> 00:15:12.036
You want to try to make
sure that the average value,

00:15:12.036 --> 00:15:14.660
average value, this is going to
be equal to the value you want.

00:15:14.660 --> 00:15:18.230
So you want to keep running P up
until you reach the point where

00:15:18.230 --> 00:15:21.470
the average chain length
is the chain length that's

00:15:21.470 --> 00:15:23.870
optimal for your application.

00:15:23.870 --> 00:15:25.910
If you make the
polymer too long,

00:15:25.910 --> 00:15:28.780
then it's going to be
hard to dissolve it.

00:15:28.780 --> 00:15:31.610
It's going to be hard to
handle and it's can be solid.

00:15:31.610 --> 00:15:33.390
If you make it
too short, then it

00:15:33.390 --> 00:15:35.140
may not have the
mechanical properties you

00:15:35.140 --> 00:15:36.470
need for the polymer to have.

00:15:36.470 --> 00:15:38.270
So there's some optimal choice.

00:15:38.270 --> 00:15:41.090
So you typically
run the conversion

00:15:41.090 --> 00:15:44.436
until P reaches a number so
that this is your optimal value,

00:15:44.436 --> 00:15:46.310
but then you care about
what's the dispersion

00:15:46.310 --> 00:15:48.380
about that optimal value.

00:15:48.380 --> 00:15:50.960
And particularly, the unreacted
monomers that are left

00:15:50.960 --> 00:15:53.300
might be a problem because
they might leach out

00:15:53.300 --> 00:15:55.280
over time because they
might still be liquids,

00:15:55.280 --> 00:15:56.740
or even gases that come out.

00:15:56.740 --> 00:15:59.930
So this famous problem,
people made baby bottles

00:15:59.930 --> 00:16:03.320
and they have some
leftover small molecules

00:16:03.320 --> 00:16:04.491
in the baby bottles.

00:16:04.491 --> 00:16:06.240
And then they can leach
out into the milk,

00:16:06.240 --> 00:16:08.210
and the mothers don't
appreciate that.

00:16:08.210 --> 00:16:10.940
So there's a lot of
real practical problems

00:16:10.940 --> 00:16:11.949
about how to do this.

00:16:11.949 --> 00:16:13.740
So anyway, you'd be
interested in the width

00:16:13.740 --> 00:16:15.550
of the distribution.

00:16:15.550 --> 00:16:18.775
So we define what's
called the variance.

00:16:23.810 --> 00:16:30.436
And the variance of n
is written this way.

00:16:30.436 --> 00:16:34.070
And it's just defined
to be the expectation

00:16:34.070 --> 00:16:38.180
value of n squared minus the
expectation value of n squared.

00:16:38.180 --> 00:16:41.150
These two are always different
or almost always different,

00:16:41.150 --> 00:16:42.130
so it's not 0.

00:16:45.490 --> 00:16:47.830
So this is equal
to the summation

00:16:47.830 --> 00:16:55.320
of n squared times the
probability of n minus--

00:17:01.690 --> 00:17:02.780
all right?

00:17:02.780 --> 00:17:05.295
And a lot of times
in the polymer field,

00:17:05.295 --> 00:17:07.670
what they'll take is they'll
take the square root of this

00:17:07.670 --> 00:17:09.530
and they'll compare
sigma n divided

00:17:09.530 --> 00:17:11.119
by expectation value of n.

00:17:11.119 --> 00:17:13.579
This is a dimensionless
number because sigma n

00:17:13.579 --> 00:17:14.940
will have the dimensions.

00:17:14.940 --> 00:17:17.359
Sigma squared is
dimensions n squared.

00:17:17.359 --> 00:17:20.420
This is dimensions of n, so
it's a dimensionless number.

00:17:20.420 --> 00:17:23.730
And that's-- I think they
call it dispersity of polymer,

00:17:23.730 --> 00:17:24.720
something like that.

00:17:29.180 --> 00:17:33.910
Now, notice that when we
use these [INAUDIBLE],,

00:17:33.910 --> 00:17:35.770
when we wrote it this
way, it's implicitly

00:17:35.770 --> 00:17:38.920
that these things are
divided by the summation

00:17:38.920 --> 00:17:41.230
of the probability of n.

00:17:41.230 --> 00:17:43.210
But because these
probabilities sum to 1,

00:17:43.210 --> 00:17:44.210
I can just leave it out.

00:17:47.320 --> 00:17:49.440
But sometimes, it
may be difficult

00:17:49.440 --> 00:17:51.950
for you to figure out exactly
what the probabilities are

00:17:51.950 --> 00:17:53.730
and you'll need a scaling
factor to force this thing

00:17:53.730 --> 00:17:54.480
to be equal to 1.

00:17:54.480 --> 00:17:57.020
So sometimes, people leave
these in the denominator.

00:17:57.020 --> 00:17:58.811
There's another thing
you might care about,

00:17:58.811 --> 00:18:05.400
which would be like, what's
the weight percent of Pn?

00:18:05.400 --> 00:18:07.920
So what fraction of the
weight of the polymer

00:18:07.920 --> 00:18:10.470
is my particular oligomer, Pn?

00:18:10.470 --> 00:18:13.717
[INAUDIBLE] sorry,
some special one, Pm.

00:18:13.717 --> 00:18:15.300
And I want to know
its weight percent.

00:18:15.300 --> 00:18:28.650
So that's going to be
equal to the weight of Pm

00:18:28.650 --> 00:18:33.060
in the mix divided
by the total weight.

00:18:39.070 --> 00:18:44.790
So that's equal to
the weight of m times

00:18:44.790 --> 00:18:49.770
the probability of m divided
by the total weight, which

00:18:49.770 --> 00:18:51.812
is going to be the
weight of all these guys,

00:18:51.812 --> 00:18:54.940
times the probability
of each of them.

00:18:54.940 --> 00:18:56.570
And you can see
this is different.

00:18:56.570 --> 00:18:59.140
This is not the same as--

00:18:59.140 --> 00:19:09.880
not equal to, right?

00:19:09.880 --> 00:19:11.090
It's not the same thing.

00:19:11.090 --> 00:19:15.812
So just watch out
when you do this.

00:19:15.812 --> 00:19:18.270
And in fact, in the polymer
world, they always have to say,

00:19:18.270 --> 00:19:19.150
I did weight average.

00:19:19.150 --> 00:19:23.210
I did number average,
because they're different.

00:19:23.210 --> 00:19:25.436
Is this OK?

00:19:25.436 --> 00:19:27.650
Yeah?

00:19:27.650 --> 00:19:30.029
So I would-- my general
confidence, at least for me,

00:19:30.029 --> 00:19:32.570
if I skip steps, I always get
it wrong when I do probability.

00:19:32.570 --> 00:19:33.745
So don't skip steps.

00:19:33.745 --> 00:19:36.890
Do it one by one by one,
what you really mean.

00:19:36.890 --> 00:19:37.640
Then you'll be OK.

00:19:40.370 --> 00:19:45.270
All right, now, this is
a cute little example.

00:19:45.270 --> 00:19:46.270
It's discrete variables.

00:19:46.270 --> 00:19:48.995
It's easy to count everything.

00:19:48.995 --> 00:19:51.120
Very often, we care about
probability distributions

00:19:51.120 --> 00:19:52.272
of continuous variables.

00:19:52.272 --> 00:19:54.480
And we have to do those
probability density functions

00:19:54.480 --> 00:19:58.430
that I talked about last time
which have units in them.

00:19:58.430 --> 00:20:03.090
And so as we
mentioned last time,

00:20:03.090 --> 00:20:04.770
if you want to know
the probability

00:20:04.770 --> 00:20:18.740
that a continuous variable, that
x is a member of this interval,

00:20:18.740 --> 00:20:30.340
the probability this is true is
equal to Px of x hat times dx.

00:20:33.190 --> 00:20:35.230
And so this quantity
has units of 1

00:20:35.230 --> 00:20:39.284
over x, whatever
the units of x are.

00:20:39.284 --> 00:20:41.200
And then you have to
multiply it by x in order

00:20:41.200 --> 00:20:42.560
to get the units to be
dimensionless, which

00:20:42.560 --> 00:20:43.685
is what the probability is.

00:20:47.850 --> 00:20:54.050
And this is like obvious things,
like P Px of x prime dx prime.

00:20:54.050 --> 00:20:56.550
[INAUDIBLE] value as possible
of x has got to be equal to 1.

00:20:56.550 --> 00:20:59.810
It's a probability,
which is the same

00:20:59.810 --> 00:21:03.380
as saying that probability of
x is some value anywhere is 1.

00:21:03.380 --> 00:21:05.651
So there's some
[INAUDIBLE] you measure.

00:21:08.480 --> 00:21:13.490
And you can also have a
probability that x is less than

00:21:13.490 --> 00:21:15.470
or equal to x prime.

00:21:15.470 --> 00:21:19.045
And that's the integral
from negative infinity

00:21:19.045 --> 00:21:23.520
to x prime of Px of x dx.

00:21:29.760 --> 00:21:35.716
And the mean is just
the integral of x Px.

00:21:42.660 --> 00:21:44.320
And you can compute
the x squared.

00:21:44.320 --> 00:21:47.070
The average of x and
x squared, same thing.

00:21:47.070 --> 00:21:48.440
You average of anything.

00:21:55.290 --> 00:21:57.250
You can put these together.

00:21:57.250 --> 00:22:02.280
You can get sigma x squared
is equal to x squared

00:22:02.280 --> 00:22:06.640
minus the average squared.

00:22:06.640 --> 00:22:09.690
So that's the variance of x.

00:22:09.690 --> 00:22:12.600
You can also do this
with any function.

00:22:12.600 --> 00:22:18.510
So you can say that the
average value of a function

00:22:18.510 --> 00:22:24.112
is equal to the integral
of f of x Px of x dx.

00:22:27.030 --> 00:22:29.370
This is an average
value of a function

00:22:29.370 --> 00:22:31.770
of a random variable described
by probability density

00:22:31.770 --> 00:22:34.350
function with P of x.

00:22:34.350 --> 00:22:37.046
And then you can get
things like sigma

00:22:37.046 --> 00:22:43.890
f squared is equal to the
integral of f of x, quantity

00:22:43.890 --> 00:22:49.555
squared, Px of x minus--

00:22:55.870 --> 00:22:59.274
all right?

00:22:59.274 --> 00:22:59.940
Everything's OK?

00:22:59.940 --> 00:23:02.101
Yeah.

00:23:02.101 --> 00:23:04.350
All right, so a lot of times,
people are going to say,

00:23:04.350 --> 00:23:06.900
we do sampling from Px.

00:23:06.900 --> 00:23:09.720
So sampling from Px means
that we have some probability

00:23:09.720 --> 00:23:13.200
distribution function,
Px of x, and we

00:23:13.200 --> 00:23:17.400
want to have one value of x that
we draw from that probability

00:23:17.400 --> 00:23:18.660
distribution.

00:23:18.660 --> 00:23:20.760
When we say it that
way, we mean that we're

00:23:20.760 --> 00:23:25.370
more likely to find x's
where Px has a high value

00:23:25.370 --> 00:23:28.430
and we're less likely to
draw an x value that Px

00:23:28.430 --> 00:23:30.459
has a low value.

00:23:30.459 --> 00:23:31.750
So that's what's sampling from.

00:23:31.750 --> 00:23:33.770
Now, you can do
that mathematically

00:23:33.770 --> 00:23:36.800
using random number generators
in MATLAB, for example,

00:23:36.800 --> 00:23:39.290
and we'll do that sometimes.

00:23:39.290 --> 00:23:41.580
But you do it all the time
when you do experiments.

00:23:41.580 --> 00:23:43.911
So the experiment has some
probability density function

00:23:43.911 --> 00:23:45.410
that you're going
observe something,

00:23:45.410 --> 00:23:47.450
you're going to
measure something.

00:23:47.450 --> 00:23:49.670
And you don't know what
that distribution is,

00:23:49.670 --> 00:23:51.470
but every time you
make a measurement,

00:23:51.470 --> 00:23:53.650
you're sampling from
that distribution.

00:23:53.650 --> 00:23:55.700
So that's the key
conceptual idea

00:23:55.700 --> 00:23:59.180
is that there is a Px of x
out there for our measurement.

00:23:59.180 --> 00:24:02.165
So you're trying to
measure how tall I am.

00:24:02.165 --> 00:24:03.540
Every time you
measure it, you're

00:24:03.540 --> 00:24:08.070
drawing from a distribution
of experimental measurements

00:24:08.070 --> 00:24:11.430
of Professor Green's height.

00:24:11.430 --> 00:24:14.340
And there is some Px of x
that exists even though you

00:24:14.340 --> 00:24:15.776
don't know what it is.

00:24:15.776 --> 00:24:17.400
And each time you
make the measurement,

00:24:17.400 --> 00:24:19.920
you're drawing numbers
from that distribution.

00:24:19.920 --> 00:24:24.990
And if you draw a lot of them,
then you can do an average.

00:24:24.990 --> 00:24:27.930
And it should be an average
that's close to this.

00:24:27.930 --> 00:24:30.360
If you drew an infinite
number of values,

00:24:30.360 --> 00:24:31.680
then you're sampling this.

00:24:31.680 --> 00:24:35.820
You can make a histogram plot of
the heights you measure of me,

00:24:35.820 --> 00:24:39.210
and it should have some shape
that's similar to Px of x.

00:24:39.210 --> 00:24:40.980
Does that makes sense?

00:24:40.980 --> 00:24:41.610
All right.

00:24:41.610 --> 00:24:43.110
So actually, everyday
you're drawing

00:24:43.110 --> 00:24:44.401
from probability distributions.

00:24:44.401 --> 00:24:45.780
You just didn't know it.

00:24:45.780 --> 00:24:46.530
It's like [INAUDIBLE] street.

00:24:46.530 --> 00:24:47.985
The probability the bus
is going to hit me or not

00:24:47.985 --> 00:24:49.890
and the bus driver
is going to stop,

00:24:49.890 --> 00:24:50.730
I think there's a
high probability,

00:24:50.730 --> 00:24:52.479
but I'm always a little
worried, actually.

00:24:52.479 --> 00:24:53.290
Good.

00:24:53.290 --> 00:24:55.890
I'm drawing from-- it's
a particular instance

00:24:55.890 --> 00:24:59.360
of that probability distribution
about whether the bus driver's

00:24:59.360 --> 00:25:01.740
really going to stop or not.

00:25:01.740 --> 00:25:04.640
And if I sample enough
times, I might be dead.

00:25:04.640 --> 00:25:07.680
But anyway, all right.

00:25:15.270 --> 00:25:17.770
Often we have
multiple variables.

00:25:17.770 --> 00:25:19.670
So you can write down--

00:25:33.740 --> 00:25:36.430
you can define Px hat.

00:25:50.300 --> 00:25:51.740
So now I have multiple x's.

00:25:51.740 --> 00:25:53.890
It's like more than
one variable of x.

00:25:53.890 --> 00:25:56.139
And I wanted the probability
density function of them.

00:25:56.139 --> 00:25:58.430
I'm going to measure this
and this and this and this,

00:25:58.430 --> 00:25:59.810
all right?

00:25:59.810 --> 00:26:04.700
And this is equal
to the probability

00:26:04.700 --> 00:26:16.760
that x1 is a member of
the set, x1, x1 plus dx1,

00:26:16.760 --> 00:26:27.136
and x2 is a member of x2,
x2 plus dx2, and that.

00:26:29.584 --> 00:26:31.250
That's what probability
density function

00:26:31.250 --> 00:26:33.230
means with multiple variables.

00:26:37.560 --> 00:26:39.800
So this is very common
for us because we often

00:26:39.800 --> 00:26:42.950
measure and experiment
more than one thing, right?

00:26:42.950 --> 00:26:45.860
So you measure the flow
rate and the temperature.

00:26:45.860 --> 00:26:49.250
You measure the yield
and the absorption

00:26:49.250 --> 00:26:52.835
at some wavelength that
corresponds to an impurity.

00:26:52.835 --> 00:26:55.460
Usually when you experiment, you
often measure multiple things.

00:26:55.460 --> 00:26:58.580
And so you're sampling
from multiple observable

00:26:58.580 --> 00:26:59.780
simultaneously.

00:26:59.780 --> 00:27:02.836
And implicitly, you're sampling
from some complicated PDF

00:27:02.836 --> 00:27:05.210
like this even though you
don't know the shape of the PDF

00:27:05.210 --> 00:27:06.126
usually to start with.

00:27:10.180 --> 00:27:14.220
And so then when you have
this multiple variable case,

00:27:14.220 --> 00:27:21.900
you can define a thing
called the covariance matrix

00:27:21.900 --> 00:27:25.130
where the elements
of the matrix Cij

00:27:25.130 --> 00:27:37.060
are equal to xi xj, the mean
of that product, minus xi xj.

00:27:37.060 --> 00:27:39.820
And so you can see that,
for example, sigma i squared

00:27:39.820 --> 00:27:43.030
is equal to Cii, but
the diagonal elements

00:27:43.030 --> 00:27:44.545
are just the variances.

00:27:44.545 --> 00:27:47.170
But now we have the covariances
because we measured, let's say,

00:27:47.170 --> 00:27:48.146
two things.

00:27:53.750 --> 00:27:59.220
All right, so suppose
we do n measurements

00:27:59.220 --> 00:28:02.040
and we compute the
average of our repeats.

00:28:02.040 --> 00:28:06.140
So we'd just repeat the same
measurements over and over.

00:28:06.140 --> 00:28:09.287
So suppose you measure
my height and my weight.

00:28:09.287 --> 00:28:10.870
Every time I go to
the medical clinic,

00:28:10.870 --> 00:28:13.370
they always measure my height,
my weight, my blood pressure.

00:28:13.370 --> 00:28:14.540
You've got three numbers.

00:28:14.540 --> 00:28:17.210
And I could go back
in there 47 times,

00:28:17.210 --> 00:28:19.010
and they'll do it 47 times.

00:28:19.010 --> 00:28:21.050
And if a different
technician measured

00:28:21.050 --> 00:28:24.199
it using different [INAUDIBLE]
and a different scale,

00:28:24.199 --> 00:28:25.490
I might get a different number.

00:28:25.490 --> 00:28:27.156
Sometimes, I forget
to take my shoes off

00:28:27.156 --> 00:28:29.281
so I'm a little bit taller
than I would have been.

00:28:29.281 --> 00:28:30.530
So the numbers go up and down.

00:28:30.530 --> 00:28:31.447
They fluctuate, right?

00:28:31.447 --> 00:28:32.488
You'd expect that, right?

00:28:32.488 --> 00:28:33.950
If you looked at
my medical chart,

00:28:33.950 --> 00:28:35.450
it's not the same
number every time.

00:28:39.910 --> 00:28:43.970
But you'd think, if
everything's right in the world,

00:28:43.970 --> 00:28:45.710
that I'm an old guy.

00:28:45.710 --> 00:28:47.960
I've been going to the medical
clinic for a long time.

00:28:47.960 --> 00:28:50.170
If I look at my chart and
average all those numbers,

00:28:50.170 --> 00:28:52.122
it should be somewhere
close to the true value

00:28:52.122 --> 00:28:52.830
of those numbers.

00:28:52.830 --> 00:29:00.200
So I should have that the
average values experimentally,

00:29:00.200 --> 00:29:03.677
which I just define
to be the averages--

00:29:15.110 --> 00:29:18.150
this is the number
of experiments.

00:29:18.150 --> 00:29:20.990
OK, so I can have
these averages.

00:29:20.990 --> 00:29:27.600
And I would expect that
as n goes to infinity,

00:29:27.600 --> 00:29:31.670
I hope that my
experimental values go

00:29:31.670 --> 00:29:34.520
to the same value
of x that I would

00:29:34.520 --> 00:29:38.590
have gotten from the true
probability distribution

00:29:38.590 --> 00:29:39.350
function.

00:29:39.350 --> 00:29:46.610
If I knew what Px of x is
and I evaluated the integral

00:29:46.610 --> 00:29:49.970
and I got x, I think it should
be the same as the experiment

00:29:49.970 --> 00:29:52.580
as long as I did enough repeats.

00:29:52.580 --> 00:29:55.262
So this is almost like an
article of faith here, yeah?

00:29:55.262 --> 00:29:56.220
It's what you'd expect.

00:30:02.550 --> 00:30:04.450
Now, the interesting
thing about this--

00:30:04.450 --> 00:30:06.570
I mean, probably
you've done this a lot.

00:30:06.570 --> 00:30:09.070
You probably did experiments
and you've averaged some things

00:30:09.070 --> 00:30:10.110
before, right?

00:30:10.110 --> 00:30:12.600
If everybody in the class tried
to measure how tall I was,

00:30:12.600 --> 00:30:14.490
you guys all wouldn't
get the same number.

00:30:14.490 --> 00:30:15.690
But you'd think that
if you took the average

00:30:15.690 --> 00:30:17.440
of the whole classroom,
it might be pretty

00:30:17.440 --> 00:30:19.380
close to my true height, right?

00:30:19.380 --> 00:30:28.280
So the key idea here
is that the sigma

00:30:28.280 --> 00:30:38.840
squared of the x measurement
experimental, which

00:30:38.840 --> 00:30:42.310
we define to be this--

00:30:55.967 --> 00:30:57.550
maybe we should do
this one at a time.

00:31:00.130 --> 00:31:01.224
[INAUDIBLE]

00:31:11.470 --> 00:31:13.840
Then I can have a vector
of these guys for all

00:31:13.840 --> 00:31:15.082
the different measurements.

00:31:15.082 --> 00:31:16.540
So there's some
error in my height.

00:31:16.540 --> 00:31:17.390
There's some error in my weight.

00:31:17.390 --> 00:31:20.350
There's some different error in
my blood pressure measurement,

00:31:20.350 --> 00:31:23.450
but each should have
their own variances.

00:31:23.450 --> 00:31:24.910
I can have the covariances.

00:31:50.280 --> 00:31:52.620
OK, so these are all the
experimental quantities.

00:31:52.620 --> 00:31:55.078
You guys maybe even computed
all these before in your life.

00:31:57.760 --> 00:31:59.610
And we expect that this
should go like this

00:31:59.610 --> 00:32:00.674
as n goes to infinity.

00:32:00.674 --> 00:32:02.340
Now what's going to
happen to these guys

00:32:02.340 --> 00:32:03.374
as n goes to infinity?

00:32:03.374 --> 00:32:04.915
That's the really
important question.

00:32:09.080 --> 00:32:13.910
So there's an amazing theory
called the central limit

00:32:13.910 --> 00:32:15.565
theorem of statistics.

00:32:26.460 --> 00:32:40.240
And what this theorem
says, that as n gets large

00:32:40.240 --> 00:32:58.470
and if trials are uncorrelated
and the x's aren't correlated,

00:32:58.470 --> 00:33:01.960
which is the same as saying
that Cij is equal to 0 off

00:33:01.960 --> 00:33:16.420
the diagonal, then the
probability of making

00:33:16.420 --> 00:33:33.696
the measurement x is
proportional to the Gaussian,

00:33:33.696 --> 00:33:34.320
the bell curve.

00:34:02.640 --> 00:34:04.470
All right?

00:34:04.470 --> 00:34:08.790
So this is only true
as n gets very large.

00:34:08.790 --> 00:34:10.929
It doesn't specify exactly
how large has to be,

00:34:10.929 --> 00:34:14.590
but it's true for any
Px, any distribution

00:34:14.590 --> 00:34:17.005
function, probability
distribution function.

00:34:17.005 --> 00:34:19.840
So everything
becomes a bell curve

00:34:19.840 --> 00:34:22.719
if you look at the averages.

00:34:22.719 --> 00:34:27.760
And sigma i squared
in that limit

00:34:27.760 --> 00:34:35.598
goes to 1 over n sigma
xi squared experimental.

00:34:41.900 --> 00:34:44.130
And this is really important.

00:34:44.130 --> 00:34:47.750
So what this says is that
the width of this Gaussian

00:34:47.750 --> 00:34:50.690
distribution gets narrower
and narrower as you

00:34:50.690 --> 00:34:55.130
increase the number of
repeated experiments

00:34:55.130 --> 00:34:57.860
or increase the
number of samples.

00:34:57.860 --> 00:35:03.460
So this is really saying that
the uncertainty in the mean

00:35:03.460 --> 00:35:09.321
is scaling as 1
over root n where

00:35:09.321 --> 00:35:10.820
n is the number of
samples or number

00:35:10.820 --> 00:35:12.440
of experiments that's repeated.

00:35:15.420 --> 00:35:19.730
Now, sigma, the variance,
is not like that at all.

00:35:19.730 --> 00:35:26.290
So this quantity, actually
as you increase n,

00:35:26.290 --> 00:35:27.430
just goes to a constant.

00:35:27.430 --> 00:35:29.982
It goes to whatever
the real variance is,

00:35:29.982 --> 00:35:31.940
which if you're measuring
me, it might how good

00:35:31.940 --> 00:35:32.940
your ruler or something.

00:35:32.940 --> 00:35:35.890
It'll tell you roughly
what the real variance is.

00:35:35.890 --> 00:35:39.840
And that number does not go
to 0 as the number of repeats

00:35:39.840 --> 00:35:40.340
happens.

00:35:40.340 --> 00:35:42.090
I mean, I could get
the whole student body

00:35:42.090 --> 00:35:45.112
to measure how tall I am
at MIT, and they're still

00:35:45.112 --> 00:35:46.320
not going to have 0 variance.

00:35:46.320 --> 00:35:48.920
It's going to still be
some variance, right?

00:35:48.920 --> 00:35:53.750
So this quantity stays
constant as n increases

00:35:53.750 --> 00:35:56.570
or goes to a constant value
once it sort of stabilizes.

00:35:56.570 --> 00:35:58.160
You have to have enough samples.

00:35:58.160 --> 00:36:02.487
But this quantity, the
uncertainty in the mean value,

00:36:02.487 --> 00:36:04.820
gets smaller and smaller and
smaller as the square of n.

00:36:09.850 --> 00:36:13.120
Now, this is only true in
the limit as n is large.

00:36:13.120 --> 00:36:17.330
Now, this is a huge problem
because experimentalists

00:36:17.330 --> 00:36:20.654
are lazy, and you don't want
to do that many measurements.

00:36:20.654 --> 00:36:22.070
And it's hard to
do a measurement.

00:36:22.070 --> 00:36:25.100
So for example, the Higgs boson
was discovered, what, a year

00:36:25.100 --> 00:36:27.090
and a half ago, two years ago?

00:36:27.090 --> 00:36:31.070
And I think altogether, they
had like nine observations

00:36:31.070 --> 00:36:33.920
or something when
they reported it, OK?

00:36:33.920 --> 00:36:36.230
So nine is not infinity.

00:36:36.230 --> 00:36:39.020
And so they don't have
infinitely small error

00:36:39.020 --> 00:36:40.155
bars on that measurement.

00:36:40.155 --> 00:36:42.530
And in fact, who knows if it
really looks like a Gaussian

00:36:42.530 --> 00:36:45.280
distribution from
such a small sample,

00:36:45.280 --> 00:36:49.640
but they still
reported 90% confidence

00:36:49.640 --> 00:36:51.950
interval using the Gaussian
distribution formula

00:36:51.950 --> 00:36:54.350
to figure out
confidence intervals.

00:36:54.350 --> 00:36:56.760
So everybody does this.

00:36:56.760 --> 00:36:59.074
If n is big, it should be right.

00:36:59.074 --> 00:37:00.990
And you could prove
mathematically it's right,

00:37:00.990 --> 00:37:04.540
but the formula doesn't really
tell you how big is big.

00:37:04.540 --> 00:37:07.060
So this is like a
general problem.

00:37:07.060 --> 00:37:13.000
And it leads to us
oftentimes misestimating

00:37:13.000 --> 00:37:15.370
how accurate our results
are because we're

00:37:15.370 --> 00:37:18.820
going to use formulas that are
based on-- assuming that we've

00:37:18.820 --> 00:37:21.820
averaged enough repeats that
we're in this limit where we

00:37:21.820 --> 00:37:25.270
can use the Gaussian formulas
and get this nice limit

00:37:25.270 --> 00:37:26.284
formula.

00:37:26.284 --> 00:37:28.450
But in fact, we haven't
really reach that because we

00:37:28.450 --> 00:37:30.480
haven't done enough repeats.

00:37:30.480 --> 00:37:33.880
So anyway, this is
just the way life is.

00:37:33.880 --> 00:37:35.337
That's the way life is.

00:37:35.337 --> 00:37:37.420
And I think there's even
discussions in statistics

00:37:37.420 --> 00:37:41.390
journals and stuff about how
to make corrections and use

00:37:41.390 --> 00:37:46.870
slightly better forms that get
the fact that your distribution

00:37:46.870 --> 00:37:49.960
of the mean doesn't narrow
down to a beautiful Gaussian

00:37:49.960 --> 00:37:50.740
so fast.

00:37:50.740 --> 00:37:52.567
It has some stuff in the tails.

00:37:52.567 --> 00:37:54.400
People talk about that,
like low probability

00:37:54.400 --> 00:37:56.816
events out in the tails of
distributions, stuff like that.

00:37:56.816 --> 00:37:59.660
So that's a big
field of statistics.

00:37:59.660 --> 00:38:01.890
I don't know too much
about it, but it's like--

00:38:01.890 --> 00:38:04.600
I mean, it's very
practical because--

00:38:04.600 --> 00:38:07.120
now unfortunately, oftentimes
in chemical engineering,

00:38:07.120 --> 00:38:10.690
we make so few repeats
that we have no chance

00:38:10.690 --> 00:38:13.480
to figure out what the tails
are doing, maybe [INAUDIBLE]

00:38:13.480 --> 00:38:15.280
our tails.

00:38:15.280 --> 00:38:18.610
And so this is a big problem
for trying to make sure

00:38:18.610 --> 00:38:20.110
you really have things right.

00:38:20.110 --> 00:38:25.690
So I would say in general,
this is an optimistic estimate

00:38:25.690 --> 00:38:30.526
of what the uncertainty
in the mean is.

00:38:30.526 --> 00:38:31.900
Uncertainties are
usually bigger.

00:38:31.900 --> 00:38:35.950
So you shouldn't be surprised
if your data doesn't

00:38:35.950 --> 00:38:41.156
match a model brilliantly well
as predicted by this formula.

00:38:41.156 --> 00:38:43.030
Now, if it's off by some
orders of magnitude,

00:38:43.030 --> 00:38:44.717
you might be a little alarmed.

00:38:44.717 --> 00:38:46.550
And that might be the
normal situation, too.

00:38:46.550 --> 00:38:50.570
But anyway, if it's just
off by a little bit,

00:38:50.570 --> 00:38:53.050
I wouldn't sweat it
because you probably

00:38:53.050 --> 00:38:56.080
haven't done enough repeats
to be entitled to such

00:38:56.080 --> 00:38:57.780
a beautiful result as this.

00:39:02.750 --> 00:39:05.970
We can write a similar--

00:39:05.970 --> 00:39:10.850
actually, so here I assumed
that the x's are uncorrelated.

00:39:10.850 --> 00:39:12.940
That's almost never true.

00:39:12.940 --> 00:39:15.350
If you actually numerically
evaluate the C's, usually

00:39:15.350 --> 00:39:17.012
they have off-diagonal elements.

00:39:17.012 --> 00:39:18.845
For example, my weight
and my blood pressure

00:39:18.845 --> 00:39:21.650
are probably correlated.

00:39:21.650 --> 00:39:25.820
And so you wouldn't expect them
to be totally uncorrelated.

00:39:25.820 --> 00:39:32.330
And so there's another
formula like this.

00:39:32.330 --> 00:39:34.490
It's given in the
notes by Joe Scott

00:39:34.490 --> 00:39:36.920
that includes the covariance.

00:39:36.920 --> 00:39:42.770
And you just get a different
form of what you'd expect, OK?

00:39:42.770 --> 00:39:48.440
And the covariance should also
converge roughly as 1 over n

00:39:48.440 --> 00:39:49.880
if you have enough samples.

00:39:49.880 --> 00:39:52.058
So you should eventually
get some covariance.

00:39:55.550 --> 00:39:59.120
You can write very
similar formulas

00:39:59.120 --> 00:40:02.790
like this for functions.

00:40:02.790 --> 00:40:11.300
So if I have a function
f of x and that's

00:40:11.300 --> 00:40:14.960
really what I care about--

00:40:14.960 --> 00:40:19.370
remember, I said that I
have the average value of f

00:40:19.370 --> 00:40:28.094
is equal to f of x Px of x dx.

00:40:28.094 --> 00:40:29.760
And I could make this
vectors if I want.

00:40:32.370 --> 00:40:35.690
And I could repeat my function,
and I'd get some number.

00:40:35.690 --> 00:40:37.310
And I could repeat the variance.

00:40:37.310 --> 00:40:38.090
I have a sigma f.

00:40:44.877 --> 00:40:46.960
And this is something I
like to do a lot of times.

00:40:49.570 --> 00:40:59.060
Then if we do
experimental delta f--

00:40:59.060 --> 00:41:02.590
so we don't know what the
probability distribution

00:41:02.590 --> 00:41:04.690
function is usually, or often.

00:41:04.690 --> 00:41:07.145
So we'll try to evaluate
this experimentally.

00:41:11.030 --> 00:41:18.282
This is going to be
1 over n, the values

00:41:18.282 --> 00:41:22.905
f of x little n, the n-th trial.

00:41:26.610 --> 00:41:29.892
And we could write a
similar thing for sigma f,

00:41:29.892 --> 00:41:31.100
which I just did right there.

00:41:31.100 --> 00:41:31.750
You can do the same thing.

00:41:31.750 --> 00:41:33.416
Just make these
experimental values now.

00:41:40.580 --> 00:41:48.950
The sigma f squared
experimental should go to 1

00:41:48.950 --> 00:41:51.470
over n times the variance.

00:41:55.860 --> 00:42:00.542
And this was the sigma in
the mean of f, 1 over n

00:42:00.542 --> 00:42:02.950
times the variance of the sigma.

00:42:05.590 --> 00:42:07.900
All right, so this is
the same beautiful thing,

00:42:07.900 --> 00:42:12.550
that the uncertainty
in the mean value of f

00:42:12.550 --> 00:42:16.570
narrows with the
number of trials.

00:42:16.570 --> 00:42:19.670
So you have some
original variance

00:42:19.670 --> 00:42:24.400
that you computed here, either
experimentally or from the PDF.

00:42:24.400 --> 00:42:25.870
Experimentally is fine.

00:42:25.870 --> 00:42:28.320
And then now you want
to know the uncertainty

00:42:28.320 --> 00:42:30.535
in the mean value,
and that drops down

00:42:30.535 --> 00:42:34.000
with the number of trials in the
number of things you average.

00:42:34.000 --> 00:42:36.730
So this all leads
in two directions.

00:42:36.730 --> 00:42:38.320
What we're going
to talk about first

00:42:38.320 --> 00:42:41.950
is about comparing
models versus experiments

00:42:41.950 --> 00:42:44.860
where we're sampling by
doing the experiment.

00:42:44.860 --> 00:42:46.600
So that's one really
important direction,

00:42:46.600 --> 00:42:47.920
maybe the most important one.

00:42:47.920 --> 00:42:52.630
But it also suggests ways you
could do numerical integration.

00:42:52.630 --> 00:42:54.370
So if I wanted to
evaluate an integral

00:42:54.370 --> 00:42:59.230
that looks like this,
f of x P of x dx,

00:42:59.230 --> 00:43:02.860
and if I had some way
to sample from Px,

00:43:02.860 --> 00:43:06.790
then one way to evaluate
this numerical integral

00:43:06.790 --> 00:43:09.139
would be to--

00:43:09.139 --> 00:43:10.930
sorry, I made this
vector [INAUDIBLE] a lot

00:43:10.930 --> 00:43:15.420
of species there, a
lot of directions.

00:43:15.420 --> 00:43:17.620
If I want to evaluate
this multiple integral--

00:43:17.620 --> 00:43:22.389
it's a lot of integrals
for every dimension of x--

00:43:22.389 --> 00:43:23.930
that would be very
hard to do, right?

00:43:23.930 --> 00:43:25.554
We talked about in
[INAUDIBLE],, if you

00:43:25.554 --> 00:43:28.930
get more than about three or
four of these integral signs,

00:43:28.930 --> 00:43:31.660
usually you're in big trouble
to evaluate the integral.

00:43:31.660 --> 00:43:34.210
But you can do it by what's
called Monte Carlo sampling

00:43:34.210 --> 00:43:38.060
where you sample from P of x
and just evaluate the value of f

00:43:38.060 --> 00:43:40.960
at some particular x
points you pull as a sample

00:43:40.960 --> 00:43:42.767
and just repeat their average.

00:43:42.767 --> 00:43:44.350
And the average of
those things should

00:43:44.350 --> 00:43:46.660
converge, according
to this formula,

00:43:46.660 --> 00:43:48.590
as you increase the
number of samples.

00:43:48.590 --> 00:43:50.590
And so that's the whole
principle of Monte Carlo

00:43:50.590 --> 00:43:53.420
methods, and we'll come back
to that a little bit later.

00:43:53.420 --> 00:43:56.650
And you can apply that
to a lot of problems.

00:43:56.650 --> 00:44:00.800
Basically, any problem you have
in numerics, you have a choice.

00:44:00.800 --> 00:44:03.930
You can use deterministic
methods or stochastic methods.

00:44:03.930 --> 00:44:05.680
Deterministic methods,
if you can do them,

00:44:05.680 --> 00:44:08.530
usually are the fastest
and more accurate,

00:44:08.530 --> 00:44:11.560
but stochastic ones are
often very easy to program

00:44:11.560 --> 00:44:14.250
and sometimes are actually
the fastest way to do it.

00:44:14.250 --> 00:44:15.750
In particular, in
this kind of case,

00:44:15.750 --> 00:44:17.572
we have lots of
dimensions, many, many x's.

00:44:17.572 --> 00:44:19.780
It turns out that stochastic
ones are pretty good way

00:44:19.780 --> 00:44:21.720
to do it.

00:44:21.720 --> 00:44:25.157
But we're going to talk
mostly about [INAUDIBLE] data

00:44:25.157 --> 00:44:27.240
because that's going to
be important to all of you

00:44:27.240 --> 00:44:28.570
in your research.

00:44:28.570 --> 00:44:30.380
So let's talk about
that for a minute.

00:44:35.740 --> 00:44:38.310
I'll just comment, there's
really good notes posted on

00:44:38.310 --> 00:44:40.440
the [INAUDIBLE] website
for all this material,

00:44:40.440 --> 00:44:41.820
so you should
definitely read it.

00:44:41.820 --> 00:44:43.705
And the textbook has
a lot of material.

00:44:43.705 --> 00:44:46.950
It's maybe not so easy to read
as the notes are, but plenty

00:44:46.950 --> 00:44:49.200
to learn, for sure.

00:44:49.200 --> 00:44:52.867
So we generally have a situation
where we have an experiment.

00:44:52.867 --> 00:44:54.450
And what do we have
in the experiment?

00:44:54.450 --> 00:44:56.370
We have some knobs.

00:44:59.130 --> 00:45:01.150
These are things
that we can change.

00:45:01.150 --> 00:45:03.810
So we can change
some valve positions.

00:45:03.810 --> 00:45:05.280
We can change how
much electricity

00:45:05.280 --> 00:45:06.900
goes into our heaters.

00:45:06.900 --> 00:45:10.920
We can change the setting on
our back pressure regulator.

00:45:10.920 --> 00:45:14.200
We can change the chemicals
we pour into the system.

00:45:14.200 --> 00:45:16.920
So there's a lot of
knobs that we control.

00:45:16.920 --> 00:45:20.850
And I'm going to
call the knobs x.

00:45:20.850 --> 00:45:22.510
And then we have parameters.

00:45:25.790 --> 00:45:28.550
And these are other
things that affect

00:45:28.550 --> 00:45:30.230
the result of the
experiment that we

00:45:30.230 --> 00:45:32.420
don't have control over.

00:45:32.420 --> 00:45:35.840
And I'm going to
call those theta.

00:45:35.840 --> 00:45:38.660
So for example, if I do
a kinetics experiment,

00:45:38.660 --> 00:45:41.729
it depends on the
rate coefficients.

00:45:41.729 --> 00:45:43.520
I have no control of
the rate coefficients.

00:45:43.520 --> 00:45:45.769
They're going to [INAUDIBLE]
by God, as far as I know.

00:45:45.769 --> 00:45:49.749
So they're some numbers,
but they definitely affect

00:45:49.749 --> 00:45:51.540
the result. And if the
rate coefficient had

00:45:51.540 --> 00:45:53.581
a different value, I would
get a different result

00:45:53.581 --> 00:45:56.540
in the kinetic experiment.

00:45:56.540 --> 00:45:59.630
The molecular weight of sulfur,
I have no control over that.

00:45:59.630 --> 00:46:00.930
That's just a parameter.

00:46:00.930 --> 00:46:02.660
But if I weigh something and it
has a certain number of atoms

00:46:02.660 --> 00:46:05.020
of sulfur, it's going to be
a very important parameter

00:46:05.020 --> 00:46:08.150
in determining the result.

00:46:08.150 --> 00:46:11.120
So we have these two things.

00:46:11.120 --> 00:46:20.550
And then we're going to have
some measurables, things

00:46:20.550 --> 00:46:21.620
that we can measure.

00:46:21.620 --> 00:46:24.590
Let's call them y.

00:46:24.590 --> 00:46:28.740
And in general, we think
that if we set the x value

00:46:28.740 --> 00:46:30.680
and we know the theta
values, we should

00:46:30.680 --> 00:46:33.030
get some measurable values.

00:46:33.030 --> 00:46:37.070
And so there's a
y that the model

00:46:37.070 --> 00:46:44.490
says that's a function of
the x's and the thetas.

00:46:44.490 --> 00:46:46.985
Now, I write this as a
simple function like this.

00:46:46.985 --> 00:46:48.380
This might be
really complicated.

00:46:48.380 --> 00:46:50.213
It might have partial
differential equations

00:46:50.213 --> 00:46:51.284
embedded inside it.

00:46:51.284 --> 00:46:53.450
It might have all kinds of
horrible stuff inside it.

00:46:53.450 --> 00:46:54.920
But you guys already know how
to solve all these problems

00:46:54.920 --> 00:46:55.940
already because you've done it.

00:46:55.940 --> 00:46:57.981
You've been in this class
through seven homeworks

00:46:57.981 --> 00:46:58.940
already.

00:46:58.940 --> 00:47:00.654
And so no problem, right?

00:47:00.654 --> 00:47:01.820
So if I give you something--

00:47:01.820 --> 00:47:02.810
I give you some knobs.

00:47:02.810 --> 00:47:06.705
I give you some parameters--
you can compute it, all right?

00:47:06.705 --> 00:47:09.080
And so then the question is--
that's what the model says.

00:47:09.080 --> 00:47:10.700
So we could predict
the forward prediction

00:47:10.700 --> 00:47:13.100
of what the model should say
if I knew what the parameter

00:47:13.100 --> 00:47:17.090
values were, if I knew
what the knob values were.

00:47:17.090 --> 00:47:22.440
And I want to--

00:47:22.440 --> 00:47:28.040
oftentimes what I
measure, y data,

00:47:28.040 --> 00:47:30.780
which is a function
of the knobs,

00:47:30.780 --> 00:47:32.834
it's implicitly a function
of the parameters.

00:47:32.834 --> 00:47:34.375
I have no control
of them, so I'm not

00:47:34.375 --> 00:47:35.510
going to even put them in here.

00:47:35.510 --> 00:47:36.593
So I set the knobs I want.

00:47:36.593 --> 00:47:37.990
I get some data.

00:47:37.990 --> 00:47:40.450
I want these two to
match each other.

00:47:40.450 --> 00:47:43.470
I think they should be the
same thing if my model is true,

00:47:43.470 --> 00:47:43.970
yeah?

00:47:43.970 --> 00:47:45.718
So this is my model, really.

00:47:51.780 --> 00:47:54.424
But I don't think they
should be exactly the same.

00:47:54.424 --> 00:47:56.590
I mean, just like when you
try to measure my height,

00:47:56.590 --> 00:47:58.290
you don't get exactly
the same numbers.

00:47:58.290 --> 00:48:02.050
So these y data are not going
to be exactly the same numbers

00:48:02.050 --> 00:48:03.482
as my model would say.

00:48:03.482 --> 00:48:04.940
So now I have to
cope with the fact

00:48:04.940 --> 00:48:08.610
that I have deviations between
the data and the model.

00:48:08.610 --> 00:48:11.160
And how am I going to
handle that, all right?

00:48:15.260 --> 00:48:18.560
And also, we have a
set of these guys,

00:48:18.560 --> 00:48:20.550
typically do some repeats.

00:48:20.550 --> 00:48:22.802
So we have like several
numbers for each setting

00:48:22.802 --> 00:48:25.010
in the x's, and they don't
even agree with each other

00:48:25.010 --> 00:48:25.820
because they're all different.

00:48:25.820 --> 00:48:27.361
Every time I repeated
the experiment,

00:48:27.361 --> 00:48:29.150
I got some different result--

00:48:29.150 --> 00:48:31.112
that's my y's-- for each x.

00:48:31.112 --> 00:48:32.570
And then I change
the x a few times

00:48:32.570 --> 00:48:33.694
at different knob settings.

00:48:33.694 --> 00:48:35.160
Then I make some
more measurements.

00:48:35.160 --> 00:48:37.370
And I have a whole
bunch of y values

00:48:37.370 --> 00:48:40.350
that are all scattered
numbers that maybe scatter

00:48:40.350 --> 00:48:42.660
around this model
possibly, if I'm lucky,

00:48:42.660 --> 00:48:44.030
if the model's right.

00:48:44.030 --> 00:48:47.057
Often, usually I also don't
know if the model's correct.

00:48:47.057 --> 00:48:49.390
So that's another thing to
hold in the back of your mind

00:48:49.390 --> 00:48:51.320
is like, we're going to
this whole comparison

00:48:51.320 --> 00:48:53.240
assuming the model's correct.

00:48:53.240 --> 00:48:55.640
And then we might, at the
end, decide, hmm, maybe

00:48:55.640 --> 00:48:56.848
the model's not really right.

00:48:56.848 --> 00:48:58.340
I may have to go
make a new model.

00:48:58.340 --> 00:49:01.110
So that's just a thing to
keep in the back your mind.

00:49:01.110 --> 00:49:02.950
But we'll be optimistic
to start with,

00:49:02.950 --> 00:49:04.340
and we'll assume that
the model is good.

00:49:04.340 --> 00:49:05.714
And our only
challenge is we just

00:49:05.714 --> 00:49:11.060
don't have the right values of
the thetas, maybe, in my model.

00:49:11.060 --> 00:49:12.370
And this is another thing, too.

00:49:12.370 --> 00:49:14.690
So the thetas are things
like rate coefficients

00:49:14.690 --> 00:49:16.610
and molecular weights
and viscosities

00:49:16.610 --> 00:49:19.387
and stuff that are like
properties of the universe,

00:49:19.387 --> 00:49:20.720
and they're real numbers, maybe.

00:49:20.720 --> 00:49:23.120
They're also things like
the length of my apparatus

00:49:23.120 --> 00:49:25.010
and stuff like that.

00:49:25.010 --> 00:49:28.809
But I don't know those numbers
to perfect precision, right?

00:49:28.809 --> 00:49:30.350
The best number I
can find, if I look

00:49:30.350 --> 00:49:32.984
in the database
is, you know, you

00:49:32.984 --> 00:49:34.400
could find like
the speed of light

00:49:34.400 --> 00:49:36.525
to like 11 significant
figures, but I don't know it

00:49:36.525 --> 00:49:38.240
to the 12th significant figure.

00:49:38.240 --> 00:49:40.144
So I don't know any of
the numbers perfectly.

00:49:40.144 --> 00:49:42.060
And a lot of numbers I
don't even know at all.

00:49:42.060 --> 00:49:43.280
So like there's some
rate coefficients

00:49:43.280 --> 00:49:45.170
that no one has ever
measured or calculated

00:49:45.170 --> 00:49:47.070
in the history of the world.

00:49:47.070 --> 00:49:48.930
And my students have
to deal with that a lot

00:49:48.930 --> 00:49:49.910
in the Green group.

00:49:49.910 --> 00:49:52.719
So a lot of these
are quite uncertain.

00:49:52.719 --> 00:49:54.510
But there are some that
are pretty certain.

00:49:54.510 --> 00:49:56.736
You have quite a big variance,
actually, of how certainly you

00:49:56.736 --> 00:49:57.819
know the parameter values.

00:50:00.900 --> 00:50:07.140
So one idea, a very popular
idea, is to say, you know,

00:50:07.140 --> 00:50:12.260
I have this deviation between
the model and the experiment.

00:50:12.260 --> 00:50:15.490
So I want to sort of do a
minimization by varying,

00:50:15.490 --> 00:50:23.110
say, parameter values of
some measure of the error

00:50:23.110 --> 00:50:24.561
between the model and the data.

00:50:32.100 --> 00:50:35.376
Somehow, I want
to minimize that.

00:50:35.376 --> 00:50:38.000
And I have to think about, well,
what should I really minimize?

00:50:38.000 --> 00:50:44.010
And the popular thing to
minimize is these guys squared

00:50:44.010 --> 00:50:48.376
and actually to weight
them by some kind of sigma

00:50:48.376 --> 00:50:49.500
for each one of these guys.

00:50:49.500 --> 00:50:51.441
So this is-- we should
change the notation,

00:50:51.441 --> 00:50:52.190
make this clearer.

00:51:23.620 --> 00:51:30.890
These guys-- one model, and
it's the i-th measurement

00:51:30.890 --> 00:51:33.853
that corresponds to
that n-th experiment.

00:51:44.220 --> 00:51:49.320
So I think that the difference
between what I measured

00:51:49.320 --> 00:51:51.690
and what the model calculated
should be sort of scaled

00:51:51.690 --> 00:51:55.650
by the variance, right?

00:51:55.650 --> 00:51:58.530
So I would expect
that this sum has

00:51:58.530 --> 00:52:00.990
a bunch of numbers that
are sort of order of one

00:52:00.990 --> 00:52:03.840
because I expect the deviation
to be approximately scaled

00:52:03.840 --> 00:52:06.800
of the variance of
my measurements.

00:52:06.800 --> 00:52:10.750
And if these deviations are
much larger than the variance,

00:52:10.750 --> 00:52:12.330
then I think my
model's not right

00:52:12.330 --> 00:52:14.080
and what I'm going to
try to do right here

00:52:14.080 --> 00:52:16.570
is I'm going to try to adjust
the thetas, the parameters,

00:52:16.570 --> 00:52:21.300
to try to force the model to
agree better to my experiment.

00:52:21.300 --> 00:52:28.000
And this form looks
a lot like this.

00:52:28.000 --> 00:52:29.880
Do you see this?

00:52:29.880 --> 00:52:31.950
You see I have a sum
of the deviations

00:52:31.950 --> 00:52:36.480
between the experiment and
a theoretical sort of thing

00:52:36.480 --> 00:52:39.210
divided by some variance?

00:52:39.210 --> 00:52:42.800
And so this is the motivation
of where this comes from,

00:52:42.800 --> 00:52:50.550
is that I want to make the
probability that I would make

00:52:50.550 --> 00:52:55.470
this observation
experimentally would be maximum

00:52:55.470 --> 00:53:00.460
if this quantity in the exponent
is as small as possible.

00:53:00.460 --> 00:53:02.550
So I'm going to try to
minimize that quantity,

00:53:02.550 --> 00:53:05.080
and that's exactly what
I'm doing over here.

00:53:05.080 --> 00:53:06.502
Is that all right?

00:53:06.502 --> 00:53:07.960
OK, so next time
when we come back,

00:53:07.960 --> 00:53:10.490
I'll talk more about
how we actually do it.