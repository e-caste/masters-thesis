WEBVTT

00:00:00.000 --> 00:00:02.430
The following content is
provided under a Creative

00:00:02.430 --> 00:00:04.000
Commons license.

00:00:04.000 --> 00:00:06.950
Your support will help MIT
OpenCourseWare continue to

00:00:06.950 --> 00:00:10.560
offer high quality educational
resources for free.

00:00:10.560 --> 00:00:13.450
To make a donation or view
additional materials from

00:00:13.450 --> 00:00:16.610
hundreds of MIT courses, visit
MIT OpenCourseWare at

00:00:16.610 --> 00:00:17.860
ocw.mit.edu.

00:00:22.740 --> 00:00:25.350
PROFESSOR: So in this second
lecture we're going to talk

00:00:25.350 --> 00:00:29.830
about some design patterns
for parallel programming.

00:00:29.830 --> 00:00:33.180
And to tell you a little bit
about what a design pattern is

00:00:33.180 --> 00:00:34.340
and why is it useful.

00:00:34.340 --> 00:00:36.440
And some of you, if you've
taken object oriented

00:00:36.440 --> 00:00:38.450
programming you've probably
already have seen design

00:00:38.450 --> 00:00:40.580
patterns before.

00:00:40.580 --> 00:00:43.600
So I ended the last lecture with
OK, so I understand some

00:00:43.600 --> 00:00:45.330
of the performance implications,
how do I go

00:00:45.330 --> 00:00:48.510
about parallelizing
my program?

00:00:48.510 --> 00:00:53.900
This is a flyer I found quite
often in books and talks on

00:00:53.900 --> 00:00:54.600
parallel programming.

00:00:54.600 --> 00:00:58.960
It essentially lays out 4 common
steps for parallelizing

00:00:58.960 --> 00:01:00.680
your program.

00:01:00.680 --> 00:01:03.630
So often, you start out with
sequential programs. This

00:01:03.630 --> 00:01:06.810
shouldn't be surprising since
for a long time as you've

00:01:06.810 --> 00:01:09.350
heard in earlier lectures,
people just coded sequential

00:01:09.350 --> 00:01:12.190
code and that was just
good enough.

00:01:12.190 --> 00:01:15.220
So now the problem is you want
to take that sequential code

00:01:15.220 --> 00:01:17.410
or you want to still write
sequential code just because

00:01:17.410 --> 00:01:19.670
it's conceptually easier and
you want to be able to

00:01:19.670 --> 00:01:23.240
parallelize it so you can map
it down to your parallel

00:01:23.240 --> 00:01:26.930
architecture, which in this
example has 4 processors.

00:01:26.930 --> 00:01:30.630
So the first step is you take
your sequential program and

00:01:30.630 --> 00:01:32.760
you divide it up into tasks.

00:01:32.760 --> 00:01:35.250
So during the project reviews,
for example, yesterday when I

00:01:35.250 --> 00:01:37.980
talked to each team individually
we sort of talked

00:01:37.980 --> 00:01:41.740
about this and you sort of
stumbled on these 4 steps

00:01:41.740 --> 00:01:44.190
whether you realized
it or not.

00:01:44.190 --> 00:01:47.750
And so you come up with these
tasks and then each one

00:01:47.750 --> 00:01:50.670
essentially incapsulates
some computation.

00:01:50.670 --> 00:01:52.870
And then you group them
together, so this is some

00:01:52.870 --> 00:01:55.530
granularity adjustment and you
map them down to processes.

00:01:55.530 --> 00:01:58.370
These are things you can pose
into threads, for example.

00:01:58.370 --> 00:02:00.790
And then you have to essentially
plot these down

00:02:00.790 --> 00:02:04.850
onto actual processors and they
have to talk with each

00:02:04.850 --> 00:02:07.250
other, so you have to
orchestrate to communication

00:02:07.250 --> 00:02:10.110
and then finally do
the execution.

00:02:10.110 --> 00:02:16.280
So step through each one
of these at a time.

00:02:16.280 --> 00:02:20.300
Sort of composition and recall
that just really effects or

00:02:20.300 --> 00:02:23.450
just really is effected
by Amdahl's Law.

00:02:23.450 --> 00:02:26.110
And that if there's not a whole
lot of parallels in the

00:02:26.110 --> 00:02:28.590
application your decomposition
is a waste of time.

00:02:28.590 --> 00:02:30.340
There's not really a
whole lot to get.

00:02:30.340 --> 00:02:33.150
But what you're trying to do
is identify concurrency in

00:02:33.150 --> 00:02:35.200
your application and
figure out at what

00:02:35.200 --> 00:02:36.570
level to exploit it.

00:02:36.570 --> 00:02:39.140
So what you're trying to do is
divide up your computation

00:02:39.140 --> 00:02:42.050
into tasks and eventually
these are going to be

00:02:42.050 --> 00:02:44.900
distributed among processors and
you want to find enough of

00:02:44.900 --> 00:02:48.190
them so that you can keep
all the processors busy.

00:02:48.190 --> 00:02:51.200
And remember that the more of
these that you have this gives

00:02:51.200 --> 00:02:54.640
you sort of an upper bound on
your potential speed up.

00:02:54.640 --> 00:02:57.500
And as in the rate tracing
example that I showed, the

00:02:57.500 --> 00:03:00.660
number of tasks that you
have may vary run time.

00:03:00.660 --> 00:03:03.220
So sometimes you might have lot
of arrays bouncing off a

00:03:03.220 --> 00:03:06.140
lot of things, and sometimes you
might not have a whole lot

00:03:06.140 --> 00:03:10.150
of reflection going on
so number of arrays

00:03:10.150 --> 00:03:12.720
will change over time.

00:03:12.720 --> 00:03:15.080
And in other applications,
interactions, for example,

00:03:15.080 --> 00:03:16.610
between molecules might
change in a

00:03:16.610 --> 00:03:19.960
molecular dynamic simulator.

00:03:19.960 --> 00:03:21.980
The assignment really
effects granularity.

00:03:21.980 --> 00:03:23.770
This is where you've partitioned
your tasks, you're

00:03:23.770 --> 00:03:27.760
trying to group them together
so that you're taking into

00:03:27.760 --> 00:03:29.300
account, what is the
communication

00:03:29.300 --> 00:03:30.830
cost going to be?

00:03:30.830 --> 00:03:34.640
What kind of locality am
I going to deal with?

00:03:34.640 --> 00:03:36.450
And what kind of synchronization
mechanisms do

00:03:36.450 --> 00:03:38.690
I need and how often do
I need to synchronize?

00:03:42.980 --> 00:03:45.020
You adjust your granularity
such that you end up with

00:03:45.020 --> 00:03:47.460
things that are load balanced
and you try to reduce

00:03:47.460 --> 00:03:50.300
communication as much
as possible.

00:03:50.300 --> 00:03:51.870
And structured approaches
might work well.

00:03:51.870 --> 00:03:54.670
You might look at the code, do
some inspection, you might

00:03:54.670 --> 00:03:57.360
understand the application, but
there are some well-known

00:03:57.360 --> 00:03:59.330
design patterns which is
essentially the thing we're

00:03:59.330 --> 00:04:00.920
going to get to try to
help you with this.

00:04:04.470 --> 00:04:07.280
As programmers really, I
think, we worry about

00:04:07.280 --> 00:04:12.510
partitioning first. This is
really independent of an

00:04:12.510 --> 00:04:14.070
architecture programming
model.

00:04:14.070 --> 00:04:16.920
Just taking my application and
figuring out well, what are

00:04:16.920 --> 00:04:19.710
different parts that I need to
compose together to build my

00:04:19.710 --> 00:04:20.350
application?

00:04:20.350 --> 00:04:22.100
So I'm going to show you
an example of that.

00:04:22.100 --> 00:04:25.550
And one thing to keep in the
back of your mind is that the

00:04:25.550 --> 00:04:29.150
complexity of how much
partitioning work you actually

00:04:29.150 --> 00:04:31.360
have to do really effects
your decision.

00:04:31.360 --> 00:04:33.610
So if you start out with some
piece of code or you wrote

00:04:33.610 --> 00:04:36.730
your code in one way and you
realize that to actually

00:04:36.730 --> 00:04:39.340
parallelize it requires so much
more work, in some user

00:04:39.340 --> 00:04:41.760
studies we've done on sort of
trying to get performance from

00:04:41.760 --> 00:04:44.600
code, it really effects how
much work you actually do.

00:04:44.600 --> 00:04:46.610
And if something requires a lot
of work, you might not do

00:04:46.610 --> 00:04:48.710
it even though it might have
really hot payoff.

00:04:48.710 --> 00:04:53.720
So you want to be able to keep
complexity down, so it pays

00:04:53.720 --> 00:04:55.980
off to really think well about
your algorithm, how you

00:04:55.980 --> 00:04:59.430
structure it ahead of time.

00:04:59.430 --> 00:05:01.880
And finally, the last two stages
I've lumped together.

00:05:01.880 --> 00:05:03.330
It's really orchestration
and mapping.

00:05:03.330 --> 00:05:07.230
I have my task, they need to
communicate, so what kind of

00:05:07.230 --> 00:05:08.770
computation primitives
do I need?

00:05:08.770 --> 00:05:11.450
What kind of communication
primitives do I need?

00:05:11.450 --> 00:05:13.590
So am I packaging things
up into threads?

00:05:13.590 --> 00:05:21.270
And they're talking together
over DMAs or shared memories.

00:05:21.270 --> 00:05:24.050
And what you want to do is try
to preserve locality and then

00:05:24.050 --> 00:05:26.860
figure out how to come up with
a scheduling order that

00:05:26.860 --> 00:05:29.400
preserves overall dependence
of the computation.

00:05:33.390 --> 00:05:36.720
Parallel program by patterns is
meant to essentially give

00:05:36.720 --> 00:05:40.750
you a cookbook or set of recipes
you can follow to help

00:05:40.750 --> 00:05:43.050
you with different steps:
decompose, assign,

00:05:43.050 --> 00:05:45.100
orchestrate and map.

00:05:45.100 --> 00:05:46.710
This could lead to really
high quality

00:05:46.710 --> 00:05:47.740
solutions in some domains.

00:05:47.740 --> 00:05:51.490
So in the scientific
computations there's a lot of

00:05:51.490 --> 00:05:53.590
problems that are well
understood and well studied

00:05:53.590 --> 00:05:55.430
and some of the frequently
occurring things have been

00:05:55.430 --> 00:06:00.240
abstracted out and sort of
recorded in patterns.

00:06:00.240 --> 00:06:02.700
And there's another purpose to
patterns too, in that they

00:06:02.700 --> 00:06:05.360
provide you with the
vocabulary for--

00:06:05.360 --> 00:06:08.100
two programmers can talk to each
other and use the right

00:06:08.100 --> 00:06:10.940
terminology and that conveys
a whole lot of information

00:06:10.940 --> 00:06:13.920
without having to actually
go through and

00:06:13.920 --> 00:06:16.280
understand all the details.

00:06:16.280 --> 00:06:21.510
You instantaneously know if
I use a particular model.

00:06:24.010 --> 00:06:26.570
It can also help with software
reusuability, malleability,

00:06:26.570 --> 00:06:27.640
and modularity.

00:06:27.640 --> 00:06:30.070
All of those things that are
software engineer perspective

00:06:30.070 --> 00:06:34.060
from an engineering perspective
are important.

00:06:34.060 --> 00:06:38.035
So brief history and I found
this in some of the talks that

00:06:38.035 --> 00:06:39.770
I was researching.

00:06:39.770 --> 00:06:43.520
There's a book by Christopher
Alexander from Berkeley in

00:06:43.520 --> 00:06:48.390
1977 that actually looked at
classifying patterns or really

00:06:48.390 --> 00:06:50.800
listing patterns from an
architectural perspective.

00:06:50.800 --> 00:06:53.915
He tried to look at what are
some patterns that occur in

00:06:53.915 --> 00:06:57.230
living designs and
recording those.

00:06:57.230 --> 00:06:59.150
So as an example, for
example, there's a 6

00:06:59.150 --> 00:07:00.850
foot balcony pattern.

00:07:00.850 --> 00:07:02.920
So if you're going to build a
balcony you should build it 6

00:07:02.920 --> 00:07:06.590
foot deep and you should have it
slightly recessed and so on

00:07:06.590 --> 00:07:08.900
because this is what's commonly
used and these are

00:07:08.900 --> 00:07:13.140
the kinds of balconies that
have good properties

00:07:13.140 --> 00:07:13.850
architecturally.

00:07:13.850 --> 00:07:16.480
Now I don't know whether this
book actually had a whole lot

00:07:16.480 --> 00:07:19.180
of impact on how people designed
architectures.

00:07:19.180 --> 00:07:23.350
Certainly not probably for the
Stata Center, but some

00:07:23.350 --> 00:07:26.360
patterns from object oriented
programming, I think, many of

00:07:26.360 --> 00:07:31.480
you have already seen these by
the Gang of Four in 1995--

00:07:31.480 --> 00:07:35.760
really sort of organized and
classified and came up with

00:07:35.760 --> 00:07:38.730
different ways of--

00:07:38.730 --> 00:07:42.240
or captured different ways of
programming that people had

00:07:42.240 --> 00:07:43.060
been using.

00:07:43.060 --> 00:07:45.230
You know, things like the
visitor pattern, for example,

00:07:45.230 --> 00:07:46.480
some of you might know.

00:07:52.460 --> 00:07:56.280
So in 2005, not too long ago
there was a new book, which

00:07:56.280 --> 00:07:59.860
I'm using to create some
of these slides.

00:07:59.860 --> 00:08:05.540
Really came up with or recorded
patterns for parallel

00:08:05.540 --> 00:08:06.480
programming.

00:08:06.480 --> 00:08:09.500
And they identified really
4 design spaces.

00:08:09.500 --> 00:08:14.260
I think these are sort of
structured to express or

00:08:14.260 --> 00:08:15.930
capture different elements.

00:08:15.930 --> 00:08:19.580
So some elements are for the
algorithm expression, I've

00:08:19.580 --> 00:08:22.080
listed those here and some are
for the actual software

00:08:22.080 --> 00:08:24.380
construction or the actual
implementation.

00:08:24.380 --> 00:08:28.380
So under algorithm expression
it's really the thing of

00:08:28.380 --> 00:08:29.920
decomposition; finding
concurrency.

00:08:29.920 --> 00:08:32.050
Where are my tasks?

00:08:32.050 --> 00:08:35.390
In the algorithm structure,
well, you might need some way

00:08:35.390 --> 00:08:39.050
of packaging those tasks
together so that they can talk

00:08:39.050 --> 00:08:46.950
to each other and be able to
use parallel architecture.

00:08:46.950 --> 00:08:49.150
On the software construction
side you're dealing slightly

00:08:49.150 --> 00:08:50.640
more low level details.

00:08:50.640 --> 00:08:53.320
So what are some things you
might need at a slightly lower

00:08:53.320 --> 00:08:55.770
level of implementation to
actually get all the

00:08:55.770 --> 00:08:58.920
computation that's expressed at
the algorithm level to work

00:08:58.920 --> 00:08:59.950
and run well?

00:08:59.950 --> 00:09:05.010
So I'm going to essentially talk
about the latter part in

00:09:05.010 --> 00:09:07.580
the next lecture and I'll cover
much of the algorithm

00:09:07.580 --> 00:09:09.320
expression stuff here--

00:09:09.320 --> 00:09:12.780
at least the fine concurrency
part in this talk.

00:09:12.780 --> 00:09:14.740
And if there's time I'll
do algorithm structure.

00:09:14.740 --> 00:09:17.750
Otherwise, just talk
about it next time.

00:09:17.750 --> 00:09:21.650
So let's say you're working
with MPEG decoding.

00:09:21.650 --> 00:09:24.690
This is a pipeline picture of an
MPEG 2 decoder or rather a

00:09:24.690 --> 00:09:27.780
blocked level diagram of
an MPEG 2 decoder.

00:09:27.780 --> 00:09:29.690
And you have this algorithm
and you say, OK, I want to

00:09:29.690 --> 00:09:30.380
parallelize this.

00:09:30.380 --> 00:09:31.830
Where's my parallelism?

00:09:31.830 --> 00:09:34.510
Where's my concurrency?

00:09:34.510 --> 00:09:37.000
You know, in MPEG 2 you have
some bit screen, you do some

00:09:37.000 --> 00:09:39.770
decoding on it and you end
up with two things.

00:09:39.770 --> 00:09:42.430
You end up with motion vectors
that tell you here's

00:09:42.430 --> 00:09:44.890
somebody's head, in the next
scene it's moved to this

00:09:44.890 --> 00:09:46.140
particular location.

00:09:49.050 --> 00:09:53.350
So these are captured by
the motion vectors.

00:09:53.350 --> 00:09:56.800
So this captures or recovers
temporal information.

00:09:56.800 --> 00:09:58.730
In here you cover spatial
information.

00:09:58.730 --> 00:10:01.730
So in somebody's head you might
have discovered some

00:10:01.730 --> 00:10:03.880
redundancies so that redundancy
is eliminated out

00:10:03.880 --> 00:10:07.180
so you need to know essentially,
uncompress or

00:10:07.180 --> 00:10:08.560
undo that compression.

00:10:08.560 --> 00:10:10.070
So you go through some stages.

00:10:10.070 --> 00:10:12.330
You combine the two together.

00:10:12.330 --> 00:10:15.030
Combine the motion estimation
and now the recovered pictures

00:10:15.030 --> 00:10:17.480
to reconstruct the image and
then you might do some

00:10:17.480 --> 00:10:18.730
additional stages.

00:10:20.850 --> 00:10:24.600
This particular stage here is
indicated to be data parallel

00:10:24.600 --> 00:10:28.120
in that I can do different
scenes for example in parallel

00:10:28.120 --> 00:10:29.850
or I might be able to do
different slices of the

00:10:29.850 --> 00:10:31.030
picture in parallel.

00:10:31.030 --> 00:10:33.820
So I can essentially take
advantage of data parallelism

00:10:33.820 --> 00:10:37.140
in the concept of taking a loop
and breaking it up as I

00:10:37.140 --> 00:10:40.520
showed in lecture 5.

00:10:40.520 --> 00:10:43.480
So in task decomposition what
we're looking for is really

00:10:43.480 --> 00:10:47.630
independent coarse-grain
computation.

00:10:47.630 --> 00:10:51.040
And these often are inherent to
the algorithm. so here I've

00:10:51.040 --> 00:10:52.850
outlined these in yellow here.

00:10:52.850 --> 00:10:54.600
You know, so this is one
particular task.

00:10:54.600 --> 00:10:57.960
I can have one thread of
execution doing all the

00:10:57.960 --> 00:10:59.370
spatial decomposition.

00:10:59.370 --> 00:11:00.830
I can have another
thread decoding

00:11:00.830 --> 00:11:03.710
all my motion vectors.

00:11:03.710 --> 00:11:07.410
And in general, you're looking
for sequences of statements

00:11:07.410 --> 00:11:09.410
that operate together
as a group.

00:11:09.410 --> 00:11:12.970
These could be loops or they
could be functions.

00:11:12.970 --> 00:11:15.990
And usually you want these to
essentially just fall out of

00:11:15.990 --> 00:11:17.670
your algorithm as
it's expressed.

00:11:17.670 --> 00:11:21.070
And a lot of cases it does, so
depending on how you think

00:11:21.070 --> 00:11:22.570
about the program you might
be able to find

00:11:22.570 --> 00:11:26.260
these quicker or easier.

00:11:26.260 --> 00:11:28.760
Data decompositions, which
I've highlighted here

00:11:28.760 --> 00:11:31.490
essentially says you have the
same computation applied to

00:11:31.490 --> 00:11:33.080
lots of small data element.

00:11:33.080 --> 00:11:35.130
You know, you can take your
large data element, partition

00:11:35.130 --> 00:11:38.150
it into smaller chunks and do
the computation over and over

00:11:38.150 --> 00:11:40.750
in parallel and so that allows
you to essentially get that

00:11:40.750 --> 00:11:44.070
kind of data parallelism,
expansion of space.

00:11:44.070 --> 00:11:47.000
And finally, I'm going to
make a case for pipeline

00:11:47.000 --> 00:11:49.960
parallelism, which essentially
says, well, I can recognize

00:11:49.960 --> 00:11:53.060
that I have a lot of stages in
my computation and it does

00:11:53.060 --> 00:11:56.430
help to actually have this kind
of decomposition just

00:11:56.430 --> 00:11:58.400
because you're familiar
with pipelining

00:11:58.400 --> 00:12:00.890
concepts from other domains.

00:12:00.890 --> 00:12:05.030
So this type of producer
consumer chain is actually

00:12:05.030 --> 00:12:06.590
beneficial.

00:12:06.590 --> 00:12:08.950
So it does help to sort of
expose these kinds of

00:12:08.950 --> 00:12:10.200
relationships.

00:12:12.170 --> 00:12:15.370
So what are some guidelines for
actually coming up with

00:12:15.370 --> 00:12:16.560
your task decomposition?

00:12:16.560 --> 00:12:17.640
Where do you start?

00:12:17.640 --> 00:12:21.600
You have your algorithm, you
understand the problem really

00:12:21.600 --> 00:12:24.970
well, you're writing some code
and the hope is that in fact,

00:12:24.970 --> 00:12:28.650
as I've pointed out, it does
happen that you can look for

00:12:28.650 --> 00:12:32.640
natural code regions that
incapsulate your computation.

00:12:32.640 --> 00:12:36.020
So function calls, distinct
loop iterations are pretty

00:12:36.020 --> 00:12:38.830
good places to start looking.

00:12:38.830 --> 00:12:42.990
And it's easier as a general
rule, it's easier to start

00:12:42.990 --> 00:12:46.880
with as many tasks as possible
and then fuse them to make the

00:12:46.880 --> 00:12:50.220
more coarse-grained than to
go the other way around.

00:12:50.220 --> 00:12:53.730
It impacts your software
engineering decisions, it

00:12:53.730 --> 00:12:56.530
impacts software implementation,
it impacts how

00:12:56.530 --> 00:12:59.050
you incapsulate things at
low level details of

00:12:59.050 --> 00:13:00.970
implementation.

00:13:00.970 --> 00:13:04.720
So it's always easier to
fuse than to fizz.

00:13:04.720 --> 00:13:06.180
And you want to consider
three things.

00:13:06.180 --> 00:13:08.380
You want to keep three things
in mind: flexibility,

00:13:08.380 --> 00:13:10.150
efficiency, and simplicity.

00:13:10.150 --> 00:13:21.760
So flexibility says if you made
some decisions, is that

00:13:21.760 --> 00:13:24.540
really going to scale well or
is it going to allow you to

00:13:24.540 --> 00:13:28.330
sort of make the decisions,
changes?

00:13:28.330 --> 00:13:30.230
So you might want to have
fixed tasks versus

00:13:30.230 --> 00:13:32.140
parameterized tasks,
for example.

00:13:32.140 --> 00:13:37.420
So the loops that I showed in
the previous talk, each loop

00:13:37.420 --> 00:13:40.070
that I parallelized had a hard
coded number that said, you're

00:13:40.070 --> 00:13:41.460
going to do 4 iterations.

00:13:41.460 --> 00:13:43.520
That may work well or it
may not work well.

00:13:43.520 --> 00:13:45.420
You know, I can't reuse that
code now if I want to

00:13:45.420 --> 00:13:48.130
essentially use that kind of
data decomposition and work

00:13:48.130 --> 00:13:51.220
sharing if I have a longer loop
and I want a longer array

00:13:51.220 --> 00:13:53.510
and I want each thread
to do more work.

00:13:53.510 --> 00:13:56.670
So you might want to
parameterize more things in

00:13:56.670 --> 00:13:58.140
your tasks.

00:13:58.140 --> 00:14:01.110
The efficiency, in that you have
to keep in mind that each

00:14:01.110 --> 00:14:04.470
of these tasks will eventually
sort of have to talk with

00:14:04.470 --> 00:14:05.410
other tasks.

00:14:05.410 --> 00:14:08.530
There's communication costs
that have to be taken into

00:14:08.530 --> 00:14:10.030
account, synchronization.

00:14:10.030 --> 00:14:12.040
So you want these tasks to
actually amortize the

00:14:12.040 --> 00:14:16.080
communication costs or other
overheads over to computation.

00:14:16.080 --> 00:14:18.340
And you want to keep in mind
that there's going to be

00:14:18.340 --> 00:14:20.720
dependencies between these tasks
and you don't want these

00:14:20.720 --> 00:14:22.260
dependencies to get
out of hand.

00:14:22.260 --> 00:14:24.230
So you want to keep things
under control.

00:14:24.230 --> 00:14:28.410
And lastly, which is probably as
important as the other two:

00:14:28.410 --> 00:14:29.220
simplicity.

00:14:29.220 --> 00:14:32.290
And that if you start
decomposing your code into

00:14:32.290 --> 00:14:35.850
different chunks and you can't
then understand your code in

00:14:35.850 --> 00:14:39.180
the end, it doesn't help you
from debugging perspective,

00:14:39.180 --> 00:14:40.970
doesn't help you from a
software engineering

00:14:40.970 --> 00:14:44.780
perspective or not being able
to reuse you code or other

00:14:44.780 --> 00:14:46.290
people being able to understand
your code.

00:14:49.040 --> 00:14:49.990
Guidelines for data

00:14:49.990 --> 00:14:52.250
decomposition are sort of similar.

00:14:52.250 --> 00:14:56.200
And you essentially have to do
task and data parallelism to

00:14:56.200 --> 00:15:00.250
sort of complete your process.

00:15:00.250 --> 00:15:03.040
And often your task
decomposition dictates your

00:15:03.040 --> 00:15:04.210
data partitioning.

00:15:04.210 --> 00:15:07.320
So if I've split up a loop into
two different processes

00:15:07.320 --> 00:15:10.590
I've essentially implied how
data should be distributed

00:15:10.590 --> 00:15:13.290
between these two threads.

00:15:13.290 --> 00:15:17.320
And data composition is a good
starting point as opposed to

00:15:17.320 --> 00:15:20.340
task parallelism as a
good starting point.

00:15:20.340 --> 00:15:22.470
If you're doing the same
computation over and over and

00:15:22.470 --> 00:15:27.050
over again, over really, really
large data sets, so you

00:15:27.050 --> 00:15:30.040
can essentially use that as your
stick to decide whether

00:15:30.040 --> 00:15:33.380
you do task decomposition first
or data decomposition

00:15:33.380 --> 00:15:37.720
first.

00:15:37.720 --> 00:15:41.460
I've just listed two common
data decompositions.

00:15:41.460 --> 00:15:44.040
I'll talk about more of these
later on when we talk about

00:15:44.040 --> 00:15:48.000
actual performance
optimizations.

00:15:48.000 --> 00:15:51.890
So you want to decompose arrays
for example, along rows

00:15:51.890 --> 00:15:52.670
or columns.

00:15:52.670 --> 00:15:55.200
You can compose arrays into
blocks, you decompose them

00:15:55.200 --> 00:15:56.640
into blocks.

00:15:56.640 --> 00:15:58.150
You have recursive
data structures.

00:15:58.150 --> 00:16:00.570
So a tree, you might partition
it into left and right

00:16:00.570 --> 00:16:02.420
sub-trees in a binary tree.

00:16:02.420 --> 00:16:06.150
And the thing you're trying to
get to is actually start with

00:16:06.150 --> 00:16:08.740
a problem, and then recursively
subdivide it until

00:16:08.740 --> 00:16:10.340
you can get to a manageable
part.

00:16:10.340 --> 00:16:12.190
Do the computation and figure
out a way to do the

00:16:12.190 --> 00:16:13.390
integration.

00:16:13.390 --> 00:16:16.970
You know, it's like merge
sort, classic example --

00:16:16.970 --> 00:16:19.660
tries to capture this
really well.

00:16:19.660 --> 00:16:23.620
So again, the three theme, key
concepts to keep in mind when

00:16:23.620 --> 00:16:25.980
you're doing data decomposition:
flexibility,

00:16:25.980 --> 00:16:29.100
efficiency, and simplicity.

00:16:29.100 --> 00:16:33.370
The first two are really just
meant to suggest that the size

00:16:33.370 --> 00:16:36.150
of the data that you've
allocated actually leads to

00:16:36.150 --> 00:16:37.670
enough work.

00:16:37.670 --> 00:16:40.250
Because you want to amortize the
cost of communication or

00:16:40.250 --> 00:16:43.570
synchronization, but you also
want the amount of work that's

00:16:43.570 --> 00:16:48.510
generated by each data chunk
to generate about the same

00:16:48.510 --> 00:16:51.040
amount of work, so
load balancing.

00:16:51.040 --> 00:16:55.360
And simplicity, just for same
reason as task decomposition

00:16:55.360 --> 00:16:57.600
can get out of hand,
data decomposition

00:16:57.600 --> 00:16:58.550
can get out of hand.

00:16:58.550 --> 00:17:01.480
You don't want data moving
around throughout and then it

00:17:01.480 --> 00:17:06.120
becomes again, hard to debug or
manage or make changes and

00:17:06.120 --> 00:17:09.110
track dependencies.

00:17:09.110 --> 00:17:11.550
Pipeline parallelism, this is
actually classified somewhere

00:17:11.550 --> 00:17:12.500
else in the book.

00:17:12.500 --> 00:17:17.850
Actually, lifted it up and tried
to make a case for it in

00:17:17.850 --> 00:17:20.480
that it's just good nature I
think, to expose producer

00:17:20.480 --> 00:17:21.720
consumer relationships
in your code.

00:17:21.720 --> 00:17:24.280
So if I have a function that's
producing data that's going to

00:17:24.280 --> 00:17:29.780
be used by another function as
with the spatial decomposition

00:17:29.780 --> 00:17:32.300
or in different stages of
classic rate tracing

00:17:32.300 --> 00:17:35.320
algorithms you want to maintain
that producer

00:17:35.320 --> 00:17:38.970
consumer relationship or that
asembly line analogy.

00:17:38.970 --> 00:17:41.910
And what are some prime examples
of pipelines in

00:17:41.910 --> 00:17:43.810
computer architecture?

00:17:43.810 --> 00:17:45.280
It's like the instruction
pipeline

00:17:45.280 --> 00:17:47.170
and your super scalar.

00:17:47.170 --> 00:17:50.070
But there might be some other
examples of pipelines, things

00:17:50.070 --> 00:17:53.590
that you might have used
in say, UNIX shell.

00:17:53.590 --> 00:17:57.740
So cat processor, pipe
it to another--

00:17:57.740 --> 00:17:59.780
to a grep word and then
word count that.

00:17:59.780 --> 00:18:02.860
So I think it's a
natural concept.

00:18:02.860 --> 00:18:05.880
We use it in many different ways
and it's good to sort of

00:18:05.880 --> 00:18:08.890
practice that at the software
level as well.

00:18:08.890 --> 00:18:11.490
And there are some computations
in specific

00:18:11.490 --> 00:18:13.990
domains, like in signal
processing and graphics that

00:18:13.990 --> 00:18:17.140
have really sort of--

00:18:17.140 --> 00:18:20.410
where the pipeline model is
really important part of how

00:18:20.410 --> 00:18:21.960
computation gets carried out.

00:18:21.960 --> 00:18:25.040
You know, you have your graphics
pipeline for example,

00:18:25.040 --> 00:18:27.070
in signal processing.

00:18:27.070 --> 00:18:29.418
How much time do I have?

00:18:29.418 --> 00:18:31.430
How am I doing on time?

00:18:31.430 --> 00:18:33.250
PROFESSOR: OK, should
I stop here?

00:18:33.250 --> 00:18:34.660
AUDIENCE: About how much more?

00:18:34.660 --> 00:18:37.550
PROFESSOR: 10 slides.

00:18:37.550 --> 00:18:42.930
PROFESSOR: OK, so this is sort
of a brief summary, which will

00:18:42.930 --> 00:18:46.270
lead into a much larger talk at
the next lecture on how you

00:18:46.270 --> 00:18:48.200
actually go about re-engineering
your code for

00:18:48.200 --> 00:18:49.090
parallelism.

00:18:49.090 --> 00:18:52.160
And this could come into play
if you start with sequential

00:18:52.160 --> 00:18:53.610
code and you're parallelizing
it.

00:18:53.610 --> 00:18:55.090
Some of you are doing that
for your projects.

00:18:55.090 --> 00:18:58.140
Or if you're actually writing
code from scratch and you want

00:18:58.140 --> 00:18:59.650
to engineer that for parallelism
as well.

00:19:02.570 --> 00:19:06.280
So I think it's important to
sort of understand the problem

00:19:06.280 --> 00:19:07.305
that you're working with.

00:19:07.305 --> 00:19:09.300
So you want to survey your
landscape, understand what

00:19:09.300 --> 00:19:12.270
other people might have done,
and look for well-known

00:19:12.270 --> 00:19:16.320
solutions and common pitfalls.

00:19:16.320 --> 00:19:18.570
And the patterns that I'm going
to talk about in more

00:19:18.570 --> 00:19:21.480
detail really provide you with a
list of questions to sort of

00:19:21.480 --> 00:19:26.620
help you assess the existing
code that you're working with

00:19:26.620 --> 00:19:29.160
or the problem that you're
trying to solve.

00:19:29.160 --> 00:19:31.250
There's something that you need
to keep in mind that sort

00:19:31.250 --> 00:19:33.450
of effect your overall
correctness.

00:19:33.450 --> 00:19:34.660
So for example, is your

00:19:34.660 --> 00:19:37.110
computation numerically stable?

00:19:37.110 --> 00:19:40.640
You might know if you have a
floating point computation you

00:19:40.640 --> 00:19:43.540
might not be able to reorder
all the operations because

00:19:43.540 --> 00:19:46.320
that might effect your
actual precision.

00:19:46.320 --> 00:19:48.320
And so your overall output might
be different and that

00:19:48.320 --> 00:19:50.730
may or may not be acceptable.

00:19:50.730 --> 00:19:53.930
So a lot of scientific codes for
example, are things that

00:19:53.930 --> 00:19:56.530
have to deal with a lot of
precision, might have to be

00:19:56.530 --> 00:19:58.720
cognizant of that fact.

00:19:58.720 --> 00:20:01.260
You want to also define the
scope of, what are you trying

00:20:01.260 --> 00:20:04.880
to do and will it
be good enough?

00:20:04.880 --> 00:20:07.160
You want to do back of the hand,
back of the envelope

00:20:07.160 --> 00:20:10.550
calculations to make sure that
things that you're suggesting

00:20:10.550 --> 00:20:13.095
of doing are actually feasible,
that they're

00:20:13.095 --> 00:20:15.310
actually practical and that
will give you the sort of

00:20:15.310 --> 00:20:19.460
performance expectations
that you've set out.

00:20:19.460 --> 00:20:24.400
You also want to understand
your input range.

00:20:24.400 --> 00:20:26.630
You might be able to specialize
if there are some

00:20:26.630 --> 00:20:29.170
cases for example, that you're
allowed to ignore.

00:20:29.170 --> 00:20:31.930
So these are good things
to keep in mind.

00:20:31.930 --> 00:20:34.140
You also want to define
a testing protocol.

00:20:34.140 --> 00:20:37.250
I think it's important
to understand--

00:20:37.250 --> 00:20:38.800
you started out with some piece
of code, you're going to

00:20:38.800 --> 00:20:40.520
make some changes to it,
how you going to go

00:20:40.520 --> 00:20:41.770
about testing it?

00:20:41.770 --> 00:20:44.650
How you might go about debugging
it and that could be

00:20:44.650 --> 00:20:47.910
essentially where you spend
a lot of your time.

00:20:47.910 --> 00:20:51.650
And then having these things
in mind, I think, the parts

00:20:51.650 --> 00:20:55.400
that are worth looking
at are the parts that

00:20:55.400 --> 00:20:56.470
make the most sense.

00:20:56.470 --> 00:20:58.140
Where is your computation
spending most of its time?

00:20:58.140 --> 00:20:59.610
Is there hot spots
in your code?

00:20:59.610 --> 00:21:02.380
And you can use profiling tools
for that and in fact,

00:21:02.380 --> 00:21:04.150
you'll see some of that for
cell in some of the

00:21:04.150 --> 00:21:08.660
recitations later
in the course.

00:21:08.660 --> 00:21:13.750
So a simple example of molecular
dynamics simulator.

00:21:13.750 --> 00:21:16.940
What you're trying to do is,
you have some space of

00:21:16.940 --> 00:21:19.930
molecules, which I'm just going
to represent in 2D.

00:21:19.930 --> 00:21:23.040
You know, look, they have water
molecules and I have

00:21:23.040 --> 00:21:30.250
some protein that I'm trying
to understand how the

00:21:30.250 --> 00:21:32.640
different atoms in that molecule
are moving around so

00:21:32.640 --> 00:21:34.880
that I can determine the
shape of the protein.

00:21:34.880 --> 00:21:37.010
So there are forces,
there are bonded

00:21:37.010 --> 00:21:39.190
forces between the molecules.

00:21:39.190 --> 00:21:42.420
So I've just shown for example,
bonded forces among

00:21:42.420 --> 00:21:45.120
my protein and then there
are non-bonded forces.

00:21:45.120 --> 00:21:47.300
So how are different atoms sort
of interacting with each

00:21:47.300 --> 00:21:49.670
other because of electrostatic
forces, for example.

00:21:52.920 --> 00:21:55.435
So what you try to do is figure
out, on each atom, what

00:21:55.435 --> 00:21:58.100
are all the forces that are
affecting it and what is its

00:21:58.100 --> 00:22:01.110
current position and then you
try to estimate where it's

00:22:01.110 --> 00:22:05.930
going to move based on
Newtonian, in the simplest

00:22:05.930 --> 00:22:09.710
case, a Newtonian f equals
m a type projection.

00:22:09.710 --> 00:22:12.950
So in a naive algorithm you have
n squared interactions.

00:22:12.950 --> 00:22:16.150
You have to calculate all the
forces on one molecule from

00:22:16.150 --> 00:22:19.210
all others.

00:22:19.210 --> 00:22:21.910
By understanding your problem
you know that you can actually

00:22:21.910 --> 00:22:24.720
exploit the properties of
forces that essentially

00:22:24.720 --> 00:22:27.490
decrease exponentially, so you
can use a cutoff distance.

00:22:27.490 --> 00:22:30.400
So if a molecule is way too far
away you can ignore this.

00:22:30.400 --> 00:22:35.220
And for people who do galaxy
calculations, you know you can

00:22:35.220 --> 00:22:38.620
ignore geometric forces between
constellations or

00:22:38.620 --> 00:22:41.160
clusters that are
too far apart.

00:22:41.160 --> 00:22:45.370
So in the sequential code, some
pseudo code for doing a

00:22:45.370 --> 00:22:48.100
molecular dynamic simulator,
you have your atoms array,

00:22:48.100 --> 00:22:50.750
your force array, your set of
neighbors in a two-dimensional

00:22:50.750 --> 00:22:54.470
space and you're going to go
through and sort of simulate

00:22:54.470 --> 00:22:55.710
different time steps.

00:22:55.710 --> 00:22:58.660
And for each time step you're
going to do-- for each atom--

00:22:58.660 --> 00:23:02.250
compute the bonded forces,
compute who are my neighbors

00:23:02.250 --> 00:23:05.250
for those neighbors, compute--
so these are things that

00:23:05.250 --> 00:23:07.100
essentially incapsulate
distance.

00:23:07.100 --> 00:23:09.500
Compute the forces between
them, update the

00:23:09.500 --> 00:23:11.360
position and end.

00:23:11.360 --> 00:23:16.180
So since this is a loop then
that might suggest essentially

00:23:16.180 --> 00:23:18.810
where to start looking
for concurrency.

00:23:18.810 --> 00:23:21.680
So you can start with the
decomposition patterns and

00:23:21.680 --> 00:23:26.360
they'll be more in depth details
about those next.

00:23:26.360 --> 00:23:28.705
I'm going to give you some
intuition and then you would

00:23:28.705 --> 00:23:31.370
try to figure out whether your
decomposition has to abide by

00:23:31.370 --> 00:23:34.120
certain dependencies and what
are those dependencies?

00:23:34.120 --> 00:23:35.140
How do you expose them?

00:23:35.140 --> 00:23:37.620
And then, how can you
design and evaluate?

00:23:37.620 --> 00:23:38.870
How can you evaluate
your design?

00:23:42.050 --> 00:23:44.000
Screwed up again.

00:23:44.000 --> 00:23:44.920
I just fixed this.

00:23:44.920 --> 00:23:48.800
OK, so this is the pseudo code
again from the previous slide.

00:23:48.800 --> 00:23:52.440
And since all you have is a
simple loop that essentially

00:23:52.440 --> 00:23:54.600
says, this is where to look
for the computation.

00:23:54.600 --> 00:23:56.980
And since you're essentially
doing the same computation for

00:23:56.980 --> 00:24:02.210
each atom then that again,
gives you the type of

00:24:02.210 --> 00:24:05.170
parallelism that we've
talked about before.

00:24:05.170 --> 00:24:08.700
So you can look for splitting
up each iteration and

00:24:08.700 --> 00:24:10.970
parallelizing those so that each
processor for example,

00:24:10.970 --> 00:24:12.850
does one atom.

00:24:12.850 --> 00:24:15.870
Or each processor does a
collection of atoms. But there

00:24:15.870 --> 00:24:16.920
are additional tasks.

00:24:16.920 --> 00:24:19.640
So data level parallelism
versus sort of control

00:24:19.640 --> 00:24:20.670
parallelism.

00:24:20.670 --> 00:24:23.320
For each atom you also want
to calculate the forces.

00:24:23.320 --> 00:24:25.410
You want to calculate long
range interactions, find

00:24:25.410 --> 00:24:27.490
neighbors, update position
and so on.

00:24:27.490 --> 00:24:30.340
And some of these have shared
data, some of them do not.

00:24:30.340 --> 00:24:31.790
So you have to factor that in.

00:24:31.790 --> 00:24:34.660
So understanding there control
dependencies essentially tells

00:24:34.660 --> 00:24:40.930
you how you need to it lay
out your orchestration.

00:24:40.930 --> 00:24:42.750
So you have your bonded forces,
you have your neighbor

00:24:42.750 --> 00:24:45.240
list, that effects your long
range calculations.

00:24:45.240 --> 00:24:48.110
But to do this update position
I need both of these tasks to

00:24:48.110 --> 00:24:49.870
have completed.

00:24:49.870 --> 00:24:53.270
And in each one of these tasks
there's different data

00:24:53.270 --> 00:24:54.660
structures that they need.

00:24:54.660 --> 00:25:00.180
So everybody essentially reads
the location of the items. So

00:25:00.180 --> 00:25:02.510
this is good because we want
time step that essentially

00:25:02.510 --> 00:25:08.470
says, I can really distribute
this really well, but then

00:25:08.470 --> 00:25:10.480
there's a right synchronization
problem

00:25:10.480 --> 00:25:12.880
because eventually I have to
update this array so I have to

00:25:12.880 --> 00:25:16.590
be careful about who goes
first. There's an

00:25:16.590 --> 00:25:19.200
accumulation, which means
I can potentially do a

00:25:19.200 --> 00:25:21.280
reduction on these.

00:25:21.280 --> 00:25:23.630
There's some write on the other
end, but that seems to

00:25:23.630 --> 00:25:25.280
be a localized data structure.

00:25:25.280 --> 00:25:30.040
So for partitioning example,
neighbors might have to be

00:25:30.040 --> 00:25:31.960
just locals at a different
processor.

00:25:31.960 --> 00:25:34.890
So coming up with this structure
and sort of block

00:25:34.890 --> 00:25:38.380
level diagram helps you
essentially figure out where

00:25:38.380 --> 00:25:39.180
are your tasks?

00:25:39.180 --> 00:25:41.410
Helps you figure out what kind
of synchronization mechanisms

00:25:41.410 --> 00:25:44.710
you need and it can also help
you suggest the data

00:25:44.710 --> 00:25:48.380
distribution that you might need
to reduce synchronization

00:25:48.380 --> 00:25:52.330
costs and problems. And
lastly, you want to

00:25:52.330 --> 00:25:54.440
essentially evaluate
your design.

00:25:54.440 --> 00:25:55.980
And you want to keep in mind,
what is your target

00:25:55.980 --> 00:25:56.990
architecture.

00:25:56.990 --> 00:25:59.130
Are you trying to really run
on shared memory and

00:25:59.130 --> 00:26:02.290
distributed memory and message
passing or are you just doing

00:26:02.290 --> 00:26:03.160
this for one architecture?

00:26:03.160 --> 00:26:06.750
So for your project, you're
doing this for self, so you

00:26:06.750 --> 00:26:08.830
can be very self specific.

00:26:08.830 --> 00:26:11.270
But if you're doing this
in other contexts, the

00:26:11.270 --> 00:26:12.770
architecture actually
might influence

00:26:12.770 --> 00:26:14.020
some of your decisions.

00:26:16.080 --> 00:26:17.760
Does data sharing have
enough special

00:26:17.760 --> 00:26:19.980
properties like a read only?

00:26:19.980 --> 00:26:21.460
There are data structures
that are read only.

00:26:21.460 --> 00:26:24.370
Are there enough accumulations
that you can exploit by

00:26:24.370 --> 00:26:25.250
reductions?

00:26:25.250 --> 00:26:29.300
Are there temporal constraints
on data sharing

00:26:29.300 --> 00:26:30.250
that you can exploit?

00:26:30.250 --> 00:26:31.630
And can you deal with
those efficiently?

00:26:31.630 --> 00:26:34.170
If you can't then you
have a problem.

00:26:34.170 --> 00:26:35.550
So you need to resolve that.

00:26:35.550 --> 00:26:37.180
If the designs OK then
you move on to

00:26:37.180 --> 00:26:38.970
the next design space.

00:26:38.970 --> 00:26:41.680
So at the next lecture
I'll go through these

00:26:41.680 --> 00:26:42.460
in a lot more detail.

00:26:42.460 --> 00:26:44.180
That's it.